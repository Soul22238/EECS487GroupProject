{"id": 0, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef reorder_rows(df, index_list):\n    df = df.set_index('Type', drop=False)\n    df = df.loc[index_list]\n    df = df.reset_index(drop=True)\n    return df\n\n# Test the function\ndf = pd.DataFrame({'Col1': [1, 4, 7, 10, 13, 16],\n                   'Col2': [2, 5, 8, 11, 14, 17],\n                   'Col3': [3, 6, 9, 12, 15, 18],\n                   'Type': [1, 1, 2, 2, 3, 3]})\n\nindex_list = [2, 4, 0, 3, 1, 5]\n\nprint(reorder_rows(df, index_list))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 0, "library_problem_id": 0, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 1, "code": ["<code>\nimport pandas as pd\n\ndef reorder_and_count_diff(df, index_list):\n    # Reorder the DataFrame using the index list\n    df = df.reindex(index_list)\n    \n    # Count the number of rows where the 'Type' column differs from the original sorted DataFrame after reordering\n    diff_count = (df['Type'] != df.sort_values('Type')['Type']).sum()\n    \n    return df, diff_count\n\n# Test the function\ndf = pd.DataFrame({'Col1': [1, 4, 7, 10, 13, 16],\n                   'Col2': [2, 5, 8, 11, 14, 17],\n                   'Col3': [3, 6, 9, 12, 15, 18],\n                   'Type': [1, 1, 2, 2, 3, 3]})\nindex_list = [2, 4, 0, 3, 1, 5]\n\ndf, diff_count = reorder_and_count_diff(df, index_list)\nprint(df)\nprint('Number of rows where the \\'Type\\' column differs from the original sorted DataFrame after reordering:', diff_count)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 1, "library_problem_id": 1, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 2, "code": ["<code>\nimport pandas as pd\n\ndef replace_values(df):\n    # Create a copy of the DataFrame to avoid modifying the original one\n    df_copy = df.copy()\n\n    # Iterate over each column\n    for col in df_copy.columns:\n        # Get the value counts of each unique value in the column\n        value_counts = df_copy[col].value_counts()\n\n        # Get the values that appear two or more times\n        frequent_values = value_counts[value_counts >= 2].index.tolist()\n\n        # Replace the values that do not appear two or more times with 'other'\n        df_copy[col] = df_copy[col].apply(lambda x: 'other' if x not in frequent_values else x)\n\n    return df_copy\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 2, "library_problem_id": 2, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 3, "code": ["<code>\nimport pandas as pd\n\ndef replace_values(df):\n    result = df.copy()\n    for col in df.columns:\n        freq = df[col].value_counts()\n        result[col] = result[col].apply(lambda x: x if freq[x] >= 3 else 'other')\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 3, "library_problem_id": 3, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 4, "code": ["<code>\nimport pandas as pd\n\ndef replace_values(df):\n    for col in df.columns:\n        freq = df[col].value_counts()\n        values_to_replace = freq[freq < 2].index\n        df[col] = df[col].replace(values_to_replace, 'other')\n    return df\n\n# Example DataFrame\ndata = {\n    'A': ['foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'foo'],\n    'B': ['one', 'one', 'two', 'three', 'two', 'two', 'one', 'three'],\n    'C': ['small', 'large', 'large', 'small', 'small', 'large', 'small', 'large'],\n    'D': [1, 2, 2, 3, 3, 4, 5, 6],\n    'E': [2, 4, 5, 5, 6, 6, 8, 9]\n}\n\ndf = pd.DataFrame(data)\n\n# Call the function\ndf = replace_values(df)\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 4, "library_problem_id": 4, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 5, "code": ["<code>\nSure, I can help you with that. However, you didn't specify what kind of completion you need. Are you looking for a function to count the frequency of each item in each column, or are you looking for a function to find the mode of each column, or something else? Please provide more details so I can assist you better.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 5, "library_problem_id": 5, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 6, "code": ["<code>\nimport pandas as pd\n\ndef modify_df(df, threshold=3):\n    for col in df.columns:\n        freq = df[col].value_counts()\n        to_replace = freq[freq < threshold].index.tolist()\n        df[col] = df[col].replace(to_replace, 'other')\n        df[col] = df[col].replace('apple', 'other', limit=1)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 6, "library_problem_id": 6, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 7, "code": ["<code>\nimport pandas as pd\n\ndef remove_duplicates(df):\n    # Create a mask where 'keep_if_dup' is 'Yes'\n    mask = df['keep_if_dup'] == 'Yes'\n    \n    # Create a new DataFrame with the rows where 'keep_if_dup' is 'Yes'\n    df_to_keep = df[mask]\n    \n    # Create a new DataFrame with the rows where 'keep_if_dup' is 'No' and 'url' is not in df_to_keep\n    df_to_remove = df[~mask & ~df['url'].isin(df_to_keep['url'])]\n    \n    # Combine the two DataFrames\n    df = pd.concat([df_to_keep, df_to_remove])\n    \n    # Reset the index\n    df.reset_index(drop=True, inplace=True)\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 7, "library_problem_id": 7, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 8, "code": ["<code>\nimport pandas as pd\n\ndef remove_duplicates(df):\n    # Filter rows where 'drop_if_dup' is 'Yes'\n    df_yes = df[df['drop_if_dup'] == 'Yes']\n    \n    # Remove duplicates in 'url' column for the filtered DataFrame\n    df_yes = df_yes.drop_duplicates(subset='url', keep='first')\n    \n    # Concatenate the filtered DataFrame with the original DataFrame where 'drop_if_dup' is 'No'\n    df = pd.concat([df_yes, df[df['drop_if_dup'] == 'No']])\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 8, "library_problem_id": 8, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 7}}
{"id": 9, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'url': ['A.com', 'A.com', 'A.com', 'B.com', 'B.com', 'C.com', 'B.com'],\n                   'keep_if_dup': ['Yes', 'Yes', 'No', 'No', 'No', 'No', 'Yes']})\n\n# Filter the DataFrame to keep only rows where 'keep_if_dup' is 'Yes'\ndf = df[df['keep_if_dup'] == 'Yes']\n\n# Remove duplicates based on 'url'\ndf = df.drop_duplicates(subset='url')\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 9, "library_problem_id": 9, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 7}}
{"id": 10, "code": ["<code>\nimport pandas as pd\n\n# Assuming df is your DataFrame\ndf = pd.DataFrame({\n    'name': ['A', 'A', 'B', 'C', 'B', 'A'],\n    'v1': ['A1', 'A2', 'B1', 'C1', 'B2', 'A2'],\n    'v2': ['A11', 'A12', 'B12', 'C11', 'B21', 'A21'],\n    'v3': [1, 2, 3, 4, 5, 6]\n})\n\ndf.set_index('name', inplace=True)\n\n# Fill missing values with 'unknown'\ndf.fillna('unknown', inplace=True)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 10, "library_problem_id": 10, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 11, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'datetime': ['2015-12-01 00:00:00-06:00', '2015-12-02 00:00:00-06:00']})\n\n# Convert the 'datetime' column to datetime type\ndf['datetime'] = pd.to_datetime(df['datetime'])\n\n# Localize the timezone to None\ndf['datetime'] = df['datetime'].dt.tz_localize(None)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 11, "library_problem_id": 11, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 12, "code": ["<code>\nimport pandas as pd\n\ndef remove_timezone_offset(df):\n    df['datetime'] = df['datetime'].dt.tz_localize(None)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 12, "library_problem_id": 12, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 13, "code": ["<code>\ndf['datetime'] = pd.to_datetime(df['datetime'])\ndf.set_index('datetime', inplace=True)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 13, "library_problem_id": 13, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 14, "code": ["<code>\nimport pandas as pd\n\ndef remove_timezone(df):\n    df['datetime'] = pd.to_datetime(df['datetime']).dt.tz_localize(None)\n    return df\n\ndf = pd.DataFrame({'datetime': ['2015-12-01 00:00:00-06:00', '2015-12-02 00:01:00-06:00', '2015-12-03 00:00:00-06:00']})\ndf = remove_timezone(df)\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 14, "library_problem_id": 14, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 15, "code": ["<code>\nHere is the completed code:\n\n```python\nimport pandas as pd\nimport ast\n\ndef process_message(df):\n    # Convert the 'message' column from string to list\n    df['message'] = df['message'].apply(ast.literal_eval)\n\n    # Extract the keys and values from the 'message' column\n    keys = set()\n    values = []\n    for message in df['message']:\n        message_dict = dict(message)\n        keys.update(message_dict.keys())\n        values.append(message_dict)\n\n    # Create a new DataFrame with each key as a column\n    new_df = pd.DataFrame(values)\n    new_df.columns = list(keys)\n\n    # Fill missing values with 'none'\n    new_df.fillna('none', inplace=True)\n\n    return new_df\n\ndf = pd.DataFrame({'name': ['matt', 'james', 'adam'],\n                   'status': ['active', 'active', 'inactive'],\n                   'number': [12345, 23456, 34567],\n                   'message': [\n                       \"[{'type': 'email', 'value': 'matt@gmail.com'}, {'type': 'phone', 'value': '1234567890'}]\",\n                       \"[{'type': 'email', 'value': 'james@gmail.com'}, {'type': 'phone', 'value': '2345678901'}]\",\n                       \"[{'type': 'email', 'value': 'adam@gmail.com\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 15, "library_problem_id": 15, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 16, "code": ["<code>\nimport pandas as pd\n\ndef multiply_scores(df, product_ids, multiplier):\n    for product_id in product_ids:\n        df.loc[df['product'] == product_id, 'score'] *= multiplier\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 16, "library_problem_id": 16, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 17, "code": ["<code>\nimport pandas as pd\n\ndef multiply_scores(df, products):\n    df.loc[~df['product'].isin(products), 'score'] *= 10\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 17, "library_problem_id": 17, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 18, "code": ["<code>\nThe given code is a pandas DataFrame. It seems like it's a data set with two columns: 'product' and 'score'. However, you didn't specify what you want to do with this data. Could you please provide more details about what you want to do with this data? For example, do you want to sort the data by the 'score' column, or do you want to calculate the average score, or something else?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 18, "library_problem_id": 18, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 19, "code": ["<code>\nimport pandas as pd\n\ndef normalize_scores(df, product_ids):\n    # Create a copy of the DataFrame to avoid modifying the original\n    df_copy = df.copy()\n    \n    # Filter the DataFrame to only include rows where the 'product' column is in the provided list\n    df_filtered = df_copy[df_copy['product'].isin(product_ids)]\n    \n    # If there are no matching rows, return the original DataFrame\n    if df_filtered.empty:\n        return df_copy\n    \n    # Perform Min-Max normalization on the 'score' column\n    df_filtered['score'] = (df_filtered['score'] - df_filtered['score'].min()) / (df_filtered['score'].max() - df_filtered['score'].min())\n    \n    # Replace the 'score' column in the original DataFrame with the normalized values\n    df_copy.loc[df_filtered.index, 'score'] = df_filtered['score']\n    \n    return df_copy\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 19, "library_problem_id": 19, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 20, "code": ["<code>\ndef binary_to_categorical(df):\n    return df.apply(lambda x: x.astype(str).replace({'0': 'Low', '1': 'High'}))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 20, "library_problem_id": 20, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 21, "code": ["<code>\nimport pandas as pd\n\ndef convert_to_category(df):\n    df['category'] = df.apply(lambda row: [col for col in df.columns if row[col] == 1][0], axis=1)\n    return df\n\ndf1 = pd.DataFrame({'A': [0, 1, 1, 1, 0, 1],\n                   'B': [1, 0, 1, 1, 1, 0],\n                   'C': [1, 1, 0, 1, 1, 1],\n                   'D': [1, 1, 1, 0, 1, 1]})\n\nconvert_to_category(df1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 21, "library_problem_id": 21, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 22, "code": ["<code>\nimport pandas as pd\n\ndata = {\n    'A': [0, 1, 2, 3, 4, 5],\n    'B': [1, 0, 0, 0, 1, 0],\n    'C': [0, 1, 0, 0, 1, 0],\n    'D': [1, 1, 0, 0, 1, 0]\n}\n\ndf = pd.DataFrame(data)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 22, "library_problem_id": 22, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 20}}
{"id": 23, "code": ["<code>\nimport pandas as pd\n\n# Create a sample DataFrame\ndf = pd.DataFrame({'Date': ['2018-01-01', '2018-02-08', '2018-02-08', '2018-02-08', '2018-02-08']})\n\n# Convert the 'Date' column to datetime\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Extract the month and year and convert to abbreviated month name\ndf['Date'] = df['Date'].dt.strftime('%b-%Y')\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 23, "library_problem_id": 23, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 24, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08','2019-02-08', '2019-03-08']})\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 24, "library_problem_id": 24, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 25, "code": ["<code>\nimport pandas as pd\n\ndef extract_dates(df, date_range):\n    # Convert the 'Date' column to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    # Filter the DataFrame to only include dates within the specified range\n    df = df[df['Date'].between(date_range[0], date_range[1])]\n\n    # Extract the month name, year, and day of the week\n    df['Date_Formatted'] = df['Date'].dt.strftime('%d-%b-%Y %A')\n\n    return df['Date_Formatted']\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 25, "library_problem_id": 25, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 23}}
{"id": 26, "code": ["<code>\nimport pandas as pd\n\ndef shift_dataframe(df):\n    # Shift the first value of the first column down one row\n    df.loc[-1] = df.loc[0]  # adding a row\n    df.index = df.index + 1  # shifting index\n    df = df.sort_index()  # sorting by index\n\n    # Shift the last value of the first column to the first row\n    last_value = df.iloc[0, 0]\n    df.iloc[0, 0] = df.iloc[-1, 0]\n    df.iloc[-1, 0] = last_value\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 26, "library_problem_id": 26, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 27, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'#1': [11.6985, 43.6431, 54.9089, 63.1225, 72.4399],\n                   '#2': [126.0, 134.0, 130.0, 126.0, 120.0]},\n                  index=['1980-01-01', '1980-01-02', '1980-01-03', '1980-01-04', '1980-01-05'])\n\n# Shift the last row of the first column to the second row\ndf.loc['1980-01-02':'1980-01-03', '#1'] = df.loc['1980-01-01', '#1']\n\n# Move the first row of the first column to the last row\ndf.loc['1980-01-05'] = df.loc['1980-01-01']\n\n# Print the DataFrame\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 27, "library_problem_id": 27, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 28, "code": ["<code>\nimport pandas as pd\n\ndef cyclic_shift(df):\n    # Shift the first row of the first column down to the last row\n    df.loc[df.index[0], '#1'] = df.loc[df.index[-1], '#1']\n    \n    # Shift the last row of the second column up to the first row\n    df.loc[df.index[-1], '#2'] = df.loc[df.index[0], '#2']\n    \n    # Shift the rest of the rows down\n    df.loc[df.index[1:], '#1'] = df.loc[df.index[:-1], '#1']\n    \n    # Shift the rest of the rows up\n    df.loc[df.index[:-1], '#2'] = df.loc[df.index[1:], '#2']\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 28, "library_problem_id": 28, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 29, "code": ["<code>\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import r2_score\n\ndef shift_and_minimize_r2(df):\n    min_r2 = float('inf')\n    min_shift = None\n    for i in range(len(df)):\n        df_shifted = df.copy()\n        df_shifted['column1'] = df_shifted['column1'].shift(i)\n        df_shifted.dropna(inplace=True)\n        r2 = r2_score(df_shifted['column2'], df_shifted['column1'])\n        if r2 < min_r2:\n            min_r2 = r2\n            min_shift = df_shifted\n    return min_shift\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 29, "library_problem_id": 29, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 30, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame(\n    {'HeaderA': [476],\n     'HeaderB': [4365],\n     'HeaderC': [457]})\n\ndf.columns = [col + 'X' for col in df.columns]\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 30, "library_problem_id": 30, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 31, "code": ["<code>\nimport pandas as pd\n\ndef rename_columns(df, prefix):\n    \"\"\"\n    Rename all columns in a Pandas DataFrame by adding a prefix to each column name.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame to rename columns in.\n    prefix (str): The prefix to add to each column name.\n\n    Returns:\n    pandas.DataFrame: The DataFrame with renamed columns.\n    \"\"\"\n    return df.rename(columns=lambda x: prefix + x)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 31, "library_problem_id": 31, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 30}}
{"id": 32, "code": ["<code>\ndef rename_columns(df):\n    for col in df.columns:\n        if not col.endswith('X'):\n            df = df.rename(columns={col: 'X' + col})\n    df = df.add_prefix('X')\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 32, "library_problem_id": 32, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 30}}
{"id": 33, "code": ["<code>\nimport pandas as pd\n\ndef calculate_mean(df):\n    # Identify value columns\n    value_cols = [col for col in df.columns if 'val' in col]\n\n    # Calculate mean for each value column\n    mean_df = df.groupby('group')[value_cols].mean().reset_index()\n\n    return mean_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 33, "library_problem_id": 33, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 34, "code": ["<code>\nimport pandas as pd\n\ndef calculate_sum(df):\n    # Identify value columns\n    value_cols = [col for col in df.columns if 'val' in col]\n\n    # Group by grouping columns and calculate sum for each value column\n    result = df.groupby(list(df.columns.drop(value_cols))).sum()\n\n    # Add 'group_color' as the first column\n    result.insert(0, 'group_color', df.groupby(list(df.columns.drop(value_cols)))['group_color'].first())\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 34, "library_problem_id": 34, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 33}}
{"id": 35, "code": ["<code>\nimport pandas as pd\n\ndef group_calculations(df, group_by_column):\n    # Select columns ending with '2'\n    columns_2 = [col for col in df.columns if col.endswith('2')]\n    \n    # Calculate grouped mean for columns ending with '2'\n    mean_df = df.groupby(group_by_column)[columns_2].mean().reset_index()\n    \n    # Select all other value columns\n    other_columns = [col for col in df.columns if col not in columns_2]\n    \n    # Calculate grouped sum for all other value columns\n    sum_df = df.groupby(group_by_column)[other_columns].sum().reset_index()\n    \n    # Merge the two DataFrames\n    result_df = pd.merge(mean_df, sum_df, on=group_by_column, how='inner')\n    \n    return result_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 35, "library_problem_id": 35, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 33}}
{"id": 36, "code": ["<code>\na = [[0, 1, 2, 3, 0, 5], [1, 1, 2, 3, 4, 5], [2, 1, 1, 1, 6, 1], [3, 1, 0, 0, 0, 0]]\n\ndef complete_function(a):\n    # Transpose the matrix\n    a_t = list(map(list, zip(*a)))\n\n    # Calculate the sum of each row\n    row_sums = [sum(row) for row in a]\n\n    # Calculate the sum of each column\n    col_sums = [sum(col) for col in a_t]\n\n    # Return the sums of rows and columns\n    return row_sums, col_sums\n\nrow_sums, col_sums = complete_function(a)\nprint(\"Row sums:\", row_sums)\nprint(\"Column sums:\", col_sums)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 36, "library_problem_id": 36, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 37, "code": ["<code>\nimport pandas as pd\n\ndef sum_values(df, rows, columns):\n    return df.loc[rows, columns].sum()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 37, "library_problem_id": 37, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 38, "code": ["<code>\nimport pandas as pd\n\n# Create a DataFrame\ndf = pd.DataFrame({\n    'A': [1, 2, 3, 4, 5],\n    'B': [10, 20, 30, 40, 50],\n    'C': [100, 200, 300, 400, 500],\n    'D': [1000, 2000, 3000, 4000, 5000]\n})\n\nprint(\"Original DataFrame:\")\nprint(df)\n\n# Specify the rows and columns to sum\nrow_indices = [0, 1, 2]\ncolumn_indices = ['A', 'B', 'C']\n\n# Calculate the sum of the specified rows and columns\nsum_series = df.loc[row_indices, column_indices].sum()\n\nprint(\"\\nSum of specified rows and columns:\")\nprint(sum_series)\n\n# Remove the largest sum\nsum_series = sum_series.drop(sum_series.idxmax())\n\nprint(\"\\nAfter removing the largest sum:\")\nprint(sum_series)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 38, "library_problem_id": 38, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 39, "code": ["<code>\nid, temp, name\n1, 34, mark\n2, 22, mark\n3, 34, mark\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 39, "library_problem_id": 39, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 40, "code": ["<code>\nimport pandas as pd\n\ndef count_null_values(df: pd.DataFrame) -> pd.Series:\n    return df.isnull().sum()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 40, "library_problem_id": 40, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 41, "code": ["<code>\nimport pandas as pd\n\ndef value_counts_columns(df):\n    result = \"\"\n    for col in df.columns:\n        result += \"---- {0} ---\\n\".format(col)\n        if df[col].nunique() == 1:\n            result += \"Uninteresting column (all values are the same)\\n\"\n        elif df[col].isnull().all() or df[col].nunique() == 0:\n            result += \"Irrelevant column (all values are null or the same)\\n\"\n        else:\n            result += df[col].value_counts().to_string() + \"\\n\"\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 41, "library_problem_id": 41, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 39}}
{"id": 42, "code": ["<code>\nimport pandas as pd\n\ndef merge_rows():\n    # Read the Excel file\n    df = pd.read_excel('nanonose.xls', sheet_name='Sheet1')\n\n    # Drop the 'Unnamed: 2' column\n    df = df.drop(columns='Unnamed: 2')\n\n    # Merge the first and second rows into a single header row\n    df.columns = df.iloc[0]\n    df = df.drop(df.index[0])\n    df = df.reset_index(drop=True)\n\n    # Rename the columns\n    df.columns = ['Sample type', 'Concentration', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 42, "library_problem_id": 42, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 43, "code": ["<code>\nimport pandas as pd\n\ndef merge_rows():\n    try:\n        # Read the Excel file\n        df = pd.read_excel('nanonose.xls', sheet_name='Sheet1')\n\n        # Drop the 'Unnamed: 2' column\n        df = df.drop(columns='Unnamed: 2')\n\n        # Merge the first and second rows into a single header row\n        df.columns = df.iloc[0]\n        df = df.drop(df.index[0])\n        df = df.drop(df.index[0])\n\n        # Rename the columns\n        df.columns = ['Nanonose', 'Concentration', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n\n        return df\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 43, "library_problem_id": 43, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 44, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    0: [0.0, np.nan, np.nan],\n    1: [1.0, 1.0, np.nan],\n    2: [2.0, 2.0, 2.0]\n})\n\ndef shift_row(row):\n    return pd.Series(np.concatenate((row.dropna().values, np.full(len(row.dropna()), np.nan))))\n\ndf = df.apply(shift_row, axis=1)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 44, "library_problem_id": 44, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 45, "code": ["<code>\nThe given code appears to be a pandas DataFrame. However, it's not clear what the function is supposed to do. Could you please provide more context or specify what the function should do?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 45, "library_problem_id": 45, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 44}}
{"id": 46, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame([[3,1,2],[np.nan,1,2],[np.nan,np.nan,2]],columns=['0','1','2'])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 46, "library_problem_id": 46, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 44}}
{"id": 47, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'lab':['A', 'B', 'C', 'D', 'E', 'F'], 'value':[50, 35, 8, 5, 1, 1]})\ndf = df.set_index('lab')\nthresh = 6\n\ndef complete_function(df, thresh):\n    return df[df['value'] > thresh]\n\nprint(complete_function(df, thresh))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 47, "library_problem_id": 47, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 48, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'lab':['A', 'B', 'C', 'D', 'E', 'F'], 'value':[50, 35, 8, 5, 1, 1]})\ndf = df.set_index('lab')\n\nthreshold = 6\n\ndef filter_df(df, threshold):\n    return df[df['value'] > threshold]\n\nfiltered_df = filter_df(df, threshold)\nprint(filtered_df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 48, "library_problem_id": 48, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 47}}
{"id": 49, "code": ["<code>\nimport pandas as pd\n\n# Create the initial DataFrame\ndf = pd.DataFrame({'lab':['A', 'B', 'C', 'D', 'E', 'F'], 'value':[50, 35, 8, 5, 1, 1]})\ndf = df.set_index('lab')\n\ndef aggregate_out_of_range(df, range_min, range_max):\n    # Filter rows not in the specified range\n    out_of_range = df[~df['value'].between(range_min, range_max)]\n    \n    # Calculate the average of the values in the out_of_range DataFrame\n    avg_out_of_range = out_of_range['value'].mean()\n    \n    # Filter rows in the specified range\n    in_range = df[df['value'].between(range_min, range_max)]\n    \n    # Calculate the average of the values in the in_range DataFrame\n    avg_in_range = in_range['value'].mean()\n    \n    # Create a new DataFrame with the average values\n    new_df = pd.DataFrame({'value': [avg_out_of_range, avg_in_range]}, index=['X', 'Y'])\n    \n    return new_df\n\n# Test the function\nprint(aggregate_out_of_range(df, 4, 38))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 49, "library_problem_id": 49, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 47}}
{"id": 50, "code": ["<code>\nimport pandas as pd\n\n# Create a DataFrame\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n\n# Add inverse columns to each existing column\ndf['inv_A'] = 1/df['A']\ndf['inv_B'] = 1/df['B']\n\n# Store the resulting DataFrame in a variable named 'result'\nresult = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 50, "library_problem_id": 50, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 51, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\n# Create a DataFrame\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n\n# Add new columns representing the element-wise exponential of each existing column\ndf['exp_A'] = np.exp(df['A'])\ndf['exp_B'] = np.exp(df['B'])\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 51, "library_problem_id": 51, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 52, "code": ["<code>\nimport pandas as pd\n\ndef add_inverse_columns(df):\n    for col in df.columns:\n        df['inv_' + col] = 1 / df[col]\n\ndf = pd.DataFrame({\"A\": [1, 0, 3], \"B\": [4, 5, 6]})\nadd_inverse_columns(df)\nresult = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 52, "library_problem_id": 52, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 53, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef sigmoid_transform(df):\n    for col in df.columns:\n        df['sigmoid_' + col] = df[col].apply(lambda x: 1 / (1 + np.exp(-x)))\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 53, "library_problem_id": 53, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 54, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef find_last_max_index(df):\n    # Find the index location of the last occurrence of the maximum value in each column, considering only values up to the index of the column-wise minimum.\n    return df.apply(lambda x: x[:x.idxmin()].idxmax() if x.idxmin() != len(x) else x.idxmax(), axis=1)\n\na = np.array([[ 1. ,  0.9,  1. ],\n              [ 0.9,  0.9,  1. ],\n              [ 0.8,  1. ,  0.5],\n              [ 1. ,  0.3,  0.2],\n              [ 1. ,  0.2,  0.1],\n              [ 0.9,  1. ,  1. ],\n              [ 1. ,  0.9,  1. ],\n              [ 0.6,  0.9,  0.7],\n              [ 1. ,  0.9,  0.8],\n              [ 1. ,  0.8,  0.9]])\n\ndf = pd.DataFrame(a, columns=['A', 'B', 'C'], index=pd.date_range('2017', periods=10))\n\nprint(find_last_max_index(df))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 54, "library_problem_id": 54, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 55, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\na = np.array([[ 1. ,  0.9,  1. ],\n              [ 0.9,  0.9,  1. ],\n              [ 0.8,  1. ,  0.5],\n              [ 1. ,  0.3,  0.2],\n              [ 1. ,  0.2,  0.1],\n              [ 0.9,  1. ,  1. ],\n              [ 1. ,  0.9,  1. ],\n              [ 0.6,  0.9,  0.7],\n              [ 1. ,  0.9,  0.8],\n              [ 1. ,  0.8,  0.9]])\n\nidx = pd.date_range('2017', periods=a.shape[0])\ndf = pd.DataFrame(a, index=idx)\n\ndef find_index(df):\n    max_col = df.idxmax(axis=1)\n    min_col = df.idxmin(axis=1)\n    for i in range(len(max_col)):\n        if max_col[i] < min_col[i]:\n            return i\n    return None\n\nprint(find_index(df))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 55, "library_problem_id": 55, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 56, "code": ["<code>\nimport pandas as pd\n\ndef expand_date_range(df):\n    df['dt'] = pd.to_datetime(df['dt'])\n    df = df.set_index('dt').groupby('user').resample('D').ffill().reset_index()\n    df['val'] = df['val'].fillna(0)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 56, "library_problem_id": 56, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 57, "code": ["<code>\nimport pandas as pd\n\ndef expand_date_range(df):\n    df['dt'] = pd.to_datetime(df['dt'])\n    df = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\n    df = df.groupby('user').apply(lambda x: pd.date_range(start=x['dt'].min(), end=x['dt'].max())).reset_index()\n    df = df.merge(df['dt'].drop_duplicates().reset_index(drop=True), on='dt').drop('dt', axis=1)\n    df = df.merge(df, on='user').drop_duplicates().sort_values(['user', 0])\n    df.columns = ['user', 'dt', 'val']\n    df['val'] = df.groupby(['user', pd.Grouper(key='dt', freq='D')])['val'].transform(lambda x: x.ffill().bfill())\n    df = df.sort_values(['user', 'dt']).reset_index(drop=True)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 57, "library_problem_id": 57, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 56}}
{"id": 58, "code": ["<code>\nimport pandas as pd\n\ndef expand_date_range(df):\n    df['dt'] = pd.to_datetime(df['dt'])\n    df = df.sort_values(['user', 'dt'])\n    df = df.set_index(['user', 'dt']).unstack(fill_value=233).stack().reset_index()\n    df = df.groupby('user').apply(lambda g: pd.date_range(g['dt'].min(), g['dt'].max())).reset_index()\n    df = df.set_index(['user', 'dt']).unstack(fill_value=233).stack().reset_index()\n    df = df.sort_values(['user', 'dt'])\n    df['val'] = df['val'].fillna(233)\n    return df\n\nx = pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\nprint(expand_date_range(x))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 58, "library_problem_id": 58, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 59, "code": ["<code>\nimport pandas as pd\n\ndef expand_date_range(df):\n    df['date'] = pd.to_datetime(df['date'])\n    df.set_index(['user_id', 'date'], inplace=True)\n    df = df.groupby(level=0).apply(lambda x: x.reindex(pd.date_range(x.index.levels[1].min(), x.index.levels[1].max())))\n    df['val'] = df.groupby(level=0)['val'].transform('max')\n    df.reset_index(inplace=True)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 59, "library_problem_id": 59, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 60, "code": ["<code>\nimport pandas as pd\n\ndef expand_df(df):\n    df['dt'] = pd.to_datetime(df['dt'])\n    df = df.set_index(['user', 'dt']).unstack(fill_value=0).stack()\n    df = df.groupby(level=0).apply(lambda x: x.reindex(pd.date_range(x.index.min(), x.index.max())))\n    df = df.reset_index()\n    df['val'] = df.groupby('user')['val'].transform('max')\n    return df\n\nx = pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\nprint(expand_df(x))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 60, "library_problem_id": 60, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 61, "code": ["<code>\nimport pandas as pd\n\ndef replace_names_with_ids(df):\n    df['name'] = pd.Categorical(df['name']).codes\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 61, "library_problem_id": 61, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 62, "code": ["<code>\n'Brave', 'David'],\n                            'a': [3, 3, 3, 4, 3, 5],\n                            'b': [5, 6, 6, 6, 6, 1],\n                            'c': [7, 9, 10, 0, 1, 4]})\n\ndef replace_with_unique_ids(df):\n    # Create a dictionary to store the unique IDs for each unique name\n    id_dict = {}\n    # Iterate over the 'name' column\n    for name in df['name'].unique():\n        # Generate unique IDs for each unique 'a' value in the current name\n        ids = range(1, df[df['name'] == name]['a'].nunique() + 1)\n        # Store the IDs in the dictionary\n        id_dict[name] = dict(zip(df[df['name'] == name]['a'].unique(), ids))\n    # Replace the 'a' values in the DataFrame with the corresponding IDs\n    df['a'] = df.apply(lambda row: id_dict[row['name']][row['a']], axis=1)\n    return df\n\ndf = replace_with_unique_ids(df)\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 62, "library_problem_id": 62, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 61}}
{"id": 63, "code": ["<code>\nimport pandas as pd\n\ndef replace_names_with_ids(df):\n    # Check if 'name' column exists in the DataFrame\n    if 'name' not in df.columns:\n        raise ValueError(\"'name' column not found in the DataFrame\")\n\n    # Create a dictionary mapping unique names to unique IDs\n    name_to_id = {name: i for i, name in enumerate(df['name'].unique())}\n\n    # Replace 'name' column with IDs\n    df['name'] = df['name'].map(name_to_id)\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 63, "library_problem_id": 63, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 64, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\n# Assuming df is your DataFrame\ndf = pd.DataFrame({\n    'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave'],\n    'a': [3, 3, 3, 4, 3],\n    'b': [5, 6, 6, 6, 6],\n    'c': [7, 9, 10, 0, 1]\n})\n\ndf.fillna(df.mean(), inplace=True)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 64, "library_problem_id": 64, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 61}}
{"id": 65, "code": ["<code>\ndef read_data(data):\n    lines = data.split('\\n')\n    headers = lines[0].split()\n    users = []\n    for line in lines[1:]:\n        if not line.strip():\n            continue\n        fields = line.split()\n        user = {\n            'user': fields[0],\n            'date1': fields[1],\n            'date2': fields[2],\n            'value': int(fields[3]),\n            'someBool': fields[4].lower() == 'true',\n        }\n        users.append(user)\n    return users\n\ndata = \"\"\"\nuser    01/12/15    02/12/15 someBool\nu1      100         300      True\nu2      200        -100      False\nu3     -50          200      True\n\"\"\"\n\nusers = read_data(data)\nfor user in users:\n    print(user)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 65, "library_problem_id": 65, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 66, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'user': ['u1', 'u2', 'u3'],\n                   '01/12/15': [100, 200, -50],\n                   '02/12/15': [300, -100, 200],\n                   'someBool': [True, False, True]})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 66, "library_problem_id": 66, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 65}}
{"id": 67, "code": ["<code>\nimport pandas as pd\n\n# Assuming df is your initial DataFrame\ndf = pd.DataFrame({\n    'user': ['u1', 'u2', 'u3'],\n    '01/12/15': [100, 200, None],\n    '02/12/15': [None, -100, 200],\n    'someBool': [True, False, True]\n})\n\n# Melt the DataFrame to long format\ndf_long = pd.melt(df, id_vars='user', var_name='date', value_name='value')\n\n# Drop rows with missing values\ndf_long = df_long.dropna(subset=['value'])\n\n# Convert 'value' column to integer\ndf_long['value'] = df_long['value'].astype(int)\n\n# Print the transformed DataFrame\nprint(df_long)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 67, "library_problem_id": 67, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 65}}
{"id": 68, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\ncolumns = ['b','e']\n\n# Select only the columns 'b' and 'e'\ndf = df[columns]\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 68, "library_problem_id": 68, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 69, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef efficient_filter(df):\n    # Filter rows where the value in column 'c' is greater than 0.45\n    filtered_df = df[df['c'] > 0.45]\n    \n    # Select specific columns 'a', 'b', and 'e'\n    filtered_df = filtered_df[['a', 'b', 'e']]\n    \n    # Convert the result to a NumPy array\n    result = filtered_df.to_numpy()\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 69, "library_problem_id": 69, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 70, "code": ["<code>\nThe function `f` is not defined in the given code. Please provide the definition of the function `f` so that I can assist you in completing the code.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 70, "library_problem_id": 70, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 71, "code": ["<code>\nimport pandas as pd\n\ndef process_data(df, columns):\n    # Filter rows where 'c' column value is greater than 0.5\n    df = df[df['c'] > 0.5]\n    \n    # Select only the specified columns\n    df = df[columns]\n    \n    # Add a new column 'sum' that represents the sum of the selected columns\n    df['sum'] = df[columns].sum(axis=1)\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 71, "library_problem_id": 71, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 72, "code": ["<code>\nimport pandas as pd\n\ndef select_rows_and_columns(df, columns):\n    threshold = 0.5\n    df_filtered = df[df['a'] > threshold][columns]\n    return df_filtered\n\n# Test the function\ndf = pd.DataFrame({\n    'a': [0.945686, 0.919359],\n    'b': [0.000710, 0.667057],\n    'c': [0.909158, 0.462478],\n    'd': [0.892892, 0.008204],\n    'e': [0.326670, 0.0]\n})\n\ncolumns = ['a', 'c', 'e']\nprint(select_rows_and_columns(df, columns))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 72, "library_problem_id": 72, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 73, "code": ["<code>\nimport pandas as pd\n\ndef remove_overlapping_dates(df, x):\n    # Convert the 'date' column to datetime format\n    df['date'] = pd.to_datetime(df['date'])\n\n    # Sort the DataFrame by the 'date' column\n    df = df.sort_values('date')\n\n    # Initialize an empty DataFrame to store the non-overlapping rows\n    non_overlapping_df = pd.DataFrame()\n\n    # Iterate over the DataFrame\n    for i in range(len(df)):\n        # If the DataFrame is empty, append the first row\n        if non_overlapping_df.empty:\n            non_overlapping_df = non_overlapping_df.append(df.iloc[i])\n        else:\n            # If the current row's date is not within X days of the last row's date, append the current row\n            if (df.iloc[i]['date'] - non_overlapping_df.iloc[-1]['date']).days > x:\n                non_overlapping_df = non_overlapping_df.append(df.iloc[i])\n\n    return non_overlapping_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 73, "library_problem_id": 73, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 74, "code": ["<code>\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndef remove_close_dates(df, weeks):\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.sort_values(by='date')\n    df = df.reset_index(drop=True)\n\n    for i in range(len(df)):\n        if i == 0:\n            continue\n        if (df.loc[i, 'date'] - df.loc[i-1, 'date']).days <= 7*weeks:\n            df = df.drop(i)\n            df = df.reset_index(drop=True)\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 74, "library_problem_id": 74, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 73}}
{"id": 75, "code": ["<code>\nimport pandas as pd\n\ndef remove_duplicate_dates(df, x):\n    # Parse the 'date' column as datetime objects\n    df['date'] = pd.to_datetime(df['date'])\n\n    # Sort the DataFrame by 'date'\n    df = df.sort_values('date')\n\n    # Initialize a new column 'diff' with the difference in weeks between each date and the previous date\n    df['diff'] = (df['date'] - df['date'].shift()).fillna(pd.Timedelta(days=0)) / pd.Timedelta(weeks=1)\n\n    # Remove rows where the 'diff' is less than or equal to X\n    df = df[df['diff'] > x]\n\n    # Reset the index and drop the 'diff' column\n    df = df.reset_index(drop=True).drop(columns='diff')\n\n    # Format the 'date' column as '%d-%b-%Y'\n    df['date'] = df['date'].dt.strftime('%d-%b-%Y')\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 75, "library_problem_id": 75, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 73}}
{"id": 76, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0]})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 76, "library_problem_id": 76, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 77, "code": ["<code>\nprint(df['col1'].value_counts())\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 77, "library_problem_id": 77, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 78, "code": ["<code>\nimport pandas as pd\n\n# Given DataFrame\ndf = pd.DataFrame({'col1': [1, 1, 4, 5, 1, 4]})\n\n# Bin the DataFrame\ndf['bin'] = (df.index / 4).astype(int)\n\n# Group by the bin and concatenate the values\nresult = df.groupby('bin')['col1'].apply(lambda x: ''.join(x.astype(str))).reset_index(drop=True)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 78, "library_problem_id": 78, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 79, "code": ["<code>\nimport pandas as pd\n\ndef bin_dataframe(df):\n    # Calculate the number of groups\n    num_groups = len(df) // 3\n\n    # Group the dataframe by every three rows starting from the end\n    grouped = df.groupby(df.index // 3)\n\n    # Calculate the mean for each group\n    result = grouped['col1'].mean().reset_index(name='mean')\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 79, "library_problem_id": 79, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 80, "code": ["<code>\nunique_count = df['col1'].nunique()\nprint(unique_count)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 80, "library_problem_id": 80, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 81, "code": ["<code>\nunique_count = df['col1'].nunique()\nprint(unique_count)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 81, "library_problem_id": 81, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 82, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef fill_zeros(df):\n    df['A'] = df['A'].replace(0, np.nan)\n    df['A'].fillna(method='pad', inplace=True)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 82, "library_problem_id": 82, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 83, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns = ['A'])\n\ndef replace_zeros(df):\n    df['A'] = df['A'].replace(0, np.nan)\n    df['A'].fillna(method='bfill', inplace=True)\n\nreplace_zeros(df)\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 83, "library_problem_id": 83, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 82}}
{"id": 84, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef fill_zeros(df):\n    mask = df['A'] == 0\n    df.loc[mask, 'A'] = df['A'].replace(0, np.nan)\n    df['A'].fillna(method='ffill', inplace=True)\n    df['A'].fillna(method='bfill', inplace=True)\n    df['A'] = df['A'].astype(int)\n\n# Create DataFrame\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data, index=index, columns=['A'])\n\n# Call the function\nfill_zeros(df)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 84, "library_problem_id": 84, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 85, "code": ["<code>\nimport pandas as pd\n\n# Create a sample DataFrame\ndata = {'duration': ['7 year', '2 day', '4 week', '8 month']}\ndf = pd.DataFrame(data, index=[1, 2, 3, 4])\n\n# Extract the numerical and time unit parts\ndf['number'] = df.duration.str.extract('(\\d+)').astype(int)\ndf['time'] = df.duration.str.extract('([a-zA-Z]+)')\n\n# Create a dictionary to map time units to their number of days\ntime_to_days = {'year': 365, 'day': 1, 'week': 7, 'month': 30}\n\n# Create the 'time_days' column\ndf['time_days'] = df['time'].map(time_to_days)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 85, "library_problem_id": 85, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 86, "code": ["<code>\ndf['time'] = df.duration.str.split().str[0]\ndf['number'] = df.duration.str.split().str[1].astype(int)\ndf['time_day'] = df['time'].replace({'year': 365, 'day': 1, 'week': 7, 'month': 30})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 86, "library_problem_id": 86, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 87, "code": ["<code>\ndf['time'] = df.duration.str.extract('([a-zA-Z]+)')\n\ntime_days = {'year': 365, 'day': 1, 'week': 7, 'month': 30}\ndf['time_days'] = df['time'].map(time_days)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 87, "library_problem_id": 87, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 88, "code": ["<code>\nimport pandas as pd\n\n# Create a sample DataFrame\ndata = {'duration': ['year 7', 'day 2', 'week 4', 'month 8']}\ndf = pd.DataFrame(data)\n\n# Extract the time unit and number\ndf['number'] = df.duration.str.extract('(\\d+)').astype(int)\ndf['time'] = df.duration.str.extract('([a-zA-Z]+)')\n\n# Create a dictionary to map time units to days\ntime_to_days = {'year': 365, 'day': 1, 'week': 7, 'month': 30}\n\n# Create the 'time_day' column\ndf['time_day'] = df['number'] * df['time'].map(time_to_days)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 88, "library_problem_id": 88, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 89, "code": ["<code>\n'C': [3, 3, 3],\n                   'D': [4, 4, 4],\n                   'E': [5, 5, 5],\n                   'F': [6, 6, 6],\n                   'Postset': ['yes', 'no', 'yes']})\ncolumns_check_list = ['A', 'B', 'C', 'D', 'E', 'F', 'Postset']\n\ndef check_identical_rows(df1, df2, columns_check_list):\n    return df1[columns_check_list].eq(df2[columns_check_list]).all(axis=1)\n\nprint(check_identical_rows(df1, df2, columns_check_list))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 89, "library_problem_id": 89, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 90, "code": ["<code>\nimport pandas as pd\n\ndef check_equal_values(df1, df2, columns_check_list):\n    return (df1[columns_check_list] == df2[columns_check_list]).all(axis=1).tolist()\n\ndf1 = pd.DataFrame({'A': [1, 1, 1], 'B': [2, 2, 2], 'C': [3, 3, 3], 'D': [4, 4, 4], 'E': [5, 5, 5], 'F': [6, 6, 6], 'Postset': ['yes', 'no', 'yes']})\ndf2 = pd.DataFrame({'A': [1, 1, 1], 'B': [2, 2, 2], 'C': [3, 3, 3], 'D': [4, 4, 4], 'E': [5, 5, 5], 'F': [6, 6, 6], 'Preset': ['yes', 'yes', 'yes']})\n\ncolumns_check_list = ['A','B','C','D','E','F']\n\nprint(check_equal_values(df1, df2, columns_check_list))  # Output: [True, True, True]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 90, "library_problem_id": 90, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 89}}
{"id": 91, "code": ["<code>\nimport pandas as pd\n\n# Assuming the DataFrame is named df\ndf = pd.DataFrame({\n    'id': ['abc', '9/1/1994', '3/1/1995'],\n    'date': ['3/1/1994', '9/1/1994', '3/1/1995'],\n    'x': [100, 90, 80],\n    'y': [7, 8, 9]\n})\n\n# Convert 'date' column to datetime\ndf['date'] = pd.to_datetime(df['date'])\n\n# Set 'date' column as index\ndf.set_index('date', inplace=True)\n\n# Sort the DataFrame by index\ndf.sort_index(inplace=True)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 91, "library_problem_id": 91, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 92, "code": ["<code>\nimport pandas as pd\n\nindex = pd.MultiIndex.from_tuples([('abc', '3/1/1994'), ('abc', '9/1/1994'), ('abc', '3/1/1995')],\n                                 names=('name', 'datetime'))\ndf = pd.DataFrame({'fee': [100, 90, 80], 'credits':[7, 8, 9]}, index=index)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 92, "library_problem_id": 92, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 93, "code": ["<code>\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\ndef convert_to_numpy(df):\n    # Parse the date index into datetime objects\n    df.index = df.index.map(lambda x: datetime.strptime(x[1], '%Y-%m-%d'))\n\n    # Select the 'x' and 'y' columns and convert to numpy array\n    df = df[['x', 'y']].to_numpy()\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 93, "library_problem_id": 93, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 94, "code": ["<code>\n### END SOLUTION\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 94, "library_problem_id": 94, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 95, "code": ["<code>\nThe given code is a pandas DataFrame. It seems like the DataFrame is already complete. If you want to add more data to it, you can use the `append()` function. Here is an example of how you can add more data:\n\n```python\nimport pandas as pd\n\n# Given data\ndata = {\n    'Index': [0, 1, 2, 3],\n    'Country': ['Argentina', 'Argentina', 'Brazil', 'Brazil'],\n    'Variable': ['var1', 'var2', 'var1', 'var2'],\n    '2000': [12, 15, 20, 23],\n    '2001': [15, 18, 23, 29],\n    '2002': [18, 20, 25, 29],\n    '2003': [17, 2, 29, 2],\n    '2004': [23, 7, 31, 3],\n    '2005': [29, 5, 32, 3]\n}\n\ndf = pd.DataFrame(data)\n\n# Add more data\nnew_data = {\n    'Index': [4],\n    'Country': ['Chile'],\n    'Variable': ['var1'],\n    '2000': [13],\n    '2001': [16],\n    '2002': [19],\n    '2003': [20],\n    '2004': [24],\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 95, "library_problem_id": 95, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 96, "code": ["<code>\nimport pandas as pd\n\n# Assuming the given dataframe is df\n\n# Melt the dataframe to long format\ndf_long = pd.melt(df, id_vars=['Index', 'Country', 'Variable'], var_name='Year')\n\n# Convert Year to integer\ndf_long['Year'] = df_long['Year'].astype(int)\n\n# Sort by Country and Year in descending order\ndf_long = df_long.sort_values(by=['Country', 'Year'], ascending=[True, False])\n\n# Reset the index\ndf_long = df_long.reset_index(drop=True)\n\nreturn df_long\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 96, "library_problem_id": 96, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 97, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'A_Name': ['AA', 'BB', 'CC', 'DD', 'EE', 'FF', 'GG'],\n                   'B_Detail': ['X1', 'Y1', 'Z1', 'L1', 'M1', 'N1', 'K1'],\n                   'Value_B': [1.2, 0.76, 0.7, 0.9, 1.3, 0.7, -2.4],\n                   'Value_C': [0.5, -0.7, -1.3, -0.5, 0.6, -1.2, 0.8]})\n\ndef filter_rows(df):\n    value_columns = [col for col in df.columns if col.startswith('Value')]\n    filtered_df = df[df[value_columns].abs().max(axis=1) < 1]\n    return filtered_df\n\nfiltered_df = filter_rows(df)\nprint(filtered_df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 97, "library_problem_id": 97, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 98, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef filter_rows(df):\n    # Select columns with 'Value_' prefix\n    value_columns = df.columns[df.columns.str.startswith('Value_')]\n\n    # Apply absolute value and check if any value exceeds 1\n    df = df[np.abs(df[value_columns]).max(axis=1) <= 1]\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 98, "library_problem_id": 98, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 97}}
{"id": 99, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'A_Name': ['AA', 'BB', 'CC', 'DD', 'EE', 'FF', 'GG'],\n                   'B_Detail': ['X1', 'Y1', 'Z1', 'L1', 'M1', 'N1', 'K1'],\n                   'Value_B': [1.2, 0.76, 0.7, 0.9, 1.3, 0.7, -2.4],\n                   'Value_C': [0.5, -0.7, -1.3, -0.5, 1.8, -0.8, -1.9],\n                   'Value_D': [-1.3, 0.6, -0.7, 1.2, -1.5, 0.8, 2.3]})\n\n# Filter rows where the absolute value of any column starting with 'Value_' exceeds 1\nfiltered_df = df.loc[:, (df.columns.str.startswith('Value_')) & (df.abs().max(axis=1) > 1)]\n\n# Remove the 'Value_' prefix from the column names\nfiltered_df.columns = filtered_df.columns.str.replace('Value_', '')\n\nprint(filtered_df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 99, "library_problem_id": 99, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 100, "code": ["<code>\nimport pandas as pd\n\n# Create a DataFrame\ndf = pd.DataFrame({\n    'A': ['Good &AMP; bad'],\n    'B': ['Some value'],\n    'C': ['Good &AMP; bad']\n})\n\ndef replace_amp(df):\n    for col in df.columns:\n        if df[col].dtype == 'object':\n            df[col] = df[col].str.replace('&AMP;', '&')\n    return df\n\n# Apply the function to the DataFrame\ndf = replace_amp(df)\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 100, "library_problem_id": 100, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 101, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'A': ['Good &LT bad', 'BB', 'CC', 'DD', 'Good &LT; bad'], 'B': range(5), 'C': ['Good &LT; bad'] * 5})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 101, "library_problem_id": 101, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 102, "code": ["<code>\nimport pandas as pd\n\nexample_df = pd.DataFrame({'A': ['Good &AMP; bad', 'BB', 'CC', 'DD', 'Good &AMP; bad'], 'B': range(5), 'C': ['Good &AMP; bad'] * 5})\ndef f(df=example_df):\n    # Replace '&AMP;' with '&'\n    df['A'] = df['A'].str.replace('&AMP;', '&')\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 102, "library_problem_id": 102, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 100}}
{"id": 103, "code": ["<code>\nimport pandas as pd\n\n# Assuming df is your DataFrame\ndf = pd.DataFrame({\n    'column1': ['This &AMP; is &AMP;AMP; test', 'This is &LT; test', 'This &GT; is test'],\n    'column2': ['This &AMP; is test', 'This is test', 'This &GT; is &AMP; test'],\n    'column3': ['This &AMP;AMP; is &LT; &GT; test', 'This is test', 'This &AMP; is test']\n})\n\n# Replace all occurrences of HTML entities\ndf = df.replace({'&AMP;': '&', '&LT;': '<', '&GT;': '>'}, regex=True)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 103, "library_problem_id": 103, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 104, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'A': ['1 &AMP; 1', 'BB', 'CC', 'DD', '1 &AMP; 0'], 'B': range(5), 'C': ['0 &AMP; 0'] * 5})\n\n# Replace '&AMP;' with '&' in the 'A' column\ndf['A'] = df['A'].str.replace('&AMP;', '&')\n\n# Convert the 'B' column to floats\ndf['B'] = df['B'].astype(float)\n\n# Split the 'C' column into two new columns\ndf['C0'], df['C1'] = df['C'].str.split(' & ').str\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 104, "library_problem_id": 104, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 105, "code": ["<code>\nimport pandas as pd\nimport re\n\ndef split_names(df):\n    df['first_name'] = df['name'].apply(lambda x: re.split(' ', x)[0] if re.split(' ', x)[1:] else None)\n    df['last_name'] = df['name'].apply(lambda x: ' '.join(re.split(' ', x)[1:]) if re.split(' ', x)[1:] else None)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 105, "library_problem_id": 105, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 106, "code": ["<code>\nimport pandas as pd\n\ndef split_names(df):\n    # Check if 'name' column exists in the DataFrame\n    if 'name' not in df.columns:\n        return df\n\n    # Split names with exactly one space into two columns\n    df['1_name'] = df['name'].apply(lambda x: x.split(' ')[0] if ' ' in x and x.count(' ') == 1 else x)\n    df['2_name'] = df['name'].apply(lambda x: x.split(' ')[1] if ' ' in x and x.count(' ') == 1 else '')\n\n    # Remove 'name' column\n    df.drop(columns=['name'], inplace=True)\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 106, "library_problem_id": 106, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 105}}
{"id": 107, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef split_names(df):\n    df['first_name'] = df['name'].apply(lambda x: np.nan if ' ' not in x else x.split(' ')[0])\n    df['middle_name'] = df['name'].apply(lambda x: np.nan if ' ' not in x else (x.split(' ')[1] if len(x.split(' ')) == 2 else np.nan))\n    df['last_name'] = df['name'].apply(lambda x: np.nan if ' ' not in x else (x.split(' ')[-1] if len(x.split(' ')) == 2 else np.nan))\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 107, "library_problem_id": 107, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 105}}
{"id": 108, "code": ["<code>\nimport pandas as pd\n\ndef join_dataframes(df1, df2):\n    if df1.empty or df2.empty:\n        return \"One or both of the dataframes are empty\"\n    else:\n        try:\n            joined_df = pd.concat([df1, df2])\n            return joined_df\n        except Exception as e:\n            return f\"An error occurred: {str(e)}\"\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 108, "library_problem_id": 108, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 109, "code": ["<code>\nimport pandas as pd\n\ndef merge_dataframes(df1, df2):\n    # Convert 'Timestamp' columns to datetime format\n    df1['Timestamp'] = pd.to_datetime(df1['Timestamp'])\n    df2['Timestamp'] = pd.to_datetime(df2['Timestamp'])\n\n    # Merge dataframes based on closest matching timestamp\n    merged_df = pd.merge_asof(df1.sort_values('Timestamp'), df2.sort_values('Timestamp'), \n                              on='Timestamp', direction='nearest', suffixes=('_df1', '_df2'))\n\n    return merged_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 109, "library_problem_id": 109, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 108}}
{"id": 110, "code": ["<code>\nimport pandas as pd\n\ndef create_state_column(df):\n    # Convert 'datetime' column to datetime format\n    df['datetime'] = pd.to_datetime(df['datetime'])\n\n    # Create 'state' column\n    df['state'] = df.apply(lambda row: row['col1'] if row['col2'] <= 50 and row['col3'] <= 50 else max(row['col1'], row['col2'], row['col3']), axis=1)\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 110, "library_problem_id": 110, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 111, "code": ["<code>\ndef complete_function(df):\n    df['state'] = df.apply(lambda row: row['col1'] if row['col2'] > 50 and row['col3'] > 50 else row['col1'] + row['col2'] + row['col3'], axis=1)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 111, "library_problem_id": 111, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 110}}
{"id": 112, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 112, "library_problem_id": 112, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 113, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 113, "library_problem_id": 113, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 112}}
{"id": 114, "code": ["<code>\nimport pandas as pd\n\n# Assuming the given data is in a dictionary\ndata = {\n    'ID': [1, 2, 3, 4, 5],\n    'Field1': [1.15, 2, 1, 25, 'and']\n}\n\n# Create DataFrame\ndf = pd.DataFrame(data)\n\n# Print the DataFrame\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 114, "library_problem_id": 114, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 112}}
{"id": 115, "code": ["<code>\nimport pandas as pd\n\n# Assuming the data is in a string format\ndata = \"\"\"\ncat  val1   val2   val3   val4\nA    7      10     0      19\nB    10     2      1      14\nC    5      15     6      16\n\"\"\"\n\n# Convert the string into a pandas DataFrame\ndf = pd.read_csv(StringIO(data), sep=\"\\s+\", index_col=0)\n\n# Calculate the sum of each row\ndf['sum'] = df.sum(axis=1)\n\n# Calculate the average of each row\ndf['average'] = df.mean(axis=1)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 115, "library_problem_id": 115, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 116, "code": ["<code>\nimport pandas as pd\n\ndef calculate_percentage_contribution(df, cat_col, num_cols):\n    \"\"\"\n    Calculate the percentage contribution of each category in each numerical column.\n\n    Parameters:\n    df (DataFrame): The input DataFrame.\n    cat_col (str): The name of the categorical column.\n    num_cols (list): The names of the numerical columns.\n\n    Returns:\n    DataFrame: A new DataFrame where each cell contains the percentage of each category's value within the corresponding numerical column's total.\n    \"\"\"\n    # Calculate the total for each numerical column\n    total = df[num_cols].sum()\n\n    # Calculate the percentage contribution of each category in each numerical column\n    result = df.groupby(cat_col)[num_cols].apply(lambda x: (x / total) * 100)\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 116, "library_problem_id": 116, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 115}}
{"id": 117, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\n# Assuming the DataFrame is named 'df'\ndf = pd.DataFrame({\n    'alleles': ['A/C', 'A/T', 'T/A', 'C/A', 'C/T'],\n    'chrom': ['A', 'A', 'T', 'C', 'C'],\n    'pos': [0, 0, 0, 0, 0],\n    'strand': ['+', '+', '+', '+', '+'],\n    'assembly#': [np.nan, np.nan, np.nan, np.nan, np.nan],\n    'center': [np.nan, np.nan, np.nan, np.nan, np.nan],\n    'protLSID': [np.nan, np.nan, np.nan, np.nan, np.nan],\n    'assayLSID': [np.nan, np.nan, np.nan, np.nan, np.nan],\n    'rs#': ['TP3', 'TP7', 'TP12', 'TP15', 'TP18']\n})\n\n# Fill NaN values with 'unknown'\ndf.fillna('unknown', inplace=True)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 117, "library_problem_id": 117, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 118, "code": ["<code>\nimport pandas as pd\n\n# Assuming the data is in a string format\ndata = \"\"\"\nTP3      A/C      0    3   \nTP7      A/T      0    7   \nTP12     T/A      0   12  \nTP15     C/A      0   15 \nTP18     C/T      0   18\n\"\"\"\n\n# Convert the string into a pandas DataFrame\ndf = pd.read_csv(StringIO(data), sep='\\s+', header=None, names=['TP', 'AB', '0', '3'])\n\n# Now you can process the DataFrame as you wish\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 118, "library_problem_id": 118, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 117}}
{"id": 119, "code": ["<code>\nimport pandas as pd\n\ndef remove_rows(df, row_names):\n    return df.drop(row_names)\n\n# Assuming the data is in a CSV file named 'data.csv' with whitespace as the delimiter\ndf = pd.read_csv('data.csv', delimiter='\\s+', index_col='rs')\n\n# List of row names to be removed\ntest = ['row1', 'row2', 'row3']\n\n# Call the function\ndf = remove_rows(df, test)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 119, "library_problem_id": 119, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 117}}
{"id": 120, "code": ["<code>\ndef get_rows(df, row_names):\n    # Create a set from the row_names to remove duplicates\n    row_names = set(row_names)\n    \n    # Filter the DataFrame to only include rows with names in the set\n    new_df = df[df.index.isin(row_names)]\n    \n    return new_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 120, "library_problem_id": 120, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 117}}
{"id": 121, "code": ["<code>\nThe given code seems to be a pandas DataFrame. However, it's not clear what the function is supposed to do. Could you please provide more details about what the function should do? For example, should it filter the data, sort it, calculate some statistics, etc.?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 121, "library_problem_id": 121, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 122, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\n# Create a DataFrame\ndf = pd.DataFrame({\n    'time': np.arange(7),\n    'car': [0, 0, 0, 1, 1, 2, 2],\n    'farthest_neighbour': [1, 2, 3, 1, 3, 4, 5],\n    'euclidean_distance': [2.236068, 2.236068, 1.414214, 10.049876, 10.049876, 53.037722, 53.037722]\n})\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 122, "library_problem_id": 122, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 123, "code": ["<code>\ndf.fillna(\"unknown\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 123, "library_problem_id": 123, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 124, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef create_keywords_all(df):\n    df['keywords_all'] = df.apply(lambda row: '-'.join(row.dropna().values.astype(str)), axis=1)\n    return df\n\ndf = pd.DataFrame({'keywords_0':[\"a\", np.nan, \"c\"], \n                'keywords_1':[\"d\", \"e\", np.nan],\n                'keywords_2':[np.nan, np.nan, \"b\"],\n                'keywords_3':[\"f\", np.nan, \"g\"]})\n\ndf = create_keywords_all(df)\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 124, "library_problem_id": 124, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 125, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef concat_keywords(df):\n    cols = [col for col in df.columns if 'keywords' in col]\n    df['keywords_all'] = df[cols].apply(lambda row: '-'.join(row.dropna().values), axis=1)\n    return df\n\n# Test the function\ndf = pd.DataFrame({\n    'users': ['Hu Tao', 'Zhongli', 'Xingqiu'],\n    'keywords_0': ['a', np.nan, 'c'],\n    'keywords_1': ['d', np.nan, np.nan],\n    'keywords_2': [np.nan, 'e', np.nan],\n    'keywords_3': ['f', np.nan, 'g']\n})\n\nprint(concat_keywords(df))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 125, "library_problem_id": 125, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 126, "code": ["<code>\nimport pandas as pd\n\n# Assuming the given DataFrame is named 'df'\n\n# Check if there are any NaN values in each column\nprint(df.isnull().sum())\n\n# Replace NaN values with a specific keyword\ndf.fillna('default_keyword', inplace=True)\n\n# Print the DataFrame after replacing NaN values\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 126, "library_problem_id": 126, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 127, "code": ["<code>\nimport pandas as pd\n\ndef modify_df(df):\n    # Randomly select 20% of rows\n    sample_df = df.sample(frac=0.2, random_state=0)\n    \n    # Set 'Quantity' column of the selected rows to zero\n    df.loc[sample_df.index, 'Quantity'] = 0\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 127, "library_problem_id": 127, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 128, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef zero_out_product_ids(df):\n    # Randomly select 20% of rows\n    rows_to_zero = np.random.choice(df.index, int(len(df) * 0.2), replace=False)\n    \n    # Set 'ProductId' column of selected rows to zero\n    df.loc[rows_to_zero, 'ProductId'] = 0\n    \n    return df\n\n# Initialize the DataFrame\ndf = pd.DataFrame({\n    'UserId': [1, 1, 1, 2, 3, 3],\n    'ProductId': [1, 4, 7, 4, 2, 1],\n    'Quantity': [6, 1, 3, 2, 7, 2]\n})\n\n# Call the function\ndf = zero_out_product_ids(df)\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 128, "library_problem_id": 128, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 127}}
{"id": 129, "code": ["<code>\nimport numpy as np\n\ndef zero_out_quantity(df):\n    for user in df['UserId'].unique():\n        user_data = df[df['UserId'] == user]\n        num_rows = user_data.shape[0]\n        num_rows_to_zero = int(num_rows * 0.2)\n        rows_to_zero = np.random.choice(user_data.index, num_rows_to_zero, replace=False)\n        df.loc[rows_to_zero, 'Quantity'] = 0\n    return df\n\ndf = zero_out_quantity(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 129, "library_problem_id": 129, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 127}}
{"id": 130, "code": ["<code>\nimport pandas as pd\n\ndef identify_duplicates(df, cols):\n    duplicate_bool = df.duplicated(subset=cols, keep='first')\n    df['duplicate_index'] = df.duplicated(subset=cols, keep=False)\n    df['duplicate_index'] = df['duplicate_index'].apply(lambda x: None if x else df.index.get_loc(x))\n    return df\n\ndf = pd.DataFrame(data=[[1,2],[3,4],[1,2],[1,4],[1,2]],columns=['col1','col2'])\ndf = identify_duplicates(df, ['col1','col2'])\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 130, "library_problem_id": 130, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 131, "code": ["<code>\nimport pandas as pd\n\ndef identify_duplicates(df):\n    df['last_occurrence'] = df.groupby(['col1', 'col2']).ngroup()\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 131, "library_problem_id": 131, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 130}}
{"id": 132, "code": ["<code>\nimport pandas as pd\n\ndef identify_duplicates(df):\n    df['index_original'] = df.duplicated(subset=['col1', 'col2'], keep=False)\n    df['index_original'] = df['index_original'].apply(lambda x: not x)\n    df['index_original'] = df.groupby(['col1', 'col2'])['index_original'].cumsum() - 1\n    df['index_original'] = df.groupby(['col1', 'col2'])['index_original'].transform(lambda x: x.idxmin())\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 132, "library_problem_id": 132, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 130}}
{"id": 133, "code": ["<code>\nSure, I can help you with that. However, you didn't provide the function `identify_duplicates`. Please provide the function `identify_duplicates` so I can help you complete the code.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 133, "library_problem_id": 133, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 134, "code": ["<code>\nimport pandas as pd\n\ndef identify_duplicates(df, columns):\n    df_duplicates = df[df.duplicated(subset=columns, keep=False)]\n    df_duplicates['index_original'] = df_duplicates.duplicated(subset=columns, keep='last').cumsum() - 1\n    return df_duplicates\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 134, "library_problem_id": 134, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 135, "code": ["<code>\nimport pandas as pd\n\ndef max_count_rows(df, group_cols):\n    # Group by the specified columns and find the maximum 'count' value in each group\n    max_counts = df.groupby(group_cols)['count'].max().reset_index()\n\n    # Merge the original DataFrame with the maximum 'count' values DataFrame\n    df = pd.merge(df, max_counts, on=group_cols, suffixes=('', '_max'))\n\n    # Filter the rows where 'count' equals the maximum 'count' value in each group\n    df = df[df['count'] == df['count_max']]\n\n    # Drop the '_max' suffix from the column names\n    df.columns = [col.rstrip('_max') for col in df.columns]\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 135, "library_problem_id": 135, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 136, "code": ["<code>\nimport pandas as pd\n\ndef max_count_rows(df):\n    # Group the DataFrame by 'Sp' and 'Mt'\n    grouped = df.groupby(['Sp', 'Mt'])\n    \n    # Find the maximum value in the 'count' column for each group\n    max_counts = grouped['count'].transform('max')\n    \n    # Filter the DataFrame to only include rows where the 'count' column is equal to the maximum count for that group\n    max_count_df = df[df['count'] == max_counts]\n    \n    return max_count_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 136, "library_problem_id": 136, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 135}}
{"id": 137, "code": ["<code>\nimport pandas as pd\n\n# Assuming df is your DataFrame\ndf = pd.DataFrame({\n    'Sp': [0, 1, 2, 3, 4, 5, 6, 7, 8],\n    'Mt': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n    'Value': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n    'count': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi']\n})\n\ndef process_data(df):\n    # Add your processing code here\n    pass\n\n# Call the function\nprocess_data(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 137, "library_problem_id": 137, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 135}}
{"id": 138, "code": ["<code>\nimport pandas as pd\n\n# Given DataFrame\ndf = pd.DataFrame({\n    'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n    'Value': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n    'Mt': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n    'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]\n})\n\ndef max_count_rows(df):\n    # Group by 'Sp' and 'Value' and find the maximum 'count' value in each group\n    max_count_df = df.groupby(['Sp', 'Value'])['count'].max().reset_index()\n\n    # Merge the original DataFrame with the maximum 'count' DataFrame on 'Sp', 'Value' and 'count'\n    result_df = pd.merge(df, max_count_df, on=['Sp', 'Value', 'count'])\n\n    return result_df\n\nprint(max_count_rows(df))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 138, "library_problem_id": 138, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 135}}
{"id": 139, "code": ["<code>\nimport pandas as pd\n\ndef filter_df_by_categories(df, categories):\n    try:\n        if not isinstance(categories, list):\n            raise ValueError(\"Categories should be a list.\")\n        if 'Category' not in df.columns:\n            raise ValueError(\"DataFrame does not have a 'Category' column.\")\n        if 'Index' in df.columns:\n            df = df.drop_duplicates(subset='Index', keep='first')\n        df = df[df['Category'].isin(categories)]\n        df = df.drop_duplicates(keep='first')\n        return df\n    except ValueError as ve:\n        print(ve)\n        return None\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 139, "library_problem_id": 139, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 140, "code": ["<code>\nimport pandas as pd\n\ndef filter_categories(df, filter_list):\n    # Check if the DataFrame is empty\n    if df.empty:\n        return df\n\n    # Check if the filter_list is empty\n    if not filter_list:\n        return df\n\n    # Filter the DataFrame based on the categories in filter_list\n    df = df[df['category'].isin(filter_list)]\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 140, "library_problem_id": 140, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 139}}
{"id": 141, "code": ["<code>\nimport pandas as pd\n\ndef melt_multiindex_df(df):\n    # Get the list of all column levels\n    all_levels = [i for i in range(df.columns.nlevels)]\n    \n    # Use pd.melt to melt the DataFrame\n    melted_df = pd.melt(df.reset_index(), id_vars='index', value_vars=all_levels)\n    \n    # Split the 'variable' column into two new columns 'level' and 'level_value'\n    melted_df[['level', 'level_value']] = melted_df['variable'].apply(lambda x: pd.Series(x)).astype(int)\n    \n    # Drop the 'variable' column\n    melted_df.drop(columns='variable', inplace=True)\n    \n    # Set the 'level' and 'level_value' as the index of the DataFrame\n    melted_df.set_index(['index', 'level', 'level_value'], inplace=True)\n    \n    # Sort the DataFrame by the index\n    melted_df.sort_index(inplace=True)\n    \n    return melted_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 141, "library_problem_id": 141, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 142, "code": ["<code>\nimport pandas as pd\n\ndef melt_multi_index(df):\n    # Use pd.melt to melt the DataFrame\n    melted_df = pd.melt(df.reset_index(), id_vars=df.index.names)\n    \n    # Rename the columns\n    melted_df.columns = ['_'.join(col) for col in melted_df.columns.to_flat_index()]\n    melted_df.columns = [col.replace('variable_', 'variable'), col.replace('value_', 'value') for col in melted_df.columns]\n    \n    # Set the index back to the original index\n    melted_df.set_index(df.index.names, inplace=True)\n    \n    return melted_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 142, "library_problem_id": 142, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 141}}
{"id": 143, "code": ["<code>\nimport pandas as pd\n\ndef calculate_cumulative_sum(df):\n    df['cumsum'] = df.groupby('id')['val'].cumsum()\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 143, "library_problem_id": 143, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 144, "code": ["<code>\nimport pandas as pd\n\n# Given DataFrame\ndf = pd.DataFrame({\n    'id': ['A', 'B', 'A', 'C', 'D', 'B', 'C'],\n    'stuff': [12, 23232, 13, 1234, 3235, 3236, 732323],\n    'val': [1, 2, -3, 1, 5, 6, -2]\n})\n\n# Calculate cumulative sum\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n\n# Sort by 'id' and 'stuff'\ndf = df.sort_values(by=['id', 'stuff'])\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 144, "library_problem_id": 144, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 145, "code": ["<code>\nimport pandas as pd\n\ndef calculate_cumulative_sum(df):\n    df['cumsum'] = df.groupby('id')['val'].cumsum()\n    return df\n\n# Test the function\ndf = pd.DataFrame({'id': ['A', 'B', 'A', 'B'],\n                   'stuff': [12, 23232, 13, 3236],\n                   'val': [1, 2, -3, 6]})\n\nprint(calculate_cumulative_sum(df))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 145, "library_problem_id": 145, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 146, "code": ["<code>\nimport pandas as pd\n\n# Given DataFrame\ndf = pd.DataFrame({\n    'id': ['A', 'B', 'A', 'C', 'D', 'B', 'C'],\n    'stuff': [12, 23232, 13, 1234, 3235, 3236, 732323],\n    'val': [1, 2, -3, 1, 5, 6, -2]\n})\n\n# Calculate cumulative maximum\ndf['cummax'] = df.groupby('id')['val'].cummax()\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 146, "library_problem_id": 146, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 147, "code": ["<code>\nimport pandas as pd\n\ndef cumsum_reset(df):\n    df['cumsum'] = df.groupby('id')['val'].cumsum().clip(0)\n    return df\n\ndf = pd.DataFrame.from_dict({'id': ['A', 'B', 'A', 'C', 'D', 'B', 'C'], 'val': [1,2,-3,1,5,6,-2], 'stuff':['12','23232','13','1234','3235','3236','732323']})\n\nprint(cumsum_reset(df))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 147, "library_problem_id": 147, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 143}}
{"id": 148, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef grouped_sum_with_nan_propagation(df):\n    return df.groupby(level=0, group_keys=False)['v'].apply(lambda x: np.nansum(x))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 148, "library_problem_id": 148, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 149, "code": ["<code>\nimport pandas as pd\n\ndef grouped_sum(df, column):\n    return df.groupby(df.index)[column].sum(skipna=False)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 149, "library_problem_id": 149, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 148}}
{"id": 150, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\nd = {'l':  ['left', 'right', 'left', 'right', 'left', 'right'],\n     'r': ['right', 'left', 'right', 'left', 'right', 'left'],\n     'v': [-1, 1, -1, 1, -1, np.nan]}\ndf = pd.DataFrame(d)\n\n# Function to replace 'left' with -1 and 'right' with 1\ndef replace_with_values(df):\n    df['l'].replace(['left'], -1, inplace=True)\n    df['r'].replace(['right'], 1, inplace=True)\n    return df\n\ndf = replace_with_values(df)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 150, "library_problem_id": 150, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 151, "code": ["<code>\nThe problem is not clear about the relationship between the columns. It's not mentioned what kind of relationship we should consider between the columns. However, I'll assume that we need to find the unique combinations of values in each pair of columns.\n\nHere is the Python function that takes a Pandas DataFrame as input and returns a list of strings describing the relationship between all pairs of columns in the DataFrame:\n\n```python\nimport pandas as pd\n\ndef describe_relationships(df):\n    relationships = []\n    for col1 in df.columns:\n        for col2 in df.columns:\n            if col1 != col2:\n                relationship = df[[col1, col2]].drop_duplicates()\n                if relationship.shape[0] == df.shape[0]:\n                    if relationship.shape[1] == 1:\n                        relationship_type = 'one-to-one'\n                    else:\n                        relationship_type = 'one-to-many'\n                elif relationship.shape[1] == 1:\n                    relationship_type = 'many-to-one'\n                else:\n                    relationship_type = 'many-to-many'\n                relationships.append(f'{col1} and {col2} are {relationship_type}')\n    return relationships\n\n# Create a DataFrame\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3,\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 151, "library_problem_id": 151, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 152, "code": ["<code>\ndf['Column6'] = [2, 2, 2, 2, 2, 2, 2, 2, 2]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 152, "library_problem_id": 152, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 153, "code": ["<code>\nimport pandas as pd\n\ndef relationship_df(df):\n    # Initialize an empty DataFrame\n    relationship_df = pd.DataFrame(columns=df.columns, index=df.columns)\n\n    # Iterate over each pair of columns\n    for col1 in df.columns:\n        for col2 in df.columns:\n            if col1 != col2:\n                # Determine the relationship between the two columns\n                if df[col1].nunique() == df[col2].nunique():\n                    relationship = 'one-to-one'\n                elif df[col1].nunique() < df[col2].nunique():\n                    relationship = 'one-to-many'\n                elif df[col1].nunique() > df[col2].nunique():\n                    relationship = 'many-to-one'\n                else:\n                    relationship = 'many-to-many'\n\n                # Set the relationship in the DataFrame\n                relationship_df.loc[col1, col2] = relationship\n\n    return relationship_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 153, "library_problem_id": 153, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 154, "code": ["<code>\nHere is the completed code:\n\n```python\nimport pandas as pd\n\ndef relationship_matrix(df):\n    # Create a DataFrame with the same shape as the input DataFrame filled with 'same'\n    relationship_df = pd.DataFrame('same', index=df.columns, columns=df.columns)\n\n    for col1 in df.columns:\n        for col2 in df.columns:\n            if col1 != col2:\n                # Check if the columns are the same\n                if df[col1].equals(df[col2]):\n                    relationship_df.loc[col1, col2] = 'same'\n                else:\n                    # Check if one column is a subset of the other\n                    if df[col1].isin(df[col2]).all():\n                        relationship_df.loc[col1, col2] = 'one-to-many' if df.shape[0] == df[col2].nunique() else 'many-to-one'\n                    elif df[col2].isin(df[col1]).all():\n                        relationship_df.loc[col1, col2] = 'many-to-one' if df.shape[0] == df[col1].nunique() else 'one-to-many'\n                    else:\n                        # If neither column is a subset of the other, check if they have the same unique values\n                        if df[col1].nunique() == df[col2].nunique() == df.shape[0]:\n                            relationship_df.loc[col1, col2] = 'one-to-one'\n                        else:\n                            relationship_df.loc[col1,\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 154, "library_problem_id": 154, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 155, "code": ["<code>\nimport pandas as pd\n\ndef deduplicate_dataframe(df):\n    # Convert all string values to lowercase and remove any white spaces from the 'firstname', 'lastname', and 'email' columns\n    df['firstname'] = df['firstname'].str.lower().str.strip()\n    df['lastname'] = df['lastname'].str.lower().str.strip()\n    df['email'] = df['email'].str.lower().str.strip()\n\n    # Sort the DataFrame by the 'bank' column in descending order, so rows with non-null 'bank' values come first\n    df = df.sort_values('bank', ascending=False)\n\n    # Drop duplicates based on the 'firstname', 'lastname', and 'email' columns\n    df = df.drop_duplicates(subset=['firstname', 'lastname', 'email'])\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 155, "library_problem_id": 155, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 155}}
{"id": 156, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ns = pd.Series(['2,144.78', '2,036.62', '1,916.60', '1,809.40', '1,711.97', '6,667.22', '5,373.59', '4,071.00', '3,050.20', '-0.06', '-1.88', '', '-0.13', '', '-0.14', '0.07', '0', '0'],\n              index=['2016-10-31', '2016-07-31', '2016-04-30', '2016-01-31', '2015-10-31', '2016-0\n\n# Replace commas with periods and convert to float\ns = s.replace(',', '.', regex=True).astype(float)\n\n# Replace NaN values with 0\ns = s.replace(np.nan, 0)\n\nprint(s)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 156, "library_problem_id": 156, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 156}}
{"id": 157, "code": ["<code>\nimport pandas as pd\n\ndef calculate_mean_survival(df):\n    df['Family'] = df.apply(lambda row: 'Has Family' if row['SibSp'] > 0 or row['Parch'] > 0 else 'No Family', axis=1)\n    return df.groupby('Family')['Survived'].mean()\n\ndf = pd.DataFrame({'Survived': [0,1,1,1,0], 'SibSp': [0,1,0,1,0], 'Parch': [1,1,0,1,1]})\nprint(calculate_mean_survival(df))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 157, "library_problem_id": 157, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 158, "code": ["<code>\nThe given code appears to be a pandas DataFrame, which is a two-dimensional labeled data structure with columns of potentially different types. The DataFrame contains information about passengers who survived the Titanic disaster. The columns are 'Survived', 'SibSp' (siblings/spouses aboard), and 'Parch' (parents/children aboard).\n\nHowever, the code you provided does not contain a function to be completed. Could you please provide more details about the function you want to be completed? For example, are you looking for a function to calculate the average number of siblings/spouses and parents/children aboard for those who survived, or are you looking for a function to count the number of passengers who have more than one sibling/spouse or parent/child?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 158, "library_problem_id": 158, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 157}}
{"id": 159, "code": ["<code>\nimport pandas as pd\n\n# Assuming df is your DataFrame\ndf = pd.DataFrame({\n    'Survived': [0, 1, 1, 0, 1, 1, 0, 1],\n    'SibSp': [1, 0, 0, 1, 0, 0, 1, 0],\n    'Parch': [1, 0, 1, 0, 1, 0, 0, 1]\n})\n\n# Create groups based on conditions\ngroups = df.groupby([(df['SibSp'] == 1) & (df['Parch'] == 1),\n                     (df['SibSp'] == 0) & (df['Parch'] == 0),\n                     (df['SibSp'] == 0) & (df['Parch'] == 1),\n                     (df['SibSp'] == 1) & (df['Parch'] == 0)])\n\n# Calculate mean of 'Survived' for each group\nresult = groups['Survived'].mean()\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 159, "library_problem_id": 159, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 157}}
{"id": 160, "code": ["<code>\nimport pandas as pd\n\ndef sort_within_groups(df, group_col, sort_col):\n    try:\n        if df[group_col].is_monotonic:\n            df = df.sort_values(group_col)\n        else:\n            df = df.sort_values(group_col)\n\n        df = df.groupby(group_col).apply(lambda x: x.sort_values(sort_col))\n        return df\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 160, "library_problem_id": 160, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 161, "code": ["<code>\nimport pandas as pd\n\ndef sort_data(data):\n    df = pd.DataFrame(data)\n    df = df.sort_values(by=['A'])\n    df = df.groupby('cokey')\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 161, "library_problem_id": 161, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 160}}
{"id": 162, "code": ["<code>\nimport pandas as pd\n\ndef restructure_columns(df):\n    # Create a MultiIndex from the tuples in the columns\n    df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Lower'])\n    return df\n\n# Example usage:\ndf = pd.DataFrame({\n    (A, a): [1, 2, 3],\n    (A, b): [2, 3, 4],\n    (B, a): [2, 3, 4],\n    (B, b): [3, 2, 1]\n}, index=[1, 2, 3])\n\nrestructured_df = restructure_columns(df)\nprint(restructured_df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 162, "library_problem_id": 162, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 162}}
{"id": 163, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\n# Define the original DataFrame with tuple column headers\ndata = {\n    ('A', '1', 'a'): np.random.rand(5),\n    ('A', '1', 'b'): np.random.rand(5),\n    ('A', '2', 'a'): np.random.rand(5),\n    ('A', '2', 'b'): np.random.rand(5),\n    ('B', '1', 'a'): np.random.rand(5),\n    ('B', '1', 'b'): np.random.rand(5)\n}\n\ndf = pd.DataFrame(data)\n\n# Convert the DataFrame with tuple column headers to a DataFrame with a MultiIndex column header\ndf.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Middle', 'Lower'])\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 163, "library_problem_id": 163, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 162}}
{"id": 164, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\nl = [('A', 'a', '1'), ('A', 'b', '2'), ('B','a', '1'), ('A', 'b', '1'),  ('B','b', '1'),  ('A', 'a', '2')]\nnp.random.seed(1)\ndf = pd.DataFrame(np.random.randn(5, 6), columns=l)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 164, "library_problem_id": 164, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 162}}
{"id": 165, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef create_dataframe(birds):\n    unique_values, counts = np.unique(birds, return_counts=True)\n    result = pd.DataFrame({'birdType': unique_values, 'birdCount': counts})\n    result.index = range(0, len(result))\n    return result\n\nbirds = ['Parrot', 'Eagle', 'Parrot', 'Sparrow', 'Eagle', 'Sparrow', 'Eagle']\nresult = create_dataframe(birds)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 165, "library_problem_id": 165, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 166, "code": ["<code>\nimport pandas as pd\n\ndef calculate_mean_std(df):\n    return df.groupby('a')['b'].agg(['mean', 'std']).reset_index()\n\n# Test the function\ndf = pd.DataFrame({'a': [1, 1, 1, 2, 2, 2, 3, 3, 3],\n                   'b': [12, 13, 23, 22, 23, 24, 30, 35, 55]})\n\nprint(calculate_mean_std(df))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 166, "library_problem_id": 166, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 166}}
{"id": 167, "code": ["<code>\nimport pandas as pd\n\ndef calculate_mean_std(df):\n    grouped = df.groupby('b')\n    result = pd.DataFrame({\n        'mean': grouped['a'].mean(),\n        'std': grouped['a'].std()\n    })\n    result.index.name = 'b'\n    return result\n\n# Test the function\ndf = pd.DataFrame({\n    'a': [12, 13, 23, 22, 23, 24, 30, 35, 55],\n    'b': [1, 1, 1, 2, 2, 2, 3, 3, 3]\n})\n\nprint(calculate_mean_std(df))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 167, "library_problem_id": 167, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 166}}
{"id": 168, "code": ["<code>\nimport pandas as pd\nimport numpy as np\nfrom scipy.special import softmax\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Given DataFrame\ndf = pd.DataFrame({'a': [1, 1, 1, 2, 2, 2, 3], 'b': [12, 13, 23, 22, 23, 24, 25]})\n\n# Group by 'a' and calculate softmax and min-max normalization of 'b' for each group\ndf['softmax'] = df.groupby('a')['b'].transform(lambda x: softmax(x))\ndf['min-max'] = df.groupby('a')['b'].transform(lambda x: MinMaxScaler().fit_transform(x.values.reshape(-1, 1)))\n\n# Print the DataFrame\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 168, "library_problem_id": 168, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 166}}
{"id": 169, "code": ["<code>\nimport pandas as pd\n\ndef remove_zero_sum_rows_cols(df):\n    row_sums = df.sum(axis=1)\n    col_sums = df.sum(axis=0)\n    rows_to_keep = row_sums[row_sums > 0].index\n    cols_to_keep = col_sums[col_sums > 0].index\n    return df.loc[rows_to_keep, cols_to_keep]\n\ndf = pd.DataFrame([[1,1,0,1],[0,0,0,0],[1,0,0,0],[0,1,0,1],[1,1,0,1]], columns=['A', 'B', 'C', 'D'])\n\nprint(remove_zero_sum_rows_cols(df))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 169, "library_problem_id": 169, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 169}}
{"id": 170, "code": ["<code>\nimport pandas as pd\n\ndef remove_zero_rows_cols(df):\n    # Remove rows with sum of zero\n    df = df[df.sum(axis=1) != 0]\n    \n    # Remove columns with sum of zero\n    df = df[df.sum(axis=0) != 0]\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 170, "library_problem_id": 170, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 169}}
{"id": 171, "code": ["<code>\nSure, I can help you with that. However, you didn't specify what kind of completion you need. Are you looking for a function to calculate the mean, median, mode, or some other statistical measures of the dataframe? Please provide more details so I can assist you better.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 171, "library_problem_id": 171, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 169}}
{"id": 172, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame([[1,2,3,1],[0,0,0,0],[1,0,0,1],[0,1,2,0],[1,1,0,1]])\n\ndf = df.apply(lambda x: [0 if i==2 else i for i in x])\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 172, "library_problem_id": 172, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 169}}
{"id": 173, "code": ["<code>\nimport pandas as pd\n\n# Assuming s is your pandas series\ns = pd.Series([1.2, 0.9, 1.0, 0.8], index=['a', 'b', 'c', 'd'])\n\n# Sorting the series\nresult = s.sort_values().sort_index()\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 173, "library_problem_id": 173, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 174, "code": ["<code>\nimport pandas as pd\n\n# Given series\ns = pd.Series({\n    \"146tf150p\": 1.000000,\n    \"havent\": 1.000000,\n    \"home\": 1.000000,\n    \"okie\": 1.000000,\n    \"thanx\": 1.000000,\n    \"er\": 1.000000,\n    \"anything\": 1.000000,\n    \"lei\": 1.000000,\n    \"nite\": 1.000000,\n    \"yup\": 1.000000,\n    \"thank\": 1.000000,\n    \"ok\": 1.000000,\n    \"where\": 1.000000,\n    \"beerage\": 1.000000,\n    \"anytime\": 1.000000,\n    \"too\": 1.000000\n})\n\n# Sort the series\ns = s.sort_values(ascending=(True, False))\n\n# Create a dataframe\ndf = pd.DataFrame({'index': s.index, '1': s.values})\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 174, "library_problem_id": 174, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 173}}
{"id": 175, "code": ["<code>\nimport pandas as pd\n\ndef filter_dataframe(df):\n    df['A'] = pd.to_numeric(df['A'], errors='coerce')\n    df = df.dropna(subset=['A'])\n    return df\n\n# Given DataFrame\ndata = {'A': [1, 2, 's', 3, 'b'], 'B': ['green', 'red', 'blue', 'yellow', 'black']}\ndf = pd.DataFrame(data)\n\nfiltered_df = filter_dataframe(df)\nprint(filtered_df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 175, "library_problem_id": 175, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 175}}
{"id": 176, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'A': [1, 2, 's', 3, 'b'],\n                   'B': ['green', 'red', 'blue', 'yellow', 'black']})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 176, "library_problem_id": 176, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 175}}
{"id": 177, "code": ["<code>\nimport pandas as pd\n\n# Assuming df is your DataFrame\ndf = pd.DataFrame({\n    'Sp': [0, 1, 2, 3, 4, 5, 6, 7, 8],\n    'Mt Value': ['MM1 S1 a', 'MM1 S1 n', 'MM1 S3 cb', 'MM2 S3 mk', 'MM2 S4 bg', 'MM2 S4 dgd', 'MM4 S2 rd', 'MM4 S2 cb', 'MM4 S2 uyi'],\n    'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]\n})\n\n# Split the 'Mt Value' column into 'Mt' and 'Value'\ndf[['Mt', 'Value']] = df['Mt Value'].str.split(' ', 1, expand=True)\n\n# Fill the 'Value' column with the count values\ndf['Value'] = df['count']\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 177, "library_problem_id": 177, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 177}}
{"id": 178, "code": ["<code>\nimport pandas as pd\n\n# Assuming df is your DataFrame\ndf = pd.DataFrame({\n    'Sp': [0, 1, 2, 3, 4, 5, 6, 7, 8],\n    'Mt': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n    'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n    'count': [2, 3, 5, 8, 5, 1, 2, 2, 7]\n})\n\n# Fill in the 'count' column based on the 'Value' column\ndf['count'] = df['Value'].apply(lambda x: len(x) if isinstance(x, str) else x)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 178, "library_problem_id": 178, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 177}}
{"id": 179, "code": ["<code>\nimport pandas as pd\n\n# Assuming df is your DataFrame\ndf = pd.DataFrame({\n    'Sp': [0, 1, 2, 3, 4, 5, 6, 7, 8],\n    'Mt': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n    'Value': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n    'count': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi']\n})\n\ndef process_data(df):\n    # Add your processing code here\n    pass\n\n# Call the function\nprocess_data(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 179, "library_problem_id": 179, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 180, "code": ["<code>\nimport pandas as pd\n\n# Given DataFrame\ndf = pd.DataFrame({\n    'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n    'Value': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n    'Mt': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n    'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]\n})\n\ndef max_count_rows(df):\n    # Group by 'Sp' and 'Value' and find the maximum 'count' value in each group\n    max_count_df = df.groupby(['Sp', 'Value'])['count'].max().reset_index()\n\n    # Merge the original DataFrame with the maximum 'count' DataFrame on 'Sp', 'Value' and 'count'\n    result_df = pd.merge(df, max_count_df, on=['Sp', 'Value', 'count'])\n\n    return result_df\n\nprint(max_count_rows(df))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 180, "library_problem_id": 180, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 181, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\n# Given dictionary\ndict = {'abc':'1/2/2003', 'def':'1/5/2017', 'ghi':'4/10/2013'}\n\n# Given DataFrame\ndf = pd.DataFrame({'Member': ['xyz', 'uvw', 'abc', 'def', 'ghi'],\n                   'Group': ['A', 'B', 'A', 'B', 'B'],\n                   'Date': [np.nan, np.nan, np.nan, np.nan, np.nan]})\n\n# Map values from the dictionary to the DataFrame column\ndf['Date'] = df['Member'].map(dict)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 181, "library_problem_id": 181, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 181}}
{"id": 182, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\n# Given dictionary\ndict = {'abc':'1/2/2003', 'def':'1/5/2017', 'ghi':'4/10/2013'}\n\n# Given DataFrame\ndf = pd.DataFrame({'Member': ['xyz', 'uvw', 'abc', 'def', 'ghi'],\n                   'Group': ['A', 'B', 'A', 'B', 'B'],\n                   'Date': [np.nan, np.nan, np.nan, np.nan, np.nan]})\n\n# Map values from the dictionary to the 'Date' column based on matching 'Member' values\ndf['Date'] = df['Member'].map(dict).fillna('17/8/1926')\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 182, "library_problem_id": 182, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 181}}
{"id": 183, "code": ["<code>\nimport pandas as pd\n\ndef f(d: dict, df: pd.DataFrame) -> pd.DataFrame:\n    df['Date'] = df['Member'].map(d)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 183, "library_problem_id": 183, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 181}}
{"id": 184, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef map_dict_to_df(df, dict, col_name, default_value='17/8/1926'):\n    # Map values from dictionary to specific column in DataFrame\n    df[col_name] = df['Member'].map(dict).fillna(default_value)\n\n    # Convert Date column to datetime format\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    # Reformat Date column to 'dd-MMM-yyyy' format\n    df['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 184, "library_problem_id": 184, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 181}}
{"id": 185, "code": ["<code>\nimport pandas as pd\n\nd = ({'Date': ['1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\ndf['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf.set_index('Date', inplace=True)\n\n# Add 'Count_d' column\ndf['Count_d'] = 1\n\n# Add 'Count_m' and 'Count_y' columns\ndf['Count_m'] = df.groupby(by=[df.index.month, df.index.year])['Count_d'].transform('count')\ndf['Count_y'] = df.groupby(by=df.index.year)['Count_d'].transform('count')\n\ndf.reset_index(inplace=True)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 185, "library_problem_id": 185, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 186, "code": ["<code>\nSure, here is the completed code:\n\n```python\nimport pandas as pd\n\nd = {'Date': ['1/1/18','1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n     'Val': ['A','B','A','B','A','B','A','B','A']}\ndf = pd.DataFrame(d)\n\n# Convert 'Date' to datetime format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Convert 'Val' to categorical\ndf['Val'] = df['Val'].astype('category')\n\n# Create new columns for year, month, and day\ndf['Year'] = df['Date'].dt.year\ndf['Month'] = df['Date'].dt.month\ndf['Day'] = df['Date'].dt.day\n\n# Count occurrences within each group\ndf_grouped = df.groupby(['Year', 'Month', 'Day', 'Val']).size().unstack(fill_value=0).reset_index()\n\n# Rename columns\ndf_grouped.columns.name = None\ndf_grouped.columns = ['Year', 'Month', 'Day', 'Val'] + [f'Count_Val_{i}' for i in df['Val'].cat.categories]\n\n# Count occurrences at the daily, monthly, and yearly levels\ndf_grouped['Count_d'] = df_grouped.groupby(['Year', 'Month'])['Day'].transform('count\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 186, "library_problem_id": 186, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 185}}
{"id": 187, "code": ["<code>\nimport pandas as pd\n\nd = {\n    'Date' : ['1/1/18','1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],                 \n    'Val' : ['A','A','B','C','D','A','B','C','D'],                                      \n}\ndf = pd.DataFrame(data = d)\ndf['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Count_d'] = df.Date.map(df.groupby('Date').size())\n\n# Extract year, month, and weekday from date\ndf['Year'] = df['Date'].dt.year\ndf['Month'] = df['Date'].dt.month\ndf['Weekday'] = df['Date'].dt.weekday\n\n# Count occurrences of each unique combination of year, month, weekday, and value\ndf['Count_y'] = df.groupby(['Year', 'Val']).size()\ndf['Count_m'] = df.groupby(['Month', 'Val']).size()\ndf['Count_w'] = df.groupby(['Weekday', 'Val']).size()\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 187, "library_problem_id": 187, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 185}}
{"id": 188, "code": ["<code>\nThe given code seems to be a part of a pandas DataFrame. However, it's not clear what the function is supposed to do. Could you please provide more details about what the function should do? For example, should it filter the data, sort it, calculate a new column, etc.?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 188, "library_problem_id": 188, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 189, "code": ["<code>\nThe given code seems to be a part of a pandas DataFrame. However, it's not clear what the function is supposed to do. Could you please provide more details about what the function should do? For example, should it filter the data, sort it, calculate a new column, etc.?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 189, "library_problem_id": 189, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 188}}
{"id": 190, "code": ["<code>\nnp.sum, np.mean])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 190, "library_problem_id": 190, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 190}}
{"id": 191, "code": ["<code>\nimport pandas as pd\n\n# Assuming df is your DataFrame\ndf = pd.DataFrame({\n   'A': ['foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'foo'],\n   'B': ['one', 'one', 'two', 'three', 'two', 'two', 'one', 'three'],\n   'C': ['small', 'large', 'large', 'small', 'small', 'large', 'small', 'large'],\n   'D': [1, 2, 2, 3, 3, 4, 5, 6],\n   'E': [2, 4, 5, 5, 6, 6, 8, 9]\n})\n\npivot_table = df.pivot_table(values=['D', 'E'], index='B', aggfunc={'D': 'sum', 'E': 'mean'})\n\nprint(pivot_table)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 191, "library_problem_id": 191, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 192, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef generate_pivot_table(df):\n    np.random.seed(0)\n    return df.pivot_table(index='B', values=['D', 'E'], aggfunc={'D': 'sum', 'E': 'mean'})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 192, "library_problem_id": 192, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 193, "code": ["<code>\n* 8,\n          'E' : np.random.rand\n          * 8\n})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 193, "library_problem_id": 193, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 190}}
{"id": 194, "code": ["<code>\nimport dask.dataframe as dd\n\ndef split_column(df, column):\n    \"\"\"\n    Splits a column containing comma-separated strings into multiple rows.\n    \n    Parameters:\n    df (dask.dataframe.DataFrame): Input DataFrame.\n    column (str): The column to be split.\n    \n    Returns:\n    dask.dataframe.DataFrame: DataFrame with split column.\n    \"\"\"\n    # Split the column by comma and explode the resulting list\n    df = df.assign(**{column: df[column].str.split(',')})\n    df = df.explode(column)\n    \n    # Remove leading/trailing whitespaces\n    df = df.assign(**{column: df[column].str.strip()})\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 194, "library_problem_id": 194, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 195, "code": ["<code>\nimport dask.dataframe as dd\n\ndef split_comma_separated_strings(df):\n    # Create a new DataFrame with 'var2' column exploded into multiple rows\n    new_df = dd.from_pandas(df, npartitions=1).assign(var2=lambda x: x['var2'].str.split(',')).explode('var2')\n    \n    # Return the new DataFrame\n    return new_df.compute()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 195, "library_problem_id": 195, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 194}}
{"id": 196, "code": ["<code>\nimport dask.dataframe as dd\n\ndef split_column_into_rows(df, column):\n    # Split the column into multiple rows\n    df = df.assign(**{column: df[column].str.split(',')})\n\n    # Explode the column into multiple rows\n    df = df.explode(column)\n\n    # Remove leading/trailing spaces\n    df = df.assign(**{column: df[column].str.strip()})\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 196, "library_problem_id": 196, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 194}}
{"id": 197, "code": ["<code>\nimport pandas as pd\n\ndef count_special_char(row):\n    string = row['str']\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\n\ndf = pd.read_csv('inn.txt', sep='\\t')\ndf[\"new\"] = df.apply(count_special_char, axis=1)\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 197, "library_problem_id": 197, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 197}}
{"id": 198, "code": ["<code>\nimport pandas as pd\nimport re\n\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\n\n# Function to remove special characters and convert to lowercase\ndef clean_string(s):\n    s = re.sub(r'\\W+', '', s)\n    return s.lower()\n\n# Apply the function to the 'str' column\ndf['str'] = df['str'].apply(clean_string)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 198, "library_problem_id": 198, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 197}}
{"id": 199, "code": ["<code>\nI'm sorry, but your question is not clear. It seems like you're asking for a continuation of a Python function, but you haven't provided the initial code. Could you please provide the initial code so I can assist you better?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 199, "library_problem_id": 199, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 199}}
{"id": 200, "code": ["<code>\nimport pandas as pd\n\ndef split_string(df, delimiter=' '):\n    df['fips'], df['row'] = df['row'].str.split(delimiter, 1).str\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 200, "library_problem_id": 200, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 199}}
{"id": 201, "code": ["<code>\nimport pandas as pd\n\n# Assuming your dataframe is named df\ndf = pd.DataFrame({'row': ['00000 UNITED STATES', '01000 ALAB AMA', '01001 Autauga County, AL', '01003 Baldwin County, AL', '01005 Barbour County, AL']})\n\n# Split the 'row' column into three parts\ndf['fips'] = df['row'].str[:5]\ndf['medi'] = df['row'].str.split().str[1]\ndf['row'] = df['row'].str.split().str[2:]\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 201, "library_problem_id": 201, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 199}}
{"id": 202, "code": ["<code>\nHere is the completed function:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndef cumulative_average(df):\n    # Initialize a new DataFrame with the same structure as the input DataFrame\n    result = pd.DataFrame(index=df.index, columns=df.columns)\n\n    # Iterate over each row in the input DataFrame\n    for i in df.index:\n        # Initialize a list to store the cumulative averages for each column\n        cum_avg = []\n        # Initialize a variable to store the sum of non-zero values encountered so far\n        sum_non_zero = 0\n        # Initialize a variable to store the count of non-zero values encountered so far\n        count_non_zero = 0\n\n        # Iterate over each column in the input DataFrame\n        for j in df.columns:\n            # If the value in the current cell is not zero\n            if df.loc[i, j] != 0:\n                # Increment the sum of non-zero values and the count of non-zero values\n                sum_non_zero += df.loc[i, j]\n                count_non_zero += 1\n\n            # Calculate the cumulative average for the current column and append it to the list\n            if count_non_zero != 0:\n                cum_avg.append(sum_non_zero / count_non_zero)\n            else:\n                cum_avg.append(np.nan)\n\n        # Assign the list of cumulative averages to the corresponding row in the result DataFrame\n        result.loc[i] = cum_avg\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 202, "library_problem_id": 202, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 203, "code": ["<code>\nimport pandas as pd\n\ndef calculate_cumulative_average(df):\n    # Reverse the DataFrame\n    df = df.iloc[::-1]\n\n    # Iterate over each row in the DataFrame\n    for i in range(len(df)):\n        # Ignore zero values\n        row = df.iloc[i][df.iloc[i] != 0]\n\n        # Calculate the cumulative average\n        df.iloc[i] = row.cumsum() / (row.index.values + 1)\n\n    # Reverse the DataFrame back to its original order\n    df = df.iloc[::-1]\n\n    # Fill NaN values with 0\n    df.fillna(0, inplace=True)\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 203, "library_problem_id": 203, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 202}}
{"id": 204, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef cumulative_average(df):\n    for i in range(len(df)):\n        row = df.iloc[i]\n        non_zero_values = row[row != 0]\n        df.iloc[i] = non_zero_values.cumsum() / (non_zero_values.count())\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 204, "library_problem_id": 204, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 202}}
{"id": 205, "code": ["<code>\nimport pandas as pd\n\ndef cumulative_average(df):\n    # Reverse the DataFrame\n    df = df.iloc[::-1]\n\n    # Initialize a new DataFrame to store the cumulative averages\n    cum_avg_df = pd.DataFrame(index=df.index, columns=df.columns)\n\n    # Iterate over each row in the reversed DataFrame\n    for i in range(len(df)):\n        # Calculate the cumulative average for each row\n        cum_avg_df.iloc[i] = df.iloc[i:].replace(0, pd.np.nan).mean(axis=0).fillna(0)\n\n    # Reverse the cumulative averages DataFrame back to its original order\n    cum_avg_df = cum_avg_df.iloc[::-1]\n\n    return cum_avg_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 205, "library_problem_id": 205, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 206, "code": ["<code>\nimport pandas as pd\n\n# Create a sample DataFrame\ndf = pd.DataFrame({\n    'DateTime': pd.date_range(start='01-01-2000', periods=5),\n    'Close': [1460, 1470, 1450, 1465, 1475]\n})\n\n# Add a 'label' column with 0s initially\ndf['label'] = 0\n\n# Fill the 'label' column with 1s or 0s based on the difference between consecutive 'Close' values\ndf.loc[1:, 'label'] = (df.loc[1:, 'Close'] - df.loc[:len(df)-2, 'Close']).apply(lambda x: 1 if x > 0 else 0)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 206, "library_problem_id": 206, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 206}}
{"id": 207, "code": ["<code>\nimport pandas as pd\n\ndef calculate_label(df):\n    df['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else -1 if x < 0 else 0)\n    df.loc[0, 'label'] = 1  # Set the label of the first row to 1\n    return df\n\n# Test the function\ndf = pd.DataFrame({\n    'DateTime': ['2000-01-04', '2000-01-05', '2000-01-06', '2000-01-07', '2000-01-08'],\n    'Close': [1460, 1470, 1480, 1480, 1450]\n})\n\ndf = calculate_label(df)\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 207, "library_problem_id": 207, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 206}}
{"id": 208, "code": ["<code>\nimport pandas as pd\n\ndef calculate_difference(df):\n    df['Close_diff'] = df['Close'].diff()\n    df['label'] = df['Close_diff'].apply(lambda x: 1 if x > 0 else -1 if x < 0 else 0)\n    df.loc[0, 'label'] = 1\n    df['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 208, "library_problem_id": 208, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 206}}
{"id": 209, "code": ["<code>\nimport pandas as pd\n\ndef calculate_duration(df):\n    df = df.copy()  # To avoid modifying the original DataFrame\n    df['arrival_time'] = pd.to_datetime(df['arrival_time'])\n    df['departure_time'] = pd.to_datetime(df['departure_time'])\n\n    df.sort_values(['id', 'arrival_time'], inplace=True)  # Sort by 'id' and 'arrival_time'\n\n    df['Duration'] = df.groupby('id')['arrival_time'].apply(lambda x: (x - x.shift())).dt.total_seconds()\n    df['Duration'].fillna(df.groupby('id')['arrival_time'].apply(lambda x: (x.max() - x.min()).total_seconds()), inplace=True)  # Handle the first arrival time\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 209, "library_problem_id": 209, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 210, "code": ["<code>\nimport pandas as pd\n\ndef calculate_duration(df):\n    df['arrival_time'] = pd.to_datetime(df['arrival_time'])\n    df['departure_time'] = pd.to_datetime(df['departure_time'])\n    df.sort_values(['id', 'arrival_time'], inplace=True)\n    df['Duration'] = df.groupby('id')['arrival_time'].apply(lambda x: (x - x.shift()) / pd.Timedelta(seconds=1))\n    df['Duration'].iloc[0] = pd.np.nan\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 210, "library_problem_id": 210, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 209}}
{"id": 211, "code": ["<code>\nimport pandas as pd\n\ndef calculate_duration(df):\n    df['arrival_time'] = pd.to_datetime(df['arrival_time'], format='%d-%b-%Y %H:%M:%S')\n    df['departure_time'] = pd.to_datetime(df['departure_time'], format='%d-%b-%Y %H:%M:%S')\n    df.sort_values(['id', 'arrival_time'], inplace=True)\n    df['Duration'] = df.groupby('id')['arrival_time'].apply(lambda x: (x - x.shift())/pd.Timedelta(seconds=1))\n    df.loc[df['id'] != df['id'].shift(), 'Duration'] = None\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 211, "library_problem_id": 211, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 209}}
{"id": 212, "code": ["<code>\nSure, I can help you with that. However, you didn't specify what you want to do with the given DataFrame. Could you please provide more details about what you want to achieve?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 212, "library_problem_id": 212, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 212}}
{"id": 213, "code": ["<code>\nSure, I can help you with that. However, you didn't specify what you want to do with the given DataFrame. Could you please provide more details about what you want to achieve?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 213, "library_problem_id": 213, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 212}}
{"id": 214, "code": ["<code>\nresult = df[df['key2'].str.endswith('e')].groupby('key1').size().reset_index(name='count')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 214, "library_problem_id": 214, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 212}}
{"id": 215, "code": ["<code>\nimport pandas as pd\n\ndef get_min_max_dates(df):\n    max_result = df.index.max().strftime('%Y-%m-%d')\n    min_result = df.index.min().strftime('%Y-%m-%d')\n    return max_result, min_result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 215, "library_problem_id": 215, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 216, "code": ["<code>\nimport pandas as pd\n\ndef mode_median_dates(df):\n    mode_date = df['value'].idxmax().strftime('%Y-%m-%d')\n    median_date = df['value'].idxmin().strftime('%Y-%m-%d')\n    return mode_date, median_date\n\ndf = pd.DataFrame({'value':[10000,2000,2000,200,5,70,200,5,25,0.02,12,11,0.022]},\n                  index=['2014-03-13','2014-03-21','2014-03-27','2014-03-17','2014-03-17','2014-03-17','2014-03-21','2014-03-27','2014-03-27','2014-03-27'])\n\nmode_date, median_date = mode_median_dates(df)\nprint(f\"Mode date: {mode_date}, Median date: {median_date}\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 216, "library_problem_id": 216, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 215}}
{"id": 217, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\n# Set seed for reproducibility\nnp.random.seed(2)\n\n# Create a DataFrame with random integers between 95 and 105 for the 'closing_price' column\ndf = pd.DataFrame(np.random.randint(low=95, high=105, size=(10, 1)), columns=['closing_price'])\n\ndef filter_df(df):\n    # Filter the DataFrame based on the 'closing_price' column's values falling within the range of 99 to 101 (inclusive)\n    filtered_df = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]\n    return filtered_df\n\n# Call the function\nfiltered_df = filter_df(df)\nprint(filtered_df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 217, "library_problem_id": 217, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 217}}
{"id": 218, "code": ["<code>\nimport pandas as pd\n\ndef filter_df(df):\n    return df[~((df['closing_price'] >= 99) & (df['closing_price'] <= 101))]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 218, "library_problem_id": 218, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 217}}
{"id": 219, "code": ["<code>\nimport pandas as pd\n\ndef min_diff_per_item(df):\n    df_min = df.groupby('item')['diff'].min().reset_index()\n    df_min = df_min.merge(df, on=['item', 'diff'])\n    return df_min\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 219, "library_problem_id": 219, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 219}}
{"id": 220, "code": ["<code>\nimport pandas as pd\n\ndef extract_substring(df, column):\n    \"\"\"\n    This function extracts the substring after the last underscore ('_') from a pandas DataFrame column named 'SOURCE_NAME'.\n    If no underscore exists, the function returns the original string.\n    \"\"\"\n    return df[column].str.split('_').str.get(-1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 220, "library_problem_id": 220, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 220}}
{"id": 221, "code": ["<code>\nimport pandas as pd\n\ndef extract_substring(df):\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].apply(lambda x: x.rsplit('_', 1)[0] if '_' in x else x)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 221, "library_problem_id": 221, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 220}}
{"id": 222, "code": ["<code>\nimport pandas as pd\n\ndef extract_substring(df, column_name):\n    df[column_name] = df[column_name].apply(lambda x: x.split('_')[-1] if '_' in x else x)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 222, "library_problem_id": 222, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 220}}
{"id": 223, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\n# Assuming df is your DataFrame and 'Column_x' is your column\n\n# Calculate the number of NaN values\nnan_count = df['Column_x'].isnull().sum()\n\n# Calculate the half of the NaN values\nhalf_count = int(np.floor(nan_count / 2))\n\n# Fill the first half of NaN values with 0\ndf.loc[df['Column_x'].isnull(), 'Column_x'] = 0\n\n# Fill the second half of NaN values with 1\ndf.loc[df['Column_x'].isnull(), 'Column_x'] = 1\n\n# Print the DataFrame to verify the changes\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 223, "library_problem_id": 223, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 223}}
{"id": 224, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef fill_nan_stratified(df, column):\n    # Calculate the number of NaNs in the column\n    num_nans = df[column].isnull().sum()\n\n    # Calculate the number of NaNs to fill with each value\n    num_zeros = int(np.floor(num_nans * 0.3))\n    num_halfs = int(np.floor(num_nans * 0.3))\n    num_ones = num_nans - num_zeros - num_halfs\n\n    # Generate the replacement values\n    replacements = [0]*num_zeros + [0.5]*num_halfs + [1]*num_ones\n\n    # Shuffle the replacements to ensure randomness\n    np.random.shuffle(replacements)\n\n    # Replace the NaNs\n    df.loc[df[column].isnull(), column] = replacements\n\n    return df\n\ndf = pd.DataFrame({'Column_x': [0,0,0,0,0,0,1,1,1,1,1,1, np.nan, np.nan, np.nan]})\n\ndf = fill_nan_stratified(df, 'Column_x')\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 224, "library_problem_id": 224, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 223}}
{"id": 225, "code": ["<code>\nimport numpy as np\nimport math\n\ndef fill_binary_column(df, column):\n    # Count the number of NaN values in the column\n    num_nan = df[column].isnull().sum()\n    \n    # If the number of NaN values is odd, round up the number of 0s to be filled\n    num_zeros = math.ceil(num_nan / 2) if num_nan % 2 != 0 else num_nan / 2\n    num_ones = num_nan / 2\n    \n    # Fill the NaN values with 0s and 1s\n    df.loc[df[column].isnull(), column] = np.random.choice([0, 1], size=int(num_zeros), replace=True)\n    df.loc[df[column].isnull(), column] = np.random.choice([0, 1], size=int(num_ones), replace=True)\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 225, "library_problem_id": 225, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 223}}
{"id": 226, "code": ["<code>\nimport pandas as pd\n\ndef combine_dataframes(dfs):\n    # Check if all dataframes have the same columns\n    if not all(dfs[0].columns == df.columns for df in dfs[1:]):\n        raise ValueError(\"All dataframes must have the same columns\")\n\n    # Use pandas.DataFrame.apply to apply a function along an axis of the DataFrame.\n    # In this case, we're applying a lambda function that returns a tuple of values\n    # from each dataframe at the same index.\n    combined = dfs[0].apply(lambda x: tuple(df.iloc[x.name] for df in dfs), axis=1)\n\n    return combined\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 226, "library_problem_id": 226, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 226}}
{"id": 227, "code": ["<code>\nimport pandas as pd\n\ndef combine_dataframes(dfs):\n    # Concatenate all dataframes into one\n    combined = pd.concat(dfs, axis=1)\n    \n    # Create a new dataframe where each element is a tuple of corresponding elements in the input dataframes\n    new_df = combined.apply(tuple, axis=1)\n    \n    return new_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 227, "library_problem_id": 227, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 226}}
{"id": 228, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef combine_dataframes(dfs):\n    max_len = max([len(df) for df in dfs])\n    combined = []\n    for i in range(max_len):\n        row = []\n        for df in dfs:\n            if i < len(df):\n                row.append(tuple(df.iloc[i]))\n            else:\n                row.append(np.nan)\n        combined.append(tuple(row))\n    return pd.DataFrame(combined, columns=[f'col_{i}' for i in range(len(dfs))])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 228, "library_problem_id": 228, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 226}}
{"id": 229, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef create_view_bins(df):\n    bins = [1, 10, 25, 50, 100]\n    labels = ['1-10', '11-25', '26-50', '51-100']\n\n    df['view_bin'] = pd.cut(df['views'], bins=bins, labels=labels, right=False)\n\n    pivot_table = pd.pivot_table(df, index='username', columns='view_bin', aggfunc='size', fill_value=0)\n\n    return pivot_table\n\n# Test the function\ndf = pd.DataFrame({\n    'username': ['john', 'john', 'john', 'john', 'jane', 'jane', 'jane', 'jane'],\n    'post_id': [1, 2, 3, 4, 7, 8, 9, 10],\n    'views': [3, 23, 44, 82, 5, 25, 46, 56]\n})\n\nprint(create_view_bins(df))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 229, "library_problem_id": 229, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 229}}
{"id": 230, "code": ["<code>\nimport pandas as pd\n\ndef create_view_bins(df):\n    bins = [1, 10, 25, 50, 100]\n    labels = ['1-10', '11-25', '26-50', '51-100']\n\n    df['view_bin'] = pd.cut(df['views'], bins=bins, labels=labels, right=False)\n    df = df.groupby(['username', 'view_bin']).size().unstack(fill_value=0)\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 230, "library_problem_id": 230, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 231, "code": ["<code>\nimport pandas as pd\n\ndef transform_data(df):\n    bins = [1, 10, 25, 50, 100]\n    labels = ['1-10', '11-25', '26-50', '51-100']\n\n    df['binned_views'] = pd.cut(df['views'], bins=bins, labels=labels, right=False)\n\n    pivot_table = df.pivot_table(index='username', columns='binned_views', aggfunc='size', fill_value=0)\n\n    return pivot_table\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 231, "library_problem_id": 231, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 232, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 232, "library_problem_id": 232, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 232}}
{"id": 233, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 233, "library_problem_id": 233, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 234, "code": ["<code>\nimport pandas as pd\n\ndef concatenate_rows(df):\n    return pd.DataFrame({'text': [', '.join(df['text'])]})\n\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = concatenate_rows(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 234, "library_problem_id": 234, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 235, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\n\ndef concat_text(df):\n    return pd.Series([', '.join(df['text'])], index=['text'])\n\nresult = concat_text(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 235, "library_problem_id": 235, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 236, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\n\nresult = pd.DataFrame({'text': [\"-\".join(df['text'][::-1])]}, index=[0])\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 236, "library_problem_id": 236, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 232}}
{"id": 237, "code": ["<code>\nimport pandas as pd\n\ndef complete_dataframes(df1, df2):\n    # Concatenate the dataframes based on the 'id' column\n    result = pd.concat([df1, df2], ignore_index=True)\n\n    # Merge df1 and df2 on 'id' column\n    result = result.merge(df1[['id', 'city', 'district']], on='id', how='left')\n\n    # Fill missing 'city' and 'district' values in df2 using corresponding values from df1\n    result['city'] = result['city_x'].combine_first(result['city_y'])\n    result['district'] = result['district_x'].combine_first(result['district_y'])\n\n    # Drop the temporary columns\n    result.drop(columns=['city_x', 'city_y', 'district_x', 'district_y'], inplace=True)\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 237, "library_problem_id": 237, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 237}}
{"id": 238, "code": ["<code>\nimport pandas as pd\n\ndef complete_dataframes(df1, df2):\n    # Concatenate df1 and df2 based on 'id' column\n    df = pd.concat([df1, df2], ignore_index=True)\n\n    # Fill missing 'city' and 'district' values from df1 into df2\n    df = df.fillna(df1)\n\n    # Sort the resulting dataframe by 'id' and 'date'\n    df = df.sort_values(by=['id', 'date'])\n\n    # Reformat the 'date' column to 'DD-Mon-YYYY' format\n    df['date'] = df['date'].dt.strftime('%d-%b-%Y')\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 238, "library_problem_id": 238, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 239, "code": ["<code>\nimport pandas as pd\n\ndef complete_function(df1, df2):\n    # Concatenate two dataframes based on the 'id' column\n    df = pd.concat([df1, df2], ignore_index=True)\n\n    # Fill missing 'city' and 'district' values from df1\n    df['city'] = df['city'].fillna(df1['city'])\n    df['district'] = df['district'].fillna(df1['district'])\n\n    # Sort the resulting dataframe to group rows with the same 'id' together, ordering by date in ascending order\n    df = df.sort_values(by=['id', 'date'])\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 239, "library_problem_id": 239, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 240, "code": ["<code>\nimport pandas as pd\n\n# Assuming C and D are your DataFrames\nresult = pd.merge(C, D, how='left', on='A')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 240, "library_problem_id": 240, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 240}}
{"id": 241, "code": ["<code>\nimport pandas as pd\n\ndef merge_dataframes(C, D):\n    merged_df = pd.merge(C, D, on='A', how='outer', suffixes=('_C', '_D'))\n    merged_df['B'] = merged_df['B_C'].combine_first(merged_df['B_D'])\n    merged_df.drop(columns=['B_C', 'B_D'], inplace=True)\n    return merged_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 241, "library_problem_id": 241, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 240}}
{"id": 242, "code": ["<code>\nimport pandas as pd\n\ndef merge_dataframes(C, D):\n    # Merge DataFrames C and D on column 'A' with an outer join\n    result = pd.merge(C, D[['A', 'B']], on='A', how='outer', suffixes=('', '_D'))\n\n    # Create a new column 'duplicated' that indicates whether a row in C has a matching 'A' value in D\n    result['duplicated'] = result['A'].apply(lambda x: x if x in D['A'].values else None)\n    result['duplicated'] = result['duplicated'].notna()\n\n    # Drop the duplicate column\n    result.drop(columns=['A_D'], inplace=True)\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 242, "library_problem_id": 242, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 240}}
{"id": 243, "code": ["<code>\nimport pandas as pd\n\ndef aggregate_transactions(df):\n    df = df.sort_values(by=['time'])\n    return df.groupby('user')[['time', 'amount']].apply(lambda x: x.values.tolist())\n\ndf = pd.DataFrame({'user':[1,1,2,2,3], \n                   'time':[20,10,11,18, 15], \n                   'amount':[10.99, 4.99, 2.99, 1.99, 10.99]})\n\nprint(aggregate_transactions(df))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 243, "library_problem_id": 243, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 243}}
{"id": 244, "code": ["<code>\nimport pandas as pd\n\ndef process_transactions(df):\n    # Group by 'user' and create a list of tuples for each user\n    df_grouped = df.groupby('user').apply(lambda x: list(zip(x['time'], x['amount'])))\n    \n    # Reset the index to make 'user' a column\n    df_grouped = df_grouped.reset_index()\n    \n    # Rename the grouped column to 'amount-time'\n    df_grouped = df_grouped.rename(columns={0: 'amount-time'})\n    \n    # Set 'user' as the index\n    df_grouped.set_index('user', inplace=True)\n    \n    # Create a new column 'amount-time-tuple' by applying a lambda function to 'amount-time'\n    df_grouped['amount-time-tuple'] = df_grouped['amount-time'].apply(lambda x: tuple(x))\n    \n    return df_grouped\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 244, "library_problem_id": 244, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 243}}
{"id": 245, "code": ["<code>\nThe given code is creating a pandas DataFrame with three columns: 'user', 'time', and 'amount'. However, it doesn't specify what the function should do with this DataFrame. \n\nPlease provide more information about the task you want to accomplish with this DataFrame.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 245, "library_problem_id": 245, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 243}}
{"id": 246, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\nseries = pd.Series([np.array([1,2,3,4]), np.array([5,6,7,8]), np.array([9,10,11,12])], index=['file1', 'file2', 'file3'])\n\n# Calculate the sum of all arrays in the series\nseries_sum = series.apply(np.sum)\n\nprint(series_sum)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 246, "library_problem_id": 246, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 246}}
{"id": 247, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\n# Given series\ns = pd.Series({\n    'file1': np.array([1, 2, 3, 4]),\n    'file2': np.array([5, 6, 7, 8]),\n    'file3': np.array([9, 10, 11, 12])\n})\n\n# Convert series to dataframe\ndf = pd.DataFrame(s.tolist(), index=s.index).reset_index().rename(columns={'index': 'name'})\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 247, "library_problem_id": 247, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 246}}
{"id": 248, "code": ["<code>\nimport pandas as pd\n\ndef find_substring_columns(df, substring):\n    return [col for col in df.columns if substring in col]\n\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9]}\ndf = pd.DataFrame(data)\n\nprint(find_substring_columns(df, 'spike'))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 248, "library_problem_id": 248, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 248}}
{"id": 249, "code": ["<code>\nimport pandas as pd\n\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n\ndef find_columns_with_string(df, s):\n    return [col for col in df.columns if s in col]\n\nprint(find_columns_with_string(df, s))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 249, "library_problem_id": 249, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 248}}
{"id": 250, "code": ["<code>\nimport pandas as pd\n\ndef rename_columns(df, substring):\n    matching_columns = [col for col in df.columns if substring in col]\n    for i, col in enumerate(matching_columns, 1):\n        df.rename(columns={col: f'{substring}{i}'}, inplace=True)\n    return df\n\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\n\ndf = rename_columns(df, 'spike')\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 250, "library_problem_id": 250, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 248}}
{"id": 251, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef transform_df(df):\n    # Get the maximum length of the lists in the 'codes' column\n    max_len = max(df['codes'].apply(len))\n\n    # Create new columns for each element in the lists\n    for i in range(max_len):\n        df[f'code_{i}'] = df['codes'].apply(lambda x: x[i] if len(x) > i else np.nan)\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 251, "library_problem_id": 251, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 251}}
{"id": 252, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\n# Assuming df is your DataFrame\ndf = pd.DataFrame({'codes': [[1, 2, 3], [4, 5], [6, 7, 8, 9], [10]]})\n\n# Get the maximum length of the lists\nmax_len = max(df['codes'].apply(len))\n\n# Create new columns for each element in the lists\nfor i in range(1, max_len+1):\n    df[f'code_{i}'] = df['codes'].apply(lambda x: x[i-1] if i <= len(x) else np.nan)\n\n# Store the result in a variable named 'result'\nresult = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 252, "library_problem_id": 252, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 251}}
{"id": 253, "code": ["<code>\nIt seems like you're trying to complete a Python function, but you didn't provide the context or the incomplete function. Could you please provide the context or the incomplete function so I can assist you better?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 253, "library_problem_id": 253, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 251}}
{"id": 254, "code": ["<code>\nimport pandas as pd\nimport ast\n\ndef convert_and_concatenate(df, column):\n    if not isinstance(df, pd.DataFrame):\n        return \"Error: The input is not a valid Pandas DataFrame.\"\n    if column not in df.columns:\n        return \"Error: The input column is not a valid column in the DataFrame.\"\n    if df[column].isnull().all():\n        return \"Error: The DataFrame column contains only NaN values.\"\n\n    result = []\n    for item in df[column]:\n        try:\n            if isinstance(item, str):\n                result.extend(ast.literal_eval(item))\n        except:\n            continue\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 254, "library_problem_id": 254, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 254}}
{"id": 255, "code": ["<code>\ndf = pd.DataFrame(dict(col1=[[\"1,2,3\", \"4,5,6\"],[\"7,8,9,10\"]]))\n\ndef reverse_and_concatenate(df, col):\n    df[col] = df[col].apply(lambda x: [','.join(i.split(',')[::-1]) for i in x])\n    df[col] = df[col].apply(lambda x: ','.join(x))\n    df[col] = df[col].apply(lambda x: [int(i) for i in x.split(',')])\n    return df\n\nresult = reverse_and_concatenate(df, 'col1')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 255, "library_problem_id": 255, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 256, "code": ["<code>\nimport pandas as pd\n\ndef convert_list_to_string(df, column):\n    df[column] = df[column].apply(lambda x: ','.join(map(str, x)))\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 256, "library_problem_id": 256, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 257, "code": ["<code>\nimport pandas as pd\n\ndef bin_and_average(df, freq='2min', method='time'):\n    \"\"\"\n    Bin the data into 2-minute intervals, calculate the average 'Value' within each bin, and optionally interpolate missing values.\n\n    Parameters:\n    df (pandas.DataFrame): The input DataFrame with 'Time' (datetime) and 'Value' columns.\n    freq (str): The frequency string or DateOffset representing the bins. Default is '2min'.\n    method (str): The method to use for interpolation. Default is 'time'.\n\n    Returns:\n    pandas.DataFrame: The binned and averaged DataFrame.\n    \"\"\"\n    # Convert 'Time' column to datetime\n    df['Time'] = pd.to_datetime(df['Time'])\n\n    # Set 'Time' as index\n    df.set_index('Time', inplace=True)\n\n    # Resample the data to the specified frequency and calculate the average 'Value'\n    df = df.resample(freq).mean()\n\n    # Interpolate missing values\n    df['Value'] = df['Value'].interpolate(method=method)\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 257, "library_problem_id": 257, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 257}}
{"id": 258, "code": ["<code>\nimport pandas as pd\n\n# Given DataFrame\ndf = pd.DataFrame({\n    'Time': [\n        '2015-04-24 06:38:49',\n        '2015-04-24 06:39:19',\n        '2015-04-24 06:43:49',\n        '2015-04-24 06:44:18',\n        '2015-04-24 06:44:49'\n    ],\n    'Value': [\n        0.023844,\n        0.019075,\n        0.023844,\n        0.019075,\n        0.019075\n    ]\n})\n\n# Convert 'Time' to datetime\ndf['Time'] = pd.to_datetime(df['Time'])\n\n# Set 'Time' as index\ndf.set_index('Time', inplace=True)\n\n# Resample to 3-minute bins and sum 'Value'\ndf = df.resample('3T').sum()\n\n# Interpolate missing bins with zero\ndf.fillna(0, inplace=True)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 258, "library_problem_id": 258, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 257}}
{"id": 259, "code": ["<code>\nimport pandas as pd\n\n# Assuming df is your DataFrame\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method='min', ascending=True)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 259, "library_problem_id": 259, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 259}}
{"id": 260, "code": ["<code>\nimport pandas as pd\n\ndef rank_time(df):\n    df['RANK'] = df.groupby('ID')['TIME'].rank(method='min', ascending=False)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 260, "library_problem_id": 260, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 259}}
{"id": 261, "code": ["<code>\nimport pandas as pd\n\ndef rank_time(df):\n    df['TIME'] = pd.to_datetime(df['TIME'])\n    df['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %a %H:%M:%S')\n    df = df.sort_values(by=['ID', 'TIME'], ascending=[True, False])\n    df['RANK'] = df.groupby('ID').cumcount() + 1\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 261, "library_problem_id": 261, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 259}}
{"id": 262, "code": ["<code>\nimport pandas as pd\n\ndef filter_df(df: pd.DataFrame, bool_series: pd.Series) -> pd.DataFrame:\n    return df[bool_series]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 262, "library_problem_id": 262, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 262}}
{"id": 263, "code": ["<code>\nimport pandas as pd\n\ndef filter_df(df, s):\n    return df.loc[s.index]\n\ndf = pd.DataFrame({'a':[1,1,1,2,2,2,3,3,3], \n                   'b':[1,2,3,1,2,3,1,2,3], \n                   'c':range(9)})\n\ns = pd.Series([True, False, True, False, True], index=[1, 2, 3, 4, 5])\n\nprint(filter_df(df, s))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 263, "library_problem_id": 263, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 262}}
{"id": 264, "code": ["<code>\nHere is the Python function that compares two rows of a DataFrame, considering NaN values in the same position as equal. It also handles potential non-numeric data gracefully.\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndef compare_rows(df, row1, row2):\n    if row1 not in df.index or row2 not in df.index:\n        raise ValueError(\"One or both of the rows are not in the DataFrame\")\n    \n    diff_columns = []\n    for col in df.columns:\n        if pd.isna(df.at[row1, col]) and pd.isna(df.at[row2, col]):\n            continue\n        if np.isnan(df.at[row1, col]) and np.isnan(df.at[row2, col]):\n            continue\n        if df.at[row1, col] != df.at[row2, col]:\n            diff_columns.append(col)\n    return diff_columns\n\n# Test the function\ndf = pd.DataFrame({\n    'c0': [np.nan, np.nan, np.nan, 3.0],\n    'c1': [6.0, 6.0, 17.0, np.nan],\n    'c2': [14.0, 5.0, np.nan, np.nan],\n    'c3': [np.nan, 17.0, 8.0, 15.0],\n    'c4': [5.0, np.nan, 6.0, np.nan],\n    '\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 264, "library_problem_id": 264, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 264}}
{"id": 265, "code": ["<code>\nHere is the completed function:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndef compare_rows(df, row1, row2):\n    \"\"\"\n    This function compares two rows of a DataFrame, considering NaN values in the same position as equal.\n    It returns an index of columns where the values in the specified rows are identical, including cases where both rows have NaN in the same column.\n    \"\"\"\n    # Get the columns where the values are equal, including cases where both rows have NaN in the same column\n    equal_columns = df.columns[(df.iloc[row1] == df.iloc[row2]) | (df.iloc[row1].isna() & df.iloc[row2].isna())]\n    \n    return equal_columns\n\n# Test the function\ndf = pd.DataFrame({\n    'c0': [np.nan, np.nan, np.nan],\n    'c1': [6.0, 6.0, 17.0],\n    'c2': [14.0, 5.0, np.nan],\n    'c3': [np.nan, 17.0, 8.0],\n    'c4': [5.0, np.nan, 6.0],\n    'c5': [np.nan, np.nan, np.nan],\n    'c6': [2.0, 13.0, np.nan],\n    'c7': [12.0, np.nan, np.nan],\n    'c8': [3.0, np.nan, np.nan],\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 265, "library_problem_id": 265, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 266, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef compare_rows(df, row1, row2):\n    diff_columns = []\n    for col in df.columns:\n        if pd.isna(df.at[row1, col]) and pd.isna(df.at[row2, col]):\n            continue\n        try:\n            if df.at[row1, col] != df.at[row2, col]:\n                diff_columns.append(col)\n        except TypeError:\n            diff_columns.append(col)\n    return diff_columns\n\n# Create a DataFrame with random integers and NaN values\nnp.random.seed(0)\ndf = pd.DataFrame(np.random.randint(0, 20, size=(10, 5)), columns=list('ABCDE'))\nfor col in df.columns:\n    df.loc[np.random.choice(df.index, np.random.randint(1, 4), replace=False), col] = np.nan\n\n# Compare rows 0 and 1\nprint(compare_rows(df, 0, 1))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 266, "library_problem_id": 266, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 267, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef compare_rows(row1, row2):\n    diffs = []\n    for i in range(len(row1)):\n        if not np.isnan(row1[i]) and not np.isnan(row2[i]):\n            if row1[i] != row2[i]:\n                diffs.append((row1[i], row2[i]))\n        elif np.isnan(row1[i]) and np.isnan(row2[i]):\n            continue\n        else:\n            diffs.append((row1[i], row2[i]))\n    return diffs\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 267, "library_problem_id": 267, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 264}}
{"id": 268, "code": ["<code>\nimport pandas as pd\n\n# Create a simple DataFrame\ndf = pd.DataFrame({\n    'Date': ['2022-01-01', '2022-01-02', '2022-01-03'],\n    'Value': [1, 2, 3]\n})\n\n# Convert 'Date' column to datetime format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Convert DataFrame to Series\ns = pd.Series(df['Value'].values, index=df['Date'])\n\nprint(s)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 268, "library_problem_id": 268, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 268}}
{"id": 269, "code": ["<code>\nimport pandas as pd\n\ndef transform_df(df):\n    df = df.T.reset_index().T\n    df.columns = [f'{col}_{i+1}' if i > 0 else f'{col}_1' for i, col in enumerate(df.columns)]\n    df = df.iloc[1:].reset_index(drop=True)\n    return df\n\n# Test the function\ndf = pd.DataFrame({\n    'A': [1, 6, 11],\n    'B': [2, 7, 12],\n    'C': [3, 8, 13],\n    'D': [4, 9, 14],\n    'E': [5, 10, 5]\n})\n\nprint(transform_df(df))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 269, "library_problem_id": 269, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 269}}
{"id": 270, "code": ["<code>\nimport pandas as pd\n\ndef transform_df(df):\n    df = df.T.reset_index().T\n    df.columns = [f'{col}_{i}' if i > 0 else col for i, col in enumerate(df.columns.tolist())]\n    df = df.iloc[1:].reset_index(drop=True)\n    return df\n\n# Test the function\ndf = pd.DataFrame({'A': [1, 6, 11], 'B': [2, 7, 12], 'C': [3, 8, 13], 'D': [4, 9, 14], 'E': [5, 10, 5]})\nprint(transform_df(df))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 270, "library_problem_id": 270, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 269}}
{"id": 271, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame([(.21, .3212), (.01, .61237), (.66123, .03), (.21, .18),(pd.NA, .18)],\n                  columns=['dogs', 'cats'])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 271, "library_problem_id": 271, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 271}}
{"id": 272, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame([(.21, .3212), (.01, .61237), (.66123, pd.NA), (.21, .18),(pd.NA, .188)],\n                  columns=['dogs', 'cats'])\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 272, "library_problem_id": 272, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 271}}
{"id": 273, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(10)\ndata = {}\nfor i in [chr(x) for x in range(65,91)]:\n    data['Col '+i] = np.random.randint(1,100,10)\ndf = pd.DataFrame(data)\nlist_of_my_columns = ['Col A', 'Col E', 'Col Z']\n\n# Select only the columns in list_of_my_columns\ndf = df[list_of_my_columns]\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 273, "library_problem_id": 273, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 273}}
{"id": 274, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef calculate_row_wise_average(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n    \"\"\"\n    This function calculates the row-wise average of a Pandas DataFrame's selected columns.\n\n    Parameters:\n    df (pd.DataFrame): The DataFrame to calculate the row-wise average.\n    columns (list): The list of columns to calculate the row-wise average.\n\n    Returns:\n    pd.DataFrame: A new DataFrame with the row-wise average of the selected columns.\n\n    Raises:\n    ValueError: If the list of columns is empty.\n    KeyError: If any of the columns in the list do not exist in the DataFrame.\n    \"\"\"\n    if not columns:\n        raise ValueError(\"The list of columns cannot be empty.\")\n    if not all(col in df.columns for col in columns):\n        raise KeyError(\"One or more columns in the list do not exist in the DataFrame.\")\n    \n    df['row_wise_average'] = df[columns].mean(axis=1)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 274, "library_problem_id": 274, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 273}}
{"id": 275, "code": ["<code>\nimport pandas as pd\n\ndef calculate_stats(df, column_names):\n    df['Avg'] = df[column_names].mean(axis=1)\n    df['Min'] = df[column_names].min(axis=1)\n    df['Max'] = df[column_names].max(axis=1)\n    df['Median'] = df[column_names].median(axis=1)\n    return df\n\n# Example usage:\n# df = pd.DataFrame({'Col A': [1, 2, 3], 'Col E': [4, 5, 6], 'Col Z': [7, 8, 9]})\n# list_of_my_columns = ['Col A', 'Col E', 'Col Z']\n# df = calculate_stats(df, list_of_my_columns)\n# print(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 275, "library_problem_id": 275, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 273}}
{"id": 276, "code": ["<code>\nimport pandas as pd\n\n# Given DataFrame\ndf = pd.DataFrame({\n    'gene': ['TGFb', 'TGFb', 'TGFb', 'TGFb', 'TGFb', 'TGFb', 'TGFb', 'TGFb'],\n    'VIM': [0.158406, 0.039158, -0.052608, 0.157153, 0.206030, 0.132580, -0.144209, -0.093910],\n    'treatment': ['TGFb', 'TGFb', 'TGFb', 'TGFb', 'TGFb', 'TGFb', 'TGFb', 'TGFb'],\n    'dose': [0.1, 1, 10, 0.1, 1, 10, 0.1, 1],\n    'time': [2, 2, 2, 24, 24, 24, 48, 48]\n})\n\ndf = df.set_index(['treatment', 'dose', 'time'])\n\n# Sort DataFrame by 'time' while preserving relative order of rows with the same 'time' value\ndf = df.sort_values(by='time')\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 276, "library_problem_id": 276, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 276}}
{"id": 277, "code": ["<code>\nimport pandas as pd\n\n# Assuming df is your DataFrame\ndf = pd.DataFrame({\n    'treatment': ['A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],\n    'dose': ['D1', 'D1', 'D2', 'D2', 'D2', 'D1', 'D1', 'D1'],\n    'time': ['T1', 'T2', 'T1', 'T2', 'T3', 'T1', 'T2', 'T3'],\n    'VIM': [0.1, 0.2, 0.15, 0.12, 0.18, 0.13, 0.14, 0.15]\n})\n\ndf.set_index(['treatment', 'dose', 'time'], inplace=True)\ndf.sort_index(level='VIM', inplace=True)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 277, "library_problem_id": 277, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 276}}
{"id": 278, "code": ["<code>\nimport pandas as pd\n\ndef remove_specific_dates(df, date_range):\n    \"\"\"\n    This function removes rows from a DataFrame that correspond to specific dates.\n\n    Parameters:\n    df (pandas.DataFrame): The DataFrame to remove rows from.\n    date_range (list of str): A list of dates to remove in the format 'YYYY-MM-DD HH:MM:SS'.\n\n    Returns:\n    pandas.DataFrame: A new DataFrame with the rows removed.\n    \"\"\"\n    # Convert the date_range to datetime\n    date_range = [pd.to_datetime(date) for date in date_range]\n\n    # Create a boolean mask where True indicates rows to be removed\n    mask = (df.index >= date_range[0]) & (df.index <= date_range[1])\n\n    # Return a new DataFrame with the rows to be removed removed\n    return df[~mask]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 278, "library_problem_id": 278, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 278}}
{"id": 279, "code": ["<code>\nimport pandas as pd\n\ndef process_data(df, dates_to_remove):\n    # Convert 'Date' column to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n\n    # Set 'Date' as index\n    df.set_index('Date', inplace=True)\n\n    # Remove specific dates\n    df = df[~df.index.isin(dates_to_remove)]\n\n    # Format date index to display the day of the week\n    df.index = df.index.strftime('%d-%b-%Y %A')\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 279, "library_problem_id": 279, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 278}}
{"id": 280, "code": ["<code>\nimport pandas as pd\n\ndef extract_values(corr_matrix):\n    # Extract the upper triangle of the correlation matrix\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n    # Find the values greater than 0.3\n    values_gt_03 = upper[upper > 0.3]\n\n    return values_gt_03\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 280, "library_problem_id": 280, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 280}}
{"id": 281, "code": ["<code>\nimport pandas as pd\n\ndef extract_correlation(corr_matrix):\n    # Convert the correlation matrix to a pandas Series\n    corr_series = corr_matrix.stack()\n    \n    # Filter the series to only include values greater than 0.3\n    corr_series = corr_series[corr_series > 0.3]\n    \n    return corr_series\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 281, "library_problem_id": 281, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 280}}
{"id": 282, "code": ["<code>\nSure, I can help you with that. However, you didn't specify what you want to do with the given DataFrame. Could you please provide more details about what you want to achieve with the DataFrame?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 282, "library_problem_id": 282, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 282}}
{"id": 283, "code": ["<code>\nSure, I can help you with that. However, you didn't specify what you want to do with the given DataFrame. Could you please provide more details about what you want to achieve with the DataFrame?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 283, "library_problem_id": 283, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 282}}
{"id": 284, "code": ["<code>\nimport pandas as pd\n\ndef most_frequent_and_count(df):\n    # Find the most frequent value in each row\n    df['most_frequent'] = df.apply(lambda row: row.value_counts().idxmax(), axis=1)\n    \n    # Count the occurrences of the most frequent value in each row\n    df['count'] = df.apply(lambda row: row.value_counts().max(), axis=1)\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 284, "library_problem_id": 284, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 284}}
{"id": 285, "code": ["<code>\nimport pandas as pd\n\ndef most_frequent(df):\n    # Create new columns to store the most frequent value and its count\n    df['most_frequent'] = 0\n    df['count'] = 0\n\n    # Iterate over each row in the DataFrame\n    for i in range(df.shape[0]):\n        # Get the series of values in the current row\n        row = df.iloc[i]\n        \n        # Find the most frequent value and its count\n        most_frequent_value = row.mode()[0]\n        count = row.value_counts()[most_frequent_value]\n        \n        # Store the most frequent value and its count in the new columns\n        df.at[i, 'most_frequent'] = most_frequent_value\n        df.at[i, 'count'] = count\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 285, "library_problem_id": 285, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 284}}
{"id": 286, "code": ["<code>\nimport pandas as pd\ndata = pd.read_csv('myData.csv', sep = ',')\ndata.head()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 286, "library_problem_id": 286, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 284}}
{"id": 287, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef calculate_mean(df):\n    # Group by 'id1' and 'id2'\n    grouped = df.groupby(['id1', 'id2'])\n\n    # Calculate mean of 'foo' and 'bar'\n    means = grouped.mean()\n\n    # Handle NULL values\n    means = means.fillna(0)\n\n    return means\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 287, "library_problem_id": 287, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 287}}
{"id": 288, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\n# Assuming df is your DataFrame\ndf = pd.DataFrame({\n    'foo': ['bar', 'id1', 'id2'],\n    'id1': [1, np.nan, 3],\n    'id2': [1, 2, np.nan],\n    'value1': [5.75, 5.50, 7.00],\n    'value2': [0.75, 2.00, 3.00]\n})\n\n# Fill missing values with 0\ndf.fillna(0, inplace=True)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 288, "library_problem_id": 288, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 287}}
{"id": 289, "code": ["<code>\nimport pandas as pd\n\ndef merge_dataframes(df1, df2):\n    # Select only specific columns from df2\n    df2 = df2[['EntityNum', 'a_col']]\n    \n    # Perform left join on df1 and df2 using 'EntityNum' as the key\n    merged_df = pd.merge(df1, df2, on='EntityNum', how='left')\n    \n    # Remove 'b_col' column from the final result\n    merged_df = merged_df.drop('b_col', axis=1)\n    \n    return merged_df\n\ndf_a = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'foo':[100,50,200]})\ndf_b = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'a_col':['alice','bob','77'], 'b_col':['b1','b2','b3']})\n\nmerged_df = merge_dataframes(df_a, df_b)\nprint(merged_df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 289, "library_problem_id": 289, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 289}}
{"id": 290, "code": ["<code>\ndf_merged = df_a.merge(df_b, on='EntityNum')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 290, "library_problem_id": 290, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 289}}
{"id": 291, "code": ["<code>\nimport numpy as np\n\ndef get_array_shape(array):\n    return np.shape(array)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 291, "library_problem_id": 0, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 292, "code": ["<code>\nx = np.array([1400, 1500, 1600, np.nan, np.nan, np.nan ,1700])\nx = np.nan_to_num(x, nan=0)  # Replace NaN with 0\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 292, "library_problem_id": 1, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 293, "code": ["<code>\nx = np.array([1400, 1500, 1600, np.nan, np.nan, np.nan ,1700])\nx = np.nan_to_num(x, nan=0)  # Replace NaN with 0\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 293, "library_problem_id": 2, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 294, "code": ["<code>\nimport numpy as np\n\ndef remove_nan_values(nested_list):\n    # Use list comprehension to remove NaN values\n    return [[val for val in sublist if not np.isnan(val)] for sublist in nested_list]\n\n# Test the function\nnested_list = [[1400, 1500, 1600, np.nan], [1800, np.nan, np.nan, 1700]]\nprint(remove_nan_values(nested_list))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 294, "library_problem_id": 3, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 1}}
{"id": 295, "code": ["<code>\nSure, I can help you with that. However, you didn't provide the function to be completed. Could you please provide the function you want me to complete?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 295, "library_problem_id": 4, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 296, "code": ["<code>\nSure, I can help you with that. However, you didn't provide the function to be completed. Could you please provide the function you want me to complete?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 296, "library_problem_id": 5, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 297, "code": ["<code>\nSure, I can help you with that. However, you didn't provide the function to be completed. Could you please provide the function you want me to complete?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 297, "library_problem_id": 6, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 298, "code": ["<code>\nSure, I can help you with that. However, you didn't provide the function to be completed. Could you please provide the function you want to be completed?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 298, "library_problem_id": 7, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 299, "code": ["<code>\nSure, I can help you with that. However, you didn't specify what you want to do with the given numpy array. Could you please provide more details about what you want to achieve with this array?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 299, "library_problem_id": 8, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 300, "code": ["<code>\nimport numpy as np\na = np.array([1,2,3,4,5])\np = 25\nresult = np.power(a, p)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 300, "library_problem_id": 9, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 301, "code": ["<code>\nimport numpy as np\n\ndef vec2matrix(vec, ncol):\n    return vec.reshape(-1, ncol)\n\nA = np.array([1,2,3,4,5,6])\nB = vec2matrix(A,ncol=2)\nprint(B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 301, "library_problem_id": 10, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 302, "code": ["<code>\nimport numpy as np\n\ndef vec2matrix(A, nrow):\n    return A.reshape(nrow, -1)\n\nA = np.array([1,2,3,4,5,6])\nB = vec2matrix(A,nrow=3)\nprint(B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 302, "library_problem_id": 11, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 303, "code": ["<code>\nimport numpy as np\n\ndef vec2matrix(vec, ncol):\n    return vec.reshape(-1, ncol)\n\nA = np.array([1,2,3,4,5,6,7])\nB = vec2matrix(A,ncol=2)\nprint(B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 303, "library_problem_id": 12, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 304, "code": ["<code>\nimport numpy as np\n\ndef vec2matrix(A, ncol):\n    # Reverse the order of the elements in the array\n    A = A[::-1]\n    # Reshape the array into a 2D matrix\n    B = A.reshape(-1, ncol)\n    return B\n\nA = np.array([1,2,3,4,5,6,7])\nB = vec2matrix(A, ncol=2)\nprint(B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 304, "library_problem_id": 13, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 305, "code": ["<code>\nimport numpy as np\n\ndef shift_array(arr, shift):\n    # Create a new array with NaN padding\n    shifted_arr = np.full(len(arr), np.nan)\n    \n    # Calculate the index where the shift should start\n    start_index = len(arr) - shift\n    \n    # If the shift is positive, start from the beginning of the array\n    if start_index < 0:\n        start_index = abs(start_index)\n        shifted_arr[:start_index] = np.nan\n    else:\n        shifted_arr[start_index:] = np.nan\n    \n    # Copy the elements from the original array to the new array\n    shifted_arr[start_index:start_index+len(arr)] = arr\n    \n    return shifted_arr\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 305, "library_problem_id": 14, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 306, "code": ["<code>\nimport numpy as np\n\ndef shift_array(arr, shift):\n    shifted_arr = np.empty_like(arr)\n    shifted_arr[:] = np.nan\n    if shift > 0:\n        shifted_arr[..., shift:] = arr[..., :-shift]\n    elif shift < 0:\n        shifted_arr[..., :shift] = arr[..., -shift:]\n    else:\n        shifted_arr = arr\n    return shifted_arr\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 306, "library_problem_id": 15, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 307, "code": ["<code>\nimport numpy as np\n\ndef shift(arr, shift_arr):\n    shifted_arrays = []\n    for i, shift in enumerate(shift_arr):\n        if shift >= 0:\n            pad_width = ((0, 0), (shift, 0))\n        else:\n            pad_width = ((0, 0), (0, -shift))\n        padded_array = np.pad(arr[i], pad_width=pad_width, mode='constant', constant_values=np.nan)\n        shifted_arrays.append(padded_array)\n    return np.array(shifted_arrays)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 307, "library_problem_id": 16, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 14}}
{"id": 308, "code": ["<code>\nimport numpy as np\n\nnp.random.seed(42)\n\narray1 = np.random.randint(-1, 2, (100, 2000))\narray2 = np.random.randint(-1, 2, (100, 2000))\n\nprint(array1)\nprint(array2)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 308, "library_problem_id": 17, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 309, "code": ["<code>\nSure, I can help you with that. However, you didn't specify what kind of completion you need. Are you looking for a function to sort the array in ascending order, or do you need to find the maximum or minimum value in the array? Please provide more details so I can assist you better.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 309, "library_problem_id": 18, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 310, "code": ["<code>\nimport numpy as np\n\ndef find_min_index(arr):\n    raveled_arr = np.ravel(arr)\n    min_index = np.argmin(raveled_arr)\n    return np.unravel_index(min_index, arr.shape, order='C')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 310, "library_problem_id": 19, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 311, "code": ["<code>\nSure, I can help you with that. However, you didn't specify what kind of completion you need. Are you looking for a function to sort the array in ascending order, or do you need to find the maximum or minimum value in the array? Please provide more details so I can assist you better.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 311, "library_problem_id": 20, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 312, "code": ["<code>\nSure, I can help you with that. However, you didn't specify what you want to do with the given numpy array. Could you please provide more details about what you want to achieve with this array?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 312, "library_problem_id": 21, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 313, "code": ["<code>\nSure, I can help you with that. However, you didn't specify what kind of completion you need. Are you looking for a function to sort the array in ascending order, or do you need to find the maximum or minimum value in the array? Please provide more details so I can assist you better.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 313, "library_problem_id": 22, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 18}}
{"id": 314, "code": ["<code>\nimport numpy as np\na = np.array([[10,50,30],[60,20,40]])\nresult = np.sum(a) # put solution in this variable\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 314, "library_problem_id": 23, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 18}}
{"id": 315, "code": ["<code>\nimport numpy as np\na = np.array([[np.nan, 2., 3., np.nan],\n              [1., 2., 3., 9]])\na = np.nan_to_num(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 315, "library_problem_id": 24, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 316, "code": ["<code>\nSure, I can help you with that. However, I need to know what you want to do with the given array. Are you looking to fill the NaN values with some other value, or are you looking to perform some operation on the array? Please provide more details so I can assist you better.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 316, "library_problem_id": 25, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 24}}
{"id": 317, "code": ["<code>\nimport numpy as np\n\ndef convert_to_numpy(lst):\n    return np.array(lst)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 317, "library_problem_id": 26, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 318, "code": ["<code>\nimport numpy as np\n\ndef reorder_columns(arr, permutation):\n    if not isinstance(arr, np.ndarray) or arr.ndim != 2:\n        raise TypeError(\"Input array must be a 2D NumPy array.\")\n    if not isinstance(permutation, list) or not all(isinstance(i, int) for i in permutation):\n        raise TypeError(\"Permutation must be a list of integers.\")\n    \n    if len(permutation) != arr.shape[1]:\n        raise ValueError(\"Permutation must have the same length as the number of columns in the array.\")\n    \n    if not all(0 <= i < arr.shape[1] for i in permutation):\n        raise ValueError(\"All elements in the permutation must be between 0 and the number of columns in the array.\")\n    \n    # Check if permutation is a valid permutation of the integers from 0 to arr.shape[1] - 1\n    if sorted(permutation) != list(range(arr.shape[1])):\n        raise ValueError(\"The permutation is not a valid permutation of the columns.\")\n    \n    # Use NumPy's advanced indexing to reorder the columns\n    return arr[:, permutation]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 318, "library_problem_id": 27, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 319, "code": ["<code>\nimport numpy as np\n\ndef rearrange_matrices(array, permutation):\n    return np.transpose(array[permutation], (1, 0, 2, 3))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 319, "library_problem_id": 28, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 27}}
{"id": 320, "code": ["<code>\nimport numpy as np\n\ndef find_min_index(arr):\n    min_value_index = np.argmin(arr)\n    return np.unravel_index(min_value_index, arr.shape)\n\na = np.array([[1, 2], [3, 0]])\nresult = find_min_index(a)\nprint(result)  # Output: (1, 1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 320, "library_problem_id": 29, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 321, "code": ["<code>\nimport numpy as np\n\ndef find_max_coordinates(a):\n    max_value = np.max(a)\n    max_coordinates = np.where(a == max_value)\n    return (max_coordinates[0][0], max_coordinates[1][0])\n\na = np.array([[1, 2], [3, 0]])\nresult = find_max_coordinates(a)\nprint(result)  # Output: (0, 1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 321, "library_problem_id": 30, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 29}}
{"id": 322, "code": ["<code>\nimport numpy as np\na = np.array([[1, 0], [0, 2]])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 322, "library_problem_id": 31, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 29}}
{"id": 323, "code": ["<code>\nimport numpy as np\n\ndef calculate_sine_in_radians(angle_in_degrees):\n    angle_in_radians = np.radians(angle_in_degrees)\n    sine_of_angle = np.sin(angle_in_radians)\n    return sine_of_angle\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 323, "library_problem_id": 32, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 324, "code": ["<code>\nimport numpy as np\n\ndef calculate_cosine(angle_in_degrees):\n    angle_in_radians = np.deg2rad(angle_in_degrees)\n    return np.cos(angle_in_radians)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 324, "library_problem_id": 33, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 325, "code": ["<code>\nimport numpy as np\n\ndef classify_number(number):\n    if isinstance(number, int) and 0 <= number <= 360:\n        number = number * np.pi / 180\n    elif isinstance(number, (int, float)) and -np.pi <= number <= np.pi:\n        pass\n    else:\n        raise ValueError(\"Invalid input. The number should be in degrees between 0 and 360, or in radians between -pi and pi.\")\n\n    if np.sin(number) > np.sin(number * np.pi / 180):\n        return 0\n    else:\n        return 1\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 325, "library_problem_id": 34, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 326, "code": ["<code>\nIt seems like you've provided a Python code snippet with a variable declaration. However, you haven't specified what the function should do. Could you please provide more details about the task or the function you want to complete?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 326, "library_problem_id": 35, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 327, "code": ["<code>\nimport numpy as np\n\ndef pad_array(A, length):\n    # Calculate the number of elements needed to reach the desired length\n    num_elements = ((length - 1) // 1024 + 1) * 1024 - len(A)\n    \n    # Pad the array with zeros\n    return np.pad(A, (0, num_elements))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 327, "library_problem_id": 36, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 328, "code": ["<code>\nimport numpy as np\n\ndef pad_array(A, length):\n    # Calculate the number of elements needed to reach the desired length\n    num_elements = ((length - 1) // 1024 + 1) * 1024 - len(A)\n    \n    # Pad the array with zeros\n    return np.pad(A, (0, num_elements))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 328, "library_problem_id": 37, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 329, "code": ["<code>\nimport numpy as np\n\na = np.arange(4).reshape(2, 2)\npower = 5\n\ndef raise_power(matrix, power):\n    return np.power(matrix, power)\n\nresult = raise_power(a, power)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 329, "library_problem_id": 38, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 330, "code": ["<code>\nimport numpy as np\n\ndef f(matrix, multiplier):\n    return matrix * multiplier\n\na = np.arange(4).reshape(2, 2)\nprint(f(a, 2))  # [[2 3]\n                 # [0 1]]\nprint(f(a, 3))  # [[0 1]\n                 # [8 27]]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 330, "library_problem_id": 39, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 331, "code": ["<code>\nimport numpy as np\n\nnumerator = 98\ndenominator = 42\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 331, "library_problem_id": 40, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 332, "code": ["<code>\nimport numpy as np\ndef reduce_fraction(numerator, denominator):\n    gcd = np.gcd(numerator, denominator)\n    return numerator//gcd, denominator//gcd\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 332, "library_problem_id": 41, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 40}}
{"id": 333, "code": ["<code>\ndef simplify_fraction(numerator, denominator):\n    if denominator == 0:\n        return (float('NaN'), float('NaN'))\n    else:\n        gcd = math.gcd(numerator, denominator)\n        return (numerator // gcd, denominator // gcd)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 333, "library_problem_id": 42, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 40}}
{"id": 334, "code": ["<code>\nimport numpy as np\n\ndef average_arrays(*arrays):\n    return np.mean(arrays, axis=0)\n\na = np.array([10, 20, 30])\nb = np.array([30, 20, 20])\nc = np.array([50, 20, 40])\nresult = average_arrays(a, b, c)\nprint(result)  # Output: [30, 20, 30]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 334, "library_problem_id": 43, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 43}}
{"id": 335, "code": ["<code>\nimport numpy as np\n\ndef max_ndarray(*args):\n    return np.amax(args)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 335, "library_problem_id": 44, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 43}}
{"id": 336, "code": ["<code>\nimport numpy as np\n\ndef get_anti_diagonal(arr):\n    # Get the shape of the array\n    shape = arr.shape\n    \n    # If the array is 1D, return an empty array\n    if len(shape) == 1:\n        return np.array([])\n    \n    # Create an empty array of the same shape as the input array\n    anti_diagonal = np.zeros(shape)\n    \n    # Iterate over the elements of the array\n    for i in range(shape[0]):\n        for j in range(shape[1]):\n            # If the indices are from the anti-diagonal, assign the value to the new array\n            if i + j == shape[1] - 1:\n                anti_diagonal[i, j] = arr[i, j]\n    \n    # Return the new array\n    return anti_diagonal\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 336, "library_problem_id": 45, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 337, "code": ["<code>\nimport numpy as np\n\ndef extract_diagonal_from_top_right(arr):\n    rows, cols = arr.shape\n    indices = np.diag_indices_from(arr[::-1, :])\n    return arr[rows-1-indices[0], indices[1]]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 337, "library_problem_id": 46, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 338, "code": ["<code>\nimport numpy as np\n\ndef get_diagonals(arr):\n    # Get the indices of the main diagonal\n    main_diag_indices = np.diag_indices(min(arr.shape))\n    \n    # Get the indices of the anti-diagonal\n    anti_diag_indices = (np.arange(arr.shape[0]), np.arange(arr.shape[0])[::-1])\n    \n    # Return the main diagonal and the anti-diagonal as numpy arrays\n    return arr[main_diag_indices], arr[anti_diag_indices]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 338, "library_problem_id": 47, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 339, "code": ["<code>\nimport numpy as np\n\ndef extract_diagonals(matrix):\n    rows, cols = matrix.shape\n    indices = np.diag_indices(min(rows, cols))\n    diagonals = np.array([matrix[i, j] for i, j in zip(indices[0], indices[1])])\n    return diagonals\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 339, "library_problem_id": 48, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 45}}
{"id": 340, "code": ["<code>\nself.cells = np.empty((r,c),dtype=np.object)\nfor i in range(r):\n    for j in range(c):\n        self.cells[i][j] = \"Object \" + str(i*c + j)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 340, "library_problem_id": 49, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 341, "code": ["<code>\nimport numpy as np\n\ndef flatten_array(arr):\n    flat_list = []\n    for row in arr:\n        flat_list.extend(row)\n    return flat_list\n\n# Test the function\narr = np.random.randint(10, size=(3, 3))\nprint(\"Original array:\")\nprint(arr)\nprint(\"Flattened list:\")\nprint(flatten_array(arr))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 341, "library_problem_id": 50, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 49}}
{"id": 342, "code": ["<code>\nimport numpy as np\n\ndef flatten_array(arr):\n    return arr.flatten().tolist()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 342, "library_problem_id": 51, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 343, "code": ["<code>\nimport numpy as np\n\ndef convert_to_list(array):\n    # Convert the array to a list\n    list_array = np.asarray(array, dtype=list)\n    # Flatten the list\n    flat_list = list_array.flatten().tolist()\n    return flat_list\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 343, "library_problem_id": 52, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 49}}
{"id": 344, "code": ["<code>\nimport numpy as np\n\ndef binary_to_numpy(binary_string):\n    # Split the binary string into individual digits\n    binary_list = list(binary_string)\n    \n    # Convert each digit to an integer\n    binary_list = [int(digit) for digit in binary_list]\n    \n    # Convert the list to a NumPy array\n    numpy_array = np.array(binary_list)\n    \n    return numpy_array\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 344, "library_problem_id": 53, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 53}}
{"id": 345, "code": ["<code>\nimport numpy as np\n\n# Create a random 8x5 array\na = np.random.rand(8, 5)\n\n# Define the column to be multiplied\ncol = 2\n\n# Define the number to multiply\nmultiply_number = 5.2\n\n# Multiply the specified column by the number\na[:, col] *= multiply_number\n\n# Print the result\nprint(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 345, "library_problem_id": 54, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 346, "code": ["<code>\ndef transform_row(matrix, row, multiplier):\n    if row < matrix.shape[0]:\n        matrix[row] *= multiplier\n        return matrix\n    else:\n        return \"Row index out of range\"\n\nimport numpy as np\na = np.random.rand(8, 5)\nrow = 2\nmultiply_number = 5.2\nresult = transform_row(a, row, multiply_number)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 346, "library_problem_id": 55, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 347, "code": ["<code>\nimport numpy as np\n\n# Create a 8x5 array with random numbers\na = np.random.rand(8, 5)\n\n# Define the row and the number to divide by\nrow = 2\ndivide_number = 5.2\n\n# Divide the specified row by the number\na[row, :] = a[row, :] / divide_number\n\n# Print the result\nprint(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 347, "library_problem_id": 56, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 54}}
{"id": 348, "code": ["<code>\nimport numpy as np\n\ndef find_independent_vectors(matrix):\n    # Find the rank of the matrix\n    rank = np.linalg.matrix_rank(matrix)\n    \n    # Find the indices of the rows (or columns) that are linearly independent\n    independent_indices = np.argwhere(np.linalg.matrix_rank(matrix, axis=0) < rank).flatten()\n    \n    # Return the rows (or columns) that are linearly independent\n    return matrix[independent_indices]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 348, "library_problem_id": 57, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 349, "code": ["<code>\nimport numpy as np\n\ndef get_row_length(shape):\n    # Extract the relevant dimension to use as the iteration range in a for loop\n    row_length = shape[1]\n    return row_length\n\n# Test the function\nshape = (3, 4)\nprint(get_row_length(shape))  # Output: 4\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 349, "library_problem_id": 58, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 350, "code": ["<code>\nSure, I can help you with that. However, you didn't provide the function to be completed. Could you please provide the function you want me to complete?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 350, "library_problem_id": 59, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 351, "code": ["<code>\nimport numpy as np\nfrom scipy import stats\n\ndef weighted_ttest(a, b):\n    # Remove NaN values\n    a = a[~np.isnan(a)]\n    b = b[~np.isnan(b)]\n    \n    # Calculate the weights\n    weights_a = np.ones_like(a)\n    weights_b = np.ones_like(b)\n    \n    # Calculate the weighted means\n    mean_a = np.average(a, weights=weights_a)\n    mean_b = np.average(b, weights=weights_b)\n    \n    # Calculate the weighted variances\n    var_a = np.average((a - mean_a)**2, weights=weights_a)\n    var_b = np.average((b - mean_b)**2, weights=weights_b)\n    \n    # Calculate the degrees of freedom\n    df = len(a) + len(b) - 2\n    \n    # Calculate the pooled standard deviation\n    s = np.sqrt(((len(a) - 1) * var_a + (len(b) - 1) * var_b) / df)\n    \n    # Calculate the t-statistic\n    t = (mean_a - mean_b) / s\n    \n    # Calculate the p-value\n    p = 2 * (1 - stats.t.cdf(abs(t), df))\n    \n    return p\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 351, "library_problem_id": 60, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 352, "code": ["<code>\nThe given code is incomplete. It seems like it's defining some variables, but there's no function to complete. However, I'll assume that you want to complete a function that calculates the confidence interval for a difference between two means. Here's how you can do it:\n\n```python\nimport numpy as np\nimport scipy.stats\n\ndef confidence_interval(amean, avar, anobs, bmean, bvar, bnobs, alpha=0.05):\n    \"\"\"\n    Calculate the confidence interval for a difference between two means.\n\n    Parameters:\n    amean (float): Mean of the first distribution.\n    avar (float): Variance of the first distribution.\n    anobs (int): Number of observations in the first distribution.\n    bmean (float): Mean of the second distribution.\n    bvar (float): Variance of the second distribution.\n    bnobs (int): Number of observations in the second distribution.\n    alpha (float): Significance level (default is 0.05).\n\n    Returns:\n    tuple: Lower and upper bounds of the confidence interval.\n    \"\"\"\n    # Calculate the pooled variance and standard deviation\n    pooled_var = (avar / anobs) + (bvar / bnobs)\n    pooled_std = np.sqrt(pooled_var)\n\n    # Calculate the z-score for the given alpha level\n    z_score = scipy.stats.norm.ppf(1 - alpha / 2)\n\n    # Calculate the margin of error\n    margin_of_error = z_score * pooled_std * np.sqrt(1/an\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 352, "library_problem_id": 61, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 353, "code": ["<code>\nimport numpy as np\n\ndef find_difference(A, B):\n    A = set(tuple(row) for row in A)\n    B = set(tuple(row) for row in B)\n    return np.array([list(row) for row in A - B])\n\nA = np.asarray([[1,1,1], [1,1,2], [1,1,3], [1,1,4]])\nB = np.asarray([[0,0,0], [1,0,2], [1,0,3], [1,0,4], [1,1,0], [1,1,1], [1,1,4]])\n\nprint(find_difference(A, B))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 353, "library_problem_id": 62, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 354, "code": ["<code>\nimport numpy as np\n\ndef find_unique_elements(A, B):\n    # Find elements in A but not in B\n    unique_in_A_not_in_B = np.setdiff1d(A, B)\n    \n    # Find elements in B but not in A\n    unique_in_B_not_in_A = np.setdiff1d(B, A)\n    \n    # Combine the two arrays and return as a numpy array\n    return np.concatenate((unique_in_A_not_in_B, unique_in_B_not_in_A))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 354, "library_problem_id": 63, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 62}}
{"id": 355, "code": ["<code>\nimport numpy as np\n\ndef sort_along_axis(a, b, axis):\n    # Obtain the indices that would sort a along the specified axis\n    indices = np.argsort(a, axis=axis)\n    \n    # Use these indices to sort b along the same axis\n    sorted_b = np.take_along_axis(b, indices, axis=axis)\n    \n    return sorted_b\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 355, "library_problem_id": 64, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 356, "code": ["<code>\nimport numpy as np\n\ndef sort_along_axis(a, b, axis=0):\n    sort_indices = np.argsort(a, axis=axis)\n    b = np.take_along_axis(b, sort_indices, axis=axis)\n    return b\n\na = np.array([[[ 1.,  1.,  1.],\n  [ 1.,  1.,  1.],\n  [ 1.,  1.,  1.]],\n [[ 3.,  3.,  3.],\n  [ 3.,  3.,  3.],\n  [ 3.,  3.,  3.]],\n [[ 2.,  2.,  2.],\n  [ 2.,  2.,  2.],\n  [ 2.,  2.,  2.]]])\n\nb = np.arange(3*3*3).reshape((3, 3, 3))\n\nb = sort_along_axis(a, b)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 356, "library_problem_id": 65, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 357, "code": ["<code>\nimport numpy as np\n\n# Assuming a and b are your 3D numpy arrays\na = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nb = np.array([[10, 20, 30], [40, 50, 60], [70, 80, 90]])\n\n# Generate the sorting indices\nindices = np.argsort(a, axis=0)[::-1]\n\n# Use the indices to sort b\nc = b[indices]\n\nprint(c)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 357, "library_problem_id": 66, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 358, "code": ["<code>\nimport numpy as np\n\ndef sort_based_on_sum(a, b):\n    # Calculate the sum of each 3D array in a\n    sums_a = np.sum(a, axis=(1, 2))\n    \n    # Sort the indices of a based on the sums\n    sorted_indices = np.argsort(sums_a)[::-1]\n    \n    # Sort b based on the sorted indices\n    b_sorted = b[sorted_indices]\n    \n    return b_sorted\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 358, "library_problem_id": 67, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 64}}
{"id": 359, "code": ["<code>\nimport numpy as np\n\n# Given array\narr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n\n# Print the original array\nprint(\"Original array:\")\nprint(arr)\n\n# Delete the 3rd column\narr = np.delete(arr, 2, axis=1)\n\n# Print the modified array\nprint(\"Modified array:\")\nprint(arr)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 359, "library_problem_id": 68, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 360, "code": ["<code>\nimport numpy as np\n\n# Given array\narr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n\n# Print the given array\nprint(\"Given array:\")\nprint(arr)\n\n# Delete the 3rd row\narr = np.delete(arr, 2, axis=0)\n\n# Print the array after deleting the 3rd row\nprint(\"Array after deleting the 3rd row:\")\nprint(arr)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 360, "library_problem_id": 69, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 361, "code": ["<code>\nimport numpy as np\n\ndef delete_columns(arr):\n    return np.delete(arr, 1, axis=1)\n\narr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\nprint(delete_columns(arr))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 361, "library_problem_id": 70, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 362, "code": ["<code>\ndef delete_columns(arr, del_col):\n    # Sort the columns to be deleted in descending order\n    del_col.sort(reverse=True)\n    \n    # Iterate over the columns to be deleted\n    for col in del_col:\n        # Delete the column\n        arr = np.delete(arr, col-1, axis=1)\n    \n    return arr\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 362, "library_problem_id": 71, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 363, "code": ["<code>\nimport numpy as np\n\na = np.asarray([1,2,3,4])\na_l = a.tolist()\na_l.insert(2,66)\na = np.asarray(a_l)\nprint(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 363, "library_problem_id": 72, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 364, "code": ["<code>\nimport numpy as np\n\ndef insert_row(arr, pos, element):\n    return np.insert(arr, pos, element, axis=0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 364, "library_problem_id": 73, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 365, "code": ["<code>\nimport numpy as np\n\nexample_a = np.asarray([1,2,3,4])\n\ndef insert_element(a, pos, element):\n    # Create a new array with one more element\n    new_a = np.zeros(len(a) + 1)\n    \n    # Copy the elements before the position\n    new_a[:pos] = a[:pos]\n    \n    # Insert the new element at the position\n    new_a[pos] = element\n    \n    # Copy the elements after the position\n    new_a[pos + 1:] = a[pos:]\n    \n    return new_a\n\n# Test the function\nprint(insert_element(example_a, 2, 5))  # Output: [1. 2. 5. 3. 4.]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 365, "library_problem_id": 74, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 72}}
{"id": 366, "code": ["<code>\nimport numpy as np\n\ndef insert_rows(arr, rows, indices):\n    # Convert the rows to a NumPy array\n    rows = np.array(rows)\n    \n    # Create a new array with the inserted rows\n    new_arr = np.insert(arr, indices, rows, axis=0)\n    \n    return new_arr\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 366, "library_problem_id": 75, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 72}}
{"id": 367, "code": ["<code>\nimport numpy as np\n\ndef deep_copy_nested_array(arr):\n    return np.copy(arr)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 367, "library_problem_id": 76, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 368, "code": ["<code>\nimport numpy as np\na = np.repeat(np.arange(1, 6).reshape(1, -1), 3, axis = 0)\nresult = np.repeat(a, 3, axis = 1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 368, "library_problem_id": 77, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 369, "code": ["<code>\nimport numpy as np\na = np.repeat(np.arange(1, 6).reshape(-1, 1), 3, axis = 1)\nresult = np.sum(a, axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 369, "library_problem_id": 78, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Semantic", "perturbation_origin_id": 77}}
{"id": 370, "code": ["<code>\nimport numpy as np\nexample_a = np.repeat(np.arange(1, 6).reshape(1, -1), 3, axis = 0)\n\ndef f(a):\n    # Your function should go here\n    return a.sum(axis=1)\n\nprint(f(example_a))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 370, "library_problem_id": 79, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Surface", "perturbation_origin_id": 77}}
{"id": 371, "code": ["<code>\nimport numpy as np\nfrom scipy import integrate\n\ndef integrate_2D(func, x_range, y_range):\n    x = np.linspace(*x_range, 20)\n    y = np.linspace(*y_range, 30)\n    xx, yy = np.meshgrid(x, y)\n    zz = func(xx, yy)\n    result = integrate.nquad(func, [[x_range[0], x_range[1]], [y_range[0], y_range[1]]])[0]\n    return result\n\n# Test the function\nfunc = lambda x, y: (np.cos(x))**4 + (np.sin(y))**2\nx_range = [-np.pi, np.pi]\ny_range = [-np.pi, np.pi]\nresult = integrate_2D(func, x_range, y_range)\nprint(f\"The result is: {result}\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 371, "library_problem_id": 80, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 372, "code": ["<code>\nSure, you can use the `scipy.integrate.dblquad` function for double integration. This function uses the double-ended precision adaptive integration scheme to integrate a function over a rectangular region. It is more accurate than the simple rectangle rule summation.\n\nHere is a Python function that performs a 2D numerical integration over a set of sample data points on a rectangular grid using the `dblquad` function:\n\n```python\nimport numpy as np\nfrom scipy.integrate import dblquad\n\ndef integrate_2D(func, x_min, x_max, y_min, y_max):\n    \"\"\"\n    Perform a 2D numerical integration over a set of sample data points on a rectangular grid.\n\n    Parameters:\n    func (function): The function to be integrated.\n    x_min (float): The lower limit of the x-axis.\n    x_max (float): The upper limit of the x-axis.\n    y_min (float): The lower limit of the y-axis.\n    y_max (float): The upper limit of the y-axis.\n\n    Returns:\n    float: The result of the integration.\n    \"\"\"\n    return dblquad(func, x_min, x_max, lambda x: y_min, lambda x: y_max)\n\n# Example usage:\ndef func(x, y):\n    return (np.cos(x)**4) + (np.sin(y)**2)\n\nresult, error = integrate_2D(func, 0, np.pi, 0, np.pi)\nprint(f\"The result of the integration is {result} with an error of\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 372, "library_problem_id": 81, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 373, "code": ["<code>\nimport numpy as np\n\ndef calculate_ecdf(data):\n    # Sort the data\n    sorted_data = np.sort(data)\n    \n    # Calculate the number of data points\n    n = len(sorted_data)\n    \n    # Create an array of indices\n    index = np.arange(1, n+1)\n    \n    # Calculate the ECDF\n    ecdf = index / n\n    \n    return ecdf\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 373, "library_problem_id": 82, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 374, "code": ["<code>\nimport numpy as np\n\ndef ecdf(x):\n    x = np.sort(x)\n    n = len(x)\n    y = np.arange(1, n+1) / n\n    return x, y\n\n# Example usage:\ngrades = np.array([85, 90, 88, 92, 89])\neval = np.array([[85, 90], [88, 92], [89, 95]])\n\nresult = ecdf(grades)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 374, "library_problem_id": 83, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 375, "code": ["<code>\nimport numpy as np\nfrom scipy.stats import cumfreq\n\ndef ecdf(data, threshold):\n    # Normalize the data\n    data = np.array(data) / np.sum(data)\n    \n    # Calculate the cumulative distribution function\n    cdf = cumfreq(data, numbins=len(data))\n    \n    # Find the longest interval where the CDF is below the threshold\n    low, high = None, None\n    max_length = 0\n    for i in range(len(cdf.cumcount)):\n        if cdf.cumcount[i] / len(data) < threshold:\n            if high is None:\n                low = cdf.lowerlimit[i]\n            high = cdf.lowerlimit[i]\n            length = high - low\n            if length > max_length:\n                max_length = length\n                \n    return (low, high)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 375, "library_problem_id": 84, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 376, "code": ["<code>\nimport numpy as np\n\ndef generate_binary_array(N, ratio):\n    # Calculate the number of 1s and 0s\n    num_ones = int(N * ratio)\n    num_zeros = N - num_ones\n\n    # Generate a random binary array with the specified number of 1s and 0s\n    array = np.zeros(N, dtype=int)\n    array[:num_ones] = 1\n    np.random.shuffle(array)\n\n    return array\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 376, "library_problem_id": 85, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 377, "code": ["<code>\nimport torch\n\ndef convert_tensor_to_numpy():\n    # Initialize a PyTorch tensor of ones with size 5\n    a_torch = torch.ones(5)\n\n    # Convert the PyTorch tensor to a NumPy array\n    a_np = a_torch.numpy()\n\n    return a_np\n\na_np = convert_tensor_to_numpy()\nprint(a_np)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 377, "library_problem_id": 86, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 378, "code": ["<code>\nimport numpy as np\nimport torch\n\ndef convert_to_tensor():\n    numpy_array = np.ones(5)\n    return torch.from_numpy(numpy_array)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 378, "library_problem_id": 87, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 86}}
{"id": 379, "code": ["<code>\nimport tensorflow as tf\n\ndef tensor_to_numpy():\n    # Create a 2x3x4 tensor filled with ones\n    tensor = tf.ones([2, 3, 4])\n\n    # Convert the TensorFlow tensor to a NumPy array\n    numpy_array = tensor.numpy()\n\n    return numpy_array\n\n# Call the function\nprint(tensor_to_numpy())\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 379, "library_problem_id": 88, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 380, "code": ["<code>\nimport numpy as np\nimport tensorflow as tf\n\ndef convert_numpy_to_tensor():\n    # Create a numpy array\n    np_array = np.random.rand(2, 3, 4)\n\n    # Convert the numpy array to a tensorflow tensor\n    tf_tensor = tf.convert_to_tensor(np_array)\n\n    return tf_tensor\n\n# Call the function\nprint(convert_numpy_to_tensor())\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 380, "library_problem_id": 89, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 88}}
{"id": 381, "code": ["<code>\nSure, I can help you with that. However, you didn't specify what kind of completion you need. Are you looking for sorting the array in ascending order, descending order, or something else? Please provide more details.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 381, "library_problem_id": 90, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 382, "code": ["<code>\nSure, I can help you with that. However, you didn't specify what kind of completion you need. Are you looking for sorting the array in ascending order, descending order, or something else? Please provide more details.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 382, "library_problem_id": 91, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 383, "code": ["<code>\nimport numpy as np\n\ndef find_n_largest_indices(arr, N):\n    # Your code goes here\n    return np.argsort(arr)[-N:][::-1]\n\na = np.array([4, 1, 0, 8, 5, 2])\nN = 3\nresult = find_n_largest_indices(a, N)\nprint(result)  # Output: [3 4 0]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 383, "library_problem_id": 92, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 90}}
{"id": 384, "code": ["<code>\nimport numpy as np\n\ndef raise_to_power(A, n):\n    return np.linalg.matrix_power(A, n)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 384, "library_problem_id": 93, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 93}}
{"id": 385, "code": ["<code>\nimport numpy as np\n\ndef extract_patches(array, patch_size):\n    # Get the shape of the input array\n    array_shape = np.array(array.shape)\n    \n    # Calculate the shape of the patches\n    patch_shape = np.array(patch_size)\n    \n    # Calculate the number of patches along each dimension\n    patch_strides = array_shape - patch_shape + 1\n    \n    # Create a view of the array with the correct shape and strides\n    patches = np.lib.stride_tricks.as_strided(array, shape=tuple(patch_shape) + tuple(patch_strides), strides=array.strides * 2)\n    \n    # Reshape the patches to have the correct shape\n    patches = patches.reshape(-1, *patch_shape)\n    \n    return patches\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 385, "library_problem_id": 94, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 386, "code": ["<code>\nimport numpy as np\n\ndef extract_patches(arr):\n    # Get the shape of the input array\n    h, w = arr.shape\n\n    # Calculate the shape of the output array\n    out_h, out_w = h - 1, w - 1\n\n    # Initialize an empty list to store the patches\n    patches = []\n\n    # Loop over the array to extract the patches\n    for i in range(out_h):\n        for j in range(out_w):\n            patch = arr[i:i+2, j:j+2]\n            patches.append(patch)\n\n    # Convert the list of patches to a 3D NumPy array\n    patches_array = np.array(patches)\n\n    return patches_array\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 386, "library_problem_id": 95, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 387, "code": ["<code>\nimport numpy as np\n\ndef extract_patches(arr):\n    h, w = arr.shape\n    patches = []\n    for i in range(h-1):\n        for j in range(w-1):\n            patch = arr[i:i+2, j:j+2]\n            patches.append(patch)\n    return patches\n\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\n\npatches = extract_patches(a)\n\nfor patch in patches:\n    print(patch)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 387, "library_problem_id": 96, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 388, "code": ["<code>\nimport numpy as np\n\ndef extract_patches(arr):\n    # Get the shape of the array\n    shape = arr.shape\n\n    # Calculate the number of patches in each dimension\n    n_patches_x = shape[0] // 2\n    n_patches_y = shape[1] // 2\n\n    # Initialize an empty list to store the patches\n    patches = []\n\n    # Loop over the array in steps of 2 in both dimensions\n    for i in range(0, n_patches_x*2, 2):\n        for j in range(0, n_patches_y*2, 2):\n            # Extract a 2x2 patch and append it to the list\n            patch = arr[i:i+2, j:j+2]\n            patches.append(patch)\n\n    # Return the list of patches\n    return patches\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 388, "library_problem_id": 97, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 389, "code": ["<code>\nimport numpy as np\n\ndef reshape_3d_to_2d(arr):\n    return np.concatenate(arr, axis=1)\n\n# Test the function\narr = np.array([[[0, 1, 2], [6, 7, 8]], [[3, 4, 5], [9, 10, 11]], [[12, 13, 14], [18, 19, 20]], [[15, 16, 17], [21, 22, 23]]])\nprint(reshape_3d_to_2d(arr))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 389, "library_problem_id": 98, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 390, "code": ["<code>\nimport numpy as np\n\ndef extract_patches(arr):\n    # Get the shape of the array\n    nrows, ncols = arr.shape\n\n    # Calculate the number of patches in each dimension\n    nrows_patches = nrows // 2\n    ncols_patches = ncols // 2\n\n    # Initialize an empty list to store the patches\n    patches = []\n\n    # Loop over the array in steps of 2\n    for i in range(0, nrows, 2):\n        for j in range(0, ncols, 2):\n            # Extract a 2x2 patch and append it to the list\n            patch = arr[i:i+2, j:j+2]\n            patches.append(patch)\n\n    # Convert the list of patches to a 3D numpy array\n    patches_array = np.array(patches)\n\n    return patches_array\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 390, "library_problem_id": 99, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 391, "code": ["<code>\nimport numpy as np\n\ndef extract_columns(arr, start, end):\n    return arr[:, start:end+1]\n\n# Test the function\na = np.array([[ 0,  1,  2,  3, 5, 6, 7, 8],\n              [ 4,  5,  6,  7, 5, 3, 2, 5],\n              [ 8,  9, 10, 11, 4, 5, 3, 5]])\n\nprint(extract_columns(a, 1, 5))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 391, "library_problem_id": 100, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 392, "code": ["<code>\nimport numpy as np\n\ndef extract_rows(arr, low, high):\n    return arr[low:high+1]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 392, "library_problem_id": 101, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 393, "code": ["<code>\nimport numpy as np\n\ndef extract_columns(arr, start, end):\n    # Ensure the start and end indices are within the array's bounds\n    start = max(0, start)\n    end = min(arr.shape[1], end)\n    \n    # Extract the columns and return a new array\n    return arr[:, start:end]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 393, "library_problem_id": 102, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 394, "code": ["<code>\nimport numpy as np\n\ndef from_string(s):\n    return np.array(eval(s))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 394, "library_problem_id": 103, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 395, "code": ["<code>\nimport numpy as np\n\ndef loguni(min, max, n):\n    return np.random.loguniform(min, max, n)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 395, "library_problem_id": 104, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 396, "code": ["<code>\nimport numpy as np\n\ndef loguni(n, min, max):\n    return np.exp(np.random.uniform(min, max, n))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 396, "library_problem_id": 105, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 104}}
{"id": 397, "code": ["<code>\nimport numpy as np\n\ndef loguni(n, min_val, max_val):\n    return np.power(10, np.random.uniform(np.log10(min_val), np.log10(max_val), n))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 397, "library_problem_id": 106, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 398, "code": ["<code>\nimport numpy as np\nimport pandas as pd\n\ndef compute_time_series(A, a, b):\n    B = pd.Series(np.zeros(len(A)))\n    B[0] = a * A[0]\n    for t in range(1, len(A)):\n        B[t] = a * A[t] + b * B[t-1]\n    return B\n\nA = pd.Series(np.random.randn(10,))\na = 0.5\nb = 0.3\n\nB = compute_time_series(A, a, b)\nprint(B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 398, "library_problem_id": 107, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 399, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef compute_time_series(A, a, b, c):\n    B = pd.Series(index=A.index)\n    B[0] = a * A[0]\n    if len(B) > 1:\n        B[1] = a * A[1] + b * B[0]\n    for t in range(2, len(B)):\n        B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n    return B\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 399, "library_problem_id": 108, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 107}}
{"id": 400, "code": ["<code>\nimport numpy as np\n\ndef initialize_empty_numpy_array():\n    return np.array([])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 400, "library_problem_id": 109, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 401, "code": ["<code>\nimport numpy as np\n\ndef initialize_empty_numpy_array(rows, cols):\n    return np.empty((rows, cols))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 401, "library_problem_id": 110, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 109}}
{"id": 402, "code": ["<code>\nimport numpy as np\n\ndef sub2ind(dims, subscripts):\n    if len(dims) != len(subscripts):\n        raise ValueError(\"Dimensions and subscripts must have the same length.\")\n    if any(s < 1 for s in subscripts):\n        raise ValueError(\"Subscripts must be greater than 0.\")\n    if any(s > dim for dim, s in zip(dims, subscripts)):\n        raise ValueError(\"Subscripts are out of bounds.\")\n    return np.ravel_multi_index(subscripts, dims, order='F')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 402, "library_problem_id": 111, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 403, "code": ["<code>\ndef sub2ind(dims, subscripts):\n    if len(dims) != len(subscripts):\n        raise ValueError(\"Number of dimensions and subscripts must match.\")\n    if any(s < 0 for s in subscripts):\n        raise ValueError(\"Subscripts must be non-negative.\")\n    if any(s >= dim for dim, s in zip(dims, subscripts)):\n        raise ValueError(\"Subscripts out of bounds.\")\n    ind = 0\n    stride = 1\n    for d, s in zip(dims[::-1], subscripts[::-1]):\n        ind += s * stride\n        stride *= d\n    return ind\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 403, "library_problem_id": 112, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 404, "code": ["<code>\nThe given code already completes the task. The `dtype` parameter in the `np.zeros` function is used to specify the data type of the array. In the first case, it's set to 'int32,float32', so the `df.values.dtype` will be 'float32'. In the second case, no `dtype` is specified, so the default data type for `np.zeros` is 'float64', which is then used to create the `df2` DataFrame, so `df2.values.dtype` will be 'float64'.\n\nSo, there's no need for any additional code to complete this task. The given code already does what is required.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 404, "library_problem_id": 113, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 405, "code": ["<code>\nimport numpy as np\n\ndef accumulate_based_on_indices(mapping_array, accumulation_array):\n    # Convert the mapping array to integers\n    mapping_array = np.array(mapping_array, dtype=int)\n    \n    # Create an array of zeros with the same length as the accumulation array\n    result_array = np.zeros(len(accumulation_array))\n    \n    # Use NumPy's built-in function np.add.at to accumulate the elements of the accumulation array\n    # at the corresponding indices in the mapping array\n    np.add.at(result_array, mapping_array, accumulation_array)\n    \n    return result_array\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 405, "library_problem_id": 114, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 406, "code": ["<code>\nimport numpy as np\n\ndef max_values_per_index(values, indices):\n    unique_indices = np.unique(indices)\n    max_values = np.zeros(len(unique_indices))\n    \n    for i, index in enumerate(unique_indices):\n        max_values[i] = np.max(values[indices == index])\n    \n    return max_values\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 406, "library_problem_id": 115, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 407, "code": ["<code>\nimport numpy as np\n\ndef accumulate_at(a, accmap):\n    result = np.zeros_like(a)\n    np.add.at(result, accmap, a)\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 407, "library_problem_id": 116, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 408, "code": ["<code>\nimport numpy as np\n\ndef min_values_at_indices(a, index):\n    # create an empty list to store the minimum values\n    min_values = []\n    \n    # iterate over the index array\n    for i in index:\n        # if the index is -1, append the minimum value of a to min_values\n        if i == -1:\n            min_values.append(np.min(a))\n        # if the index is 0 or greater, append the value at that index in a to min_values\n        elif i >= 0:\n            min_values.append(a[i])\n    \n    # convert the list to a numpy array and return it\n    return np.array(min_values)\n\na = np.arange(1,11)  # data array\nindex = np.array([0,1,0,0,0,-1,-1,2,2,1])  # index array\n\nresult = min_values_at_indices(a, index)\nprint(result)  # should return array([1, 2, 6])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 408, "library_problem_id": 117, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 114}}
{"id": 409, "code": ["<code>\nimport numpy as np\n\n# Given arrays\nx = np.array([[2, 2, 2], [2, 2, 2], [2, 2, 2]])\ny = np.array([[3, 3, 3], [3, 3, 3], [3, 3, 1]])\n\n# Define the elementwise function\ndef elementwise_function(element_1, element_2):\n    return element_1 + element_2\n\n# Apply the function to the arrays\nz = np.vectorize(elementwise_function)(x, y)\n\nprint(z)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 409, "library_problem_id": 118, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 410, "code": ["<code>\nimport numpy as np\n\ndef random_sampling(tuples, probabilities):\n    try:\n        return list(np.random.choice(tuples, p=probabilities))\n    except ValueError:\n        print(\"Error: The input lists are not one-dimensional.\")\n        return []\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 410, "library_problem_id": 119, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 411, "code": ["<code>\nimport numpy as np\n\ndef complete_function(a, low_index, high_index):\n    # Create a mask for the indices between low_index and high_index\n    mask = np.ones(a.shape, dtype=bool)\n    mask[low_index:high_index, low_index:high_index] = False\n\n    # Use the mask to set the values in the array to zero\n    a[mask] = 0\n\n    return a\n\na = np.ones((3, 3))\nlow_index = -1\nhigh_index = 2\n\nprint(complete_function(a, low_index, high_index))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 411, "library_problem_id": 120, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 412, "code": ["<code>\nimport numpy as np\n\ndef filter_non_negative(arr):\n    return arr[arr >= 0]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 412, "library_problem_id": 121, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 413, "code": ["<code>\nimport numpy as np\n\ndef extract_complex_numbers(arr):\n    # Create a boolean mask where True indicates a complex number\n    mask = np.iscomplex(arr)\n    # Use the mask to index the array and return only the complex numbers\n    return arr[mask]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 413, "library_problem_id": 122, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 414, "code": ["<code>\nimport numpy as np\n\ndef calculate_bin_means(data, bin_size):\n    # Calculate the number of bins\n    num_bins = len(data) // bin_size\n    \n    # Reshape the data into bins\n    bins = data[:num_bins*bin_size].reshape(num_bins, bin_size)\n    \n    # Calculate the mean of each bin\n    bin_means = np.mean(bins, axis=1)\n    \n    return bin_means\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 414, "library_problem_id": 123, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 415, "code": ["<code>\nimport numpy as np\n\ndef bin_data(data, bin_size):\n    # Calculate the number of bins\n    num_bins = len(data) // bin_size\n    # Create an array of maximum values for each bin\n    bin_data_max = np.max(data[:num_bins*bin_size].reshape(-1, bin_size), axis=1)\n    return bin_data_max\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 415, "library_problem_id": 124, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 416, "code": ["<code>\nimport numpy as np\n\ndef bin_data(data, bin_size):\n    # Calculate the number of bins\n    num_bins = data.shape[1] // bin_size\n    if data.shape[1] % bin_size != 0:\n        num_bins -= 1\n\n    # Initialize the output array\n    bin_data_mean = np.zeros((data.shape[0], num_bins))\n\n    # Calculate the mean of each bin\n    for i in range(num_bins):\n        bin_data_mean[:, i] = np.mean(data[:, i*bin_size : (i+1)*bin_size], axis=1)\n\n    return bin_data_mean\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 416, "library_problem_id": 125, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 417, "code": ["<code>\ndef bin_and_mean(data, bin_size):\n    bin_data = [tuple(data[i:i+bin_size]) for i in range(0, len(data), bin_size)]\n    bin_data_mean = [sum(bin)/len(bin) for bin in bin_data]\n    return bin_data, bin_data_mean\n\ndata = [4,2,5,6,7,5,4,3,5,7]\n\nfor bin_size in [2, 3]:\n    bin_data, bin_data_mean = bin_and_mean(data, bin_size)\n    print(f\"For a bin size of {bin_size}:\")\n    print(f\"bin_data = {bin_data}\")\n    print(f\"bin_data_mean = {bin_data_mean}\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 417, "library_problem_id": 126, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 418, "code": ["<code>\nimport numpy as np\n\ndef bin_and_calculate_mean(data, bin_size):\n    # Calculate the number of bins\n    num_bins = data.size // bin_size\n\n    # Reshape the data into the bins\n    binned_data = data.reshape(num_bins, -1)\n\n    # Calculate the mean of each bin\n    bin_means = np.mean(binned_data, axis=1)\n\n    return bin_means\n\ndata = np.array([[4, 2, 5, 6, 7], [5, 4, 3, 5, 7]])\nbin_size = 3\nbin_data_mean = bin_and_calculate_mean(data, bin_size)\n\nprint(bin_data_mean)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 418, "library_problem_id": 127, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 419, "code": ["<code>\nimport numpy as np\n\ndef bin_data(data, bin_size):\n    # Calculate the number of bins\n    num_bins = data.shape[1] // bin_size\n    if data.shape[1] % bin_size != 0:\n        num_bins += 1\n\n    # Initialize the binned data and mean arrays\n    bin_data = np.zeros((data.shape[0], num_bins, bin_size))\n    bin_data_mean = np.zeros((data.shape[0], num_bins))\n\n    # Bin the data\n    for i in range(data.shape[0]):\n        for j in range(num_bins):\n            start = max(0, j * bin_size - bin_size)\n            end = min(data.shape[1], (j + 1) * bin_size)\n            bin_data[i, j] = data[i, start:end]\n            bin_data_mean[i, j] = np.mean(bin_data[i, j])\n\n    return bin_data, bin_data_mean\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 419, "library_problem_id": 128, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 420, "code": ["<code>\nimport numpy as np\nx = 0.25\nx_min = 0\nx_max = 1\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 420, "library_problem_id": 129, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 421, "code": ["<code>\nimport numpy as np\n\ndef smoothclamp(x, x_min, x_max):\n    t = (x - x_min) / (x_max - x_min)\n    t = np.clip(t, 0.0, 1.0)\n    return x_min + (x_max - x_min) * (6 * t**5 - 15 * t**4 + 10 * t**3)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 421, "library_problem_id": 130, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 129}}
{"id": 422, "code": ["<code>\nimport numpy as np\n\ndef circular_crosscorr(a, b):\n    # Calculate the length of the arrays\n    n = len(a)\n    m = len(b)\n    \n    # Initialize an empty array to store the results\n    results = np.zeros(n)\n    \n    # Perform the circular cross-correlation\n    for lag in range(n):\n        # Slide b over a\n        b_shifted = np.roll(b, lag)\n        \n        # Perform the cross-correlation\n        results[lag] = np.sum(a * b_shifted)\n    \n    return results\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 422, "library_problem_id": 131, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 423, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef convert_to_3d_array(df):\n    # Get the unique categories and sort them\n    categories = df.index.get_level_values('major').unique()\n    categories.sort()\n\n    # Get the unique timestamps and sort them\n    timestamps = df.index.get_level_values(0).unique()\n    timestamps.sort()\n\n    # Initialize the 3D numpy array\n    array_3d = np.zeros((len(df.columns), len(categories), len(timestamps)))\n\n    # Fill the 3D numpy array\n    for i, category in enumerate(categories):\n        for j, timestamp in enumerate(timestamps):\n            array_3d[:, i, j] = df.loc[(category, timestamp), :].values\n\n    return array_3d\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 423, "library_problem_id": 132, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 424, "code": ["<code>\nimport numpy as np\n\ndef convert_to_3d_numpy(df):\n    # Get the unique number of categories in the 'major' index\n    num_categories = df.index.get_level_values('major').nunique()\n    \n    # Get the number of columns in the DataFrame\n    num_columns = df.shape[1]\n    \n    # Get the length of the time index\n    time_length = len(df.index.get_level_values('timestamp'))\n    \n    # Initialize a 3D NumPy array with the appropriate dimensions\n    array_3d = np.zeros((num_categories, num_columns, time_length))\n    \n    # Iterate over the DataFrame and fill the 3D NumPy array\n    for i, category in enumerate(df.index.get_level_values('major').unique()):\n        for j, column in enumerate(df.columns):\n            array_3d[i, j, :] = df.loc[(category, slice(None)), column].values\n    \n    return array_3d\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 424, "library_problem_id": 133, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 132}}
{"id": 425, "code": ["<code>\nimport numpy as np\n\ndef convert_to_binary(arr, m):\n    # Convert each integer in the array to binary representation\n    binary_arr = [list(np.binary_repr(num, width=m)) for num in arr]\n    \n    # Convert the binary array to a numpy array and return\n    return np.array(binary_arr, dtype=int)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 425, "library_problem_id": 134, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 426, "code": ["<code>\nimport numpy as np\n\ndef int_to_bin_array(arr, m):\n    # Initialize an empty list to store the binary representations\n    bin_list = []\n\n    # Iterate over each integer in the input array\n    for num in arr:\n        # Convert the integer to binary and remove the '0b' prefix\n        bin_str = bin(num)[2:]\n\n        # If the binary string is shorter than m, pad it with zeros\n        if len(bin_str) < m:\n            bin_str = '0' * (m - len(bin_str)) + bin_str\n\n        # If the binary string is longer than m, truncate it to length m\n        if len(bin_str) > m:\n            bin_str = bin_str[-m:]\n\n        # Convert the binary string to a list of integers and append it to the list\n        bin_list.append([int(bit) for bit in bin_str])\n\n    # Convert the list of binary strings to a NumPy array and return it\n    return np.array(bin_list)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 426, "library_problem_id": 135, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 427, "code": ["<code>\nimport numpy as np\n\ndef binary_matrix(arr, m):\n    # Convert each integer to its binary representation of length m\n    binary_arr = [format(num, '0' + str(m) + 'b') for num in arr]\n    \n    # Convert binary strings to lists of integers\n    binary_arr = [list(map(int, list(binary_str))) for binary_str in binary_arr]\n    \n    # Compute the element-wise XOR of all resulting binary arrays\n    xor_matrix = np.array(binary_arr[0])\n    for i in range(1, len(binary_arr)):\n        xor_matrix = np.bitwise_xor(xor_matrix, np.array(binary_arr[i]))\n    \n    return xor_matrix.reshape(1, -1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 427, "library_problem_id": 136, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 428, "code": ["<code>\nimport numpy as np\n\ndef calculate_interval(arr):\n    mean = np.mean(arr)\n    std_dev = np.std(arr)\n    start = mean - 3*std_dev\n    end = mean + 3*std_dev\n    return (start, end)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 428, "library_problem_id": 137, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 429, "code": ["<code>\nimport numpy as np\n\n# Given array\narr = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\n\n# Calculate mean (\u03bc)\nmean = np.mean(arr)\n\n# Calculate standard deviation (\u03c3)\nstd_dev = np.std(arr)\n\n# Determine the interval (\u03bc - 2\u03c3, \u03bc + 2\u03c3)\ninterval = (mean - 2*std_dev, mean + 2*std_dev)\n\nprint(\"Mean: \", mean)\nprint(\"Standard Deviation: \", std_dev)\nprint(\"Interval: \", interval)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 429, "library_problem_id": 138, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 137}}
{"id": 430, "code": ["<code>\nimport numpy as np\n\ndef calculate_interval(arr=None):\n    if arr is None:\n        arr = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\n    mean = np.mean(arr)\n    std_dev = np.std(arr)\n    return (mean - 3*std_dev, mean + 3*std_dev)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 430, "library_problem_id": 139, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 137}}
{"id": 431, "code": ["<code>\nimport numpy as np\n\ndef calculate_outliers(arr):\n    # Calculate mean and standard deviation\n    mean = np.mean(arr)\n    std_dev = np.std(arr)\n\n    # Create a boolean array where True indicates an element that falls outside the interval (\u03bc - 2\u03c3, \u03bc + 2\u03c3)\n    outliers = np.abs(arr - mean) > 2 * std_dev\n\n    return outliers\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 431, "library_problem_id": 140, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 137}}
{"id": 432, "code": ["<code>\nimport numpy as np\n\ndef calculate_percentile(arr):\n    # Mask the array to exclude values less than 0 and equal to -3.40282347e+38\n    mask = (arr >= 0) & (arr != -3.40282347e+38)\n    arr_masked = arr[mask]\n\n    # Calculate the 50th percentile\n    percentile_50 = np.percentile(arr_masked, 50)\n\n    return percentile_50\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 432, "library_problem_id": 141, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 433, "code": ["<code>\nimport numpy as np\n\ndef zero_row_col(a, zero_rows, zero_cols):\n    a[zero_rows, :] = 0\n    a[:, zero_cols] = 0\n    return a\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 433, "library_problem_id": 142, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 142}}
{"id": 434, "code": ["<code>\nimport numpy as np\n\na = np.array([[0, 3, 1, 3], [3, 0, 0, 0], [1, 0, 0, 0], [3, 0, 0, 0]])\nzero_rows = [0, 1]\nzero_cols = [0, 1]\n\ndef zero_matrix(a, zero_rows, zero_cols):\n    for row in zero_rows:\n        a[row, :] = 0\n    for col in zero_cols:\n        a[:, col] = 0\n    return a\n\na = zero_matrix(a, zero_rows, zero_cols)\nprint(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 434, "library_problem_id": 143, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 435, "code": ["<code>\nprint(np.sum(a))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 435, "library_problem_id": 144, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 436, "code": ["<code>\ntotal_sum = np.sum(a)\nprint(total_sum)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 436, "library_problem_id": 145, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 145}}
{"id": 437, "code": ["<code>\nimport numpy as np\n\n# Given array\narr = np.array([[0, 1], [2, 1], [4, 8]])\n\n# Specified axis\naxis = 1\n\n# Create a mask where only the minimum value along the specified axis is True\nmask = np.zeros_like(arr, dtype=bool)\nmask[np.arange(arr.shape[0])[:, None], np.argmin(arr, axis=axis)] = True\n\nprint(mask)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 437, "library_problem_id": 146, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 145}}
{"id": 438, "code": ["<code>\nimport numpy as np\n\ndef calculate_pearson_correlation(postal_codes, distances):\n    # Convert the distance ranges to a list of upper bounds\n    if isinstance(distances[0], str):\n        distance_bounds = [int(range.split('-')[1].split('k')[0]) for range in distances]\n    else:\n        distance_bounds = distances\n\n    # Calculate the mean of the number of postal codes and the distance bounds\n    postal_codes_mean = np.mean(postal_codes)\n    distance_bounds_mean = np.mean(distance_bounds)\n\n    # Calculate the standard deviation of the number of postal codes and the distance bounds\n    postal_codes_stddev = np.std(postal_codes)\n    distance_bounds_stddev = np.std(distance_bounds)\n\n    # Calculate the Pearson correlation coefficient\n    numerator = 0\n    for i in range(len(postal_codes)):\n        numerator += (postal_codes[i] - postal_codes_mean) * (distance_bounds[i] - distance_bounds_mean)\n    denominator = len(postal_codes) * postal_codes_stddev * distance_bounds_stddev\n    pearson_correlation = numerator / denominator\n\n    return pearson_correlation\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 438, "library_problem_id": 147, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 439, "code": ["<code>\nimport numpy as np\nX = np.random.randint(2, 10, (5, 6))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 439, "library_problem_id": 148, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 440, "code": ["<code>\nimport numpy as np\n\ndef recover_matrix(Y):\n    N, M, _ = Y.shape\n    X = np.empty((M, N))\n    for i in range(N):\n        _, U = np.linalg.eig(Y[i])\n        X[:, i] = np.sqrt(U[:, 0]**2 + U[:, 1]**2 + U[:, 2]**2)\n    return X\n\nY = np.array([[[81, 63, 63],\n        [63, 49, 49],\n        [63, 49, 49]],\n\n       [[ 4, 12,  8],\n        [12, 36, 24],\n        [ 8, 24, 16]],\n\n       [[25, 35, 25],\n        [35, 50, 35],\n        [25, 35, 25]]])\n\nX = recover_matrix(Y)\nprint(X)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 440, "library_problem_id": 149, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 441, "code": ["<code>\nSure, I can help you with that. However, you didn't provide the function to be completed. Could you please provide the function you want to be completed?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 441, "library_problem_id": 150, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 442, "code": ["<code>\nimport numpy as np\n\n# Assuming A and B are your input arrays\nA = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\nB = np.array([2, 4, 6, 8, 10])\n\n# Use numpy's setdiff1d function to find the elements in A that are not in B\nC = np.setdiff1d(A, B)\n\nprint(C)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 442, "library_problem_id": 151, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 443, "code": ["<code>\nSure, I can help you with that. However, you didn't specify what kind of completion you need. Are you looking for a function to count the frequency of each element in the arrays, or are you looking for a function to find the unique elements in the arrays? Please provide more details so I can assist you better.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 443, "library_problem_id": 152, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 444, "code": ["<code>\nSure, I can help you with that. However, you didn't specify what kind of completion you need. Are you looking for a function to count the frequency of each element in the array, or are you looking for a function to find the unique elements in the array? Please provide more details so I can assist you better.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 444, "library_problem_id": 153, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 445, "code": ["<code>\nimport numpy as np\nfrom scipy.stats import rankdata\n\na = [1,2,3,4,3,2,3,4]\nranks = rankdata(a)\n\nprint(ranks)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 445, "library_problem_id": 154, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 446, "code": ["<code>\nimport numpy as np\nfrom scipy import stats\n\ndef rank_values(input_list):\n    return stats.rankdata(input_list) - 1\n\ninput_list = [1,2,3,4,3,2,3,4]\nprint(rank_values(input_list))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 446, "library_problem_id": 155, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 154}}
{"id": 447, "code": ["<code>\nranks = rankdata(a)\n    result = len(a) + 1 - ranks\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 447, "library_problem_id": 156, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 154}}
{"id": 448, "code": ["<code>\nimport numpy as np\n\ndef create_dists_array(x_dists, y_dists):\n    # Check if the input arrays have the same shape\n    if x_dists.shape != y_dists.shape:\n        raise ValueError(\"x_dists and y_dists must have the same shape\")\n\n    # Create a 3D array where each element is a tuple of the corresponding x and y distance\n    dists = np.dstack((x_dists, y_dists))\n\n    return dists\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 448, "library_problem_id": 157, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 449, "code": ["<code>\nimport numpy as np\n\nx_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\n\ny_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 449, "library_problem_id": 158, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 157}}
{"id": 450, "code": ["<code>\nimport numpy as np\n\ndef select_elements(arr, second_dim_indices, third_dim_indices):\n    try:\n        return arr[:, second_dim_indices, third_dim_indices]\n    except IndexError:\n        print(\"IndexError: One or more indices are out of range.\")\n        return None\n\n# Test the function\narr = np.arange(12).reshape(2, 2, 3)\nsecond_dim_indices = [0, 1]\nthird_dim_indices = [0, 2]\nprint(select_elements(arr, second_dim_indices, third_dim_indices))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 450, "library_problem_id": 159, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 159}}
{"id": 451, "code": ["<code>\nimport numpy as np\n\n# Create a 4-dimensional NumPy array with dimensions (20, 10, 10, 2) filled with zeros\narray = np.zeros((20, 10, 10, 2))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 451, "library_problem_id": 160, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 452, "code": ["<code>\nimport numpy as np\n\ndef normalize_l1(X):\n    # Calculate the L1 norm of each row\n    l1_norm = np.sum(np.abs(X), axis=1)\n\n    # Reshape the l1_norm to match the shape of X\n    l1_norm = l1_norm.reshape(-1, 1)\n\n    # Divide X by the corresponding L1 norm for each row\n    X = X / l1_norm\n\n    return X\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 452, "library_problem_id": 161, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 161}}
{"id": 453, "code": ["<code>\nimport numpy as np\n\ndef normalize_rows(X):\n    return np.array([x / np.linalg.norm(x) for x in X])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 453, "library_problem_id": 162, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 454, "code": ["<code>\nimport numpy as np\n\ndef normalize_rows(X):\n    # Compute the L-infinity norm of each row\n    row_norms = np.abs(X).max(axis=1)\n\n    # Reshape the row norms to match the shape of X\n    row_norms = row_norms.reshape(-1, 1)\n\n    # Divide each element in X by the corresponding row norm\n    result = X / row_norms\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 454, "library_problem_id": 163, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 455, "code": ["<code>\nimport numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame({'a': ['/blog/', 'credit-card-readers/', 'signup', 'complete', '/za/', 'promo']})\n\nconditions = [\n    df['a'].str.contains('blog'),\n    df['a'].str.contains('credit-card-readers/|machines|poss|team|transaction_fees'),\n    df['a'].str.contains('signup|sign-up|create-account|continue|checkout'),\n    df['a'].str.contains('complete'),\n    df['a'].str.contains('/za/|/'),\n    df['a'].str.contains('promo')\n]\n\nchoices = ['blog', 'info_pages', 'signup', 'completed', 'home_page', 'promo']\n\ndf['new_column'] = np.select(conditions, choices, default=np.nan)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 455, "library_problem_id": 164, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 164}}
{"id": 456, "code": ["<code>\nimport numpy as np\n\ndef calculate_pairwise_distances(dataset):\n    # Calculate the squared Euclidean distance between each pair of the two collections of inputs\n    diff = np.expand_dims(dataset, axis=1) - np.expand_dims(dataset, axis=0)\n    squared_diff = diff ** 2\n    sum_squared_diff = squared_diff.sum(axis=-1)\n\n    # Take the square root of the sum to get the Euclidean distance\n    distances = np.sqrt(sum_squared_diff)\n\n    return distances\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 456, "library_problem_id": 165, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 457, "code": ["<code>\nimport numpy as np\n\ndef pairwise_distances(X):\n    # Calculate the squared Euclidean distance for each pair of points\n    # The formula is: d(x, y) = sqrt((x - y)^2)\n    # We use the broadcasting feature of numpy to avoid explicit for loops\n    differences = X[:, np.newaxis, :] - X[np.newaxis, :, :]\n    squares = differences ** 2\n    distances = np.sqrt(squares.sum(axis=-1))\n\n    # Make the distance matrix symmetric by averaging the upper and lower triangles\n    # This is done to avoid double counting of distances\n    mask = np.tril(np.ones_like(distances), -1)\n    distances = (distances + distances.T) / 2\n    distances = distances * mask\n\n    return distances\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 457, "library_problem_id": 166, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 165}}
{"id": 458, "code": ["<code>\nimport numpy as np\nfrom scipy.spatial import distance_matrix\n\ndef calculate_pairwise_distances(arr=None):\n    if arr is None:\n        np.random.seed(0)\n        arr = np.random.rand(np.random.randint(5, 11), np.random.randint(4, 9))\n    elif arr.shape[0] < 5 or arr.shape[0] > 10 or arr.shape[1] < 4 or arr.shape[1] > 8:\n        raise ValueError(\"Input array should have between 5 and 10 rows and between 4 and 8 columns.\")\n    \n    return distance_matrix(arr, arr)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 458, "library_problem_id": 167, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 165}}
{"id": 459, "code": ["<code>\nimport numpy as np\n\nA = ['33.33', '33.33', '33.33', '33.37']\n\n# Convert the list to a numpy array and specify the data type as float\nA_array = np.array(A, dtype=float)\n\n# Calculate the mean without using explicit loops\nAVG = np.mean(A_array, axis=0)\n\nprint(AVG)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 459, "library_problem_id": 168, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 168}}
{"id": 460, "code": ["<code>\nimport numpy as np\nA = ['inf', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 460, "library_problem_id": 169, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 168}}
{"id": 461, "code": ["<code>\nimport numpy as np\n\ndef calculate_mean(arr):\n    # Convert all strings to float, replacing 'np.inf' with infinity\n    arr = [float(i) if i != 'np.inf' else np.inf for i in arr]\n    \n    # Calculate the mean\n    mean = np.mean(arr)\n    \n    return mean\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 461, "library_problem_id": 170, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 168}}
{"id": 462, "code": ["<code>\nSure, I can help you with that. However, you didn't specify what kind of completion you want to do. Are you looking for a function to count the unique values in the array, or to remove duplicates, or something else? Please provide more details so I can assist you better.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 462, "library_problem_id": 171, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 171}}
{"id": 463, "code": ["<code>\nSure, I can help you with that. However, you didn't provide the function to be completed. Could you please provide the function you want me to complete?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 463, "library_problem_id": 172, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 171}}
{"id": 464, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\nlat=np.array([[10, 20, 30],\n              [20, 11, 33],\n              [21, 20, 10]])\n\nlon=np.array([[100, 102, 103],\n              [105, 101, 102],\n              [100, 102, 103]])\n\nval=np.array([[17, 2, 11],\n              [86, 84, 1],\n              [9, 5, 10]])\n\ndf = pd.DataFrame({\n    'lat': lat.flatten(),\n    'lon': lon.flatten(),\n    'val': val.flatten()\n})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 464, "library_problem_id": 173, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 465, "code": ["<code>\nimport numpy as np\nimport pandas as pd\n\ndef create_dataframe(lat=np.array([1, 2, 3]), lon=np.array([4, 5, 6]), val=np.array([7, 8, 9])):\n    df = pd.DataFrame({\n        'lat': lat.flatten(),\n        'lon': lon.flatten(),\n        'val': val.flatten()\n    })\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 465, "library_problem_id": 174, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 173}}
{"id": 466, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\n# Given arrays\nlatitude = np.array([[10, 20, 30], [20, 11, 33], [21, 20, 10]])\nlongitude = np.array([[100, 102, 103], [105, 101, 102], [100, 102, 103]])\nvalue = np.array([[17, 2, 11], [86, 84, 1], [9, 5, 10]])\n\n# Create DataFrame\ndf = pd.DataFrame({\n    'latitude': latitude.flatten(),\n    'longitude': longitude.flatten(),\n    'value': value.flatten(),\n})\n\n# Add new column with maximum value of each row\ndf['max_value'] = df[['latitude', 'longitude', 'value']].max(axis=1)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 466, "library_problem_id": 175, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 173}}
{"id": 467, "code": ["<code>\nimport numpy as np\n\ndef moving_window(arr, window_size):\n    # Get the shape of the array\n    arr_shape = np.shape(arr)\n    \n    # Get the window dimensions\n    window_height, window_width = window_size\n    \n    # Check if the window dimensions are larger than the array dimensions\n    if window_height > arr_shape[0] or window_width > arr_shape[1]:\n        raise ValueError(\"Window size is larger than the array size.\")\n    \n    # Initialize an empty list to store the window views\n    window_views = []\n    \n    # Iterate over the array\n    for i in range(arr_shape[0]):\n        for j in range(arr_shape[1]):\n            # Get the start and end indices for the window\n            start_i = max(0, i - window_height // 2)\n            end_i = min(arr_shape[0], i + window_height // 2 + 1)\n            start_j = max(0, j - window_width // 2)\n            end_j = min(arr_shape[1], j + window_width // 2 + 1)\n            \n            # Get the window view and append it to the list\n            window_views.append(arr[start_i:end_i, start_j:end_j])\n    \n    return window_views\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 467, "library_problem_id": 176, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 176}}
{"id": 468, "code": ["<code>\nimport numpy as np\n\ndef moving_window(arr, window_size):\n    # Get the shape of the array\n    arr_shape = np.array(arr.shape)\n    \n    # Get the window size\n    win_size = np.array(window_size)\n    \n    # Calculate the start and end indices for each dimension\n    start_indices = np.maximum(0, win_size // 2)\n    end_indices = np.minimum(arr_shape, arr_shape + win_size // 2)\n    \n    # Initialize an empty list to store the windows\n    windows = []\n    \n    # Iterate over each dimension\n    for i in range(arr.ndim):\n        # Get the start and end index for this dimension\n        start, end = start_indices[i], end_indices[i]\n        \n        # Iterate over the range of indices for this dimension\n        for j in range(start, end):\n            # Get the start and end index for this dimension\n            start_j, end_j = max(0, j - win_size[i] // 2), min(arr_shape[i], j + win_size[i] // 2)\n            \n            # Get the window for this dimension\n            window = arr[..., start_j:end_j]\n            \n            # Append the window to the list\n            windows.append(window)\n            \n    return windows\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 468, "library_problem_id": 177, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 176}}
{"id": 469, "code": ["<code>\nimport numpy as np\nimport warnings\n\ndef mean_of_complex_array(arr):\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n        return np.nanmean(arr, dtype=np.complex)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 469, "library_problem_id": 178, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 178}}
{"id": 470, "code": ["<code>\nimport numpy as np\n\ndef mean_of_complex_array(arr):\n    # Replace complex infinity with NaN\n    arr = np.where(np.isinf(arr), np.nan, arr)\n\n    # Compute mean ignoring NaN values\n    mean = np.nanmean(arr, dtype=np.complex128)\n\n    return mean\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 470, "library_problem_id": 179, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 178}}
{"id": 471, "code": ["<code>\nimport numpy as np\n\ndef get_last_element(arr):\n    return arr[tuple(slice(-1) for _ in range(arr.ndim))]\n\n# Testing the function\narr1 = np.array([1, 2, 3, 4, 5])\narr2 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\narr3 = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n\nprint(get_last_element(arr1))  # Output: 5\nprint(get_last_element(arr2))  # Output: 9\nprint(get_last_element(arr3))  # Output: 12\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 471, "library_problem_id": 180, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 180}}
{"id": 472, "code": ["<code>\nimport numpy as np\n\ndef select_last_element(arr):\n    return arr[tuple(slice(-1) if i != 0 else None for i in range(arr.ndim))]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 472, "library_problem_id": 181, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 180}}
{"id": 473, "code": ["<code>\nimport numpy as np\n\ndef is_array_in_list(arr, arr_list):\n    for item in arr_list:\n        if np.array_equal(item, arr):\n            return True\n    return False\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 473, "library_problem_id": 182, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 182}}
{"id": 474, "code": ["<code>\nimport numpy as np\n\ndef is_array_in_list(arr, arr_list):\n    for item in arr_list:\n        if np.array_equal(arr, item):\n            return True\n    return False\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 474, "library_problem_id": 183, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 182}}
{"id": 475, "code": ["<code>\nimport numpy as np\nfrom scipy.interpolate import interp2d\n\ndef upsample_2d_array(arr):\n    if arr.size == 0:\n        return arr\n    n, m = arr.shape\n    x = np.arange(m)\n    y = np.arange(n)\n    f = interp2d(x, y, arr, kind='linear')\n    x_new = np.linspace(0, m-1, 2*m)\n    y_new = np.linspace(0, n-1, 2*n)\n    return f(x_new, y_new)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 475, "library_problem_id": 184, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 184}}
{"id": 476, "code": ["<code>\nimport pandas as pd\n\n# Create a dictionary\ndata = {'D': [2015, 2015, 2015, 2015, 2016, 2016, 2016, 2017, 2017, 2017],\n        'Q': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}\n\n# Create a DataFrame\ndf = pd.DataFrame(data)\n\n# Compute the cumulative sum of 'Q' column values based on the unique values in the 'D' column\ndf['Q_cum'] = df.groupby('D')['Q'].cumsum()\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 476, "library_problem_id": 185, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 477, "code": ["<code>\nimport numpy as np\n\na = np.matrix([[3, 4, 3, 1],[1,3,2,6],[2,4,1,5],[3,3,5,2]])\nU, s, V = np.linalg.svd(a,full_matrices=True)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 477, "library_problem_id": 186, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 186}}
{"id": 478, "code": ["<code>\nSure, I can help you with that. However, you didn't specify what kind of completion you want to do. Are you looking for a function to calculate the determinant of the matrix, or do you want to find the inverse of the matrix, or something else? Please provide more details so I can assist you better.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 478, "library_problem_id": 187, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 187}}
{"id": 479, "code": ["<code>\nimport pandas as pd\n\ndef generate_datetime_array(start, end, num_elements):\n    \"\"\"\n    Generate an array of equally spaced datetime objects between a start and end time, given the number of desired elements.\n\n    Parameters:\n    start (datetime): The start time.\n    end (datetime): The end time.\n    num_elements (int): The number of desired elements.\n\n    Returns:\n    pandas.DatetimeIndex: An array of equally spaced datetime objects.\n    \"\"\"\n    # Calculate the total time difference\n    total_seconds = (end - start).total_seconds()\n\n    # Calculate the step size\n    step_size = total_seconds / (num_elements - 1)\n\n    # Generate the array\n    array = [start + pd.Timedelta(seconds=step_size * i) for i in range(num_elements)]\n\n    return pd.DatetimeIndex(array)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 479, "library_problem_id": 188, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 480, "code": ["<code>\nimport numpy as np\n\ndef find_index(x, y):\n    for i in range(len(x)):\n        if (x[i], y[i]) == (1, 4):\n            return i\n    return -1\n\nx = np.array([0, 1, 1, 1, 3, 1, 5, 5, 5])\ny = np.array([0, 2, 3, 4, 2, 4, 3, 4, 5])\n\nprint(find_index(x, y))  # Output: 3\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 480, "library_problem_id": 189, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 189}}
{"id": 481, "code": ["<code>\nimport numpy as np\n\ndef find_indices(x, y, a, b):\n    indices = np.where((x == a) & (y == b))[0]\n    return indices\n\nx = np.array([0, 1, 1, 1, 3, 1, 5, 5, 5])\ny = np.array([0, 2, 3, 4, 2, 4, 3, 4, 5])\n\nprint(find_indices(x, y, 1, 4))  # Output: [3 5]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 481, "library_problem_id": 190, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 189}}
{"id": 482, "code": ["<code>\nimport numpy as np\n\ndef quadratic_fit(x, y):\n    # Convert the lists to numpy arrays\n    x = np.array(x)\n    y = np.array(y)\n\n    # Create the matrix A using the given formula\n    A = np.array([np.ones(len(x)), x, x**2]).T\n\n    # Use the least squares method to solve for the coefficients\n    coeff, _, _, _ = np.linalg.lstsq(A, y, rcond=None)\n\n    return coeff\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 482, "library_problem_id": 191, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 191}}
{"id": 483, "code": ["<code>\nimport numpy as np\n\ndef fit_polynomial(x, y, degree):\n    # Create a Vandermonde matrix of the input x values\n    vander_matrix = np.vander(x, N=degree+1)\n    \n    # Use the least squares method to find the coefficients\n    coefficients = np.linalg.lstsq(vander_matrix, y, rcond=None)[0]\n    \n    # Return the coefficients in descending order\n    return coefficients[::-1]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 483, "library_problem_id": 192, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 191}}
{"id": 484, "code": ["<code>\nimport pandas as pd\n\ndef subtract_corresponding_elements(temp_df, temp_arr):\n    temp_df.sub(temp_arr, axis=0)\n\n# Test the function\ntemp_df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ntemp_arr = [1, 2, 3]\n\nsubtract_corresponding_elements(temp_df, temp_arr)\n\nprint(temp_df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 484, "library_problem_id": 193, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 193}}
{"id": 485, "code": ["<code>\nimport numpy as np\n\n# Assuming A and B are your input matrices\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nB = np.array([[10, 11, 12], [13, 14, 15], [16, 17, 18]])\n\n# Element-wise multiplication\nprint(np.multiply(B, A))\n\n# Matrix multiplication using einsum\nprint(np.einsum('ijk,jl->ilk', B, A))\n\n# Another matrix multiplication using einsum\nprint(np.einsum('ijk,jl->ilk', A, B))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 485, "library_problem_id": 194, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 486, "code": ["<code>\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\na = np.array([[-1, 2], [-0.5, 6]])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 486, "library_problem_id": 195, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 195}}
{"id": 487, "code": ["<code>\nimport numpy as np\n\ndef rescale_array(arr):\n    # Compute the minimum and maximum values along each row\n    row_min = np.min(arr, axis=1)[:, np.newaxis]\n    row_max = np.max(arr, axis=1)[:, np.newaxis]\n    \n    # Subtract the minimum and divide by the range to rescale the values\n    rescaled_arr = (arr - row_min) / (row_max - row_min)\n    \n    return rescaled_arr\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 487, "library_problem_id": 196, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 488, "code": ["<code>\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\na = np.array([[[1, 0.5, -2], [-0.5,1, 6], [1,1,1]], [[-2, -3, 1], [-0.5, 10, 6], [1,1,1]]])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 488, "library_problem_id": 197, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 489, "code": ["<code>\nimport numpy as np\n\n# Initialize a 2D NumPy array with random values between -0.5 and 24.5\narr = np.random.uniform(-0.5, 24.5, (10, 10))\n\n# Use Boolean indexing to solve the problem\narr[(arr < 15) & (arr >= -10)] += 5\narr[arr >= 15] = 30\narr[arr < -10] = 0\n\nprint(arr)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 489, "library_problem_id": 198, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 198}}
{"id": 490, "code": ["<code>\narr_temp = arr.copy()\nmask = arr_temp < n1\nmask2 = arr_temp < n2\nmask3 = mask ^ mask2\narr[mask] = 0\narr[mask3] = arr[mask3] + 5\narr[~mask2] = 30\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 490, "library_problem_id": 199, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 198}}
{"id": 491, "code": ["<code>\nimport numpy as np\n\ndef compare_sums(arr1, arr2):\n    # Calculate the sums of elements in each row\n    sums1 = np.sum(arr1, axis=1)\n    sums2 = np.sum(arr2, axis=1)\n\n    # Initialize the count of elements that are different\n    count = 0\n\n    # Compare the sums\n    for i in range(len(sums1)):\n        # Account for the limitations of floating-point arithmetic\n        if not np.isclose(sums1[i], sums2[i]):\n            count += 1\n\n    return count\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 491, "library_problem_id": 200, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 200}}
{"id": 492, "code": ["<code>\nimport numpy as np\n\ndef compare_arrays(arr1, arr2):\n    # Append a NaN value to the end of each array\n    arr1 = np.append(arr1, np.nan)\n    arr2 = np.append(arr2, np.nan)\n\n    # Convert the arrays to a set to remove duplicates and NaN values\n    arr1_set = set(arr1[~np.isnan(arr1)])\n    arr2_set = set(arr2[~np.isnan(arr2)])\n\n    # Compare the sets and return the number of elements that are truly different\n    return len(arr1_set ^ arr2_set)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 492, "library_problem_id": 201, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 200}}
{"id": 493, "code": ["<code>\nimport numpy as np\n\ndef check_equal_arrays(arrays):\n    try:\n        return all(np.array_equal(arr, arrays[0]) for arr in arrays)\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 493, "library_problem_id": 202, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 494, "code": ["<code>\nimport numpy as np\n\na = [np.array([np.nan,2,3]),np.array([1,np.nan,3]),np.array([1,2,np.nan])]\n\n# Convert the list of arrays to a numpy array\na = np.array(a)\n\n# Calculate the mean of each column\nmean_values = np.nanmean(a, axis=0)\n\n# Replace np.nan values in the original array with the mean values\nfor i in range(a.shape[1]):\n    a[:,i][np.isnan(a[:,i])] = mean_values[i]\n\nprint(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 494, "library_problem_id": 203, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 495, "code": ["<code>\nimport numpy as np\n\ndef zero_pad_array(array, target_shape):\n    # Get the current shape of the array\n    current_shape = np.shape(array)\n    \n    # Calculate the padding needed for each dimension\n    pad_width = [(0, max(0, target_shape[i] - current_shape[i])) for i in range(2)]\n    \n    # Pad the array with zeros\n    padded_array = np.pad(array, pad_width)\n    \n    return padded_array\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 495, "library_problem_id": 204, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 204}}
{"id": 496, "code": ["<code>\nimport numpy as np\n\n# Assuming 'a' is your input array\na = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n# Calculate the padding dimensions\npad_width = ((0, 93 - a.shape[0]), (0, 13 - a.shape[1]))\n\n# Pad the array\nresult = np.pad(a, pad_width)\n\n# Print the result\nprint(result.shape)  # Should print (93, 13)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 496, "library_problem_id": 205, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 497, "code": ["<code>\nimport numpy as np\n\ndef pad_array(arr, target_shape, pad_value):\n    # Create a new array with the target shape and pad value\n    padded_arr = np.full(target_shape, pad_value)\n    \n    # Get the original array's shape\n    original_shape = arr.shape\n    \n    # Copy the original array into the padded array\n    padded_arr[:original_shape[0], :original_shape[1]] = arr\n    \n    return padded_arr\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 497, "library_problem_id": 206, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 204}}
{"id": 498, "code": ["<code>\nimport numpy as np\nexample_arr = np.ones((41, 13))\ndef f(arr = example_arr, shape=(93,13)):\n    # Reshape the array to the desired shape\n    arr = np.reshape(arr, shape)\n    return arr\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 498, "library_problem_id": 207, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 499, "code": ["<code>\nimport numpy as np\n\ndef symmetric_padding(arr, target_shape):\n    # Calculate the difference between the target shape and the current shape\n    diff_shape = np.subtract(target_shape, arr.shape)\n    \n    # Calculate the symmetric padding for each dimension\n    pad_width = tuple((diff_shape // 2, diff_shape - diff_shape // 2))\n    \n    # Apply the padding\n    return np.pad(arr, pad_width)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 499, "library_problem_id": 208, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 204}}
{"id": 500, "code": ["<code>\nimport numpy as np\n\ndef reshape_array(length):\n    # Create a list with the given length\n    lst = list(range(length))\n    \n    # Convert the list to a NumPy array\n    arr = np.array(lst)\n    \n    # Calculate the number of rows for the reshaped array\n    # If the length of the list is not a multiple of 3, the number of rows will be the quotient plus one\n    # If the length of the list is a multiple of 3, the number of rows will be the quotient\n    rows = length // 3 if length % 3 == 0 else length // 3 + 1\n    \n    # Reshape the array into a matrix with 3 columns\n    reshaped_arr = arr.reshape(rows, 3)\n    \n    return reshaped_arr\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 500, "library_problem_id": 209, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 501, "code": ["<code>\nimport numpy as np\n\ndef select_from_3d(a, b):\n    # Check if the shapes of a and b are compatible\n    if a.shape[:2] != b.shape:\n        raise ValueError(\"Shapes of a and b are not compatible\")\n\n    # Create a mask from b\n    mask = b[:, :, np.newaxis]\n\n    # Select elements from a based on the mask\n    result = np.where(mask, a[:, :, 0], a[:, :, 1])\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 501, "library_problem_id": 210, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 210}}
{"id": 502, "code": ["<code>\nimport numpy as np\n\ndef select_from_3d(a, b):\n    return np.where(b==0, a[:,:,0], a[:,:,1])\n\na = np.array(\n    [[[ 0,  1],\n     [ 2,  3],\n     [ 4,  5]],\n    [[ 6,  7],\n     [ 8,  9],\n     [10, 11]],\n    [[12, 13],\n     [14, 15],\n     [16, 17]]]\n)\n\nb = np.array(\n    [[1, 1, 1],\n    [1, 1, 0]]\n)\n\nprint(select_from_3d(a, b))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 502, "library_problem_id": 211, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 210}}
{"id": 503, "code": ["<code>\nimport numpy as np\n\ndef select_from_3d(a, b):\n    return np.take_along_axis(a, b[:, :, np.newaxis], axis=2).squeeze(-1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 503, "library_problem_id": 212, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 210}}
{"id": 504, "code": ["<code>\nimport numpy as np\n\ndef compute_sum(a, b):\n    return np.sum(a[np.arange(a.shape[0])[:, None], np.arange(a.shape[1]), b])\n\na = np.array(\n    [[[ 0,  1, 2, 3],\n     [ 2,  3, 4, 5],\n     [ 4,  5, 6, 7]],\n    [[ 6,  7, 8, 9],\n     [ 8,  9, 10, 11],\n     [10, 11, 12, 13]],\n    [[12, 13, 14, 15],\n     [14, 15, 16, 17],\n     [16, 17, 18, 19]]])\n\nb = np.array(\n    [[0, 1, 2, 3],\n     [2, 3, 0, 1],\n     [1, 2, 3, 0]])\n\nprint(compute_sum(a, b))  # Output: 130\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 504, "library_problem_id": 213, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 505, "code": ["<code>\nimport numpy as np\n\ndef calculate_sum(a, b):\n    # Create a boolean mask where True indicates the indices in b are valid\n    mask = np.arange(a.shape[2]) != b\n\n    # Apply the mask to the 3D array a\n    a_masked = a[:,:,mask]\n\n    # Return the sum of all elements in a_masked\n    return np.sum(a_masked)\n\na = np.array(\n    [[[ 0,  1, 2, 3],\n     [ 2,  3, 4, 5],\n     [ 4,  5, 6, 7]],\n    [[ 6,  7, 8, 9],\n     [ 8,  9, 10, 11],\n     [10, 11, 12, 13]],\n    [[12, 13, 14, 15],\n     [14, 15, 16, 17],\n     [16, 17, 18, 19]]]\n)\n\nb = np.array(\n    [[0, 1, 2],\n     [0, 1, 2],\n     [0, 1, 2]]\n)\n\nprint(calculate_sum(a, b))  # Output: 130\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 505, "library_problem_id": 214, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 506, "code": ["<code>\nIt seems like you've provided a table of data, but you haven't specified what kind of function you're trying to complete. Are you trying to create a function that can process this data, or are you trying to create a function that can manipulate it in some way? \n\nPlease provide more context or details so I can assist you better.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 506, "library_problem_id": 215, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 507, "code": ["<code>\nimport numpy as np\n\ndef remove_black_borders(image):\n    # Find the rows and columns that contain at least one non-zero pixel\n    rows = np.any(image, axis=1)\n    cols = np.any(image, axis=0)\n\n    # Find the first and last row and column that contain a non-zero pixel\n    rmin, rmax = np.where(rows)[0][[0, -1]]\n    cmin, cmax = np.where(cols)[0][[0, -1]]\n\n    # Return the central non-zero region\n    return image[rmin:rmax+1, cmin:cmax+1]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 507, "library_problem_id": 216, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 216}}
{"id": 508, "code": ["<code>\nimport numpy as np\n\ndef remove_zero_rows_cols(arr):\n    # Remove rows with all zeros\n    arr = arr[~np.all(arr==0, axis=1)]\n    \n    # Remove columns with all zeros\n    arr = arr[:, ~np.all(arr==0, axis=0)]\n    \n    return arr\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 508, "library_problem_id": 217, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 216}}
{"id": 509, "code": ["<code>\nimport numpy as np\n\ndef process_image(im):\n    # Get the non-zero rows and columns\n    non_zero_rows = np.any(im, axis=1)\n    non_zero_cols = np.any(im, axis=0)\n\n    # Get the indices of the non-zero rows and columns\n    row_indices, = np.where(non_zero_rows)\n    col_indices, = np.where(non_zero_cols)\n\n    # If all rows and columns are removed, return an empty array\n    if len(row_indices) == 0 or len(col_indices) == 0:\n        return np.array([])\n\n    # Get the first and last indices of the non-zero rows and columns\n    first_row, last_row = row_indices[0], row_indices[-1]\n    first_col, last_col = col_indices[0], col_indices[-1]\n\n    # Return the processed image\n    result = im[first_row:last_row+1, first_col:last_col+1]\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 509, "library_problem_id": 218, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 510, "code": ["<code>\nimport numpy as np\n\ndef crop_image(image):\n    # Find the rows and columns that contain at least one non-zero pixel\n    rows = np.any(image, axis=1)\n    cols = np.any(image, axis=0)\n\n    # Use these to crop the image\n    cropped_image = image[np.ix_(rows, cols)]\n\n    return cropped_image\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 510, "library_problem_id": 219, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 511, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate 10 random numbers\nx = np.random.rand(10)\ny = x\n\n# Plot x against y\nplt.plot(x, y)\n\n# Label the x and y axes\nplt.xlabel('x-y')\nplt.ylabel('x-y')\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 511, "library_problem_id": 0, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 512, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef scatter_plot():\n    # Generate some data\n    x = np.linspace(0, 10, 100)\n    y = np.sin(x)\n\n    # Create the scatter plot\n    plt.scatter(x, y)\n\n    # Set the y-axis ticks to be minor\n    plt.gca().yaxis.set_tick_params(which='minor', size=5, width=1, color='black')\n\n    # Display the plot\n    plt.show()\n\n# Call the function\nscatter_plot()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 512, "library_problem_id": 1, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 513, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create some data\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\n# Create the scatter plot\nplt.scatter(x, y)\n\n# Enable minor ticks\nplt.gca().xaxis.set_minor_locator(plt.MultipleLocator(1))\nplt.gca().yaxis.set_minor_locator(plt.MultipleLocator(0.1))\n\n# Customize the appearance of the minor ticks\nplt.gca().xaxis.set_minor_formatter(plt.FuncFormatter(lambda x, pos: '%.1f' % (x)))\nplt.gca().yaxis.set_minor_formatter(plt.FuncFormatter(lambda y, pos: '%.1f' % (y)))\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 513, "library_problem_id": 2, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 514, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef scatter_plot():\n    # Generate some data\n    x = np.linspace(0, 10, 100)\n    y = np.sin(x)\n\n    # Create the scatter plot\n    plt.scatter(x, y)\n\n    # Set the x-axis ticks to be only minor\n    plt.xticks(np.arange(0, max(x), 0.5), minor=True)\n\n    # Show the plot\n    plt.show()\n\n# Call the function\nscatter_plot()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 514, "library_problem_id": 3, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 515, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef generate_line_plot():\n    # Generate random y-values\n    y_values = np.random.rand(10, 5)\n\n    # Create a dataframe\n    df = pd.DataFrame(y_values, columns=['Line 1', 'Line 2', 'Line 3', 'Line 4', 'Line 5'])\n\n    # Set x-values\n    df['x'] = range(10)\n\n    # Set line styles\n    line_styles = ['-', '--', '-.', ':', '-']\n\n    # Create the plot\n    plt.figure(figsize=(10, 6))\n\n    for i, style in enumerate(line_styles):\n        plt.plot('x', 'Line '+str(i+1), data=df, linestyle=style)\n\n    plt.title('Line Plot with Random Y-Values')\n    plt.xlabel('X-Values')\n    plt.ylabel('Y-Values')\n\n    return plt.show()\n\ngenerate_line_plot()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 515, "library_problem_id": 4, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 516, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef generate_line_plot():\n    # Generate random y-values\n    y_values = np.random.rand(10, 5)\n\n    # Create a dataframe\n    df = pd.DataFrame(y_values, columns=['Line 1', 'Line 2', 'Line 3', 'Line 4', 'Line 5'])\n\n    # Set x-values\n    df['x'] = range(10)\n\n    # Set line styles\n    line_styles = ['-', '--', '-.', ':', '-']\n\n    # Create the plot\n    plt.figure(figsize=(10, 6))\n\n    for i, style in enumerate(line_styles):\n        plt.plot('x', 'Line '+str(i+1), data=df, linestyle=style)\n\n    plt.title('Line Plot with Random Y-Values')\n    plt.xlabel('X-Values')\n    plt.ylabel('Y-Values')\n\n    return plt.show()\n\ngenerate_line_plot()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 516, "library_problem_id": 5, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 517, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate x and y values\nx = np.arange(0, 10)\ny = np.random.rand(10)\n\n# Create line plot\nplt.plot(x, y, marker='d', linestyle='-', linewidth=1)\n\n# Show plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 517, "library_problem_id": 6, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 518, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create x and y data\nx = np.arange(0, 10)\ny = np.random.rand(10)\n\n# Create line plot\nplt.plot(x, y, marker='D', markersize=10, linestyle='-')\n\n# Show plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 518, "library_problem_id": 7, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 519, "code": ["<code>\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef create_boxplot():\n    # Load the 'tips' dataset from Seaborn\n    tips = sns.load_dataset('tips')\n\n    # Create a boxplot using the 'total_bill' column\n    sns.boxplot(y='total_bill', data=tips)\n\n    # Set the y-axis limit to 0 to 40\n    plt.ylim(0, 40)\n\n    # Return the final plot\n    return plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 519, "library_problem_id": 8, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 8}}
{"id": 520, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_highlighted_range(data):\n    plt.figure(figsize=(10, 5))\n    plt.plot(data)\n\n    # Highlight the range from 2 to 4 in red\n    plt.axvspan(2, 4, color='red', alpha=0.3)\n\n    plt.show()\n    return plt\n\n# Test the function\ndata = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\nplot_highlighted_range(data)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 520, "library_problem_id": 9, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 521, "code": ["<code>\nimport matplotlib.pyplot as plt\n\ndef draw_line():\n    # Create a new figure\n    plt.figure()\n    \n    # Define the coordinates of the line\n    x_coords = [0, 1]\n    y_coords = [0, 2]\n    \n    # Plot the line\n    plt.plot(x_coords, y_coords)\n    \n    # Show the plot\n    plt.show()\n\n# Call the function\ndraw_line()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 521, "library_problem_id": 10, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 522, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef draw_line():\n    # Create a figure and an axes\n    fig, ax = plt.subplots()\n\n    # Set the x and y data\n    x = np.array([0, 1])\n    y = np.array([0, 2])\n\n    # Plot the line\n    ax.plot(x, y)\n\n    # Show the plot\n    plt.show()\n\n# Call the function\ndraw_line()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 522, "library_problem_id": 11, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 523, "code": ["<code>\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming df is your DataFrame and 'Height (cm)', 'Weight (kg)', and 'Gender' are the columns\nplt.figure(figsize=(10, 6))\nsns.set_style(\"ticks\")\nsns.relplot(x='Height (cm)', y='Weight (kg)', hue='Gender', data=df)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 523, "library_problem_id": 12, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 12}}
{"id": 524, "code": ["<code>\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef create_scatter_plot():\n    x = np.arange(0, 10)\n    y = np.random.rand(10)\n\n    sns.set_style(\"darkgrid\")\n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(x=x, y=y, color=\"blue\")\n    plt.title(\"Scatter Plot\")\n    plt.xlabel(\"X\")\n    plt.ylabel(\"Y\")\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 524, "library_problem_id": 13, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 525, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Generate x values\nx = np.arange(10)\n\n# Calculate y values as sine of x\ny = np.sin(x)\n\n# Create a dataframe\ndf = pd.DataFrame({'x': x, 'y': y})\n\n# Create a line plot\nsns.lineplot(x='x', y='y', data=df)\n\n# Display the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 525, "library_problem_id": 14, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 526, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create numpy arrays 'x' and 'y' with 10 random numbers each\nx = np.random.rand(10)\ny = np.random.rand(10)\n\n# Plot a line graph with 'x' as x-coordinates, 'y' as y-coordinates, \n# '+' marker, line thickness of 7\nplt.plot(x, y, '+-', linewidth=7)\n\n# Display the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 526, "library_problem_id": 15, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 527, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_cosine():\n    x = np.linspace(0, 2*np.pi, 100)\n    y = np.cos(x)\n\n    plt.plot(x, y, label='cos')\n    plt.legend(fontsize=20)\n    plt.show()\n\nplot_cosine()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 527, "library_problem_id": 16, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 528, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_cosine():\n    x = np.linspace(0, 2*np.pi, 10)\n    y = np.cos(x)\n\n    plt.plot(x, y)\n    plt.legend(['cosine'], title='xyz', title_fontsize=20)\n    plt.show()\n\nplot_cosine()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 528, "library_problem_id": 17, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 529, "code": ["<code>\nimport matplotlib.pyplot as plt\n\ndef set_marker_alpha():\n    fig, ax = plt.subplots()\n    line = ax.plot([0, 1, 2, 3], [0, 1, 2, 3], marker='o', color='b', linestyle='None')\n    plt.setp(line, markersize=10, markerfacecolor='none', markeredgecolor='b', alpha=0.2)\n    return line\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 529, "library_problem_id": 18, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 530, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef generate_plot():\n    x = np.arange(10)\n    y = np.random.randn(10)\n\n    line, = plt.plot(x, y, 'ko-', linewidth=5, markersize=30)\n\n    return line\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 530, "library_problem_id": 19, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 531, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef generate_line_plot():\n    x = np.linspace(0, 10, 100)\n    y = np.sin(x)\n\n    line, = plt.plot(x, y, color='red', linewidth=5, marker='o', markersize=30, linestyle='-')\n\n    return line\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 531, "library_problem_id": 20, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 532, "code": ["<code>\nimport matplotlib.pyplot as plt\n\ndef rotate_x_axis_labels():\n    fig, ax = plt.subplots()\n\n    # Your plotting code here\n\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n\n    plt.show()\n\nrotate_x_axis_labels()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 532, "library_problem_id": 21, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 21}}
{"id": 533, "code": ["<code>\nimport matplotlib.pyplot as plt\n\n# Assuming you have a plot already created\nplt.plot([1, 2, 3, 4, 5], [1, 4, 9, 16, 25])\n\n# Rotate the x-axis labels counterclockwise by 45 degrees\nplt.xticks(rotation=-45)\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 533, "library_problem_id": 22, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 534, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_cosine_wave():\n    x = np.linspace(0, 2*np.pi, 100)\n    y = np.cos(x)\n\n    plt.plot(x, y, label='cos')\n    plt.xticks([0, np.pi/2, np.pi, 3*np.pi/2, 2*np.pi], ['0', '$\\pi/2$', '$\\pi$', '$3\\pi/2$', '$2\\pi$'])\n    plt.legend()\n    plt.show()\n\nplot_cosine_wave()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 534, "library_problem_id": 23, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 535, "code": ["<code>\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_distributions(x, y):\n    plt.figure(figsize=(10, 6))\n    sns.lineplot(x=x, y=np.zeros_like(x), label='a', color='0.25')\n    sns.lineplot(x=y, y=np.zeros_like(y), label='b', color='0.25')\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Distribution of Values')\n    plt.legend()\n    return plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 535, "library_problem_id": 24, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 536, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate a 2D numpy array\nH = np.random.randn(10, 10)\n\n# Create a color plot\nplt.imshow(H, cmap='viridis')\nplt.colorbar(label='Intensity')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 536, "library_problem_id": 25, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 537, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef display_2D_array():\n    # Create a 10x10 matrix with random values\n    array = np.random.rand(10, 10)\n\n    # Display the array in black and white\n    plt.imshow(array, cmap='gray')\n    plt.show()\n\n# Call the function\ndisplay_2D_array()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 537, "library_problem_id": 26, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 538, "code": ["<code>\nimport matplotlib.pyplot as plt\n\ndef set_xlabel_right():\n    plt.xlabel('X')\n    plt.xticks(rotation=90)\n    plt.subplots_adjust(right=0.9)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 538, "library_problem_id": 27, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 539, "code": ["<code>\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the 'planets' dataset\nplanets = sns.load_dataset('planets')\n\n# Create a boxplot\nplt.figure(figsize=(10,6))\nsns.boxplot(x='method', y='orbital_period', data=planets)\n\n# Rotate the x-axis labels by 90 degrees counterclockwise\nplt.xticks(rotation=90)\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 539, "library_problem_id": 28, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 540, "code": ["<code>\ndef fit_title_into_multiple_lines(title, max_length=80):\n    try:\n        if not isinstance(title, str):\n            raise ValueError(\"Title must be a string.\")\n        if not isinstance(max_length, int) or max_length <= 0:\n            raise ValueError(\"Max length must be a positive integer.\")\n        \n        words = title.split()\n        lines = []\n        line = \"\"\n        for word in words:\n            if len(line) + len(word) > max_length:\n                lines.append(line.strip())\n                line = \"\"\n            line += word + \" \"\n        if line:\n            lines.append(line.strip())\n        \n        return \"\\n\".join(lines)\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n# Test the function\ntitle = \"This is a very long title that needs to be split into multiple lines to fit within a certain length limit\"\nprint(fit_title_into_multiple_lines(title))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 540, "library_problem_id": 29, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 541, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate a random array of 10 elements\ndata = np.random.rand(10)\n\n# Create a figure and axis\nfig, ax = plt.subplots()\n\n# Plot the data\nax.plot(data)\n\n# Invert the y-axis\nax.invert_yaxis()\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 541, "library_problem_id": 30, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 542, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef generate_scatter_plot():\n    # Generate random x and y values\n    x = np.random.rand(50)\n    y = np.random.rand(50)\n\n    # Create a scatter plot\n    plt.scatter(x, y)\n\n    # Place two ticks at x=0 and x=1.5\n    plt.xticks([0, 1.5])\n    plt.yticks([0, 1.5])\n\n    # Return the plot\n    return plt\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 542, "library_problem_id": 31, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 31}}
{"id": 543, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef generate_scatter_plot():\n    # Generate random normally distributed x and y values\n    x = np.random.normal(0, 1, 100)\n    y = np.random.normal(0, 1, 100)\n\n    # Create scatter plot\n    plt.scatter(x, y)\n\n    # Place ticks at -1 and 1 on the y-axis\n    plt.yticks([-1, 1])\n\n    # Return the plot\n    return plt\n\n# Call the function to display the plot\ngenerate_scatter_plot()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 543, "library_problem_id": 32, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 31}}
{"id": 544, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Initialize the arrays\nx = np.random.rand(10)\ny = np.random.rand(10)\nz = np.random.rand(10)\n\n# Ensure that x covers y and y covers z\nx = np.sort(x)\ny = np.sort(y)\nz = np.sort(z)\n\n# Create a dataframe\ndf = pd.DataFrame({'x': x, 'y': y, 'z': z})\n\n# Plot the dataframe\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=df)\n\nplt.xlabel('Index')\nplt.ylabel('Value')\nplt.title('Plot of x, y, and z')\nplt.legend(title='Arrays', loc='upper right')\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 544, "library_problem_id": 33, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 545, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate random x and y coordinates\nx = np.random.randn(10)\ny = np.random.randn(10)\n\n# Create a scatter plot\nplt.scatter(x, y, facecolor='blue', edgecolor='black')\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 545, "library_problem_id": 34, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 546, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Generate a random array 'y' of size 10 and a range from 0 to 9 for 'x'\nx = np.arange(0, 10, 1)\ny = np.random.rand(10)\n\n# Create a DataFrame from 'x' and 'y'\ndf = pd.DataFrame({'x': x, 'y': y})\n\n# Plot the data\nplt.plot('x', 'y', data=df)\n\n# Set the x-axis ticks to integers\nplt.xticks(df['x'])\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 546, "library_problem_id": 35, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 547, "code": ["<code>\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.ticker as ticker\n\ndef create_bar_chart(data):\n    # Convert the data into a pandas DataFrame\n    df = pd.DataFrame(data)\n\n    # Create the bar chart using seaborn\n    chart = sns.catplot(x='reports', y='coverage', data=df, kind='bar')\n\n    # Set the y-axis labels to not be in scientific notation\n    chart.set(yticklabels=[])\n    chart.ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.0f'))\n\n    # Set the chart label\n    chart.fig.suptitle('Total')\n\n    return chart\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 547, "library_problem_id": 36, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 548, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ny = 2 * np.random.rand(10)\nx = np.arange(10)\nax = sns.lineplot(x=x, y=y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 548, "library_problem_id": 37, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 549, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_sine_cosine():\n    x = np.linspace(0, 2*np.pi, 400)\n    y_sin = np.sin(x)\n    y_cos = np.cos(x)\n\n    fig, axs = plt.subplots(2, sharex=True)\n    axs[0].plot(x, y_sin)\n    axs[1].plot(x, y_cos)\n\n    return fig\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 549, "library_problem_id": 38, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 550, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_sine_cosine():\n    x = np.linspace(0, 2*np.pi, 400)\n    y_sin = np.sin(x)\n    y_cos = np.cos(x)\n\n    fig, axs = plt.subplots(2)\n    axs[0].plot(x, y_sin)\n    axs[0].set_title('Sine Function')\n    axs[0].spines['top'].set_color('none')\n    axs[0].spines['bottom'].set_color('none')\n    axs[0].spines['left'].set_color('none')\n    axs[0].spines['right'].set_color('none')\n\n    axs[1].plot(x, y_cos)\n    axs[1].set_title('Cosine Function')\n    axs[1].spines['top'].set_color('none')\n    axs[1].spines['bottom'].set_color('none')\n    axs[1].spines['left'].set_color('none')\n    axs[1].spines['right'].set_color('none')\n\n    plt.show()\n\nplot_sine_cosine()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 550, "library_problem_id": 39, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 551, "code": ["<code>\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef generate_plot():\n    # Create a pandas DataFrame\n    df = pd.DataFrame({\n        'x': np.arange(0, 10, 1),\n        'y': np.sin(np.arange(0, 10, 1))\n    })\n\n    # Generate a line plot using seaborn\n    sns.lineplot(x='x', y='y', data=df)\n\n    # Remove the x-axis label\n    plt.xlabel('')\n\n    # Display the plot\n    plt.show()\n\n# Call the function\ngenerate_plot()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 551, "library_problem_id": 40, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 552, "code": ["<code>\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef create_line_plot():\n    x = np.arange(0, 10, 1)\n    y = np.sin(x)\n\n    plt.figure(figsize=(10, 6))\n    sns.lineplot(x=x, y=y)\n\n    plt.xticks([])\n    plt.show()\n\ncreate_line_plot()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 552, "library_problem_id": 41, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 40}}
{"id": 553, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate random y-values\ny = np.random.rand(10)\n\n# Create the scatter plot\nplt.scatter(range(10), y)\n\n# Set xticks at x positions 3 and 4\nplt.xticks([3, 4])\n\n# Add vertical grid lines at these x positions\nplt.gca().xaxis.grid(True, which='minor')\n\n# Remove y-axis ticks and grid lines\nplt.yticks([])\nplt.grid(False)\n\n# Set x-axis range from 0 to 9\nplt.xlim(0, 9)\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 553, "library_problem_id": 42, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 554, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Create a numpy array of 10 random numbers\ny = np.random.rand(10)\n\n# Create a pandas DataFrame with x ranging from 0 to 9 and y being the random numbers\ndf = pd.DataFrame({'x': np.arange(10), 'y': y})\n\n# Create a scatter plot using seaborn\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=\"x\", y=\"y\", data=df)\n\n# Show yticks at positions 3 and 4 on the y-axis\nplt.yticks([3, 4])\n\n# Include a horizontal grid\nplt.grid(axis='y', linestyle='-')\n\n# Display the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 554, "library_problem_id": 43, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 555, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_scatter():\n    # Generate some data\n    x = np.linspace(0, 10, 100)\n    y = x + np.random.normal(0, 2, 100)\n\n    # Create a scatter plot\n    plt.scatter(x, y)\n\n    # Set yticks and horizontal grid\n    plt.yticks([3, 4])\n    plt.grid(axis='y')\n\n    # Set xticks and vertical grid\n    plt.xticks([1, 2])\n    plt.grid(axis='x')\n\n    # Show the plot\n    plt.show()\n\n# Call the function\nplot_scatter()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 555, "library_problem_id": 44, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 556, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef generate_scatter_plot():\n    x = np.arange(0, 10)\n    y = np.random.rand(10)\n\n    plt.scatter(x, y)\n    plt.show()\n\ngenerate_scatter_plot()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 556, "library_problem_id": 45, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 557, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef scatter_plot(x, y):\n    df = pd.DataFrame({'x': x, 'y': y})\n    sns.scatterplot(x='x', y='y', data=df)\n    plt.legend(['x-y'])\n    return plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 557, "library_problem_id": 46, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 46}}
{"id": 558, "code": ["<code>\nHere is the completed function:\n\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create a figure and a set of subplots\nfig, axs = plt.subplots(2, 2)\n\n# Set the labels for the subplots\naxs[0, 0].set_xlabel(r'$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$')\naxs[0, 0].set_ylabel(r'$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$')\naxs[0, 1].set_xlabel(r'$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$')\naxs[0, 1].set_ylabel(r'$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$')\naxs[1, 0].set_xlabel(r'$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$')\naxs[1, 0].set_ylabel(r'$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$')\naxs[1, 1].set_xlabel(r'$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$')\naxs[1,\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 558, "library_problem_id": 47, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 559, "code": ["<code>\nimport matplotlib.pyplot as plt\n\ndef plot_arrays(x, y, z):\n    plt.plot(x, y, label='Y')\n    plt.plot(x, z, label='Z')\n    plt.xlabel('X')\n    plt.ylabel('Y and Z')\n    plt.legend()\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 559, "library_problem_id": 48, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 560, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define the data\ndata = np.random.rand(4, 4)\n\n# Define the column labels and row labels\ncolumns = 'ABCD'\nrows = 'WXYZ'\n\n# Create the heatmap\nfig, ax = plt.subplots()\nim = ax.imshow(data, cmap='Blues')\n\n# We want to show all ticks...\nax.set_xticks(np.arange(len(columns)))\nax.set_yticks(np.arange(len(rows)))\n# ... and label them with the respective list entries\nax.set_xticklabels(columns)\nax.set_yticklabels(rows)\n\n# Rotate the tick labels and set their alignment.\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n         rotation_mode=\"anchor\")\n\n# Loop over data dimensions and create text annotations.\nfor i in range(len(rows)):\n    for j in range(len(columns)):\n        text = ax.text(j, i, data[i, j],\n                       ha=\"center\", va=\"center\", color=\"w\")\n\nax.set_title(\"Heatmap with x-axis at the top\")\nfig.tight_layout()\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 560, "library_problem_id": 49, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 561, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate x and y values\nx = np.arange(0, 10)\ny = x\n\n# Plot the values\nplt.plot(x, y)\n\n# Label the x-axis\nplt.xlabel('X', labelpad=20)\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 561, "library_problem_id": 50, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 562, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_without_xticks(x, y):\n    plt.figure(figsize=(10, 5))\n    plt.plot(x, y)\n    plt.xticks([])\n    plt.show()\n\n# Test the function\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\nplot_without_xticks(x, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 562, "library_problem_id": 51, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 563, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_line(x, y):\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.yaxis.tick_right()\n    ax.yaxis.set_label_position(\"right\")\n    plt.show()\n\n# Test the function\nx = np.array([1, 2, 3, 4, 5])\ny = np.array([1, 4, 9, 16, 25])\nplot_line(x, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 563, "library_problem_id": 52, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 564, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef generate_plot():\n    x = np.arange(10)\n    y = np.arange(10)\n\n    fig, ax = plt.subplots()\n\n    ax.plot(x, y)\n\n    ax.set_ylabel('Y')\n    ax.yaxis.set_label_position(\"right\")\n\n    plt.show()\n\n    return fig\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 564, "library_problem_id": 53, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 52}}
{"id": 565, "code": ["<code>\nSure, I can help you with that. However, you didn't provide the function you want to complete. Could you please provide the function you want to complete?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 565, "library_problem_id": 54, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 566, "code": ["<code>\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Load the tips dataset\ntips = sns.load_dataset(\"tips\")\n\n# Create a seaborn joint regression plot of 'total_bill' and 'tip'\njoint_reg_plot = sns.jointplot(x='total_bill', y='tip', data=tips, kind='reg', \n                               scatter_kws={\"color\": \"green\"}, \n                               hist_kws={\"color\": \"blue\"})\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 566, "library_problem_id": 55, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 567, "code": ["<code>\nimport seaborn as sns\n\n# Load the tips dataset\ntips = sns.load_dataset(\"tips\")\n\n# Create a seaborn joint regression plot\njoint_reg_plot = sns.jointplot(x='total_bill', y='tip', data=tips, kind='reg')\n\n# Display the plot\nsns.plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 567, "library_problem_id": 56, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 568, "code": ["<code>\nimport matplotlib.pyplot as plt\n\ndef create_bar_plot(df):\n    # Create a bar plot\n    plt.bar(df['celltype'], df['s1'], color='b', label='s1')\n    plt.bar(df['celltype'], df['s2'], color='r', label='s2')\n\n    # Set x-axis tick labels horizontal for better readability\n    plt.xticks(rotation=90)\n\n    # Set labels and title\n    plt.xlabel('Cell Type')\n    plt.ylabel('Values')\n    plt.title('Bar plot of s1 and s2 values by cell type')\n\n    # Add legend\n    plt.legend()\n\n    # Show the plot\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 568, "library_problem_id": 57, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 569, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.DataFrame(\n    {\n        \"celltype\": [\"foo\", \"bar\", \"qux\", \"woz\"],\n        \"s1\": [5, 9, 1, 7],\n        \"s2\": [12, 90, 13, 87],\n    }\n)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 569, "library_problem_id": 58, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 57}}
{"id": 570, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate x and y arrays\nx = np.arange(10)\ny = np.arange(10)\n\n# Create the plot\nplt.plot(x, y)\n\n# Set x-axis label and ticks color to red\nplt.xlabel('X', color='red')\nplt.xticks(color='red')\n\n# Set y-axis ticks color to red\nplt.yticks(color='red')\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 570, "library_problem_id": 59, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 571, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate x and y values\nx = np.arange(0, 10)\ny = x\n\n# Plot y over x\nplt.plot(x, y)\n\n# Label x-axis\nplt.xlabel('X', color='red')\n\n# Show plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 571, "library_problem_id": 60, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 59}}
{"id": 572, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_y_over_x(x, y):\n    plt.figure(figsize=(10, 6))\n    plt.plot(x, y)\n    plt.xticks(rotation=90, fontsize=10)\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 572, "library_problem_id": 61, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 573, "code": ["<code>\nimport matplotlib.pyplot as plt\n\ndef draw_vertical_lines():\n    # Define the x-coordinates of the vertical lines\n    x_coordinates = [0.22058956, 0.33088437, 2.20589566]\n\n    # Create a figure and a set of subplots\n    fig, ax = plt.subplots()\n\n    # Draw vertical lines at the specified x-coordinates\n    for xc in x_coordinates:\n        ax.axvline(x=xc, color='r')\n\n    # Display the graph\n    plt.show()\n\n# Call the function\ndraw_vertical_lines()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 573, "library_problem_id": 62, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 574, "code": ["<code>\nSure, I can help you with that. However, you didn't provide the function to be completed. Could you please provide the function you want me to complete?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 574, "library_problem_id": 63, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 575, "code": ["<code>\nimport matplotlib.pyplot as plt\n\ndef plot_curves(Swdown, Rn, temp, time):\n    fig, ax1 = plt.subplots()\n\n    ax1.set_xlabel('Time (h)')\n    ax1.set_ylabel('Radiation (MJ m^-2 d^-1)')\n    ax1.set_ylim(-20, 100)\n    ax1.plot(time, Swdown, color='blue', label='Swdown')\n    ax1.plot(time, Rn, color='green', label='Rn')\n    ax1.legend(loc='upper left')\n\n    ax2 = ax1.twinx()\n    ax2.set_ylabel('Temperature (^\\circ C)')\n    ax2.set_ylim(0, 35)\n    ax2.plot(time, temp, color='red', label='temp')\n    ax2.legend(loc='upper right')\n\n    plt.show()\n    plt.clf()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 575, "library_problem_id": 64, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 576, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_graph(x, y):\n    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n    axs[0].plot(x, y)\n    axs[0].set_title('Y')\n    axs[1].plot(x, y)\n    axs[1].set_title('Y')\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 576, "library_problem_id": 65, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 577, "code": ["<code>\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the penguins dataset\npenguins = sns.load_dataset(\"penguins\")\n\n# Create a scatter plot\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=penguins, x='bill_length_mm', y='bill_depth_mm', s=30)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 577, "library_problem_id": 66, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 578, "code": ["<code>\nimport matplotlib.pyplot as plt\n\ndef scatter_plot(a, b, c):\n    plt.scatter(a, b)\n\n    for i, txt in enumerate(c):\n        plt.annotate(txt, (a[i], b[i]))\n\n    plt.xlabel('x-axis')\n    plt.ylabel('y-axis')\n    plt.title('Scatter Plot')\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 578, "library_problem_id": 67, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
{"id": 579, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef generate_line_chart():\n    x = np.array([i for i in range(10)])\n    y = np.array([i for i in range(10)])\n\n    plt.plot(x, y, label=\"y over x\")\n    plt.legend(title=\"y over x\")\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('y over x')\n    plt.show()\n\ngenerate_line_chart()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 579, "library_problem_id": 68, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 580, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_function(x, y):\n    plt.plot(x, y)\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Plot of y against x')\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 580, "library_problem_id": 69, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 581, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.random.rand(10)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 581, "library_problem_id": 70, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 582, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate x and y data\nx = np.arange(10)\ny = np.arange(10)\n\n# Create a figure with two subplots\nfig = plt.figure()\n\n# Create the first subplot\nax1 = fig.add_subplot(121)\nax1.plot(x, y)\n\n# Create the second subplot\nax2 = fig.add_subplot(122, sharey=ax1)\nax2.plot(x, y)\n\n# Make the second subplot three times wider than the first\nfig.set_size_inches(12, 5)\n\n# Display the figure\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 582, "library_problem_id": 71, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 583, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate 10 random values for x and y\nx = np.random.rand(10) * 2 - 1\ny = np.random.rand(10) * 2 - 1\n\n# Create bins for the histograms\nbins = np.linspace(-1, 1, 100)\n\n# Plot the histograms\nplt.hist(x, bins=bins, alpha=0.5, label='x', color='blue')\nplt.hist(y, bins=bins, alpha=0.5, label='y', color='red')\n\n# Add legend\nplt.legend()\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 583, "library_problem_id": 72, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 584, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_grouped_histograms(x, y):\n    # Create a figure with two subplots\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,10))\n\n    # Plot the histogram for 'x'\n    ax1.hist(x, bins=np.arange(min(x), max(x) + 1), alpha=0.5, label='x')\n    ax1.legend(loc='upper right')\n\n    # Plot the histogram for 'y'\n    ax2.hist(y, bins=np.arange(min(y), max(y) + 1), alpha=0.5, label='y')\n    ax2.legend(loc='upper right')\n\n    # Adjust the layout\n    plt.tight_layout()\n\n    # Show the plot\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 584, "library_problem_id": 73, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 585, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_line(a, b, c, d):\n    # Create a range of x values from 0 to 5\n    x = np.linspace(0, 5, 1000)\n    \n    # Calculate the slope and y-intercept of the line\n    m = (d - b) / (c - a)\n    y_intercept = b - m * a\n\n    # Calculate the corresponding y values\n    y = m * x + y_intercept\n\n    # Plot the line\n    plt.plot(x, y)\n\n    # Set the xlim and ylim of the plot\n    plt.xlim(0, 5)\n    plt.ylim(0, 5)\n\n    # Show the plot\n    plt.show()\n\n# Test the function\nplot_line(1, 2, 4, 3)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 585, "library_problem_id": 74, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 586, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create two random arrays of size 10x10\nx = np.random.rand(10, 10)\ny = np.random.rand(10, 10)\n\n# Create a figure and a set of subplots\nfig, axs = plt.subplots(2, 1)\n\n# Create a colormap for the first subplot\ncax1 = axs[0].imshow(x, cmap='viridis')\naxs[0].set_title('Array X')\n\n# Create a colormap for the second subplot\ncax2 = axs[1].imshow(y, cmap='viridis')\naxs[1].set_title('Array Y')\n\n# Add a colorbar\nfig.colorbar(cax1, ax=axs)\n\n# Display the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 586, "library_problem_id": 75, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 587, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate a random 2D numpy array\nx = np.random.rand(10, 2)\n\n# Plot the data\nplt.figure(figsize=(10, 5))\n\n# Plot the first column\nplt.plot(x[:, 0], label='a')\n\n# Plot the second column\nplt.plot(x[:, 1], label='b')\n\n# Set the labels\nplt.xlabel('Index')\nplt.ylabel('Value')\n\n# Add a legend\nplt.legend()\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 587, "library_problem_id": 76, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 588, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_graph():\n    x = np.arange(0, 10, 1)\n    y = np.arange(0, 10, 1)\n    a = np.arange(0, 10, 1)\n    z = np.arange(0, 10, 1)\n\n    fig, axs = plt.subplots(2)\n    fig.suptitle('Y and Z')\n\n    axs[0].plot(x, y)\n    axs[1].plot(a, z)\n\n    plt.show()\n\nplot_graph()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 588, "library_problem_id": 77, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 589, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_points(points):\n    # Convert the points to a pandas DataFrame\n    df = pd.DataFrame(points, columns=['x', 'y'])\n\n    # Plot the data\n    plt.figure(figsize=(10, 6))\n    plt.plot(df['x'], df['y'])\n\n    # Set the y-axis to a logarithmic scale\n    plt.yscale('log')\n\n    # Set the labels and title\n    plt.xlabel('X')\n    plt.ylabel('Y (log scale)')\n    plt.title('Line plot of points')\n\n    # Show the plot\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 589, "library_problem_id": 78, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 78}}
{"id": 590, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef generate_plot():\n    x = np.arange(10)\n    y = np.arange(10)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(x, y)\n    plt.title('Plot of y over x', fontsize=20)\n    plt.xlabel('x', fontsize=18)\n    plt.ylabel('y', fontsize=16)\n    plt.show()\n\ngenerate_plot()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 590, "library_problem_id": 79, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 79}}
{"id": 591, "code": ["<code>\ndef plot_function():\n    x = np.arange(1, 11)\n    y = np.arange(1, 11)\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.plot(x, y)\n\n    return fig, ax\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 591, "library_problem_id": 80, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 592, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_line_segments(line_segments, colors):\n    for i, segment in enumerate(line_segments):\n        color = colors[i]\n        plt.plot([segment[0][0], segment[1][0]], [segment[0][1], segment[1][1]], color=(color[0], color[1], color[2], color[3]))\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 592, "library_problem_id": 81, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 593, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate x and y values\nx = np.arange(0, 1001, 50)\ny = np.arange(0, 1001, 50)\n\n# Create a log-log plot\nplt.loglog(x, y)\n\n# Mark the x and y axes with numbers at 1, 10, 100\nplt.xticks([1, 10, 100, 1000])\nplt.yticks([1, 10, 100, 1000])\n\n# Disable scientific notation\nplt.gca().get_xaxis().get_major_formatter().labelOnlyBase = False\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 593, "library_problem_id": 82, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 594, "code": ["<code>\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef generate_plots():\n    # Create a date range\n    date_range = pd.date_range(start='1/1/2000', periods=50)\n\n    # Create a DataFrame with random numbers\n    df = pd.DataFrame(np.random.randn(50, 4), index=date_range, columns=['A', 'B', 'C', 'D'])\n\n    # Calculate the cumulative sum\n    df = df.cumsum()\n\n    # Create a list to store the plots\n    plots = []\n\n    # Generate the plots\n    for col in df.columns:\n        plt.figure()\n        plt.plot(df[col])\n        plt.title(f'Cumulative sum of {col}')\n        plt.xlabel('Date')\n        plt.ylabel('Cumulative sum')\n        plots.append(plt.gcf())\n\n    return plots\n\n# Call the function\nplots = generate_plots()\n\n# Display the plots\nfor plot in plots:\n    plt.show(plot)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 594, "library_problem_id": 83, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 595, "code": ["<code>\nSure, I can help you with that. However, you didn't specify what kind of completion you need. Are you looking for a function to calculate the mean, median, mode, standard deviation, or some other statistical measure of the data? Please provide more details so I can assist you better.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 595, "library_problem_id": 84, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 84}}
{"id": 596, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_line(x, y):\n    plt.plot(x, y, marker='o', markersize=5, markerfacecolor='none', markeredgecolor='blue', markeredgewidth=1.5, alpha=0.5)\n    plt.show()\n\n# Example usage:\nx = np.array([1, 2, 3, 4, 5])\ny = np.array([1, 4, 9, 16, 25])\nplot_line(x, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 596, "library_problem_id": 85, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 597, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_graphs(x, y, a, z):\n    fig, ax = plt.subplots()\n\n    ax.plot(x, y, label='Line 1')\n    ax.plot(x, a, label='Line 2')\n    ax.plot(x, z, label='Line 3')\n\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_title('Graphs')\n    ax.legend()\n\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 597, "library_problem_id": 86, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 598, "code": ["<code>\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the penguins dataset\npenguins = sns.load_dataset(\"penguins\")\n\n# Create a figure with 2 subplots\nfig, axs = plt.subplots(2, figsize=(10,10))\n\n# First subplot: bill_depth_mm vs bill_length_mm\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=penguins, ax=axs[0])\n\n# Second subplot: flipper_length_mm vs bill_length_mm\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=penguins, ax=axs[1])\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 598, "library_problem_id": 87, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 87}}
{"id": 599, "code": ["<code>\nimport matplotlib.pyplot as plt\n\ndef plot_function():\n    fig, ax = plt.subplots()\n\n    x = range(0,10)\n    y = range(0,10)\n\n    ax.plot(x, y)\n\n    ax.set_xlim(1,10)\n    ax.set_xticks(range(1,11))\n    ax.set_xticklabels(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'])\n    ax.set_xticklabels(['1', 'second', '3', '4', '5', '6', '7', '8', '9', '10'], minor=True)\n\n    return fig, ax\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 599, "library_problem_id": 88, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 600, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the function f(x)\ndef f(x):\n    return x\n\n# Generate x values from 0 to 9\nx = np.linspace(0, 9, 100)\n\n# Calculate y values\ny = f(x)\n\n# Plot the function\nplt.plot(x, y, label='$\\lambda$')\n\n# Add legend\nplt.legend()\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 600, "library_problem_id": 89, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 601, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef add_extra_ticks():\n    # Generate data\n    x = np.linspace(0, 10, 100)\n    y = np.sin(x)\n\n    # Create plot\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n\n    # Get existing xticks\n    xticks = ax.get_xticks()\n\n    # Add extra ticks\n    new_ticks = np.concatenate([xticks, [2.1, 3, 7.6]])\n    ax.set_xticks(new_ticks)\n\n    # Re-draw plot\n    plt.draw()\n\n    return plt\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 601, "library_problem_id": 90, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 602, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate x-axis values\nx = np.arange(2010, 2021)\n\n# Generate y-axis values\ny = np.arange(10)\n\n# Create a line plot\nplt.plot(x, y)\n\n# Rotate the xticklabels on the plot\nplt.xticks(rotation=-60)\n\n# Set the xticks horizontal alignment to left\nplt.gcf().autofmt_xdate()\n\n# Display the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 602, "library_problem_id": 91, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 603, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate x and y values\nx = np.arange(2010, 2020)\ny = np.arange(10)\n\n# Create the plot\nplt.plot(x, y)\n\n# Rotate the y-tick labels by -60 degrees\nplt.yticks(rotation=-60)\n\n# Align the x-tick labels to the top\nplt.tick_params(axis='x', which='both', labeltop=True)\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 603, "library_problem_id": 92, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 604, "code": ["<code>\nimport matplotlib.pyplot as plt\n\ndef set_xtick_transparency(plot_obj):\n    for label in plot_obj.xaxis.get_ticklabels():\n        label.set_alpha(0.5)\n    return plot_obj\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 604, "library_problem_id": 93, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 605, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 605, "library_problem_id": 94, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 606, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef generate_plot(x_values, y_values):\n    # Check if the inputs are numpy arrays\n    if not isinstance(x_values, np.ndarray):\n        x_values = np.array(x_values)\n    if not isinstance(y_values, np.ndarray):\n        y_values = np.array(y_values)\n\n    # Create the plot\n    fig, ax = plt.subplots()\n\n    # Plot the data\n    ax.plot(x_values, y_values)\n\n    # Set the margins\n    ax.margins(x=0.01, y=0.1)\n\n    # Return the plot\n    return plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 606, "library_problem_id": 95, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 607, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_figure(x, y):\n    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n    fig.suptitle('Figure')\n\n    axs[0].plot(x, y)\n    axs[0].set_title('Subplot 1')\n\n    axs[1].plot(x, y)\n    axs[1].set_title('Subplot 2')\n\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 607, "library_problem_id": 96, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 96}}
{"id": 608, "code": ["<code>\ndef process_data(data):\n    # Initialize an empty dictionary to store the sums\n    sums = {}\n\n    # Iterate over the rows in the data\n    for i, row in enumerate(data):\n        # Initialize the sum for this row\n        row_sum = 0\n\n        # Iterate over the values in the row\n        for j, value in enumerate(row):\n            # Add the value to the row sum\n            row_sum += value\n\n        # Store the row sum in the sums dictionary\n        sums[f'row_{i+1}'] = row_sum\n\n    # Return the sums\n    return sums\n\n# Test the function with your data\ndata = [\n    [1, 2],\n    [3, 4]\n]\nprint(process_data(data))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 608, "library_problem_id": 97, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 609, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate 10 elements numpy arrays for x and y coordinates\nx = np.random.rand(10)\ny = np.random.rand(10)\n\n# Create a scatter plot with vertical line hatch for the marker\nplt.scatter(x, y, marker='|', hatch='|')\n\n# Display the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 609, "library_problem_id": 98, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 610, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef scatter_plot(x, y):\n    plt.scatter(x, y, edgecolors='none', marker='|')\n    plt.show()\n\n# Example usage:\nx = np.array([1, 2, 3, 4, 5])\ny = np.array([2, 3, 5, 7, 11])\nscatter_plot(x, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 610, "library_problem_id": 99, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 611, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create numpy arrays with 10 elements each\nx = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\ny = np.array([10, 9, 8, 7, 6, 5, 4, 3, 2, 1])\n\n# Create scatter plot\nplt.scatter(x, y, marker='*')\n\n# Display the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 611, "library_problem_id": 100, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 612, "code": ["<code>\nplt.plot(x, y)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 612, "library_problem_id": 101, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 613, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_heatmap():\n    # Generate random data\n    data = np.random.rand(4, 5)\n\n    # Create a new figure and set the xlim and ylim\n    plt.figure()\n    plt.xlim(1, 5)\n    plt.ylim(0, 10)\n\n    # Plot the heatmap\n    plt.pcolor(data, cmap='hot', edgecolors='k', linewidths=1)\n\n    # Add colorbar\n    plt.colorbar()\n\n    return plt.show()\n\nplot_heatmap()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 613, "library_problem_id": 102, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 614, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create x values\nx = np.linspace(0.1, 2*np.pi, 100)\n\n# Calculate y values\ny = np.exp(np.sin(x))\n\n# Create stem plot\nplt.stem(y, x, orientation='horizontal')\n\n# Show plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 614, "library_problem_id": 103, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 615, "code": ["<code>\nimport matplotlib.pyplot as plt\n\ndef create_bar_plot(d, c):\n    plt.figure(figsize=(10,5))\n    plt.bar(d.keys(), d.values(), color=[c[key] for key in d.keys()])\n    plt.xlabel('Keys')\n    plt.ylabel('Values')\n    plt.title('Bar plot using data from d and colors from c')\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 615, "library_problem_id": 104, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 616, "code": ["<code>\nimport matplotlib.pyplot as plt\n\ndef plot_vertical_line():\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot a vertical line at x=3\n    ax.axvline(x=3, color='r', label='cutoff')\n\n    # Add a legend\n    ax.legend()\n\n    # Show the plot\n    plt.show()\n\n# Call the function\nplot_vertical_line()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 616, "library_problem_id": 105, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 617, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef create_polar_plot(labels, height):\n    # Create a figure and an axes\n    fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n\n    # Create a list of angles\n    theta = np.linspace(0, 2*np.pi, len(labels), endpoint=False)\n\n    # Plot the bar plot\n    ax.bar(theta, height)\n\n    # Set the labels\n    ax.set_xticks(theta)\n    ax.set_xticklabels(labels)\n\n    return fig, ax\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 617, "library_problem_id": 106, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 106}}
{"id": 618, "code": ["<code>\nimport matplotlib.pyplot as plt\n\nl = [\"a\", \"b\", \"c\"]\ndata = [225, 90, 50]\n\nplt.bar(l, data)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 618, "library_problem_id": 107, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 619, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create numpy arrays\nx = np.arange(0, 10)\ny = np.arange(0, 10)\n\n# Create the plot\nplt.plot(x, y)\n\n# Add blue dashed grid lines\nplt.grid(True, linestyle='--', color='blue')\n\n# Display the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 619, "library_problem_id": 108, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 620, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the x and y values\nx = np.arange(10)\ny = np.arange(10)\n\n# Create the plot\nplt.plot(x, y)\n\n# Enable minor ticks\nplt.minorticks_xenable()\nplt.minorticks_yenable()\n\n# Set the grid lines\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='gray')\nplt.grid(which='major', linestyle='', linewidth='0')\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 620, "library_problem_id": 109, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 621, "code": ["<code>\nimport matplotlib.pyplot as plt\n\ndef create_pie_chart(sizes, labels, colors):\n    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140, textprops={'fontsize': 14, 'fontweight': 'bold'})\n    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 621, "library_problem_id": 110, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 622, "code": ["<code>\nimport matplotlib.pyplot as plt\n\ndef create_pie_chart(sizes, labels, colors):\n    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140, textprops={'fontsize': 14, 'fontweight': 'bold'})\n    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 622, "library_problem_id": 111, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 623, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_line_chart(x, y):\n    plt.figure(figsize=(5, 5))\n    plt.plot(x, y, marker='o', markersize=10, markerfacecolor='none', markeredgecolor='blue')\n    plt.axis('off')\n    plt.show()\n\n# Example usage:\nx = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\ny = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\nplot_line_chart(x, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 623, "library_problem_id": 112, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 624, "code": ["<code>\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the penguins dataset\npenguins = sns.load_dataset(\"penguins\")\n\n# Select the 'bill_length_mm' column\nbill_length = penguins['bill_length_mm']\n\n# Plot a vertical line at 55 with green color on a graph\nplt.axvline(x=55, color='green')\n\n# Plot a distribution plot of the 'bill_length_mm' column\nsns.distplot(bill_length, kde=False)\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 624, "library_problem_id": 113, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 625, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nblue_bar = (23, 25, 17)\norange_bar = (19, 18, 14)\n\n# Create a new figure\nplt.figure()\n\n# Create a bar chart with blue bars\nplt.bar(np.arange(len(blue_bar)), blue_bar, color='b', width=0.3)\n\n# Create a bar chart with orange bars\nplt.bar(np.arange(len(orange_bar)) + 0.3, orange_bar, color='orange', width=0.3)\n\n# Set the x-axis labels\nplt.xticks(np.arange(len(blue_bar)) + 0.15, ('A', 'B', 'C'))\n\n# Set the y-axis label\nplt.ylabel('Values')\n\n# Set the chart title\nplt.title('Bar Chart')\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 625, "library_problem_id": 114, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 626, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the arrays\nx = np.arange(0, 10, 1)\ny = np.random.rand(10)\na = np.arange(0, 10, 1)\nz = np.random.rand(10)\n\n# Create a plot with two subplots\nfig, ax = plt.subplots(2, 1)\n\n# Plot y against x in the first subplot\nax[0].plot(x, y, label='y')\n\n# Plot z against a in the second subplot\nax[1].plot(a, z, label='z')\n\n# Label each line chart in the first subplot and include them in a single legend\nax[0].legend()\n\n# Display the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 626, "library_problem_id": 115, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 627, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create x and y values\nx = np.arange(0, 10, 1)\ny = np.arange(0, 1, 0.1)\n\n# Create a scatter plot\nplt.scatter(x, y, c=y, cmap='Spectral')\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 627, "library_problem_id": 116, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 116}}
{"id": 628, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_function(x, y):\n    plt.plot(x, y)\n    plt.xticks(np.arange(min(x), max(x)+1, 1.0))\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 628, "library_problem_id": 117, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 629, "code": ["<code>\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming df is your DataFrame\ndf = sns.load_dataset('penguins')\n\ng = sns.catplot(data=df, x='species', y='bill_length_mm', \n                col='sex', kind='bar', sharey=False)\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 629, "library_problem_id": 118, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 630, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\ndef draw_circle():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n\n    circle = patches.Circle((0.5, 0.5), 0.2, edgecolor='b', facecolor='none')\n    ax.add_patch(circle)\n\n    plt.xlim(0, 1)\n    plt.ylim(0, 1)\n\n    plt.show()\n\ndraw_circle()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 630, "library_problem_id": 119, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 631, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef generate_plot():\n    x = np.arange(0, 10)\n    y = np.arange(0, 10)\n\n    plt.plot(x, y)\n    plt.title(r'$\\boldsymbol{\\Phi}$', fontsize=20)\n    plt.show()\n\ngenerate_plot()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 631, "library_problem_id": 120, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 632, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.arange(0, 10)\ny = x\n\nplt.plot(x, y, label='Line')\nplt.legend(labelspacing=0.1)\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 632, "library_problem_id": 121, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 633, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate x and y values\nx = np.arange(0, 10)\ny = x\n\n# Create the plot\nplt.plot(x, y, label='Line')\n\n# Add legend\nlegend = plt.legend(loc='lower left', handles=[plt.Line2D([0], [0], color='black', linewidth=0, linestyle='')], handlelength=0.3)\nplt.gca().add_artist(legend)\n\n# Labels and title\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Line Plot')\n\n# Remove extra axes and grid lines\nplt.axis('off')\nplt.grid(False)\n\n# Display the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 633, "library_problem_id": 122, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 634, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Create arrays x and y with numpy's arange function\nx = np.arange(10)\ny = np.arange(10)\n\n# Plot the lines\nplt.plot(x, label='Line')\nplt.plot(y[::-1], label='Flipped')\n\n# Add a legend\nplt.legend(loc='upper right', ncol=2)\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 634, "library_problem_id": 123, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 635, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef generate_plot():\n    x = np.arange(10)\n    y = x\n\n    plt.plot(x, y, '*', label='Line')\n    plt.legend()\n    plt.show()\n\ngenerate_plot()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 635, "library_problem_id": 124, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 636, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create a 10x10 array filled with random numbers\ndata = np.random.rand(10, 10)\n\n# Create a figure and an axes\nfig, ax = plt.subplots()\n\n# Create a colormap\ncmap = plt.get_cmap('viridis')\n\n# Create a colorbar\ncbar = plt.colorbar(plt.matshow(data, cmap=cmap), ax=ax)\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 636, "library_problem_id": 125, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 125}}
{"id": 637, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming x and y are numpy arrays with 10 elements each\nx = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\ny = np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20])\n\nplt.plot(x, y)\n\n# Set the title\nplt.title(r\"$\\bf{\" + 'Figure' + \"}$ \" + '1')\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 637, "library_problem_id": 126, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 126}}
{"id": 638, "code": ["<code>\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef create_pairplot(df, x, y, id):\n    pairplot = sns.pairplot(df, x_vars=x, y_vars=y, hue=id)\n    pairplot.legend.remove()\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 638, "library_problem_id": 127, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 639, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create x and y values\nx = np.arange(0, 10)\ny = np.arange(0, 10)\n\n# Plot y over x\nplt.plot(x, y)\n\n# Invert the x axis\nplt.gca().invert_xaxis()\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 639, "library_problem_id": 128, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 128}}
{"id": 640, "code": ["<code>\nimport matplotlib.pyplot as plt\n\ndef scatter_plot(x, y):\n    plt.figure(figsize=(10, 6))\n    plt.scatter(x, y, clip_on=False)\n    plt.xlim(0, 10)\n    plt.ylim(0, 10)\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 640, "library_problem_id": 129, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 641, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate x and y values\nx = np.arange(0, 10, 0.1)\ny = np.arange(0, 10, 0.1)\n\n# Create the scatter plot\nplt.scatter(x, y, c='red', edgecolors='black')\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 641, "library_problem_id": 130, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 642, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_data(x, y):\n    fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n    for i in range(2):\n        for j in range(2):\n            axs[i, j].plot(x, y)\n            axs[i, j].set_title('Plot of y over x')\n    plt.tight_layout()\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 642, "library_problem_id": 131, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 643, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Assuming x is a numpy array\nx = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n# Create the histogram\nplt.hist(x, bins=np.arange(0, 11, 2), edgecolor='black')\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 643, "library_problem_id": 132, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 644, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate x and y\nx = np.arange(0, 10)\ny = np.arange(1, 11)\n\n# Generate error values\nerror = np.random.rand(10)\n\n# Plot y against x\nplt.plot(x, y, marker='o')\n\n# Generate shaded region for error values\nplt.fill_between(x, y - error, y + error, color='grey', alpha=0.2)\n\n# Set labels and title\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Plot of y against x with error region')\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 644, "library_problem_id": 133, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 133}}
{"id": 645, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define the x and y axes\nx = np.linspace(-5.0, 5.0, 100)\ny = np.linspace(-5.0, 5.0, 100)\n\n# Create a meshgrid from x and y\nX, Y = np.meshgrid(x, y)\n\n# Define the hypothetical function\nZ = -np.hypot(X, Y)\n\n# Create a contour plot\nplt.contourf(X, Y, Z, cmap='Greys')\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 645, "library_problem_id": 134, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 646, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_error_bars(box_errors, box_position, box_height, c):\n    plt.figure(figsize=(10, 6))\n    plt.errorbar(box_position, box_height, yerr=box_errors, fmt='o', color='yellow', ecolor=c, capsize=5, elinewidth=2)\n    plt.show()\n\n# Test the function\nbox_errors = np.array([0.1, 0.2, 0.3, 0.4, 0.5])\nbox_position = np.array([1, 2, 3, 4, 5])\nbox_height = np.array([10, 11, 12, 13, 14])\nc = ['red', 'green', 'blue', 'cyan', 'magenta']\n\nplot_error_bars(box_errors, box_position, box_height, c)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 646, "library_problem_id": 135, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 647, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_subplots():\n    x = np.arange(0, 10)\n    y = x\n    z = x\n    a = x\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n    ax1.plot(x, y)\n    ax1.set_title('Y')\n\n    ax2.plot(a, z)\n    ax2.set_title('Z')\n\n    plt.tight_layout()\n\n    return fig\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 647, "library_problem_id": 136, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 136}}
{"id": 648, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create a 4x4 subplot grid with a figure size of (5,5)\nfig, axs = plt.subplots(4, 4, figsize=(5, 5))\n\n# Ensure there is sufficient spacing between the subplots\nplt.tight_layout()\n\n# In each subplot, plot y against x and include axis tick labels\nx = np.linspace(0, 10, 100)\nfor i in range(4):\n    for j in range(4):\n        y = np.sin(x + i * np.pi / 2) * (1 + 0.2 * np.random.rand(100))\n        axs[i, j].plot(x, y)\n        axs[i, j].set_xlabel('x')\n        axs[i, j].set_ylabel('y')\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 648, "library_problem_id": 137, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 649, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_2d_array(d):\n    plt.figure(figsize=(8, 8))\n    plt.matshow(d, fignum=1)\n    plt.show()\n\n# Example usage:\nd = np.random.rand(10, 10)\nplot_2d_array(d)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 649, "library_problem_id": 138, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 138}}
{"id": 650, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the penguins dataset\npenguins = sns.load_dataset(\"penguins\")\n\n# Select the first 10 rows and the specified columns\ndata = penguins.loc[:9, ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']]\n\n# Create a figure and a set of subplots\nfig, ax = plt.subplots()\n\n# Create a table from the data\ntable = plt.table(cellText=data.values,\n                  colLabels=data.columns,\n                  cellLoc='center',\n                  bbox=[0, 0, 1, 1])\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 650, "library_problem_id": 139, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 651, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_line_chart(x, y):\n    # Generate the x and y values\n    x = np.arange(x[0], x[1], x[2])\n    y = np.arange(y[0], y[1], y[2])\n\n    # Create the figure and the line chart\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n\n    # Set the x-axis tick labels on both the top and bottom of the figure\n    ax.xaxis.tick_top()\n    ax.xaxis.tick_bottom()\n\n    # Show the plot\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 651, "library_problem_id": 140, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 140}}
{"id": 652, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate x and y values\nx = np.arange(0, 10)\ny = np.arange(0, 10)\n\n# Create the plot\nplt.plot(x, y)\n\n# Set the x-axis ticks\nplt.tick_params(axis='x', which='both', top=True, bottom=True)\n\n# Display the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 652, "library_problem_id": 141, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 653, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_line_chart(x, y):\n    plt.figure(figsize=(10, 6))\n    plt.plot(x, y)\n    plt.xticks(x)\n    plt.tick_params(axis='x', which='both', length=0)\n    plt.show()\n\n# Example usage:\nx = np.array([1, 2, 3, 4, 5])\ny = np.array([2, 4, 6, 8, 10])\nplot_line_chart(x, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 653, "library_problem_id": 142, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 654, "code": ["<code>\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the exercise dataset\ndf = sns.load_dataset('exercise')\n\n# Create a scatter plot with seaborn\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\",\n            kind=\"scatter\", col_wrap=2,\n            data=df,\n            height=4, aspect=.75,\n            legend=False)\n\n# Set the titles\nplt.subplot(211)\nplt.title(\"Group: Fat\")\nplt.subplot(212)\nplt.title(\"Group: No Fat\")\n\nplt.tight_layout()\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 654, "library_problem_id": 143, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 655, "code": ["<code>\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the exercise dataset\nexercise = sns.load_dataset('exercise')\n\n# Create a seaborn catplot with a scatter plot\nsns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=exercise, kind='scatter')\n\n# Set the x-axis label\nplt.xlabel(\"Exercise Time\")\n\n# Set the y-axis label\nplt.ylabel(\"Exercise Pulse\")\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 655, "library_problem_id": 144, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 656, "code": ["<code>\n# Display the first 5 rows of the dataframe\nprint(df.head())\n\n# Display the summary statistics of the dataframe\nprint(df.describe())\n\n# Display the information of the dataframe\nprint(df.info())\n\n# Count the number of unique values in each column\nprint(df.nunique())\n\n# Check for missing values\nprint(df.isnull().sum())\n\n# Plot a histogram of the 'pulse' column\nplt.hist(df['pulse'], bins=10, color='c')\nplt.title('Histogram of Pulse')\nplt.xlabel('Pulse')\nplt.ylabel('Frequency')\nplt.show()\n\n# Plot a boxplot of the 'pulse' column\nsns.boxplot(df['pulse'])\nplt.title('Boxplot of Pulse')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 656, "library_problem_id": 145, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 657, "code": ["<code>\nimport matplotlib.pyplot as plt\n\ndef plot_data(x, y, label='y', legend_fontsize=8):\n    plt.plot(x, y, label=label)\n    plt.legend(fontsize=legend_fontsize)\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 657, "library_problem_id": 146, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 146}}
{"id": 658, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_graph(x, y):\n    plt.figure(figsize=(5, 5), dpi=300)\n    plt.plot(x, y)\n    plt.show()\n\n# Example usage:\nx = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\ny = np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20])\nplot_graph(x, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 658, "library_problem_id": 147, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 659, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate x and y values\nx = np.arange(0, 10)\ny = x\n\n# Plot y over x\nplt.plot(x, y, label='y')\n\n# Include a legend\nplt.legend(frameon=False)\n\n# Label the plot\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Plot of y over x')\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 659, "library_problem_id": 148, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 660, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the time variable\nt = np.linspace(0, 2*np.pi, 400)\n\n# Define the functions a, b and c\na = np.sin(t)\nb = np.cos(t)\nc = a + b\n\n# Create the plot\nplt.figure(figsize=(10, 6))\nplt.plot(t, a, label='sin(t)')\nplt.plot(t, b, label='cos(t)')\nplt.plot(t, c, label='sin(t) + cos(t)')\nplt.legend()\nplt.xlabel('Time (t)')\nplt.ylabel('Value')\nplt.title('Plot of three functions')\nplt.grid(True)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 660, "library_problem_id": 149, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 149}}
{"id": 661, "code": ["<code>\nimport seaborn as sns\n\ndef create_stripplot(df):\n    sns.stripplot(x='sex', y='bill_length_mm', hue='species', data=df, jitter=True)\n    plt.legend([],[], frameon=False)\n\n# Call the function\ncreate_stripplot(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 661, "library_problem_id": 150, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 662, "code": ["<code>\nSure, I can help you with that. However, you didn't provide the function you want to complete. Could you please provide the function you want to complete?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 662, "library_problem_id": 151, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 663, "code": ["<code>\nSure, I can help you with that. However, you didn't specify what kind of completion you need. Are you looking for a way to plot these points in 3D, or are you looking for a way to generate more random numbers? Please provide more details so I can assist you better.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 663, "library_problem_id": 152, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 152}}
{"id": 664, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_line_chart(x, y):\n    plt.figure()\n    plt.plot(x, y)\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.xticks([])\n    plt.yticks([])\n    plt.show()\n\n# Example usage:\nx = np.arange(0, 10, 1)\ny = np.arange(0, 10, 1)\nplot_line_chart(x, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 664, "library_problem_id": 153, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 153}}
{"id": 665, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.random.random((10, 10))\n\nfrom matplotlib import gridspec\nnrow = 2\nncol = 2\nfig = plt.figure(figsize=(ncol + 1, nrow + 1))\ngs = gridspec.GridSpec(nrow, ncol)\n\nfor i in range(nrow):\n    for j in range(ncol):\n        ax = fig.add_subplot(gs[i, j])\n        ax.imshow(x, cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 665, "library_problem_id": 154, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 666, "code": ["<code>\nimport tensorflow as tf\n\ndef assign_and_retrieve():\n    x = tf.Variable(0)\n    # Assign a new value to the variable\n    x.assign(10)\n    # Retrieve the value of the variable\n    return x.numpy()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 666, "library_problem_id": 0, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 667, "code": ["<code>\nimport tensorflow as tf\n\nx = tf.Variable(0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 667, "library_problem_id": 1, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 668, "code": ["<code>\nimport numpy as np\n\ndef one_hot_encoding(labels, num_classes):\n    return np.eye(num_classes, dtype=np.int32)[labels]\n\n# Test the function\nlabels = [0, 6, 5, 4, 2]\nnum_classes = 10\nprint(one_hot_encoding(labels, num_classes))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 668, "library_problem_id": 2, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 669, "code": ["<code>\nimport numpy as np\n\ndef one_hot_encoding(indices, num_classes):\n    # Create a numpy array of zeros with the shape of (number of indices) x num_classes\n    one_hot = np.zeros((len(indices), num_classes), dtype=np.int32)\n    \n    # For each index in the indices list, set the corresponding element in the one_hot array to 1\n    for i, index in enumerate(indices):\n        one_hot[i, index] = 1\n    \n    return one_hot\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 669, "library_problem_id": 3, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 670, "code": ["<code>\nimport tensorflow as tf\n\ndef one_hot_encode(labels, num_classes):\n    # Convert the labels to a tensor\n    labels_tensor = tf.constant(labels, dtype=tf.int32)\n    \n    # Use tf.one_hot to encode the labels\n    one_hot_tensor = tf.one_hot(labels_tensor, depth=num_classes, dtype=tf.int32)\n    \n    return one_hot_tensor\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 670, "library_problem_id": 4, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 671, "code": ["<code>\nimport numpy as np\n\ndef one_hot_encode(class_indices):\n    n = len(class_indices)\n    m = 10  # number of classes\n    one_hot = np.zeros((n, m), dtype=np.int32)\n    one_hot[np.arange(n), class_indices] = 1\n    return one_hot\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 671, "library_problem_id": 5, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 672, "code": ["<code>\nimport numpy as np\n\ndef one_hot_encode(labels, num_classes):\n    n = len(labels)\n    one_hot = np.zeros((n, num_classes), dtype='int32')\n    one_hot[np.arange(n), labels] = 1\n    return one_hot\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 672, "library_problem_id": 6, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 673, "code": ["<code>\ndef my_map_func(i):\n  return [i, i+1, i+2]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 673, "library_problem_id": 7, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 674, "code": ["<code>\nimport tensorflow as tf\n\ndef f(input):\n    # Convert the input list to a tensor\n    input_tensor = tf.constant(input, dtype=tf.int32)\n\n    # Define a custom map function\n    def map_func(x):\n        return [x+1, x+2, x+3]\n\n    # Use the map function to expand the dataset\n    expanded_dataset = tf.map_fn(map_func, input_tensor, dtype=tf.int32)\n\n    # Flatten the output of the map function\n    flattened_list = tf.reshape(expanded_dataset, [-1]).numpy().tolist()\n\n    return flattened_list\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 674, "library_problem_id": 8, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 7}}
{"id": 675, "code": ["<code>\nimport tensorflow as tf\n\nlengths = [4, 3, 5, 2]\ntensor = tf.constant(lengths)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 675, "library_problem_id": 9, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 676, "code": ["<code>\nimport tensorflow as tf\n\ndef create_mask(lengths):\n    max_length = tf.reduce_max(lengths)\n    mask = tf.sequence_mask(lengths, max_length, dtype=tf.int32)\n    result = tf.cast(mask, tf.int32)\n    return result\n\n# Test the function\nlengths = tf.constant([3, 5, 2, 7, 1, 6, 4, 8])\nresult = create_mask(lengths)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 676, "library_problem_id": 10, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 677, "code": ["<code>\nimport tensorflow as tf\n\ndef create_mask(lengths, total_length):\n    # Create a range tensor of size total_length\n    range_tensor = tf.range(total_length)\n    \n    # Expand dimensions to match the shape of lengths tensor\n    range_tensor = tf.expand_dims(range_tensor, 0)\n    \n    # Broadcast lengths tensor to match the shape of range tensor\n    lengths = tf.broadcast_to(lengths, range_tensor.shape)\n    \n    # Create a mask tensor where each row is a range tensor less than the corresponding length\n    mask = tf.less(range_tensor, lengths)\n    \n    # Convert the mask tensor to float32\n    mask = tf.cast(mask, tf.float32)\n    \n    return mask\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 677, "library_problem_id": 11, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 678, "code": ["<code>\nimport tensorflow as tf\n\ndef create_mask(lengths, max_length=None):\n    if max_length is None:\n        max_length = tf.reduce_max(lengths)\n    mask = tf.sequence_mask(lengths, max_length, dtype=tf.int32)\n    return tf.cast(mask, tf.float32)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 678, "library_problem_id": 12, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 679, "code": ["<code>\nimport tensorflow as tf\n\nlengths = [4, 3, 5, 2]\ntensor = tf.constant(lengths)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 679, "library_problem_id": 13, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 680, "code": ["<code>\nimport tensorflow as tf\n\na = tf.constant([1,2,3])\nb = tf.constant([4,5,6,7])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 680, "library_problem_id": 14, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 681, "code": ["<code>\nimport tensorflow as tf\n\nexample_a = tf.constant([1,2,3])\nexample_b = tf.constant([4,5,6,7])\n\ndef f(a=example_a,b=example_b):\n    # Sum the tensors\n    result = tf.add(a, b)\n    return result\n\n# Call the function with the example tensors\nresult = f(example_a, example_b)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 681, "library_problem_id": 15, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 682, "code": ["<code>\nimport tensorflow as tf\n\n# Assuming 'a' is your tensor\na = tf.random.normal([50, 100, 1, 512])\n\n# Reshape the tensor\nresult = tf.reshape(a, [50, 100, 512])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 682, "library_problem_id": 16, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 683, "code": ["<code>\nimport tensorflow as tf\n\n# Assuming 'tensor' is the given TensorFlow tensor\ntensor = tf.random.normal([50, 100, 512])\n\n# Reshape the tensor\nresult = tf.reshape(tensor, [50, 100, 1, 512])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 683, "library_problem_id": 17, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 684, "code": ["<code>\nimport tensorflow as tf\n\n# Given tensor\ntensor = tf.constant(np.random.rand(50, 100, 512), dtype=tf.float32)\n\n# Reshape the tensor\nresult = tf.reshape(tensor, [1, 50, 100, 1, 512])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 684, "library_problem_id": 18, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 685, "code": ["<code>\nSure, I can help you with that. However, you didn't specify what you want to do with the given code. Could you please provide more details about what you want to achieve with the function?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 685, "library_problem_id": 19, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 686, "code": ["<code>\nSure, I can help you with that. However, you didn't specify what you want to do with the given code. Could you please provide more details about what you want to achieve with the function?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 686, "library_problem_id": 20, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 687, "code": ["<code>\nimport tensorflow as tf\n\n# Define the tensor A\nA = tf.constant([-0.5, -0.1, 0, 0.1, 0.5, 2])\n\n# Calculate the reciprocal of each element in the tensor\nreciprocal_A = tf.math.reciprocal(A)\n\n# Print the result\nprint(reciprocal_A)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 687, "library_problem_id": 21, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 19}}
{"id": 688, "code": ["<code>\nimport tensorflow as tf\n\ndef calculate_l2_distance(a, b):\n    return tf.norm(a - b, axis=-1, ord=2)\n\na = tf.constant([[1,1,1],[1,1,1]])\nb = tf.constant([[0,0,0],[1,1,1]])\n\nresult = calculate_l2_distance(a, b)\n\nprint(result)  # Output: <tf.Tensor: shape=(2,), dtype=float32, numpy=array([3.7416573, 3.7416573], dtype=float32)>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 688, "library_problem_id": 22, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 689, "code": ["<code>\nimport tensorflow as tf\n\na = tf.constant([\n  [1,1,1],\n  [0,1,1]\n])\nb = tf.constant([\n  [0,0,1],\n  [1,1,1]\n])\n\n# Perform element-wise addition\nc = tf.add(a, b)\n\n# Print the result\nprint(c)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 689, "library_problem_id": 23, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 22}}
{"id": 690, "code": ["<code>\nimport tensorflow as tf\n\ndef f(A, B):\n    return tf.reduce_sum(tf.square(tf.subtract(A, B)), axis=1)\n\n# Test the function\nexample_a = tf.constant([\n  [1,1,1],\n  [1,1,1]\n])\nexample_b = tf.constant([\n  [0,0,0],\n  [1,1,1]\n])\n\nprint(f(example_a, example_b))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 690, "library_problem_id": 24, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 691, "code": ["<code>\nimport tensorflow as tf\n\nx = [[1,2,3],[4,5,6]]\ny = [0,1]\nz = [1,2]\nx = tf.constant(x)\ny = tf.constant(y)\nz = tf.constant(z)\nm = tf.gather_nd(x, tf.stack([y, z], axis=1))\n\nprint(m)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 691, "library_problem_id": 25, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 692, "code": ["<code>\nimport tensorflow as tf\n\nx = [[1,2,3],[4,5,6]]\nrow = [0,1]\ncol = [0,2]\nx = tf.constant(x)\nrow = tf.constant(row)\ncol = tf.constant(col)\nm = tf.gather_nd(x, tf.stack([row, col], axis=1))\n\nprint(m)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 692, "library_problem_id": 26, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 693, "code": ["<code>\nimport tensorflow as tf\n\nx = [[1,2,3],[4,5,6]]\ny = [0,1]\nz = [1,2]\nx = tf.constant(x)\ny = tf.constant(y)\nz = tf.constant(z)\nm = x[y,z]\n\nprint(m)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 693, "library_problem_id": 27, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 694, "code": ["<code>\nC = A + B\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 694, "library_problem_id": 28, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 695, "code": ["<code>\nC = A + B\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 695, "library_problem_id": 29, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 696, "code": ["<code>\nimport tensorflow as tf\n\ndef decode_byte_strings(byte_strings):\n    byte_strings_tensor = tf.constant(byte_strings)\n    decoded_strings = tf.strings.unicode_transcode(byte_strings_tensor, \"UTF-8\", \"UTF-32-BE\")\n    decoded_strings = tf.strings.unicode_transcode(decoded_strings, \"UTF-32-BE\", \"UTF-8\")\n    return decoded_strings.numpy().tolist()\n\nbyte_strings = ['\\xd8\\xa8\\xd9\\x85\\xd8\\xb3\\xd8\\xa3\\xd9\\x84\\xd8\\xa9', '\\xd8\\xa5\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1', '\\xd9\\x82\\xd8\\xb6\\xd8\\xa7\\xd8\\xa1', '\\xd8\\xac\\xd9\\x86\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a', '\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a']\nprint(decode_byte_strings(byte_strings))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 696, "library_problem_id": 30, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 697, "code": ["<code>\nimport tensorflow as tf\n\ndef convert_bytes_to_unicode(byte_strings):\n    # Convert byte strings to unicode strings\n    unicode_strings = [s.decode('utf-8') for s in byte_strings]\n    \n    # Convert list of unicode strings to TensorFlow tensor\n    tensor = tf.constant(unicode_strings)\n    \n    return tensor\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 697, "library_problem_id": 31, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 698, "code": ["<code>\nimport tensorflow as tf\nimport numpy as np\n\ndef average_second_last_dim(tensor):\n    tensor = tf.constant(tensor)\n    mask = tf.not_equal(tensor, 0)\n    tensor = tf.boolean_mask(tensor, mask)\n    tensor = tf.reshape(tensor, tf.shape(tensor))\n    avg = tf.reduce_mean(tensor, axis=-2)\n    avg = tf.expand_dims(avg, axis=-2)\n    return avg\n\ntensor = np.array([[[[1,2,3], [2,3,4], [0,0,0]],\n  [[1,2,3], [2,0,4], [3,4,5]],\n  [[1,2,3], [0,0,0], [0,0,0]],\n  [[1,2,3], [1,2,3], [0,0,0]]],\n [[[1,2,3], [0,1,0], [0,0,0]],\n  [[1,2,3], [2,3,4], [0,0,0]],\n  [[1,2,3], [0,0,0], [0,0,0]]]])\n\nprint(average_second_last_dim(tensor))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 698, "library_problem_id": 32, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 699, "code": ["<code>\nimport torch\n\ndef calculate_variance(tensor):\n    # Ignore zero-padded values\n    mask = tensor!=0\n    # Calculate the variance along the second to last dimension\n    variance = torch.var(tensor[mask], dim=-2)\n    # Expand the variance tensor to match the input tensor's shape\n    variance = variance.unsqueeze(-2).expand_as(tensor)\n    return variance\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 699, "library_problem_id": 33, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 700, "code": ["<code>\nimport tensorflow as tf\n\ndef calculate_average(tensor):\n    # Convert the list of lists to a TensorFlow tensor\n    tensor = tf.constant(tensor, dtype=tf.float32)\n\n    # Get the second to last dimension of the tensor\n    tensor = tf.reshape(tensor, [tensor.shape[0], -1, tensor.shape[-1]])\n\n    # Exclude zero-padded values\n    mask = tf.math.count_nonzero(tensor, axis=-1, dtype=tf.bool, keepdims=True)\n    tensor = tf.boolean_mask(tensor, mask)\n\n    # Compute the average over the second to last dimension\n    tensor = tf.reduce_sum(tensor, axis=-2) / tf.reduce_sum(tf.cast(mask, tf.float32), axis=-2)\n\n    return tensor\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 700, "library_problem_id": 34, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 701, "code": ["<code>\nimport tensorflow as tf\n\n# Generate random matrices A and B\nA = tf.random.normal([100, 100])\nB = tf.random.normal([100, 100])\n\n# Perform matrix multiplication\nmatrix_mul = tf.matmul(A, B)\n\n# Reduce the result to a scalar\nresult = tf.reduce_sum(matrix_mul)\n\n# Initialize the TensorFlow session\n# Note: In TensorFlow 2.x, you don't need to explicitly create a session.\n# The operations are automatically executed when you call the `tf.Tensor.numpy()` method.\n\n# Print the result\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 701, "library_problem_id": 35, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 702, "code": ["<code>\nimport tensorflow as tf\n\ndef get_max_indices(prediction_scores):\n    \"\"\"\n    This function takes a TensorFlow tensor of shape [N, C] representing prediction scores for N samples across C classes,\n    and returns a tensor of shape [N] where each element is the index of the maximum value in the corresponding row of the input tensor.\n    \"\"\"\n    max_indices = tf.argmax(prediction_scores, axis=1)\n    return max_indices\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 702, "library_problem_id": 36, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 703, "code": ["<code>\nimport tensorflow as tf\n\ndef get_max_indices(scores):\n    # Use tf.argmax to find the indices of the maximum values in each column\n    indices = tf.argmax(scores, axis=0)\n    # Ensure the output tensor is of type int32\n    indices = tf.cast(indices, tf.int32)\n    return indices\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 703, "library_problem_id": 37, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 704, "code": ["<code>\nimport tensorflow as tf\n\ndef get_max_indices(prediction_scores):\n    max_indices = tf.argmax(prediction_scores, axis=1)\n    return max_indices\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 704, "library_problem_id": 38, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 705, "code": ["<code>\ntf.Tensor([[3], [3], [2]], shape=(3, 1), dtype=int32)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 705, "library_problem_id": 39, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 706, "code": ["<code>\nfrom keras import optimizers\nfrom keras import backend\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.activations import relu,tanh,sigmoid\n\nnetwork_layout = []\nfor i in range(3):\n    network_layout.append(8)\n\nmodel = Sequential()\n\ninputdim = 4\nactivation = 'relu'\n\n# Continue building your model here...\n\n# Save the model\nmodel.save(\"my_model/\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 706, "library_problem_id": 40, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 707, "code": ["<code>\nimport tensorflow as tf\n\n# Set the random seed\ntf.random.set_seed(10)\n\n# Generate a TensorFlow tensor with 10 random integers\nrandom_integers = tf.random.uniform(shape=[10], minval=1, maxval=5, dtype=tf.int32)\n\nprint(random_integers)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 707, "library_problem_id": 41, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 708, "code": ["<code>\nimport tensorflow as tf\n\nseed_x = 10\ntf.random.set_seed(seed_x)\n\nresult = tf.random.uniform(shape=[114], minval=2, maxval=6, dtype=tf.int32)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 708, "library_problem_id": 42, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 709, "code": ["<code>\nimport tensorflow as tf\n\n# Set the random seed\ntf.random.set_seed(10)\n\n# Generate a TensorFlow tensor with 10 random integers\ntensor = tf.random.uniform(shape=(10,), minval=1, maxval=5, dtype=tf.int32)\n\nprint(tensor)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 709, "library_problem_id": 43, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 710, "code": ["<code>\nimport tensorflow as tf\n\n# Define a simple sequential model\ndef create_model():\n  model = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(10)\n  ])\n\n  model.compile(optimizer='adam',\n                loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n                metrics=['accuracy'])\n\n  return model\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 710, "library_problem_id": 44, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 711, "code": ["<code>\nimport numpy as np\n\ndef find_coefficients(x, y):\n    # Convert x to log(x)\n    x_log = np.log(x)\n    # Use polyfit to find coefficients\n    coeff = np.polyfit(x_log, y, 1)\n    return coeff\n\nx = [1, 7, 20, 50, 79]\ny = [10, 19, 30, 35, 51]\n\nprint(find_coefficients(x, y))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 711, "library_problem_id": 0, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 712, "code": ["<code>\nimport numpy as np\nimport scipy\n\nx = np.array([1, 7, 20, 50, 79])\ny = np.array([10, 19, 30, 35, 51])\n\n# Calculate the correlation coefficient\ncorrelation_coefficient = np.corrcoef(x, y)[0, 1]\n\nprint(f\"The correlation coefficient is: {correlation_coefficient}\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 712, "library_problem_id": 1, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 713, "code": ["<code>\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\n# Define the function you want to fit to the data\ndef func(x, a, b, c):\n    return a * np.exp(-b * x) + c\n\ny = np.array([1, 7, 20, 50, 79])\nx = np.array([10, 19, 30, 35, 51])\np0 = (4, 0.1, 1)\n\n# Use curve_fit to fit the function to the data\npopt, pcov = curve_fit(func, x, y, p0)\n\nprint(\"Optimized parameters: \", popt)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 713, "library_problem_id": 2, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 714, "code": ["<code>\nfrom scipy import stats\nimport numpy as np\n\ndef ks_test(sample1, sample2):\n    # Perform KS test\n    ks_test_statistic, p_value = stats.kstest(sample1, sample2)\n\n    return ks_test_statistic, p_value\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 714, "library_problem_id": 3, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 3}}
{"id": 715, "code": ["<code>\nimport numpy as np\nfrom scipy import stats\n\ndef ks_test(data1, data2, alpha=0.01):\n    try:\n        # Perform KS test\n        ks_stat, p_value = stats.kstest(data1, data2)\n        \n        # Check if the data arrays are of the same length\n        if len(data1) != len(data2):\n            return \"Error: The two data arrays must have the same length.\"\n        \n        # Check if the p-value is less than the significance level\n        if p_value < alpha:\n            return f\"Reject the null hypothesis. The distributions of the two data sets are significantly different. KS statistic = {ks_stat:.4f}, p-value = {p_value:.4f}\"\n        else:\n            return f\"Fail to reject the null hypothesis. The distributions of the two data sets are not significantly different. KS statistic = {ks_stat:.4f}, p-value = {p_value:.4f}\"\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 715, "library_problem_id": 4, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 3}}
{"id": 716, "code": ["<code>\nfrom scipy.optimize import minimize\nimport numpy as np\n\n# Define the function\ndef f(x):\n    a, b, c = x\n    return ((a + b - c) - 2)**2 + ((3 * a - b - c))**2 + np.sin(b) + np.cos(b) + 4\n\n# Define the initial guess\nx0 = [-1, 0, -3]\n\n# Use the minimize function\nresult_obj = minimize(f, x0)\n\n# Store the values of the variables at the minimum\nresult = result_obj.x.tolist()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 716, "library_problem_id": 5, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 5}}
{"id": 717, "code": ["<code>\nimport numpy as np\nfrom scipy.stats import norm\n\ndef z_to_p(z_scores):\n    # Convert the input to a numpy array if it's not already\n    if not isinstance(z_scores, np.ndarray):\n        z_scores = np.array(z_scores)\n\n    # Calculate the p-values using the CDF of the standard normal distribution\n    p_values = norm.cdf(z_scores)\n\n    return p_values\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 717, "library_problem_id": 6, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 718, "code": ["<code>\nimport scipy.stats\nimport numpy as np\n\nz_scores = [-3, -2, 0, 2, 2.5]\nmu = 3\nsigma = 4\n\n# Convert z-scores to values in the normal distribution\nvalues = scipy.stats.norm.ppf(z_scores, loc=mu, scale=sigma)\n\nprint(values)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 718, "library_problem_id": 7, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 719, "code": ["<code>\nimport numpy as np\nimport scipy.stats\n\np_values = [0.1, 0.225, 0.5, 0.75, 0.925, 0.95]\n\ndef p_to_z(p_values):\n    z_scores = scipy.stats.norm.ppf(p_values)\n    return z_scores\n\nz_scores = p_to_z(p_values)\nprint(z_scores)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 719, "library_problem_id": 8, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 720, "code": ["<code>\nfrom scipy.stats import lognorm\n\ndef lognormal_cdf(mu, stddev, x):\n    dist = lognorm(s=stddev, scale=np.exp(mu))\n    return dist.cdf(x)\n\nstddev = 2.0785\nmu = 1.744\nx = 25\n\nprint(lognormal_cdf(mu, stddev, x))  # yields 0.7434\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 720, "library_problem_id": 9, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 721, "code": ["<code>\nfrom scipy.stats import lognorm, norm\nimport numpy as np\n\ndef calculate_lognormal_stats(mu, sigma, total):\n    # Convert mu and sigma to shape and scale parameters for lognorm\n    shape = sigma / np.sqrt(total)\n    scale = np.exp(mu + sigma**2/2)\n\n    # Calculate expected value and median of lognormal distribution\n    expected_value = np.exp(np.log(scale) + lognorm.stats(s=shape, scale=scale, moments='m'))\n    median = np.exp(np.log(scale) + lognorm.stats(s=shape, scale=scale, moments='m') - 0.57721566490153286060651209008240243104215933593992)\n\n    return expected_value, median\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 721, "library_problem_id": 10, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 722, "code": ["<code>\nfrom scipy.sparse import csr_matrix\nimport numpy as np\n\ndef elementwise_multiply(csr_mat, vec):\n    # Check if the input is of the correct type\n    if not isinstance(csr_mat, csr_matrix):\n        raise TypeError(\"The first argument must be a SciPy sparse CSR matrix.\")\n    if not isinstance(vec, np.ndarray):\n        raise TypeError(\"The second argument must be a one-dimensional NumPy array.\")\n    if vec.ndim != 1:\n        raise ValueError(\"The second argument must be a one-dimensional NumPy array.\")\n\n    # Check if the dimensions of the input matrices are compatible for multiplication\n    if csr_mat.shape[1] != vec.shape[0]:\n        raise ValueError(\"The dimensions of the input matrices are not compatible for multiplication.\")\n\n    # Perform element-wise multiplication\n    result = csr_matrix((csr_mat.shape[0], 1), dtype=np.float64)\n    for i in range(csr_mat.shape[0]):\n        result[i, 0] = csr_mat[i, :].multiply(vec).sum()\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 722, "library_problem_id": 11, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 723, "code": ["<code>\nfrom scipy.sparse import csr_matrix\n\ndef multiply_csr_matrices(A: csr_matrix, B: csr_matrix) -> csr_matrix:\n    if A.shape[1] != B.shape[0]:\n        raise ValueError(\"Matrices are not compatible for multiplication\")\n    return A.dot(B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 723, "library_problem_id": 12, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 724, "code": ["<code>\nimport numpy as np\nfrom scipy.interpolate import NearestNDInterpolator\n\ndef interpolate_moisture(points, V, request):\n    # Create the interpolator\n    interp = NearestNDInterpolator(points, V)\n    \n    # Interpolate the moisture value at the given point\n    V_interp = interp(request)\n    \n    return V_interp\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 724, "library_problem_id": 13, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 725, "code": ["<code>\nimport numpy as np\nfrom scipy.interpolate import RegularGridInterpolator\n\ndef interpolate_moisture(data_points, request):\n    # Separate the data points into x, y, z, and V\n    x = data_points[:, 0]\n    y = data_points[:, 1]\n    z = data_points[:, 2]\n    V = data_points[:, 3]\n\n    # Create a grid from the data points\n    grid = (x, y, z)\n\n    # Create the interpolator\n    interpolator = RegularGridInterpolator(grid, V)\n\n    # Interpolate the moisture values at the request points\n    interpolated_V = interpolator(request)\n\n    # Return the interpolated values as a list\n    return interpolated_V.tolist()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 725, "library_problem_id": 14, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 726, "code": ["<code>\nimport numpy as np\nfrom scipy.ndimage import rotate\n\ndef rotate_point(data_orig, x0, y0, angle):\n    # Rotate the image\n    data_rot = rotate(data_orig, angle)\n    \n    # Calculate the center of the image\n    center_x = data_orig.shape[0] / 2\n    center_y = data_orig.shape[1] / 2\n    \n    # Calculate the new coordinates of the point\n    xrot = center_x + (x0 - center_x) * np.cos(np.radians(angle)) - (y0 - center_y) * np.sin(np.radians(angle))\n    yrot = center_y + (x0 - center_x) * np.sin(np.radians(angle)) + (y0 - center_y) * np.cos(np.radians(angle))\n    \n    return data_rot, xrot, yrot\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 726, "library_problem_id": 15, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 727, "code": ["<code>\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\narr = np.random.rand(4, 4)\nM = csr_matrix(arr)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 727, "library_problem_id": 16, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 728, "code": ["<code>\nfrom scipy.stats import kstest\nimport numpy as np\n\ndef poisson_simul(rate, T):\n    return np.random.exponential(scale=1/rate, size=int(T*rate))\n\nrate = 1.0\nT = 100.0\ntimes = poisson_simul(rate, T)\nprint(kstest(times, \"uniform\"))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 728, "library_problem_id": 17, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 729, "code": ["<code>\nfrom scipy import stats\nimport random\nimport numpy as np\n\ndef poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n    times = times[1:]\n\n    # Generate uniform distribution data\n    uniform_data = np.random.uniform(0, T, len(times))\n\n    # Perform K-S test\n    ks_test_result = stats.kstest(times, 'uniform', args=(0, T))\n\n    return ks_test_result.pvalue\n\nexample_rate = 1.0\nexample_T = 10\n\np_value = poisson_simul(example_rate, example_T)\nprint(f\"The p-value from the K-S test is: {p_value}\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 729, "library_problem_id": 18, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 730, "code": ["<code>\nfrom scipy import stats\nimport random\nimport numpy as np\n\ndef poisson_simul(rate, T):\n    times = [0]\n    while (times[-1] < T):\n        time = random.expovariate(rate)\n        if (times[-1] + time) <= T:\n            times.append(times[-1] + time)\n        else:\n            break\n    return times[1:]\n\ndef test_uniformity(times, T):\n    uniform_data = np.random.uniform(0, T, size=len(times))\n    ks_test_result = stats.kstest(times, 'uniform', args=(0, T))\n    return ks_test_result.pvalue > 0.05\n\nrate = 1.0\nT = 10.0\ntimes = poisson_simul(rate, T)\nprint(test_uniformity(times, T))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 730, "library_problem_id": 19, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 17}}
{"id": 731, "code": ["<code>\nfrom scipy.sparse import hstack\n\ndef concatenate_sparse_matrices(c1, c2):\n    # Horizontally concatenate the matrices\n    Feature = hstack((c1, c2), format='csr')\n    return Feature\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 731, "library_problem_id": 20, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 732, "code": ["<code>\nfrom scipy import sparse\n\nc1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\nc2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\n\n# Concatenate along the column axis\nfeature = sparse.hstack((c1, c2)).toarray()\nprint(feature)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 732, "library_problem_id": 21, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 733, "code": ["<code>\ndef count_open_spaces(grid):\n    count = 0\n    for row in grid:\n        for cell in row:\n            if cell == 0:\n                count += 1\n    return count\n\ngrid = [[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]]\nprint(count_open_spaces(grid))  # Output: 3\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 733, "library_problem_id": 22, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 734, "code": ["<code>\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nfrom scipy.optimize import linear_sum_assignment\n\ndef find_optimal_mapping(set1, set2):\n    # Calculate pairwise distances\n    distances = cdist(set1, set2)\n    \n    # Use the Hungarian algorithm (also known as the Kuhn-Munkres or KM algorithm) to solve the assignment problem\n    row_ind, col_ind = linear_sum_assignment(distances)\n    \n    # Check if a solution exists\n    if len(row_ind) != len(set1):\n        return None\n    \n    # Return the indices of the corresponding points in the first set\n    return col_ind\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 734, "library_problem_id": 23, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 735, "code": ["<code>\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\n\ndef find_optimal_mapping(points1, points2):\n    # Calculate the Manhattan distance between each pair of points\n    distances = np.abs(points1[:, np.newaxis, :] - points2[np.newaxis, :, :])\n    distances = distances.sum(axis=-1)\n\n    # Use the Hungarian algorithm to find the optimal one-to-one mapping\n    row_ind, col_ind = linear_sum_assignment(distances)\n\n    # Return the indices of the points in the first set that are mapped to the points in the second set\n    return col_ind\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 735, "library_problem_id": 24, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 736, "code": ["<code>\nfrom scipy.sparse import csr_matrix, lil_matrix\nimport numpy as np\n\ndef remove_diagonal(sparse_matrix):\n    if isinstance(sparse_matrix, lil_matrix):\n        sparse_matrix = lil_matrix(sparse_matrix)\n        np.fill_diagonal(sparse_matrix.A, 0)\n        return sparse_matrix\n    elif isinstance(sparse_matrix, csr_matrix):\n        sparse_matrix = csr_matrix(sparse_matrix)\n        n = min(sparse_matrix.shape)\n        row_indices = np.arange(n)\n        sparse_matrix = sparse_matrix[np.ix_(row_indices, row_indices)]\n        return sparse_matrix\n    else:\n        raise ValueError(\"Input matrix must be either lil_matrix or csr_matrix\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 736, "library_problem_id": 25, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 737, "code": ["<code>\nimport numpy as np\nfrom scipy import ndimage\n\ndef count_connected_regions(img, threshold=0.75):\n    # Apply a threshold to the image\n    img = img > threshold\n\n    # Label the connected regions in the image\n    labels, num_labels = ndimage.label(img)\n\n    return num_labels\n\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nimg /= img.max()\n\nprint(count_connected_regions(img))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 737, "library_problem_id": 26, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 738, "code": ["<code>\ndef apply_threshold(img, threshold):\n    return img > threshold\n\nresult = apply_threshold(img, threshold)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 738, "library_problem_id": 27, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 739, "code": ["<code>\ndef f(img):\n        threshold = 0.75\n        mask = img > threshold\n        labels, num_labels = ndimage.label(mask)\n        return num_labels\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 739, "library_problem_id": 28, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 740, "code": ["<code>\ndef apply_threshold(img, threshold):\n    return img > threshold\n\nresult = apply_threshold(img, threshold)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 740, "library_problem_id": 29, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 741, "code": ["<code>\nfrom scipy.sparse import csr_matrix, lil_matrix\n\ndef make_symmetric(sA):\n    if not isinstance(sA, (csr_matrix, lil_matrix)):\n        raise TypeError(\"Input must be a SciPy sparse matrix (either lil_matrix or csr_matrix)\")\n\n    if not sA.shape[0] == sA.shape[1]:\n        raise ValueError(\"Input matrix must be square\")\n\n    # Convert to lil_matrix for easy manipulation\n    sA = sA.tolil()\n\n    for i in range(sA.shape[0]):\n        for j in range(i+1, sA.shape[1]):\n            # If only one of the (i,j) and (j,i) entries is filled, fill the other one\n            if sA[i, j] != sA[j, i]:\n                sA[j, i] = sA[i, j]\n\n    # Convert back to csr_matrix for efficiency\n    sA = sA.tocsr()\n\n    return sA\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 741, "library_problem_id": 30, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 742, "code": ["<code>\nfrom scipy.sparse import csr_matrix, lil_matrix\n\ndef make_symmetric(mat):\n    if not isinstance(mat, (csr_matrix, lil_matrix)):\n        raise TypeError(\"Input must be a SciPy sparse matrix (either lil_matrix or csr_matrix)\")\n\n    if isinstance(mat, lil_matrix):\n        mat = csr_matrix(mat)\n\n    n = mat.shape[0]\n    for i in range(n):\n        for j in range(i+1, n):\n            mat[i, j] = mat[j, i] = (mat[i, j] + mat[j, i]) / 2\n\n    return mat\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 742, "library_problem_id": 31, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 743, "code": ["<code>\nimport numpy as np\nfrom scipy import ndimage\n\ndef remove_isolated_ones(arr):\n    # Convert the array to boolean\n    arr = arr.astype(bool)\n    \n    # Create a structuring element for 8-connectedness\n    struct = ndimage.generate_binary_structure(2, 2)\n    \n    # Erode the array\n    eroded = ndimage.binary_erosion(arr, structure=struct)\n    \n    # Create a mask of isolated '1' cells\n    isolated_ones_mask = arr & ~eroded\n    \n    # Set isolated '1' cells to '0'\n    arr[isolated_ones_mask] = False\n    \n    # Convert the array back to int\n    arr = arr.astype(int)\n    \n    return arr\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 743, "library_problem_id": 32, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 744, "code": ["<code>\nimport numpy as np\nfrom scipy.ndimage import binary_dilation, binary_erosion\n\ndef remove_isolated_cells(image):\n    # Convert the image to a binary image\n    binary_image = image != 0\n\n    # Perform binary dilation\n    dilated_image = binary_dilation(binary_image)\n\n    # Perform binary erosion\n    eroded_image = binary_erosion(binary_image)\n\n    # Identify isolated cells by comparing the dilated and eroded images\n    isolated_cells = np.logical_and(dilated_image, np.logical_not(eroded_image))\n\n    # Remove isolated cells from the original image\n    image[isolated_cells] = 0\n\n    return image\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 744, "library_problem_id": 33, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 745, "code": ["<code>\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\n\n# Calculate the sum of the elements in the first column of the sparse matrix\ncol_sum = col.sum()\nprint(col_sum)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 745, "library_problem_id": 34, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 746, "code": ["<code>\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\n\n# Calculate the sum of the elements in the first column of the sparse matrix\ncol_sum = col.sum()\nprint(col_sum)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 746, "library_problem_id": 35, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 34}}
{"id": 747, "code": ["<code>\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\n\n# Calculate the sum of the elements in the first column of the sparse matrix\ncol_sum = col.sum()\nprint(col_sum)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 747, "library_problem_id": 36, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 34}}
{"id": 748, "code": ["<code>\nHere is the completed function:\n\n```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\ndef fourier_series_fit(filename, harmonics=1, fourier='fourier8'):\n    # Load data from file\n    data = np.loadtxt(filename, delimiter=',')\n    x, y = data[:, 0], data[:, 1]\n\n    # Define Fourier series function\n    if fourier == 'fourier8':\n        def fourier_series(x, a0, a1, w1, a2, w2, a3, w3, a4, w4, a5, w5, a6, w6, a7, w7, a8, w8):\n            series = a0\n            for i in range(1, harmonics+1):\n                series += ai * np.cos(wi * x) + bi * np.sin(wi * x)\n            return series\n    elif fourier == 'fourier16':\n        def fourier_series(x, a0, a1, w1, a2, w2, a3, w3, a4, w4, a5, w5, a6, w6, a7, w7, a8, w8, a9, w9, a10, w10, a11, w11, a12, w12, a13, w13, a14, w14, a15, w15):\n            series = a0\n            for i in range(1, harmonics+1):\n                series += ai * np\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 748, "library_problem_id": 37, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 749, "code": ["<code>\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef calculate_distances(example_array):\n    # Flatten the array and remove zeros\n    patches = example_array[example_array != 0]\n\n    # Calculate pairwise distances\n    result = cdist(patches, patches, metric='euclidean')\n\n    # Replace the diagonal with inf to avoid self-comparison\n    np.fill_diagonal(result, np.inf)\n\n    # Find the minimum distance for each pair of patches\n    result = np.min(result, axis=1)\n\n    # Reshape the result back to the original array size\n    result = result.reshape(example_array.shape)\n\n    return result\n\n# Test the function\nexample_array = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nresult = calculate_distances(example_array)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 749, "library_problem_id": 38, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 750, "code": ["<code>\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\ndef calculate_distances(arr):\n    # Flatten the array and remove zeros\n    flat = arr[arr > 0].flatten()\n    # Get unique regions\n    regions = np.unique(flat)\n    # Initialize output array\n    distances = np.zeros((len(regions), len(regions)))\n    # Calculate distances for each region pair\n    for i in range(len(regions)):\n        for j in range(i+1, len(regions)):\n            # Get coordinates of regions i and j\n            coords_i = np.argwhere(arr == regions[i])\n            coords_j = np.argwhere(arr == regions[j])\n            # Calculate Manhattan distances\n            dist_i = pdist(coords_i, metric='cityblock')\n            dist_j = pdist(coords_j, metric='cityblock')\n            # Get minimum distance\n            min_dist = min(np.min(dist_i), np.min(dist_j))\n            # Store minimum distance\n            distances[i, j] = min_dist\n            distances[j, i] = min_dist\n    return distances\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 750, "library_problem_id": 39, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 751, "code": ["<code>\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\ndef calculate_distances(arr):\n    # Flatten the array and get unique IDs\n    unique_ids = np.unique(arr)\n    unique_ids = unique_ids[unique_ids != 0]  # Ignore zeros\n\n    # Initialize output array\n    output = np.zeros((len(unique_ids), len(unique_ids)))\n\n    # Calculate pairwise distances for each unique ID\n    for i, id1 in enumerate(unique_ids):\n        mask = arr == id1\n        coords = np.argwhere(mask)\n        distances = pdist(coords, 'euclidean')\n        min_distance = np.min(distances)\n        output[i, i] = min_distance\n\n        # Calculate pairwise distances with other unique IDs\n        for j, id2 in enumerate(unique_ids[i+1:]):\n            mask = arr == id2\n            coords = np.argwhere(mask)\n            distances = pdist(coords, 'euclidean')\n            min_distance = np.min(distances)\n            output[i, j+i+1] = min_distance\n            output[j+i+1, i] = min_distance\n\n    return output\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 751, "library_problem_id": 40, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 752, "code": ["<code>\nimport numpy as np\nfrom scipy.interpolate import splrep, splev\n\ndef extrapolate_curves(x):\n    # Initialize an empty list to store the extrapolated curves\n    extrapolated_curves = []\n\n    # Loop over each curve\n    for curve in x:\n        # Use splrep to get the spline representation of the curve\n        tck = splrep(curve, np.arange(len(curve)))\n\n        # Use splev to evaluate the spline at the desired points for extrapolation\n        extrapolated_curve = splev(np.linspace(0, len(curve)-1, 100), tck)\n\n        # Append the extrapolated curve to the list\n        extrapolated_curves.append(extrapolated_curve)\n\n    # Convert the list of extrapolated curves to a numpy array and return it\n    return np.array(extrapolated_curves)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 752, "library_problem_id": 41, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 753, "code": ["<code>\nHere is the completed code:\n\n```python\nimport numpy as np\nimport scipy.stats as ss\n\nx1 = [38.7, 41.5, 43.8, 44.5, 45.5, 46.0, 47.7, 58.0]\nx2 = [39.2, 39.3, 39.7, 41.4, 41.8, 42.9, 43.3, 45.8]\nx3 = [34.0, 35.0, 39.0, 40.0, 43.0, 43.0, 44.0, 45.0]\nx4 = [38.0, 38.5, 39.5, 40.0, 41.0, 41.5, 42.5, 44.0]\n\n# Convert the datasets to numpy arrays\nx1 = np.array(x1)\nx2 = np.array(x2)\nx3 = np.array(x3)\nx4 = np.array(x4)\n\n# Pass the datasets to the anderson_ksamp function\nresult = ss.anderson_ksamp([x1, x2, x3, x4])\n\n# Print the result\nprint('Statistic: %.3f' % result.statistic)\nfor i in range(len(result.critical_values)):\n    sl, cv = result.significance_level[i], result.crit\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 753, "library_problem_id": 42, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 754, "code": ["<code>\nfrom scipy.stats import anderson_ksamp\n\ndef two_sample_anderson_darling_test(sample1, sample2):\n    # Combine the two samples into a list of lists\n    samples = [sample1, sample2]\n    \n    # Perform the Anderson-Darling test\n    result = anderson_ksamp(samples)\n    \n    # The critical values for a 5% significance level are the last element of the result\n    critical_value = result.critical_values[-1]\n    \n    # If the test statistic is greater than the critical value, we reject the null hypothesis\n    # and conclude that the samples are not drawn from the same population\n    if result.statistic > critical_value:\n        return False\n    else:\n        return True\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 754, "library_problem_id": 43, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 755, "code": ["<code>\nimport pandas as pd\nimport scipy.stats as stats\n\ndef rolling_kendall_tau(df, window):\n    # Initialize an empty DataFrame to store the results\n    result = pd.DataFrame()\n\n    # Iterate over each pair of columns in the DataFrame\n    for i in range(len(df.columns)):\n        for j in range(i+1, len(df.columns)):\n            # Calculate the rolling Kendall Tau correlation between the two columns\n            rolling_tau = df.iloc[:, i].rolling(window=window).apply(lambda x: stats.kendalltau(x, df.iloc[:, j])[0])\n            \n            # Add the rolling Kendall Tau correlation to the result DataFrame\n            result[f'{df.columns[i]}_{df.columns[j]}'] = rolling_tau\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 755, "library_problem_id": 44, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 756, "code": ["<code>\nfrom scipy.sparse import csr_matrix\n\ndef is_csr_matrix_only_zeroes(matrix: csr_matrix) -> bool:\n    return len(matrix.nonzero()[0]) == 0\n\n# Testing the function\nprint(is_csr_matrix_only_zeroes(csr_matrix([[1,2,0],[0,0,3],[4,0,5]])))  # should return False\nprint(is_csr_matrix_only_zeroes(csr_matrix([[0,0,0],[0,0,0],[0,0,0]])))  # should return True\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 756, "library_problem_id": 45, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 757, "code": ["<code>\nfrom scipy.sparse import lil_matrix\n\ndef is_lil_matrix_only_zeroes(matrix):\n    non_zero_indices = matrix.nonzero()\n    if len(non_zero_indices[0]) == 0:\n        return True\n    else:\n        return False\n\n# Testing the function\nprint(is_lil_matrix_only_zeroes(lil_matrix([[1,2,0],[0,0,3],[4,0,5]])))  # should return False\nprint(is_lil_matrix_only_zeroes(lil_matrix([[0,0,0],[0,0,0],[0,0,0]])))  # should return True\nprint(is_lil_matrix_only_zeroes(lil_matrix((2,3))))  # should return True\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 757, "library_problem_id": 46, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 758, "code": ["<code>\nimport numpy as np\n\ndef convert_to_block_diagonal(arr):\n    return np.block(arr)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 758, "library_problem_id": 47, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 759, "code": ["<code>\nfrom scipy import stats\nimport numpy as np\n\ndef calculate_p_value(pre_course_scores, during_course_scores):\n    t_stat, p_val = stats.ttest_ind(pre_course_scores, during_course_scores)\n    return p_val\n\npre_course_scores = np.random.randn(10)\nduring_course_scores = np.random.randn(10)\np_value = calculate_p_value(pre_course_scores, during_course_scores)\nprint(p_value)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 759, "library_problem_id": 48, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 760, "code": ["<code>\nimport numpy as np\nfrom scipy import stats\n\ndef wilcoxon_ranksums(pre_scores, during_scores):\n    _, p_value = stats.ranksums(pre_scores, during_scores)\n    return p_value\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 760, "library_problem_id": 49, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 761, "code": ["<code>\nimport numpy as np\n\ndef calculate_kurtosis(data):\n    # Remove NaN and infinite values\n    data = data[np.isfinite(data)]\n    \n    # Calculate mean\n    mean = np.mean(data)\n    \n    # Calculate variance\n    variance = np.mean((data - mean)**2)\n    \n    # Calculate skewness\n    skewness = np.mean((data - mean)**3) / (variance**1.5)\n    \n    # Calculate kurtosis\n    kurtosis = np.mean((data - mean)**4) / (variance**2) - 3\n    \n    return kurtosis\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 761, "library_problem_id": 50, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 762, "code": ["<code>\nimport numpy as np\nfrom scipy.stats import kurtosis\n\n# Example dataset\na = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n# Calculate kurtosis\nkurtosis_result = kurtosis(a)\n\nprint(kurtosis_result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 762, "library_problem_id": 51, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 763, "code": ["<code>\nimport numpy as np\nimport scipy.interpolate\n\ndef fuelConsumption(speed, temperature):\n    # Given data points and values\n    s = np.linspace(-1, 1, 50)\n    t = np.linspace(-2, 0, 50)\n    x, y = np.meshgrid(s, t)\n    z = np.sin(np.sqrt(x**2 + y**2))\n\n    # Interpolation\n    interp_function = scipy.interpolate.interp2d(x, y, z, kind='cubic')\n    interpolated_values = interp_function(speed, temperature)\n\n    return interpolated_values\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 763, "library_problem_id": 52, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 764, "code": ["<code>\nimport numpy as np\nimport scipy.interpolate\n\ndef cubic_interpolation_3d(f, s, t):\n    # Create a grid of points (s, t)\n    s_grid, t_grid = np.meshgrid(s, t)\n\n    # Interpolate the function f on the grid\n    interp_func = scipy.interpolate.interp2d(s, t, f(s, t), kind='cubic')\n\n    # Evaluate the interpolated function on the grid\n    interp_values = interp_func(s_grid, t_grid)\n\n    # Flatten the 2D array of interpolated values into a 1D vector\n    interp_values = interp_values.flatten()\n\n    return interp_values\n\n# Define a 3D function\ndef f(x, y):\n    return x**2 + y**2\n\n# Define the 1D vectors s and t\ns = np.linspace(-1, 1, 50)\nt = np.linspace(-2, 2, 50)\n\n# Call the function\ninterp_values = cubic_interpolation_3d(f, s, t)\n\nprint(interp_values)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 764, "library_problem_id": 53, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 52}}
{"id": 765, "code": ["<code>\nimport scipy.spatial\n\npoints = [[0,0], [1,4], [2,3], [4,1], [1,1], [2,2], [5,3]]\nvor = scipy.spatial.Voronoi(points)\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\n\n# Add the extra points to the Voronoi diagram\nvor.add_points(extraPoints)\n\n# Recompute the diagram\nvor.compute()\n\n# Print the regions of the Voronoi diagram\nfor region in vor.regions:\n    if not -1 in region:\n        print(region)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 765, "library_problem_id": 54, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 766, "code": ["<code>\nimport scipy.spatial\n\ndef count_extra_points(points, extraPoints):\n    vor = scipy.spatial.Voronoi(points)\n    extraPoints = scipy.spatial.Voronoi(extraPoints)\n\n    vor_regions = vor.regions\n    extra_regions = extraPoints.regions\n\n    vor_counts = {i: len(vor_regions[i]) for i in range(len(vor_regions)) if i >= 0}\n    extra_counts = {i: len(extra_regions[i]) for i in range(len(extra_regions)) if i >= 0}\n\n    return {point: extra_counts[region] for point, region in zip(extraPoints.points, extraPoints.ridge_dict.keys())}\n\npoints = [[0,0], [1,4], [2,3], [4,1], [1,1], [2,2], [5,3]]\nextraPoints = [[0.5, 2]]\n\nprint(count_extra_points(points, extraPoints))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 766, "library_problem_id": 55, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 767, "code": ["<code>\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\ndef create_sparse_matrix(vectors, max_vector_size):\n    # Convert the vectors to a NumPy array\n    vectors = np.array(vectors)\n\n    # Pad the vectors with zeros to match the length of the longest vector\n    padded_vectors = np.zeros((len(vectors), max_vector_size))\n    for i, vector in enumerate(vectors):\n        padded_vectors[i, :len(vector)] = vector\n\n    # Create a sparse matrix from the padded vectors\n    result = csr_matrix(padded_vectors)\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 767, "library_problem_id": 56, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 768, "code": ["<code>\nimport numpy as np\nimport scipy.ndimage as nd\n\na = np.random.binomial(n=1, p=1/2, size=(9, 9))\nb = nd.median_filter(a, 3)\n\n# Shift the filter one cell to the right\nb_shifted = np.zeros_like(b)\nb_shifted[:, 1:] = b[:, :-1]\nb_shifted[:, 0] = b[:, -1]\n\nprint(\"Original array:\\n\", a)\nprint(\"\\nMedian filtered array:\\n\", b)\nprint(\"\\nMedian filtered array shifted one cell to the right:\\n\", b_shifted)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 768, "library_problem_id": 57, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 769, "code": ["<code>\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\ndef get_element(matrix, row, col):\n    if row < 0 or row >= matrix.shape[0] or col < 0 or col >= matrix.shape[1]:\n        raise IndexError('Index out of range')\n    row_indices = matrix.indices[matrix.indptr[row]:matrix.indptr[row+1]]\n    if col in row_indices:\n        return matrix.data[np.where(row_indices == col)][0]\n    else:\n        return 0\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 769, "library_problem_id": 58, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 770, "code": ["<code>\nHere is the completed code:\n\n```python\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\ndef get_values(sparse_matrix, row_indices, col_indices):\n    # Get the data, row indices, and column indices of the sparse matrix\n    data = sparse_matrix.data\n    row_ptr = sparse_matrix.indptr\n    col_ind = sparse_matrix.indices\n\n    # Initialize an empty list to store the values\n    values = []\n\n    # Iterate over each row index and column index pair\n    for row, col in zip(row_indices, col_indices):\n        # Get the start and end indices of the row in the column indices array\n        start = row_ptr[row]\n        end = row_ptr[row+1]\n\n        # Find the index of the column in the column indices array\n        col_index = np.where((col_ind[start:end] == col) & (row_indices[start:end] == row))[0]\n\n        # If the column index is found, append the corresponding data value to the values list\n        if col_index.size > 0:\n            values.append(data[start + col_index[0]])\n        # If the column index is not found, append 0 to the values list\n        else:\n            values.append(0)\n\n    return values\n\n# Test the function\narr = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 770, "library_problem_id": 59, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 771, "code": ["<code>\nimport numpy as np\nfrom scipy.interpolate import interp1d\n\ndef interpolate_3d_array(arr):\n    # Create an empty array to store the interpolated values\n    interpolated_arr = np.empty((100, 10, 10))\n\n    # Loop over the first axis of the array\n    for i in range(10):\n        # Loop over the second and third axes of the array\n        for j in range(10):\n            for k in range(10):\n                # Create an interpolation function for each element in the second and third axes\n                f = interp1d(x, arr[i, j, k])\n                # Apply the interpolation function to the new x values\n                interpolated_arr[:, j, k] = f(x_new)\n\n    return interpolated_arr\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 771, "library_problem_id": 60, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 60}}
{"id": 772, "code": ["<code>\nfrom scipy.stats import norm\n\ndef NormalDistro(u, o2, x):\n    std_dev = o2 ** 0.5\n    return norm.cdf(x, u, std_dev)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 772, "library_problem_id": 61, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 773, "code": ["<code>\nimport scipy.integrate as spi\nimport math\n\ndef NormalDistro(u, o2, x):\n    if not isinstance(u, (int, float)) or not isinstance(o2, (int, float)) or not isinstance(x, (int, float)):\n        raise TypeError(\"All inputs must be of type int or float\")\n    if o2 < 0:\n        raise ValueError(\"Variance must be non-negative\")\n    if o2 == 0:\n        if x < u:\n            return 0.0\n        else:\n            return 1.0\n\n    z = (x - u) / math.sqrt(o2)\n    return 0.5 * (1 + spi.erf(z / math.sqrt(2)))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 773, "library_problem_id": 62, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 774, "code": ["<code>\nSure, I'll need more information about the function you want to complete. Could you please provide more details? For example, what is the purpose of the function? What is the input and output? What is the expected behavior?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 774, "library_problem_id": 63, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 775, "code": ["<code>\nimport numpy as np\nfrom scipy.sparse import diags\n\nv1 = [3*i**2 +(i/2) for i in range(1, 6)]\nv2 = [-(6*i**2 - 1) for i in range(1, 6)]\nv3 = [3*i**2 -(i/2) for i in range(1, 6)]\n\ndiagonals = [v1, v2, v3]\noffsets = [-1, 0, 1]\n\ntridiagonal_matrix = diags(diagonals, offsets).toarray()\n\nprint(tridiagonal_matrix)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 775, "library_problem_id": 64, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 776, "code": ["<code>\nimport numpy as np\nimport scipy.stats\n\ndef generate_binomial_matrix(N, p):\n    # Generate a binomial matrix of size N x N with probability p\n    binomial_matrix = np.random.binomial(1, p, (N, N))\n    return binomial_matrix\n\nN = 3\np = 0.5\nresult = generate_binomial_matrix(N, p)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 776, "library_problem_id": 65, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 777, "code": ["<code>\nimport pandas as pd\nfrom scipy import stats\n\ndef normalize_zscore(df: pd.DataFrame) -> pd.DataFrame:\n    return pd.DataFrame(stats.zscore(df, axis=1), index=df.index, columns=df.columns)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 777, "library_problem_id": 66, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 778, "code": ["<code>\nimport pandas as pd\nfrom scipy import stats\n\ndef calculate_z_scores(df):\n    return pd.DataFrame(stats.zscore(df, axis=0), index=df.index, columns=df.columns)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 778, "library_problem_id": 67, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 66}}
{"id": 779, "code": ["<code>\nimport csv\n\ndef process_data(filename):\n    with open(filename, 'r') as file:\n        reader = csv.reader(file)\n        next(reader)  # Skip the header\n        data = list(reader)\n    return data\n\ndata = process_data('data.csv')\nfor row in data:\n    print(row)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 779, "library_problem_id": 68, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 780, "code": ["<code>\nimport csv\n\ndef process_data(filename):\n    with open(filename, 'r') as file:\n        reader = csv.reader(file)\n        next(reader)  # Skip the header\n        data = list(reader)\n    return data\n\ndata = process_data('data.csv')\nfor row in data:\n    print(row)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 780, "library_problem_id": 69, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 781, "code": ["<code>\nThe `line_search` function requires the following arguments:\n\n- `func`: The function to be minimized.\n- `my_func`: The function to be minimized.\n- `grad`: The gradient of the function to be minimized.\n- `my_grad`: The gradient of the function to be minimized.\n- `xk`: The current point.\n- `p`: The direction of the next step.\n- `c1` and `c2`: Constants for sufficient decrease and curvature conditions.\n\nThe `line_search` function returns the following:\n\n- `alpha`: The step size.\n- `p`: The direction of the next step.\n- `dists`: The distances traveled along the line.\n- `desc`: A flag indicating whether the function decreased.\n- `asc`: A flag indicating whether the function increased.\n- `stp`: A flag indicating whether the function is a sufficient decrease.\n\nHere is the corrected code:\n\n```python\nimport scipy\nimport scipy.optimize\nimport numpy as np\n\ndef test_func(x):\n    return (x[0])**2+(x[1])**2\n\ndef test_grad(x):\n    return [2*x[0],2*x[1]]\n\nstarting_point = np.array([0, 0])\ndirection_of_descent = np.array([1, 1])\n\nresult = scipy.optimize.line_search(test_func, test_grad, starting_point, direction_of_descent)\n\nprint(f\"The optimal step size is: {result[0\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 781, "library_problem_id": 70, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 782, "code": ["<code>\nimport numpy as np\nfrom scipy.spatial import distance\n\ndef create_distance_array(shape):\n    # Create a 2D array of zeros with the given shape\n    arr = np.zeros(shape)\n\n    # Calculate the center of the array\n    center = (shape[0] // 2, shape[1] // 2)\n\n    # Iterate over each element in the array\n    for i in range(shape[0]):\n        for j in range(shape[1]):\n            # Calculate the Euclidean distance from the center\n            arr[i, j] = distance.euclidean((i, j), center)\n\n    return arr\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 782, "library_problem_id": 71, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 783, "code": ["<code>\nimport numpy as np\n\ndef manhattan_distance_3d_to_2d(shape_3d):\n    # Create a 2D array with the same shape as the first two dimensions of the 3D array\n    array_2d = np.zeros((shape_3d[0], shape_3d[1]))\n    \n    # Calculate the center of the 2D array\n    center = (shape_3d[0] // 2, shape_3d[1] // 2)\n    \n    # Iterate over each element in the 2D array\n    for i in range(shape_3d[0]):\n        for j in range(shape_3d[1]):\n            # Calculate the Manhattan distance from the center\n            array_2d[i, j] = abs(i - center[0]) + abs(j - center[1])\n    \n    return array_2d\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 783, "library_problem_id": 72, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 71}}
{"id": 784, "code": ["<code>\nimport numpy as np\nfrom scipy.spatial import distance_matrix\n\ndef generate_euclidean_distances(shape):\n    # Create a 2D array of indices\n    indices = np.indices(shape[:2])\n    \n    # Calculate the center of the array\n    center = np.array([(x-1)//2 for x in shape[:2]])\n    \n    # Calculate the Euclidean distances from the center\n    distances = distance_matrix(indices.reshape(2, -1).T, center.reshape(1, -1))\n    \n    return distances.reshape(shape[:2])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 784, "library_problem_id": 73, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 785, "code": ["<code>\nimport numpy as np\nfrom scipy.ndimage import zoom\n\ndef resample_array(x, target_shapes):\n    original_shape = x.shape\n    best_mse = np.inf\n    best_resampled = None\n\n    for target_shape in target_shapes:\n        if target_shape == original_shape:\n            return x\n        resampled = zoom(x, target_shape, order=1)\n        mse = np.mean((x.ravel() - resampled.ravel())**2)\n        if mse < best_mse:\n            best_mse = mse\n            best_resampled = resampled\n\n    return best_resampled\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 785, "library_problem_id": 74, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 786, "code": ["<code>\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef minimize_function(coefficients, initial_guess):\n    \"\"\"\n    This function minimizes a function of the form yn = a_11*x1**2 + a_12*x2**2 + ... + a_m*xn**2 using the scipy.optimize library.\n\n    Parameters:\n    coefficients (list): The coefficients a_m.\n    initial_guess (list): The initial guess for the unknowns x1, x2,..., xn.\n\n    Returns:\n    x (numpy array): The optimal x array.\n    \"\"\"\n\n    # Define the function to be minimized\n    def func_to_minimize(x):\n        return np.sum(coefficients * np.array([i**2 for i in x]))\n\n    # Use scipy.optimize.minimize to find the optimal values\n    result = minimize(func_to_minimize, initial_guess)\n\n    # Return the optimal x array\n    return result.x\n\n# Test the function with a simple example\ncoefficients = [1, 2, 3]\ninitial_guess = [1, 1, 1]\nprint(minimize_function(coefficients, initial_guess))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 786, "library_problem_id": 75, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 787, "code": ["<code>\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef minimize_squared_differences(a, x, y, x0):\n    # Define the quadratic function\n    def quad_func(x):\n        return a[0] + a[1]*x + a[2]*x**2\n\n    # Define the objective function to minimize squared differences\n    def objective_func(a):\n        return np.sum((y - quad_func(x))**2)\n\n    # Define the lower bounds for x\n    lb = np.full(len(x0), -np.inf)\n\n    # Use scipy.optimize.minimize to find the optimal values of a\n    result = minimize(objective_func, x0, method='L-BFGS-B', bounds=lb)\n\n    return result.x\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 787, "library_problem_id": 76, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 75}}
{"id": 788, "code": ["<code>\nI'm sorry, but your question is not clear. Could you please provide more details about the function you want to complete? Are you looking for a function to integrate a specific function? Or are you looking for a way to solve a differential equation? The more specific you are, the better I can assist you.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 788, "library_problem_id": 77, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 789, "code": ["<code>\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve_ode_with_variable_input(fun, t_span, y0, input_func):\n    def fun_wrapper(t, y):\n        return fun(t, y, input_func(t))\n\n    sol = solve_ivp(fun_wrapper, t_span, y0, t_eval=np.linspace(t_span[0], t_span[1], 1000))\n\n    return sol.y[:, sol.t.size // 2:]\n\ndef dN1_dt_simple(t, N1, input_func):\n    return -100 * N1 + input_func(t)\n\ninput_func = lambda t: t - np.sin(t) if 0 < t < 2*np.pi else 0\nt_span = (0, 2*np.pi)\ny0 = [1]\n\nprint(solve_ode_with_variable_input(dN1_dt_simple, t_span, y0, input_func))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 789, "library_problem_id": 78, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 790, "code": ["<code>\nimport scipy\n\ndef dN1_dt_simple(t, N1):\n    return -100 * N1\n\ntime_span = [0, 10]  # Define the time span\nN0 = 100  # Initial condition\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\n\n# Print the solution\nprint(sol)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 790, "library_problem_id": 79, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 791, "code": ["<code>\ndef function(x):\n    if not isinstance(x, list) or len(x) != 4:\n        raise ValueError(\"Input must be a list of exactly four numbers\")\n    for i in x:\n        if not isinstance(i, (int, float)):\n            raise ValueError(\"All elements of the input list must be numbers\")\n    return -1*(18*x[0]+16*x[1]+12*x[2]+11*x[3])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 791, "library_problem_id": 80, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 792, "code": ["<code>\nfrom scipy import sparse\n\n# Given example\na = sparse.csr_matrix([[1,2,3],[4,5,6]])\nb = sparse.csr_matrix([[7,8,9],[10,11,12]])\n\n# Solution\nresult = sparse.vstack([a, b])\n\n# Starter Code example\nsa = sparse.random(10, 10, density = 0.01, format = 'csr')\nsb = sparse.random(10, 10, density = 0.01, format = 'csr')\n\n# Solution\nresult = sparse.vstack([sa, sb])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 792, "library_problem_id": 81, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 793, "code": ["<code>\nfrom scipy.sparse import csr_matrix, hstack\n\ndef merge_csr_matrices(matrix1: csr_matrix, matrix2: csr_matrix) -> csr_matrix:\n    \"\"\"\n    Merge two SciPy sparse CSR matrices horizontally without converting to dense NumPy arrays.\n\n    Parameters:\n    matrix1 (csr_matrix): The first matrix to be merged.\n    matrix2 (csr_matrix): The second matrix to be merged.\n\n    Returns:\n    csr_matrix: The merged matrix.\n    \"\"\"\n    if matrix1.shape[0] != matrix2.shape[0]:\n        raise ValueError(\"The number of rows of the two matrices must be the same.\")\n\n    return hstack([matrix1, matrix2])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 793, "library_problem_id": 82, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 81}}
{"id": 794, "code": ["<code>\nIt seems like you've provided a Python code snippet with import statements and variable assignments, but you haven't provided a function to complete. Could you please provide the function that you want to be completed?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 794, "library_problem_id": 83, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 795, "code": ["<code>\nimport numpy as np\nfrom scipy import integrate\n\ndef calculate_integrals(filename):\n    c = np.loadtxt(filename, unpack=True, usecols=[1])\n    I = []\n    for n in range(len(c)):\n        eqn = lambda x: 2*x*c[n]\n        result, error = integrate.quad(eqn, 0, 1)\n        I.append(result)\n    return np.array(I)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 795, "library_problem_id": 84, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 83}}
{"id": 796, "code": ["<code>\nfrom scipy.sparse import dok_matrix\n\ndef add_scalar_to_sparse(V, x):\n    if not isinstance(V, dok_matrix):\n        raise ValueError(\"Input matrix must be in dok_matrix format.\")\n    if not isinstance(x, (int, float)):\n        raise ValueError(\"Scalar must be a number.\")\n\n    # Create a new dok_matrix with the same shape as V\n    new_matrix = dok_matrix(V.shape)\n\n    # Iterate over the non-zero elements in V\n    for (i, j), value in V.items():\n        # Add x to the value of the non-zero element\n        new_matrix[i, j] = value + x\n\n    return new_matrix\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 796, "library_problem_id": 85, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 797, "code": ["<code>\nfrom scipy.sparse import coo_matrix\n\ndef add_scalar_to_sparse(V, x):\n    if not isinstance(V, coo_matrix):\n        raise ValueError(\"Input matrix must be in COO format\")\n\n    # Get the row, col and data arrays from the COO matrix\n    rows, cols, data = V.row, V.col, V.data\n\n    # Add the scalar value to the data array\n    data += x\n\n    # Return a new COO matrix with the modified data array\n    return coo_matrix((data, (rows, cols)), shape=V.shape)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 797, "library_problem_id": 86, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 798, "code": ["<code>\nfrom scipy.sparse import coo_matrix\n\ndef add_scalars_to_sparse(V, x, y):\n    # Get the non-zero elements of the sparse matrix\n    rows, cols, values = V.nonzero()\n    \n    # Add the scalar values to the non-zero elements\n    values += x\n    values += y\n    \n    # Create a new sparse matrix with the updated values\n    new_V = coo_matrix((values, (rows, cols)), shape=V.shape)\n    \n    return new_V\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 798, "library_problem_id": 87, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 799, "code": ["<code>\nfrom scipy import sparse\nimport numpy as np\nimport math\n\ndef normalize_columns(sparse_matrix):\n    sparse_matrix = sparse_matrix.tocsc()\n    for Col in range(sparse_matrix.shape[1]):\n        column = sparse_matrix[:,Col].data\n        column_length = math.sqrt(sum([x**2 for x in column]))\n        sparse_matrix[:,Col].data = column / column_length\n    return sparse_matrix\n\n# Test the function\nsparse_matrix = sparse.random(10, 10, density = 0.3, format = 'csc')\nnormalized_matrix = normalize_columns(sparse_matrix)\nprint(normalized_matrix)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 799, "library_problem_id": 88, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 800, "code": ["<code>\nfrom scipy import sparse\nimport numpy as np\nimport math\n\n# Create a sparse matrix\nsa = sparse.random(10, 10, density = 0.3, format = 'csr', random_state = 42)\n\n# Calculate the sum of all elements in the sparse matrix\nsum_of_elements = sa.sum()\n\nprint(f\"The sum of all elements in the sparse matrix is: {sum_of_elements}\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 800, "library_problem_id": 89, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 88}}
{"id": 801, "code": ["<code>\nimport numpy as np\nfrom scipy.sparse import coo_matrix\n\ndef email_to_binary_adjacency_matrix(email_matrix):\n    email_matrix = np.array(email_matrix)\n    rows, cols = email_matrix.shape\n    row_indices, col_indices = np.where(email_matrix > 0)\n    data = np.ones(len(row_indices))\n    adjacency_matrix = coo_matrix((data, (row_indices, col_indices)), shape=(rows, cols)).toarray()\n    adjacency_matrix = (adjacency_matrix + adjacency_matrix.T)/2\n    adjacency_matrix[adjacency_matrix > 0] = 1\n    return adjacency_matrix\n\nemail_matrix = [[26, 3, 0], [3, 195, 1], [0, 1, 17]]\nprint(email_to_binary_adjacency_matrix(email_matrix))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 801, "library_problem_id": 90, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 802, "code": ["<code>\nimport numpy as np\n\ndef convert_to_binary(matrix):\n    return (matrix > 0).astype(int)\n\n# Test the function\ninput_matrix = np.array([[26, 3, 0], [3, 195, 1], [0, 1, 17]])\noutput_matrix = convert_to_binary(input_matrix)\nprint(output_matrix)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 802, "library_problem_id": 91, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 803, "code": ["<code>\nimport numpy as np\n\ndef find_closest_to_centroids(data, centroids, labels):\n    result = []\n    for i in range(centroids.shape[0]):\n        cluster_points = data[labels == i]\n        distances = np.linalg.norm(cluster_points - centroids[i], axis=1)\n        closest_point_index = np.argmin(distances)\n        result.append(closest_point_index)\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 803, "library_problem_id": 92, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 92}}
{"id": 804, "code": ["<code>\ndistances = scipy.spatial.distance.cdist(centroids, data)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 804, "library_problem_id": 93, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 92}}
{"id": 805, "code": ["<code>\nimport numpy as np\nfrom scipy.cluster.hierarchy import linkage, cut_tree\n\ndef find_kth_closest(distance_matrix, cluster_assignments, k):\n    # Calculate centroids\n    centroids = np.array([np.mean(distance_matrix[cluster_assignments == i], axis=0) for i in np.unique(cluster_assignments)])\n    \n    # Initialize an empty list to store the indices of k-th closest data points\n    kth_closest_indices = []\n    \n    # Iterate over each cluster\n    for i in np.unique(cluster_assignments):\n        # Get the indices of data points in the current cluster\n        indices = np.where(cluster_assignments == i)[0]\n        \n        # Calculate the distances from the centroid to the data points in the current cluster\n        distances = np.linalg.norm(distance_matrix[indices] - centroids[i], axis=1)\n        \n        # Get the index of the k-th closest data point\n        kth_closest_index = indices[np.argsort(distances)][k]\n        \n        # Append the index to the list\n        kth_closest_indices.append(kth_closest_index)\n    \n    return kth_closest_indices\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 805, "library_problem_id": 94, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 92}}
{"id": 806, "code": ["<code>\nfrom scipy.optimize import fsolve\nimport numpy as np\n\ndef solve_equation(a, xdata, bdata):\n    # Define the function to be solved\n    def func(x):\n        return x + 2*a - bdata**2\n\n    # Use fsolve to solve the function for each x value in xdata\n    result = [fsolve(func, x) for x in xdata]\n\n    return np.array(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 806, "library_problem_id": 95, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 807, "code": ["<code>\nimport numpy as np\nfrom scipy.optimize import fsolve\n\ndef eqn(x, a, b):\n    return x + 2*a - b**2\n\nxdata = np.arange(4)+3\nadata = np.random.randint(0, 10, (4,))\n\nresult = fsolve(eqn, xdata, args=(adata, xdata[0]))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 807, "library_problem_id": 96, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 808, "code": ["<code>\np(x) = a * exp(-(x^(1/3) - m)^2 / (2d^2)) * x^(-2/3)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 808, "library_problem_id": 97, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 809, "code": ["<code>\np(x) = a * exp(-(x^(1/3) - m)^2 / (2d^2)) * x^(-2/3)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 809, "library_problem_id": 98, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 810, "code": ["<code>\nimport pandas as pd\nimport numpy as np\nfrom scipy import integrate\n\ndef rolling_integral(df, column):\n    # Fill NaN values with 0\n    df_filled = df.fillna(0)\n    \n    # Ensure the time index is uniformly spaced\n    if df_filled.index.freq is None:\n        df_filled = df_filled.resample('25S').mean()\n    \n    # Calculate rolling integral using scipy.integrate.trapz\n    integral = df_filled[column].rolling(window=len(df_filled)).apply(integrate.trapz, args=(np.arange(len(df_filled)),))\n    \n    # Return the rolling integral as a new DataFrame\n    return pd.DataFrame(integral, columns=[f'{column}_integral'])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 810, "library_problem_id": 99, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 811, "code": ["<code>\nx = [(2,2), (1,2), (2,3), (3,2), (2,1)]\ny = [5,7,8,10,3]\n\n# Sort the tuples in x based on the second element of each tuple\nx.sort(key=lambda tup: tup[1])\n\n# Sort the elements in y based on the order of the sorted x\ny.sort(key=lambda tup: x.index(tup))\n\nprint(x)  # Output: [(2, 1), (1, 2), (2, 2), (3, 2), (2, 3)]\nprint(y)  # Output: [3, 7, 5, 10, 8]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 811, "library_problem_id": 100, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 812, "code": ["<code>\ndef complete_function(n):\n    data = {\n        0: 0,\n        1: 1,\n        2: 2,\n        3: 3,\n        4: 2,\n        5: 1,\n        6: 6,\n        7: 0,\n        8: 1,\n        9: 1,\n        10: 7,\n        11: 10\n    }\n    return data.get(n, \"No data found\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 812, "library_problem_id": 101, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 101}}
{"id": 813, "code": ["<code>\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef optimize_parameters(objective_function, lower_bounds, upper_bounds, additional_args=()):\n    # Check if bounds are valid\n    if len(lower_bounds) != len(upper_bounds):\n        raise ValueError(\"Lower and upper bounds must have the same length.\")\n    if np.any(lower_bounds > upper_bounds):\n        raise ValueError(\"Lower bounds must be less than or equal to upper bounds.\")\n\n    # Convert bounds to numpy arrays\n    lower_bounds = np.array(lower_bounds)\n    upper_bounds = np.array(upper_bounds)\n\n    # Check if bounds are valid for the objective function\n    if np.any(objective_function(lower_bounds, *additional_args) >= objective_function(upper_bounds, *additional_args)):\n        raise ValueError(\"Objective function must be strictly unimodal within the given bounds.\")\n\n    # Use SciPy's optimization function to find the parameters that minimize the objective function\n    result = minimize(objective_function, lower_bounds, bounds=list(zip(lower_bounds, upper_bounds)), args=additional_args)\n\n    # If the optimization function is not able to find a solution within the given bounds, raise a ValueError\n    if not result.success:\n        raise ValueError(\"Optimization function failed to find a solution within the given bounds.\")\n\n    # Return the optimized parameters\n    return result.x\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 813, "library_problem_id": 102, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 814, "code": ["<code>\nimport numpy as np\n\ndef find_relative_extrema(arr, n):\n    # Initialize an empty list to store the indices of the relative extrema\n    extrema_indices = []\n\n    # Iterate over the array\n    for i in range(n, len(arr)-n):\n        # Check if the current element is a relative extremum\n        if np.max(arr[i-n:i+n+1]) == arr[i] or np.min(arr[i-n:i+n+1]) == arr[i]:\n            extrema_indices.append(i)\n\n    return extrema_indices\n\narr = np.array([-624.59309896, -624.59309896, -624.59309896, -625., -625., -625.])\nn = 2\nresult = find_relative_extrema(arr, n)\nprint(result)  # Output: [2, 3]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 814, "library_problem_id": 103, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 815, "code": ["<code>\nSure, here is the completed function:\n\n```python\nimport numpy as np\nfrom scipy import signal\n\ndef find_relative_extrema(arr, n):\n    # Create a kernel for convolution\n    kernel = np.ones(n*2+1)\n    # Convolve the array with the kernel\n    convolved = signal.convolve2d(arr, kernel, mode='same', boundary='fill', fillvalue=0)\n    # Find the indices where the original array is less than or equal to its n neighboring elements\n    indices = np.argwhere((arr <= convolved[:, :-n*2]) & (arr >= convolved[:, n*2:]))\n    # Return the indices\n    return indices.tolist()\n\narr = np.array([[-624.59309896, -624.59309896, -624.59309896, -624.59309896, -624.59309896, -624.59309896],\n                [-624.59309896, -624.59309896, -624.59309896, -624.59309896, -624.59309896, -624.59309896],\n                [-624.59309896, -624.59309896\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 815, "library_problem_id": 104, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 103}}
{"id": 816, "code": ["<code>\nimport pandas as pd\nfrom scipy import stats\n\ndef remove_outliers(df):\n    try:\n        # Calculate Z-score for each numerical column\n        z_scores = stats.zscore(df.select_dtypes(include=[int, float]))\n\n        # Create a new DataFrame with rows where all Z-scores are within 3 standard deviations from the mean\n        df_no_outliers = df[(z_scores < 3).all(axis=1)]\n\n        return df_no_outliers\n    except TypeError:\n        print(\"TypeError: Cannot calculate Z-score for mixed data types.\")\n        return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 816, "library_problem_id": 105, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 817, "code": ["<code>\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd.DataFrame(data.data, columns=data.feature_names)\nprint(data1.head())\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 817, "library_problem_id": 0, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 818, "code": ["<code>\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd.DataFrame(data=data.data, columns=data.feature_names)\ndata1['target'] = data.target\nprint(data1.head())\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 818, "library_problem_id": 1, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 819, "code": ["<code>\nfrom sklearn.datasets import fetch_california_housing\nimport pandas as pd\n\ndata = fetch_california_housing()\nprint(type(data))\ndata1 = pd.DataFrame(data=data['data'], columns=data['feature_names'])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 819, "library_problem_id": 2, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 820, "code": ["<code>\nfrom sklearn.datasets import load_iris\nimport pandas as pd\n\ndata = load_iris()\nprint(type(data))\ndata1 = pd.DataFrame(data=data.data, columns=data.feature_names)\nprint(data1.head())\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 820, "library_problem_id": 3, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 821, "code": ["<code>\nimport pandas as pd\n\ndef load_data():\n    # Load your data here\n    pass\n\ndef one_hot_encode(df):\n    df_out = df.copy()\n    unique_elements = set()\n    for list_ in df_out['Col3']:\n        unique_elements.update(list_)\n    for element in unique_elements:\n        df_out[element] = df_out['Col3'].apply(lambda x: 1 if element in x else 0)\n    return df_out\n\ndf = load_data()\ndf_out = one_hot_encode(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 821, "library_problem_id": 4, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 822, "code": ["<code>\nimport pandas as pd\n\ndef load_data():\n    # Load your data here\n    pass\n\ndef one_hot_encode(df, column):\n    # Get the unique strings\n    unique_strings = df[column].apply(pd.Series).stack().unique()\n\n    # Create a DataFrame with 0s\n    df_out = pd.DataFrame(0, index=df.index, columns=unique_strings)\n\n    # For each row, set the corresponding column to 1 if the string is present\n    for i in range(df.shape[0]):\n        for string in df.loc[i, column]:\n            df_out.loc[i, string] = 1\n\n    return df_out\n\n# Load your data\ndf = load_data()\n\n# One-hot encode the 'column'\ndf_out = one_hot_encode(df, 'column')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 822, "library_problem_id": 5, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 823, "code": ["<code>\nimport pandas as pd\n\ndef one_hot_encode(df):\n    # Get the last column\n    last_column = df.columns[-1]\n    \n    # Get the unique elements in the last column\n    unique_elements = set(df[last_column].sum())\n    \n    # For each unique element, create a new column in the DataFrame\n    for element in unique_elements:\n        # Create a new column that is 1 if the element is in the list and 0 otherwise\n        df[f'{last_column}_{element}'] = df[last_column].apply(lambda x: 1 if element in x else 0)\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 823, "library_problem_id": 6, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 824, "code": ["<code>\nimport pandas as pd\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\ndef one_hot_encode_last_column(df):\n    if df.empty:\n        return df\n\n    if df.shape[1] > 1:\n        df = df.iloc[:, :-1]\n\n    mlb = MultiLabelBinarizer()\n    encoded_data = mlb.fit_transform(df.iloc[:, -1].apply(lambda x: x.split(',')))\n    encoded_df = pd.DataFrame(encoded_data, columns=mlb.classes_)\n\n    df = pd.concat([df.iloc[:, :-1], encoded_df], axis=1)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 824, "library_problem_id": 7, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 825, "code": ["<code>\nimport pandas as pd\n\ndef load_data():\n    # Load your data here\n    pass\n\ndef one_hot_encode_last_column(df):\n    # Get the last column\n    last_column = df.columns[-1]\n\n    # Get the unique elements in the last column\n    unique_elements = df[last_column].explode().unique()\n\n    # Create new columns for each unique element\n    for element in unique_elements:\n        df[f'{last_column}_{element}'] = df[last_column].apply(lambda x: 0 if element in x else 1)\n\n    # Drop the original last column\n    df = df.drop(columns=[last_column])\n\n    return df\n\n# Load the data\ndf = load_data()\n\n# One-hot encode the last column\ndf_out = one_hot_encode_last_column(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 825, "library_problem_id": 8, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 826, "code": ["<code>\nimport sklearn.svm as suppmach\n\n# Fit model:\nsvmmodel=suppmach.LinearSVC(penalty='l1',C=1)\nsvmmodel.fit(x_train, y_train)\n\npredicted_test= svmmodel.predict(x_test)\npredicted_test_scores= svmmodel.decision_function(x_test)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 826, "library_problem_id": 9, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 827, "code": ["<code>\nSure, here is the Python function that applies the logistic function to the decision scores and compares the results with the calibrated probabilities obtained from `CalibratedClassifierCV`.\n\n```python\nfrom sklearn.svm import LinearSVC\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\ndef compare_scores(X, y):\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Train a LinearSVC model\n    svc = LinearSVC(random_state=42)\n    svc.fit(X_train, y_train)\n\n    # Get the decision scores\n    decision_scores = svc.decision_function(X_test)\n\n    # Apply the logistic function to the decision scores\n    logistic_scores = 1 / (1 + np.exp(-decision_scores))\n\n    # Calibrate the LinearSVC model\n    calibrator = CalibratedClassifierCV(svc, method='sigmoid', cv=5)\n    calibrator.fit(X_train, y_train)\n\n    # Get the calibrated probabilities\n    calibrated_probs = calibrator.predict_proba(X_test)[:, 1]\n\n    # Compare the\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 827, "library_problem_id": 10, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 828, "code": ["<code>\nI'm sorry, but your question is not clear. You didn't provide the details of the function you want to complete. Please provide more information so I can assist you better.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 828, "library_problem_id": 11, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 829, "code": ["<code>\nimport pandas as pd\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\n# Load your data\ndf_origin, transform_output = load_data()\n\n# Convert the sparse matrix to a dense one\ndense_matrix = transform_output.toarray()\n\n# Convert the dense matrix to a pandas DataFrame\ndf_matrix = pd.DataFrame(dense_matrix)\n\n# Now, you can concatenate the DataFrame with the original DataFrame\ndf = pd.concat([df_origin, df_matrix], axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 829, "library_problem_id": 12, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 830, "code": ["<code>\nfrom sklearn.preprocessing import OneHotEncoder\nimport pandas as pd\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\ndf_origin, transform_output = load_data()\n\n# Use OneHotEncoder to transform some data\nencoder = OneHotEncoder(sparse=True)\ntransform_output = encoder.fit_transform(df_origin[['column_name']])\n\n# Convert the sparse matrix to a dense matrix\ndense_output = transform_output.toarray()\n\n# Get the column names from the encoder\nencoder_columns = encoder.get_feature_names(['column_name'])\n\n# Convert the dense matrix to a dataframe\ndf_transform_output = pd.DataFrame(dense_output, columns=encoder_columns)\n\n# Merge the transformed dataframe with the original dataframe\ndf_merged = pd.concat([df_origin.drop('column_name', axis=1), df_transform_output], axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 830, "library_problem_id": 13, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 831, "code": ["<code>\n# Assume X_train and y_train are your training data\nclf.fit(X_train, y_train)\n\n# Now you can make predictions on your test data\n# X_test is your test data\npredictions = clf.predict(X_test)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 831, "library_problem_id": 14, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 832, "code": ["<code>\nfrom sklearn.datasets import load_iris\n\n# Load iris dataset as an example\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Fit the pipeline to the data\nclf.fit(X, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 832, "library_problem_id": 15, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 833, "code": ["<code>\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\n\n# Create the Pipeline object\nclf = Pipeline([\n    ('reduce_dim', PCA()),\n    ('pOly', PolynomialFeatures()),\n    ('svdm', SVC())\n])\n\n# Add a new step\nclf.steps.append(('new_step', NewStep()))\n\n# Remove a step\nclf.steps.remove(('reduce_dim', PCA()))\n\n# Replace a step\nclf.steps[clf.steps.index(('pOly', PolynomialFeatures()))] = ('new_poly', NewStep())\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 833, "library_problem_id": 16, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 834, "code": ["<code>\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.svm import SVC\n\ndef modify_pipeline(clf, add_step=None, remove_step=None):\n    steps = clf.steps\n    new_steps = []\n\n    for step in steps:\n        if step[0] != remove_step:\n            new_steps.append(step)\n\n    if add_step is not None:\n        new_steps.insert(new_steps.index(remove_step) if remove_step else len(new_steps), add_step)\n\n    return Pipeline(new_steps)\n\n# Example usage:\nclf = Pipeline([\n    ('reduce_dim', PCA()),\n    ('poly', PolynomialFeatures()),\n    ('svm', SVC())\n])\n\n# Add ('reduce_dim', PCA()) before 'poly'\nclf = modify_pipeline(clf, add_step=('reduce_dim', PCA()), remove_step='poly')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 834, "library_problem_id": 17, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 835, "code": ["<code>\nfrom sklearn.model_selection import train_test_split\n\n# Assuming that you have a DataFrame df with features in columns and target in column 'target'\nX = df.drop('target', axis=1)\ny = df['target']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Fit the pipeline to the training data\nclf.fit(X_train, y_train)\n\n# Predict the labels of the test set\ny_pred = clf.predict(X_test)\n\n# Print the accuracy of the model\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 835, "library_problem_id": 18, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 836, "code": ["<code>\n# Assume X_train and y_train are your training data\nclf.fit(X_train, y_train)\n\n# Now you can make predictions on your test data\n# X_test is your test data\npredictions = clf.predict(X_test)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 836, "library_problem_id": 19, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 17}}
{"id": 837, "code": ["<code>\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRegressor\n\n# Initialize XGBRegressor model\nmodel = XGBRegressor()\n\n# Define hyperparameters to search\nparam_grid = {\n    'n_estimators': [100, 500, 1000],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'max_depth': [3, 5, 7]\n}\n\n# Initialize GridSearchCV\ngrid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n\n# Fit GridSearchCV with early stopping\ngrid_search.fit(X_train, y_train, \n                fit_params={\n                    'early_stopping_rounds': 42, \n                    'eval_set': [(testX, testY)]\n                })\n\n# Print best parameters\nprint(grid_search.best_params_)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 837, "library_problem_id": 20, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 838, "code": ["<code>\nfrom sklearn.model_selection import GridSearchCV, TimeSeriesSplit\nfrom xgboost import XGBRegressor\nimport multiprocessing\n\n# Define the hyperparameter grid\nparamGrid = {'n_estimators': [100, 500, 1000],\n             'learning_rate': [0.01, 0.1, 0.2],\n             'max_depth': [2, 4, 6],\n             'subsample': [0.5, 0.7, 1]}\n\n# Set up the GridSearchCV\ngsearch = GridSearchCV(estimator=XGBRegressor(objective='reg:squarederror'), \n                       param_grid=paramGrid, \n                       cv=TimeSeriesSplit(n_splits=3), \n                       verbose=1, \n                       n_jobs=multiprocessing.cpu_count())\n\n# Set up the fit parameters\nfit_params = {'early_stopping_rounds': 42, \n              'eval_metric': 'mae'}\n\n# Fit the GridSearchCV\ngsearch.fit(X_train, y_train, fit_params=fit_params)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 838, "library_problem_id": 21, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 839, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\n\ndef complete_function():\n    X, y = load_data()\n    assert type(X) == np.ndarray\n    assert type(y) == np.ndarray\n    cv = StratifiedKFold(5).split(X, y)\n    logreg = LogisticRegression()\n\n    for train_index, test_index in cv:\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n        logreg.fit(X_train, y_train)\n        print(\"Accuracy: \", logreg.score(X_test, y_test))\n\n# Call the function\ncomplete_function()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 839, "library_problem_id": 22, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 840, "code": ["<code>\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\n\ndef get_logistic_regression_proba(X, y, n_folds=5):\n    cv = StratifiedKFold(n_folds)\n    model = LogisticRegression(max_iter=1000)\n    all_probas = []\n\n    for train_index, test_index in cv.split(X, y):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n        model.fit(X_train, y_train)\n        probas = model.predict_proba(X_test)\n        all_probas.append(probas)\n\n    return np.concatenate(all_probas)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 840, "library_problem_id": 23, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 841, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Load your data\ndata = load_data()\n\n# Initialize the StandardScaler\nscaler = StandardScaler()\n\n# Fit and transform the data\nscaled = scaler.fit_transform(data)\n\n# Inverse transform the scaled data back to the original scale\ninversed = scaler.inverse_transform(scaled)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 841, "library_problem_id": 24, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 842, "code": ["<code>\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\n# StandardScaler\nscaler = StandardScaler()\nscaler.fit(train_df['t'].values.reshape(-1,1))\ntrain_df['t'] = scaler.transform(train_df['t'].values.reshape(-1,1))\n\n# Regression model\nmodel = LinearRegression()\nmodel.fit(train_df.drop('target', axis=1), train_df['target'])\n\n# Check score\nprint(model.score(train_df.drop('target', axis=1), train_df['target']))\n\n# Predict\npredictions = model.predict(train_df.drop('target', axis=1))\n\n# Inverse transform\npredictions = scaler.inverse_transform(predictions.reshape(-1,1))\n\n# Print predictions\nprint(predictions)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 842, "library_problem_id": 25, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 24}}
{"id": 843, "code": ["<code>\nfrom sklearn.linear_model import LinearRegression\n\ndef get_model_name(model):\n    model_name = model.__class__.__name__\n    return model_name\n\nmodel = LinearRegression()\nprint(\"Model Name:\", get_model_name(model))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 843, "library_problem_id": 26, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 844, "code": ["<code>\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\n# Load iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Initialize Logistic Regression model\nmodel = LogisticRegression()\n\n# Perform cross-validation\nscores = cross_val_score(model, X, y, cv=5)\n\n# Extract the name of the machine learning model\nmodel_name = model.__class__.__name__\n\n# Print the model name without its parameters\nprint(f\"Model: {model_name}\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 844, "library_problem_id": 27, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 845, "code": ["<code>\ndef get_model_name(model):\n    return model.__class__.__name__\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 845, "library_problem_id": 28, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 846, "code": ["<code>\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\nfrom sklearn.pipeline import Pipeline\nimport pandas as pd\n\ndef get_tfidf_output(df):\n    # Create a pipeline with TfidfVectorizer and NMF\n    pipeline = Pipeline([\n        ('tfidf', TfidfVectorizer()),\n        ('nmf', NMF())\n    ])\n\n    # Fit the pipeline on the data\n    pipeline.fit(df['text'])\n\n    # Get the TfidfVectorizer from the pipeline\n    tfidf = pipeline.named_steps['tfidf']\n\n    # Transform the data using TfidfVectorizer\n    tfidf_output = tfidf.transform(df['text'])\n\n    return tfidf_output\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 846, "library_problem_id": 29, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 847, "code": ["<code>\nfrom sklearn.linear_model import LogisticRegression\n\npipe = Pipeline([\n    (\"tf_idf\", TfidfVectorizer()),\n    (\"nmf\", NMF()),\n    (\"clf\", LogisticRegression())\n])\n\ndata = pd.DataFrame([[\"Salut comment tu vas\", \"Hey how are you today\", \"I am okay and you ?\"]]).T\ndata.columns = [\"test\"]\n\n# Assuming you have some labels for your data\nlabels = [0, 1, 0]\n\npipe.fit(data.test, labels)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 847, "library_problem_id": 30, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 848, "code": ["<code>\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.linear_model import LogisticRegression\n\ndef get_select_kbest_output(data, target):\n    # Create a pipeline\n    pipeline = Pipeline([\n        ('feature_selection', SelectKBest(k=2)),\n        ('classifier', LogisticRegression())\n    ])\n\n    # Fit the pipeline\n    pipeline.fit(data, target)\n\n    # Transform the data\n    transformed_data = pipeline.named_steps['feature_selection'].transform(data)\n\n    return transformed_data\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 848, "library_problem_id": 31, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 849, "code": ["<code>\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\n\ndt = DecisionTreeClassifier(max_depth = 1)\nbc = BaggingClassifier(dt, n_estimators = 20, max_samples = 0.5, max_features = 0.5)\nbc = bc.fit(X_train, y_train)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 849, "library_problem_id": 32, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 850, "code": ["<code>\n# Let's assume y has 600 samples and X has 1 feature\ny = np.array([0, 1, 2, ..., 599])  # 1D array\nX = np.array([735233.])  # 1D array\n\n# Reshape the data to match the number of samples and features in the y data\nX = X.reshape(-1, 1)  # -1 tells numpy to calculate the dimension based on the length of the array and the number of features\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 850, "library_problem_id": 33, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 851, "code": ["<code>\nX = [[233.176],\n     [234.270],\n     [235.270],\n     [523.176],\n     ...]\n\ny = [0.00, 1.36, 4.46, ...]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 851, "library_problem_id": 34, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 33}}
{"id": 852, "code": ["<code>\ndef preprocess(s):\n    return s.upper()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 852, "library_problem_id": 35, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 853, "code": ["<code>\nSure, I can help you with that. However, I need to know what the function is supposed to do. Please provide more details.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 853, "library_problem_id": 36, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Semantic", "perturbation_origin_id": 35}}
{"id": 854, "code": ["<code>\nimport pandas as pd\nfrom sklearn import preprocessing\n\ndef scale_data(csv_path, delimiter):\n    # Load the data\n    df = pd.read_csv(csv_path, delimiter=delimiter)\n\n    # Scale the data\n    scaled_data = preprocessing.scale(df)\n\n    # Create a new DataFrame with the scaled data\n    scaled_df = pd.DataFrame(scaled_data, columns=df.columns, index=df.index)\n\n    return scaled_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 854, "library_problem_id": 37, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 855, "code": ["<code>\nimport pandas as pd\nfrom sklearn import preprocessing\n\ndef scale_data(data):\n    scaled_data = preprocessing.scale(data)\n    df_out = pd.DataFrame(scaled_data, index=data.index, columns=data.columns)\n    return df_out\n\ndf_out = scale_data(data)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 855, "library_problem_id": 38, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 37}}
{"id": 856, "code": ["<code>\nfrom sklearn.datasets import load_iris\n\n# Load iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Fit the pipeline\npipe.fit(X, y)\n\n# Now the pipeline is fitted and can be used for prediction\ny_pred = pipe.predict(X)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 856, "library_problem_id": 39, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 857, "code": ["<code>\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n# Load the iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=24)\n\n# Fit the pipeline\npipe.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = pipe.predict(X_test)\n\n# Evaluate the model\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 857, "library_problem_id": 40, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 858, "code": ["<code>\ndef get_selected_features(df, clf):\n    sfm = SelectFromModel(clf, prefit=True)\n    sfm.fit(df)\n    mask = sfm.get_support() #list of booleans for selected features\n    return df.columns[mask]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 858, "library_problem_id": 41, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 859, "code": ["<code>\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport pandas as pd\n\ndef feature_selection(df):\n    # Convert DataFrame to NumPy array\n    X = df.values\n\n    # Get the target variable\n    y = df.target\n\n    # Initialize ExtraTreesClassifier\n    clf = ExtraTreesClassifier(n_estimators=50)\n\n    # Fit the classifier\n    clf = clf.fit(X, y)\n\n    # Get the feature importances\n    feature_importances = clf.feature_importances_\n\n    # Initialize SelectFromModel\n    sel = SelectFromModel(clf, prefit=True)\n\n    # Transform the data\n    X_new = sel.transform(X)\n\n    # Get the mask of selected features\n    mask = sel.get_support()\n\n    # Get the names of the selected features\n    selected_features = df.columns[mask]\n\n    return selected_features.values\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 859, "library_problem_id": 42, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 860, "code": ["<code>\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport numpy as np\n\ndef load_data():\n    # Load your data here\n    # For example:\n    # X = pd.read_csv('data.csv')\n    # y = X.pop('target')\n    # return X, y\n    pass\n\nX, y = load_data()\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\n\n# Feature importance\nimportances = clf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Select features according to the importance\nselector = SelectFromModel(clf, prefit=True)\nX_new = selector.transform(X)\n\nprint(\"X.shape: %s, X_new.shape: %s\" % (X.shape, X_new.shape))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 860, "library_problem_id": 43, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 861, "code": ["<code>\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport numpy as np\n\ndf = pd.read_csv('los_10_one_encoder.csv')\ny = df['LOS'] # target\nX= df.drop('LOS',axis=1) # drop LOS column\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\n\nsfm = SelectFromModel(clf, prefit=True)\nX_new = sfm.transform(X)\n\nselected_features = sfm.get_support()\n\nselected_features_list = X.columns[selected_features].tolist()\nprint(selected_features_list)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 861, "library_problem_id": 44, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 862, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\n\n# Assuming load_data() returns a pandas DataFrame and a numpy array\np, X = load_data()\n\n# Check if X is a numpy array\nassert type(X) == np.ndarray\n\n# Initialize KMeans\nkm = KMeans()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 862, "library_problem_id": 45, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 863, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\n\n# Assuming load_data() returns a pandas DataFrame and a numpy array\np, X = load_data()\n\n# Check if X is a numpy array\nassert type(X) == np.ndarray\n\n# Initialize KMeans\nkm = KMeans()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 863, "library_problem_id": 46, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 864, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\n\n# Assuming load_data() returns a pandas DataFrame and a numpy array\np, X = load_data()\n\n# Check if X is a numpy array\nassert type(X) == np.ndarray\n\n# Initialize KMeans\nkm = KMeans()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 864, "library_problem_id": 47, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 865, "code": ["<code>\ndef get_samples(p, X, km):\n    # Fit the KMeans model\n    km.fit(X)\n\n    # Initialize an empty DataFrame\n    df = pd.DataFrame(X, columns=['feature1', 'feature2', 'feature3', ...])\n    df['cluster'] = km.labels_\n\n    # Get the indices of the samples for each cluster\n    samples = []\n    for i in range(km.n_clusters):\n        cluster_samples = df[df['cluster'] == i].index.tolist()\n        samples.append(np.random.choice(cluster_samples, size=1))\n\n    # Return the samples as a numpy array\n    return np.array(samples).flatten()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 865, "library_problem_id": 48, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 866, "code": ["<code>\nSure, here is the completed code:\n\n```python\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\n# Load iris dataset\niris = load_iris()\ndf = pd.DataFrame(data=iris.data, columns=iris.feature_names)\ndf['target'] = iris.target\n\n# Split the data into training and testing sets\ntrain, test = train_test_split(df, test_size=0.2, random_state=42)\n\n# Define preprocessing for numeric columns (normalize them so they're on the same scale)\nnumeric_features = df.select_dtypes(include=['int64', 'float64']).columns\nnumeric_transformer = StandardScaler()\n\n# Define preprocessing for categorical features (convert text categories to numeric)\ncategorical_features = df.select_dtypes(include=['object']).columns\ncategorical_transformer = OneHotEncoder()\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)])\n\n# Append classifier to preprocessing pipeline.\n# Now we have full prediction pipeline.\nclf = Pipeline(steps=\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 866, "library_problem_id": 49, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 867, "code": ["<code>\nimport pandas as pd\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Assume we have a dataframe df with a categorical column 'color'\ndf = pd.DataFrame({\n    'color': ['blue', 'green', 'red', 'blue', 'green'],\n    'size': [10, 20, 30, 40, 50],\n    'price': [100, 200, 300, 400, 500]\n})\n\n# Perform one-hot encoding on the categorical column 'color'\ndf_encoded = pd.get_dummies(df, columns=['color'])\n\n# Define the features and target\nX = df_encoded.drop('size', axis=1)\ny = df['size']\n\n# Create a GradientBoostingClassifier\nclf = GradientBoostingClassifier()\n\n# Train the classifier\nclf.fit(X, y)\n\n# Now the classifier can handle categorical features without one-hot encoding\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 867, "library_problem_id": 50, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 868, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_data\n\nX, y = load_data()\nassert isinstance(X, np.ndarray)\nassert isinstance(y, np.ndarray)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 868, "library_problem_id": 51, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 869, "code": ["<code>\nfrom sklearn.datasets import load_data\nfrom sklearn.svm import SVR\n\ndef support_vector_regression():\n    # Load data\n    data = load_data()\n    X = data.data\n    y = data.target\n\n    # Initialize SVR with Gaussian kernel\n    svr = SVR(kernel='rbf')\n\n    # Fit the model\n    svr.fit(X, y)\n\n    # Predict the values\n    y_pred = svr.predict(X)\n\n    return y_pred\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 869, "library_problem_id": 52, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 51}}
{"id": 870, "code": ["<code>\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Assuming load_data is a function that loads a dataset and returns it as a numpy array\nX, y = load_data()\n\n# Create a RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=100)\n\n# Fit the model\nclf.fit(X, y)\n\n# Predict X\npredict = clf.predict(X)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 870, "library_problem_id": 53, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 51}}
{"id": 871, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.svm import SVR\n\n# Load your data\nX, y = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\n\n# Create a polynomial kernel SVR model\nsvr = SVR(kernel='poly', degree=2)\n\n# Fit the model to your data\nsvr.fit(X, y)\n\n# Predict the output for X\npredict = svr.predict(X)\n\n# Evaluate the model\nscore = svr.score(X, y)\nprint(f'Model score: {score}')\n\n# Save the model\nwith open('model.pkl', 'wb') as f:\n    pickle.dump(svr, f)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 871, "library_problem_id": 54, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 53}}
{"id": 872, "code": ["<code>\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n    query_matrix = tfidf.transform(queries)\n    similarity_matrix = cosine_similarity(query_matrix, tfidf_matrix)\n\n    return similarity_matrix\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 872, "library_problem_id": 55, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 55}}
{"id": 873, "code": ["<code>\nquery_vector = vectorizer.transform([query])\n    similarity = cosine_similarity(tfidf, query_vector).flatten()\n    return similarity\n\ndef get_most_similar_document(documents, query):\n    similarity = get_tf_idf_query_similarity(documents, query)\n    most_similar = documents[np.argmax(similarity)]\n    return most_similar\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 873, "library_problem_id": 56, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 874, "code": ["<code>\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef solve(queries, documents):\n    tfidf = TfidfVectorizer()\n    tfidf.fit(documents)\n    doc_vectors = tfidf.transform(documents)\n    query_vectors = tfidf.transform(queries)\n    cosine_similarities_of_queries = cosine_similarity(query_vectors, doc_vectors)\n    return cosine_similarities_of_queries\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 874, "library_problem_id": 57, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 875, "code": ["<code>\nimport pandas as pd\nimport numpy as np\nimport sklearn\n\ndef load_data():\n    # Load data from a CSV file\n    data = pd.read_csv('data.csv')\n    return data\n\nfeatures = load_data()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 875, "library_problem_id": 58, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 876, "code": ["<code>\ndef convert_to_array(input_list):\n    return list(input_list)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 876, "library_problem_id": 59, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 877, "code": ["<code>\nimport numpy as np\n\ndef convert_to_binary(features):\n    # Get all unique features\n    unique_features = set(feature for sublist in features for feature in sublist)\n    \n    # Initialize new_features as a zero matrix with shape (number of samples, number of unique features)\n    new_features = np.zeros((len(features), len(unique_features)))\n    \n    # Iterate over each sample\n    for i, sample in enumerate(features):\n        # Iterate over each feature in the sample\n        for feature in sample:\n            # If the feature is in the unique features, set the corresponding entry in new_features to 1\n            if feature in unique_features:\n                new_features[i, unique_features.index(feature)] = 1\n    \n    return new_features\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 877, "library_problem_id": 60, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 878, "code": ["<code>\nI'm sorry, but your question is not clear. The function `solve` is not defined with any specific task or requirements. Please provide more details about the task or the problem you want to solve.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 878, "library_problem_id": 61, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 879, "code": ["<code>\nimport pandas as pd\nimport numpy as np\nimport sklearn\n\ndef load_data():\n    # Load data from a file or a database\n    # Replace 'file_path' with the path to your data file\n    data = pd.read_csv('file_path')\n    return data\n\nfeatures = load_data()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 879, "library_problem_id": 62, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 58}}
{"id": 880, "code": ["<code>\ndef complete_matrix(data_matrix):\n    n = len(data_matrix)\n    for i in range(n):\n        row_sum = sum(data_matrix[i])\n        row_avg = row_sum / n\n        for j in range(n):\n            if data_matrix[i][j] == 0:\n                data_matrix[i][j] = row_avg\n    return data_matrix\n\ndata_matrix=[[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]]\nprint(complete_matrix(data_matrix))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 880, "library_problem_id": 63, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 881, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Given data matrix\ndata_matrix = np.array([[0,0.8,0.9],[0.8,0,0.2],[0.9,0.2,0]])\n\n# Create an instance of AgglomerativeClustering with 2 clusters\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit and predict the clusters\nlabels = cluster.fit_predict(data_matrix)\n\n# Print the labels\nprint(labels)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 881, "library_problem_id": 64, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 882, "code": ["<code>\nfrom sklearn.cluster import AgglomerativeClustering\nimport numpy as np\n\ndef hierarchical_clustering(simM):\n    # Convert the list of lists to a numpy array\n    simM = np.array(simM)\n    \n    # Create a new AgglomerativeClustering object with 2 clusters\n    clustering = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n    \n    # Fit the model to the distance matrix and get the cluster labels\n    labels = clustering.fit_predict(simM)\n    \n    # Return the labels as a list\n    return list(labels)\n\n# Test the function\nsimM=[[0,0.6,0.8],[0.6,0,0.111],[0.8,0.111,0]]\nprint(hierarchical_clustering(simM))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 882, "library_problem_id": 65, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 883, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport scipy.cluster\n\ndef load_data():\n    # Load data from CSV file\n    data_matrix = pd.read_csv('data.csv')\n    return data_matrix\n\ndata_matrix = load_data()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 883, "library_problem_id": 66, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 884, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport scipy.cluster\n\ndef load_data():\n    # Load data from CSV file\n    data_matrix = pd.read_csv('data.csv')\n    return data_matrix\n\ndata_matrix = load_data()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 884, "library_problem_id": 67, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 885, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport scipy.cluster\n\ndef load_data():\n    # Load your data here. This is just a placeholder.\n    # Replace it with your actual data loading code.\n    data = pd.read_csv('your_data_file.csv')\n    return data\n\nsimM = load_data()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 885, "library_problem_id": 68, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 886, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\n\nnp.random.seed(1)\npredictors = pd.DataFrame(\n    {'x1': np.random.normal(5, 2, 1000),\n     'x2': np.random.exponential(10, 1000)})\n\ntrans = preprocessing.StandardScaler().fit(predictors)\npredictors_trans = pd.DataFrame(trans.transform(predictors))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 886, "library_problem_id": 69, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 69}}
{"id": 887, "code": ["<code>\nfrom sklearn import preprocessing\nimport numpy as np\n\n# Assume we have a numpy array 'data'\ndata = np.array([[1., -1.,  2.],\n                 [2.,  0.,  0.],\n                 [0.,  1., -1.]])\n\n# Create a scaler object\nscaler = preprocessing.StandardScaler()\n\n# Fit the scaler to the data\nscaler.fit(data)\n\n# Now we can transform the data\ncentered_scaled_data = scaler.transform(data)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 887, "library_problem_id": 70, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 69}}
{"id": 888, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\n\ndef load_data():\n    # Load data from a CSV file\n    data = pd.read_csv('data.csv')\n    return data.values\n\ndata = load_data()\nassert type(data) == np.ndarray\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 888, "library_problem_id": 71, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 889, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import PowerTransformer\n\ndata = load_data()\nassert isinstance(data, np.ndarray)\n\n# Applying PowerTransformer\npt = PowerTransformer()\ndata_transformed = pt.fit_transform(data)\n\n# Checking if the transformation was successful\nassert isinstance(data_transformed, np.ndarray)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 889, "library_problem_id": 72, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 890, "code": ["<code>\nfrom sklearn.preprocessing import PowerTransformer\nimport numpy as np\n\n# Create a PowerTransformer object with Yeo-Johnson method\npt = PowerTransformer(method='yeo-johnson')\n\n# Create a random sample data\nnp.random.seed(1)\nX = np.random.normal(5, 2, (1000, 2))\n\n# Apply the transformation\nX_transformed = pt.fit_transform(X)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 890, "library_problem_id": 73, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 891, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport sklearn\n\ndef load_data():\n    data = pd.read_csv('data.csv')  # Assuming the data is in a CSV file named 'data.csv'\n    return data.values\n\ndata = load_data()\nassert type(data) == np.ndarray\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 891, "library_problem_id": 74, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 73}}
{"id": 892, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Assuming load_data() is a function that loads text data\ndef load_data():\n    # Insert your code to load data here\n    pass\n\ntext = load_data()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 892, "library_problem_id": 75, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 893, "code": ["<code>\nimport numpy as np\nimport pandas as pd\n\ndataset = pd.read_csv('example.csv', header=None, sep=',')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 893, "library_problem_id": 76, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 894, "code": ["<code>\n```python\nimport numpy as np\nimport pandas as pd\n\n# Assuming data is your DataFrame\nx = data.iloc[:, :-1]  # all columns except the last one\ny = data.iloc[:, -1]  # the last column\n\n# Now split the data into training and testing sets\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n# Now, let's perform some basic data preprocessing\n\n# Fill missing values with the mean of the column\nx_train = x_train.fillna(x_train.mean())\nx_test = x_test.fillna(x_test.mean())\n\n# Normalize the data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\n\n# Convert the data to numpy arrays\nx_train = np.array(x_train)\ny_train = np.array(y_train)\nx_test = np.array(x_test)\ny_test = np.array(y_test)\n\n# Now, let's train a simple linear regression model\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(x_train, y_train)\n\n# Now, let's evaluate the model\nfrom sklearn.metrics import mean_squared_\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 894, "library_problem_id": 77, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 895, "code": ["<code>\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef load_data():\n    # Load your data here\n    pass\n\n# Load the data\ndata = load_data()\n\n# Split the data into features (x) and labels (y)\nx = data.iloc[:, :-1]\ny = data.iloc[:, -1]\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 895, "library_problem_id": 78, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 896, "code": ["<code>\nimport pandas as pd\n\ndataset = pd.read_csv('example.csv', header=None, sep=',')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 896, "library_problem_id": 79, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 897, "code": ["<code>\n```python\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Load the data\ndf = pd.read_csv('all_data_device.csv', parse_dates=['date'])\n\n# Extract the 'mse' values\nmse_values = df['mse'].values.reshape(-1, 1)\n\n# Initialize KMeans\nkmeans = KMeans(n_clusters=2, n_init=10)\n\n# Fit the model\nkmeans.fit(mse_values)\n\n# Get the cluster assignments\ncluster_assignments = kmeans.labels_\n\n# Add the cluster assignments to the dataframe\ndf['cluster'] = cluster_assignments\n\n# Calculate the mean of each cluster\ncluster_means = df.groupby('cluster')['mse'].mean()\n\n# Print the cluster means\nprint(cluster_means)\n\n# Visualize the clusters\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Scatter plot of 'mse' values\nscatter = ax.scatter(df['mse'], np.arange(len(df)), df['cluster'], cmap='viridis')\n\n# Mark the centroids\ncentroids = kmeans.cluster_centers_\nax.scatter(centroids[:, 0], centroids[:, 1], centroids[:, 2], marker='*\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 897, "library_problem_id": 80, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 898, "code": ["<code>\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport pandas as pd\n\ndf = pd.read_csv(\"file.csv\", parse_dates=[\"date\"])\nf1 = df['mse'].values\nf2 = list(range(0, len(f1)))\nX = np.array(list(zip(f1, f2)))\nkmeans = KMeans(n_clusters=2, n_init=10).fit(X)\nlabels = kmeans.predict(X)\ncentroids = kmeans.cluster_centers_\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 898, "library_problem_id": 81, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 899, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\n\n# Assuming load_data is a function that returns a list of documents and their corresponding labels\ncorpus, y = load_data()\nassert type(corpus) == list\nassert type(y) == list\n\n# Transform the text data into a format that can be understood by the SVM\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(corpus)\n\n# Train the SVM\nclf = LinearSVC()\nclf.fit(X, y)\n\n# Now you can use the trained classifier to predict the labels of new documents\n# For example:\nnew_corpus = [\"New document to predict\"]\nnew_X = vectorizer.transform(new_corpus)\npredictions = clf.predict(new_X)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 899, "library_problem_id": 82, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 900, "code": ["<code>\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import make_pipeline\n\n# Given corpus and labels\ncorpus = ['This is a document about machine learning',\n          'This is a document about natural language processing',\n          'This is a document about computer vision',\n          'This is a document about deep learning']\ny = ['Machine Learning',\n     'Natural Language Processing',\n     'Computer Vision',\n     'Deep Learning']\n\n# Create a TfidfVectorizer and a LinearSVC\nvectorizer = TfidfVectorizer()\nclassifier = LinearSVC(penalty='l1', dual=False)\n\n# Create a pipeline\npipeline = make_pipeline(vectorizer, classifier)\n\n# Fit the pipeline\npipeline.fit(corpus, y)\n\n# Get the TfidfVectorizer from the pipeline\nvectorizer = pipeline.named_steps['tfidfvectorizer']\n\n# Get the LinearSVC from the pipeline\nclassifier = pipeline.named_steps['linearsvc']\n\n# Get the feature names from the TfidfVectorizer\nfeature_names = vectorizer.get_feature_names_out()\n\n# Get the coefficients from the LinearSVC\ncoefficients = classifier.coef_\n\n# Create a dictionary to store the selected features\nselected_features = {}\n\n# Iterate over the coefficients\nfor i, coef in enumerate(coefficients):\n    # Get the indices of the non-zero coefficients\n    indices =\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 900, "library_problem_id": 83, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 901, "code": ["<code>\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nimport numpy as np\n\ndef get_selected_features(corpus, y):\n    vectorizer = TfidfVectorizer()\n    X = vectorizer.fit_transform(corpus)\n    svc = LinearSVC(C=1.0, penalty='l1', dual=False)\n    svc.fit(X, y)\n\n    # Get the support vectors\n    support_vectors = svc.coef_ != 0\n\n    # Get the indices of the selected features\n    selected_features = np.where(support_vectors)[1]\n\n    return selected_features\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 901, "library_problem_id": 84, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 902, "code": ["<code>\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef vectorize_text(corpus, vocabulary, stop_words='english', lowercase=True, token_pattern=r\"(?u)\\b\\w\\w+\\b\"):\n    \"\"\"\n    Vectorize a text corpus using scikit-learn's `CountVectorizer`.\n\n    Parameters:\n    corpus (list of str): The text corpus to be vectorized.\n    vocabulary (list of str): The vocabulary terms in the order they should be maintained.\n    stop_words (str or list of str, default='english'): If 'english', a built-in stop word list for English is used. If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n    lowercase (bool, default=True): If True, all terms are converted to lowercase before tokenizing.\n    token_pattern (str, default=r\"(?u)\\b\\w\\w+\\b\"): Regular expression denoting what constitutes a \"token\".\n\n    Returns:\n    tuple: A tuple containing the feature names and the transformed X matrix.\n    \"\"\"\n    vectorizer = CountVectorizer(vocabulary=vocabulary, stop_words=stop_words, lowercase=lowercase, token_pattern=token_pattern)\n    X = vectorizer.fit_transform(corpus)\n    feature_names = vectorizer.get_feature_names_out()\n\n    return feature_names, X\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 902, "library_problem_id": 85, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 903, "code": ["<code>\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport numpy as np\n\ndef create_document_term_matrix(documents, vocabulary=None, binary=False, lowercase=True, stop_words='english'):\n    # Tokenize the documents\n    documents = [word_tokenize(doc) for doc in documents]\n    \n    # Convert to lowercase if specified\n    if lowercase:\n        documents = [' '.join(doc).lower() for doc in documents]\n    \n    # Remove stop words\n    if stop_words:\n        stop_words = set(stopwords.words(stop_words))\n        documents = [' '.join([word for word in doc if word not in stop_words]) for doc in documents]\n    \n    # Create the document-term matrix\n    vectorizer = CountVectorizer(vocabulary=vocabulary, binary=binary)\n    dtm = vectorizer.fit_transform(documents)\n    \n    # Return the feature names and the document-term matrix\n    return vectorizer.get_feature_names_out(), dtm.toarray()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 903, "library_problem_id": 86, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 904, "code": ["<code>\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nimport numpy as np\n\ndef create_document_term_matrix(corpus, vocabulary):\n    # Define stop words\n    stop_words = set(stopwords.words('english'))\n    \n    # Create a CountVectorizer object\n    vectorizer = CountVectorizer(stop_words=stop_words, lowercase=True)\n    \n    # Fit the vectorizer to the corpus\n    vectorizer.fit(corpus)\n    \n    # Transform the corpus to a document-term matrix\n    X = vectorizer.transform(corpus)\n    \n    # Create a binary matrix\n    X_binary = X.toarray()\n    X_binary[X_binary > 0] = 1\n    \n    # Get the feature names\n    feature_names = vectorizer.get_feature_names_out()\n    \n    # Reorder the features to match the given vocabulary\n    vocabulary = np.array(vocabulary)\n    feature_indices = np.array([np.where(feature_names == term)[0][0] for term in vocabulary if term in feature_names])\n    X_binary = X_binary[:, feature_indices]\n    feature_names = vocabulary[np.in1d(vocabulary, feature_names)]\n    \n    return feature_names, X_binary\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 904, "library_problem_id": 87, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 905, "code": ["<code>\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\n\ndef create_document_term_matrix(corpus, vocabulary):\n    # Define the vectorizer\n    vectorizer = CountVectorizer(input='content', \n                                 vocabulary=vocabulary, \n                                 binary=True, \n                                 stop_words='english', \n                                 lowercase=True)\n\n    # Fit and transform the corpus\n    X = vectorizer.fit_transform(corpus)\n\n    # Get the feature names\n    feature_names = vectorizer.get_feature_names_out()\n\n    return feature_names, X.toarray()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 905, "library_problem_id": 88, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 906, "code": ["<code>\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndef perform_linear_regression(df):\n    # Load the data\n    df = df.load_data()\n\n    # Initialize an empty list to store the slope coefficients\n    slope_coefficients = []\n\n    # Loop through each column in the DataFrame\n    for col in df.columns:\n        # Check if the column is not the 'Time' column\n        if col != 'Time':\n            # Create a new DataFrame with the 'Time' and the current column\n            new_df = df[['Time', col]]\n\n            # Drop rows with NaN values\n            new_df = new_df.dropna()\n\n            # Split the DataFrame into features (X) and target (y)\n            X = new_df['Time'].values.reshape(-1, 1)\n            y = new_df[col].values.reshape(-1, 1)\n\n            # Initialize the LinearRegression model\n            model = LinearRegression()\n\n            # Fit the model to the data\n            model.fit(X, y)\n\n            # Append the slope coefficient to the list\n            slope_coefficients.append(model.coef_[0][0])\n\n    # Convert the list to a numpy array and return it\n    return np.array(slope_coefficients)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 906, "library_problem_id": 89, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 907, "code": ["<code>\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\n\ndef calculate_slopes(df):\n    # Replace NaN values with the mean of the column\n    df.fillna(df.mean(), inplace=True)\n\n    # Calculate the slope coefficient for each column\n    slopes = df.apply(lambda x: stats.linregress(range(len(x)), x).slope)\n\n    # Return the slope coefficients in a numpy array\n    return slopes.values\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 907, "library_problem_id": 90, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 89}}
{"id": 908, "code": ["<code>\nSure, I can help you with that. However, I need to know what the function `load_data()` is supposed to do, and what the task of completing the function is. Could you please provide more details?\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 908, "library_problem_id": 91, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 909, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndf = pd.read_csv('data.csv')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 909, "library_problem_id": 92, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 910, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef Transform(df):\n    le = LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 910, "library_problem_id": 93, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 911, "code": ["<code>\nHere is the completed function:\n\n```python\nimport numpy as np\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.metrics import r2_score\n\ndef elastic_net_regression():\n    # Load the data\n    X_train, y_train, X_test, y_test = load_data()\n\n    # Check if the loaded data are numpy arrays\n    assert isinstance(X_train, np.ndarray)\n    assert isinstance(y_train, np.ndarray)\n    assert isinstance(X_test, np.ndarray)\n    assert isinstance(y_test, np.ndarray)\n\n    # Create the Elastic Net regression model\n    model = ElasticNet(alpha=0.1, l1_ratio=0.7)\n\n    # Fit the model to the training data\n    model.fit(X_train, y_train)\n\n    # Calculate the R^2 scores for the training and test sets\n    training_set_score = r2_score(y_train, model.predict(X_train))\n    test_set_score = r2_score(y_test, model.predict(X_test))\n\n    return training_set_score, test_set_score\n\ndef load_data():\n    # Load the diabetes dataset\n    diabetes = load_diabetes()\n\n    # Split the data into training and test sets\n    X_train, y_train = diabetes.data[:-20], diabetes.target[:-20]\n    X_test, y_test = diabetes.data[-2\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 911, "library_problem_id": 94, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 912, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Assuming load_data() loads some data and returns a numpy array\ndef load_data():\n    # Your code to load data goes here\n    pass\n\nnp_array = load_data()\n\n# Let's say we have a numpy array and we want to scale it\nscaler = MinMaxScaler()\nnp_array_scaled = scaler.fit_transform(np_array)\n\n# Now we can use the scaled numpy array\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 912, "library_problem_id": 95, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 913, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Assuming load_data() loads some data and returns a numpy array\ndef load_data():\n    # Your code to load data goes here\n    pass\n\nnp_array = load_data()\n\n# Let's say we have a numpy array and we want to scale it\nscaler = MinMaxScaler()\nnp_array_scaled = scaler.fit_transform(np_array)\n\n# Now we can use the scaled numpy array\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 913, "library_problem_id": 96, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 914, "code": ["<code>\nscaler = MinMaxScaler()\n    new_a = scaler.fit_transform(a)\n    return new_a\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 914, "library_problem_id": 97, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 915, "code": ["<code>\nfrom sklearn.tree import DecisionTreeRegressor\nimport numpy as np\n\ndef predict_values(clf, new_data):\n    \"\"\"\n    Use a trained scikit-learn DecisionTreeRegressor model to predict values using new data.\n\n    Parameters:\n    clf (DecisionTreeRegressor): A trained DecisionTreeRegressor model.\n    new_data (numpy.ndarray): New data to predict.\n\n    Returns:\n    numpy.ndarray: Predicted values.\n    \"\"\"\n    # Check if the input data is a 2D array\n    if len(new_data.shape) != 2:\n        raise ValueError(\"The input data should be a 2D array where each row represents a sample and each column represents a feature.\")\n\n    # Check if the number of features in the input data matches the number of columns in the training data\n    if new_data.shape[1] != clf.n_features_in_:\n        raise ValueError(\"The number of features in the input data should match the number of columns in the training data.\")\n\n    # Use the model to predict the values\n    predicted_values = clf.predict(new_data)\n\n    return predicted_values\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 915, "library_problem_id": 98, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 916, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\n# Assuming X is a list of lists where each sublist represents a sample with two features\nX = [['asdf', '1'], ['asdf', '0']]\n\n# Convert the data to a pandas DataFrame\ndf = pd.DataFrame(X, columns=['feature1', 'feature2'])\n\n# Initialize a LabelEncoder\nle = LabelEncoder()\n\n# Apply the LabelEncoder to each column\ndf['feature1'] = le.fit_transform(df['feature1'])\ndf['feature2'] = le.fit_transform(df['feature2'])\n\n# Convert the DataFrame to a numpy array\nX = df.values\n\n# Initialize the DecisionTreeClassifier\nclf = DecisionTreeClassifier()\n\n# Fit the classifier to the data\nclf.fit(X, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 916, "library_problem_id": 99, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 917, "code": ["<code>\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\n\ndef refactor_classifier(data, labels):\n    try:\n        # Convert string inputs to numerical data\n        numerical_data = np.array([list(map(ord, s)) for s in data])\n\n        # Initialize DecisionTreeClassifier\n        clf = DecisionTreeClassifier()\n\n        # Fit the classifier\n        clf.fit(numerical_data, labels)\n\n        return clf\n\n    except ValueError:\n        print(\"Error: The input data should be a list or NumPy array of strings.\")\n        return None\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 917, "library_problem_id": 100, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 918, "code": ["<code>\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Assuming X and y are your features and target variable\nX = [['dsa', '2'], ['sato', '3']]\ny = ['math', 'english']  # This is a hypothetical target variable\n\nclf = DecisionTreeClassifier()\nclf = clf.fit(X, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 918, "library_problem_id": 101, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 919, "code": ["<code>\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load the data\nfilename = \"animalData.csv\"\ndataframe = pd.read_csv(filename, dtype='category')\n\n# Remove the \"Name\" column\ndataframe = dataframe.drop([\"Name\"], axis=1)\n\n# Convert \"Class\" column to numerical values\ncleanup = {\"Class\": {\"Primary Hunter\": 0, \"Primary Scavenger\": 1}}\ndataframe.replace(cleanup, inplace=True)\n\n# Split the data into features (X) and target variable (y)\nX = dataframe.drop([\"Class\"], axis=1)\ny = dataframe[\"Class\"]\n\n# Convert categorical data to numerical data\nle = LabelEncoder()\nX = X.apply(le.fit_transform)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a logistic regression model\nmodel = LogisticRegression()\n\n# Fit the model to the training data\nmodel.fit(X_train, y_train)\n\n# Now the model is ready to predict the class of new animal data\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 919, "library_problem_id": 102, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 920, "code": ["<code>\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\n\ndef logistic_regression_on_csv():\n    # Load the CSV file into a pandas DataFrame\n    df = pd.read_csv('animalData.csv')\n\n    # Drop the 'Name' column\n    df = df.drop('Name', axis=1)\n\n    # Replace 'Class' column with 0s and 1s\n    le = LabelEncoder()\n    df['Class'] = le.fit_transform(df['Class'])\n\n    # Split the DataFrame into features (X) and target variable (y)\n    X = df.drop('Class', axis=1)\n    y = df['Class']\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Initialize the LogisticRegression model\n    model = LogisticRegression()\n\n    # Train the model\n    model.fit(X_train, y_train)\n\n    # Predict the Class of the animal\n    predictions = model.predict(X_test)\n\n    return predictions\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 920, "library_problem_id": 103, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 102}}
{"id": 921, "code": ["<code>\ndef split_dataset(dataset, split_ratio=0.2):\n    \"\"\"\n    Splits a chronologically sorted dataset into a training and testing set.\n\n    Parameters:\n    dataset (list): A chronologically sorted dataset.\n    split_ratio (float): The ratio of the dataset to be used for testing.\n\n    Returns:\n    tuple: A tuple containing the training and testing sets.\n    \"\"\"\n    split_index = int(len(dataset) * split_ratio)\n    training_set = dataset[:-split_index]\n    testing_set = dataset[-split_index:]\n    return training_set, testing_set\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 921, "library_problem_id": 104, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 922, "code": ["<code>\nimport pandas as pd\n\ndef split_dataset(df):\n    # Ensure the dataset is sorted by date\n    df = df.sort_values('date')\n\n    # Calculate the split index\n    split_index = int(len(df) * 0.8)\n\n    # Split the dataset\n    train_set = df.iloc[:split_index]\n    test_set = df.iloc[split_index:]\n\n    return train_set, test_set\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 922, "library_problem_id": 105, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 923, "code": ["<code>\nimport pandas as pd\n\ndef split_train_test(df):\n    # Ensure the dataframe is indexed by dates\n    assert isinstance(df.index, pd.DatetimeIndex), \"The dataframe must be indexed by dates\"\n\n    # Sort the dataframe by date\n    df = df.sort_index()\n\n    # Calculate the split point\n    split_point = int(len(df) * 0.2)\n\n    # Split the dataframe into training and testing sets\n    train_df = df.iloc[:-split_point]\n    test_df = df.iloc[-split_point:]\n\n    return train_df, test_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 923, "library_problem_id": 106, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 924, "code": ["<code>\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Initialize MinMaxScaler\nscaler = MinMaxScaler()\n\n# Fit and transform 'X2' and 'X3' columns\ndf[['X2_scale', 'X3_scale']] = scaler.fit_transform(df[['X2', 'X3']])\n\n# Group by 'Month' and apply the transformation\ndf_grouped = df.groupby('Month').apply(lambda x: scaler.fit_transform(x[['X2', 'X3']]))\n\n# Reset the index\ndf_grouped.reset_index(level=0, inplace=True)\n\n# Rename the columns\ndf_grouped.columns = ['Month', 'X2_scale', 'X3_scale']\n\n# Concatenate the original DataFrame with the transformed DataFrame\ndf = pd.concat([df, df_grouped], axis=1)\n\nreturn df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 924, "library_problem_id": 107, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 925, "code": ["<code>\ndef scale_columns(df):\n    # Initialize MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Define the columns to be scaled\n    cols = ['A2', 'A3']\n\n    # Apply MinMaxScaler to the columns grouped by 'Month'\n    for col in cols:\n        df['new_' + col] = df.groupby('Month')[col].apply(lambda x: scaler.fit_transform(x))\n\n    return df\n\nmyData = scale_columns(myData)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 925, "library_problem_id": 108, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 107}}
{"id": 926, "code": ["<code>\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef tokenize_string(input_string):\n    vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w\\w+\\b\", lowercase=False)\n    vectorizer.fit_transform([input_string])\n    return vectorizer.get_feature_names_out()\n\n# Test the function\nprint(tokenize_string('Hello @friend, this is a good day. #good.'))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 926, "library_problem_id": 109, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 927, "code": ["<code>\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef create_vocabulary(input_string):\n    vectorizer = CountVectorizer()\n    vectorizer.fit_transform([input_string])\n    vocabulary = vectorizer.get_feature_names_out()\n    return sorted(vocabulary)\n\ninput_string = 'ha @ji me te no ru bu ru wa, @na n te ko to wa na ka tsu ta wa. wa ta shi da ke no mo na ri za, mo u to kku ni #de a \\'t te ta ka ra'\nprint(create_vocabulary(input_string))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 927, "library_problem_id": 110, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 109}}
{"id": 928, "code": ["<code>\nimport pandas as pd\n\ndef get_grid_search_results(grid_search_fitted):\n    \"\"\"\n    Given a scikit-learn GridSearchCV object that has already been fit, \n    create a pandas DataFrame that includes the accuracy scores for all \n    candidate models. The DataFrame should include columns for the parameters \n    and the corresponding accuracy scores.\n    \"\"\"\n    # Get the results from the grid search\n    cv_results = grid_search_fitted.cv_results_\n\n    # Convert the results to a pandas DataFrame\n    df = pd.DataFrame(cv_results)\n\n    # Select the columns we're interested in\n    df = df[['params', 'mean_test_score', 'std_test_score']]\n\n    # Rename the columns for clarity\n    df.columns = ['parameters', 'mean_accuracy', 'std_accuracy']\n\n    return df\n\n# Example usage:\n# grid_search_fitted = load_data()\n# df = get_grid_search_results(grid_search_fitted)\n# print(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 928, "library_problem_id": 111, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 929, "code": ["<code>\n# Assuming GridSearchCV_fitted is your fitted GridSearchCV object\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\n\n# Sort the DataFrame by mean_fit_time\nfull_results = full_results.sort_values(by='mean_fit_time')\n\n# Display the DataFrame\nprint(full_results)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 929, "library_problem_id": 112, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 930, "code": ["<code>\nfrom sklearn.ensemble import IsolationForest\nimport joblib\n\n# Load your data\ndata = load_data()\n\n# Load the trained model from a file named \"sklearn_model\"\nmodel = joblib.load('sklearn_model.pkl')\n\n# Use the model to make predictions\npredictions = model.predict(data)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 930, "library_problem_id": 113, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 931, "code": ["<code>\ndf['items'])\n\nprint(df_similarity)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 931, "library_problem_id": 114, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 932, "code": ["<code>\nimport torch\n\n# Assuming that we have a model and its parameters\n# model = ...\n# params = model.parameters()\n\noptim = torch.optim.SGD(params, lr=0.01, momentum=0.9, weight_decay=0.005, dampening=0.0, nesterov=True)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 932, "library_problem_id": 0, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 933, "code": ["<code>\nHere is a Python code block that implements a custom PyTorch optimizer that allows for direct modification of the learning rate after each epoch. The learning rate is updated based on observed training loss. The optimizer is initialized with a starting learning rate of 0.01. The optimizer is used to train a model on a given dataset. The learning rate is updated after each epoch based on the observed training loss. The updated learning rate values are stored in a list. The function returns this list of updated learning rate values. The function can handle any PyTorch model and dataset.\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass CustomOptimizer:\n    def __init__(self, model, lr=0.01):\n        self.model = model\n        self.optimizer = optim.SGD(model.parameters(), lr=lr)\n        self.lr = lr\n        self.lr_history = [lr]\n\n    def train(self, dataloader, criterion, epochs=10):\n        for epoch in range(epochs):\n            running_loss = 0.0\n            for i, data in enumerate(dataloader, 0):\n                inputs, labels = data\n                self.optimizer.zero_grad()\n\n                outputs = self.model(inputs)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                self.optimizer.step()\n\n                running_loss += loss.item()\n\n            avg_loss = running_loss / len(dataloader)\n            self.update_lr(avg_loss)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 933, "library_problem_id": 1, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 934, "code": ["<code>\n# First, we need to load the optimizer.\noptim = load_data()\n\n# Now, we can adjust the learning rate by accessing the param_groups attribute of the optimizer.\nfor param_group in optim.param_groups:\n    param_group['lr'] = 0.0005\n\n# Now, the learning rate of the optimizer has been dynamically adjusted to 0.0005.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 934, "library_problem_id": 2, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 935, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\ndef load_data():\n    # Load a CSV file\n    df = pd.read_csv('your_file.csv')\n    return df\n\n# Load the data\ndata = load_data()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 935, "library_problem_id": 3, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 936, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom gensim.models import Word2Vec\nfrom gensim.test.utils import common_texts\n\n# Load data\ndef load_data():\n    # Load your data here\n    pass\n\n# Initialize Word2Vec model\nword2vec = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n\n# Create PyTorch embedding layer with the same size as the Word2Vec model\nvocab_size = len(word2vec.wv.key_to_index)\nembedding_dim = word2vec.wv.vector_size\nembedding = torch.nn.Embedding(vocab_size, embedding_dim)\n\n# Load pre-trained weights into the embedding layer\nvectors = torch.tensor(word2vec.wv.vectors, dtype=torch.float32)\nembedding.weight.data.copy_(vectors)\n\n# Set the embedding layer to not be trainable\nembedding.weight.requires_grad = False\n\n# Now you can use the embedding layer in your PyTorch model\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 936, "library_problem_id": 4, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 937, "code": ["<code>\nimport torch\nfrom gensim.models import Word2Vec\nfrom gensim.test.utils import common_texts\n\ndef get_embedded_input(input_Tensor):\n    word2vec = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n    weights = torch.FloatTensor(word2vec.wv.vectors)\n    num_embeddings, embedding_dim = weights.shape\n    embedding = torch.nn.Embedding.from_pretrained(weights)\n    return embedding(input_Tensor)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 937, "library_problem_id": 5, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 938, "code": ["<code>\nimport pandas as pd\n\ndef convert_tensor_to_df(x):\n    # Convert PyTorch tensor to numpy array\n    x_np = x.numpy()\n\n    # Convert numpy array to pandas DataFrame\n    px = pd.DataFrame(x_np)\n\n    return px\n\n# Load PyTorch tensor\nx = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n# Convert tensor to DataFrame\npx = convert_tensor_to_df(x)\n\nprint(px)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 938, "library_problem_id": 6, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 939, "code": ["<code>\nimport pandas as pd\nimport torch\n\ndef convert_tensor_to_df(tensor):\n    # Convert the tensor to a numpy array\n    numpy_array = tensor.numpy()\n    \n    # Create a pandas DataFrame from the numpy array\n    df = pd.DataFrame(numpy_array)\n    \n    # Replace all tensor objects within the DataFrame with their numerical values\n    df.replace(to_replace=r'^tensor\\([0-9]*\\)$', value='', regex=True, inplace=True)\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 939, "library_problem_id": 7, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 940, "code": ["<code>\nimport pandas as pd\n\ndef convert_tensor_to_df(x):\n    px = pd.DataFrame(x.detach().numpy())\n    return px\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 940, "library_problem_id": 8, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 941, "code": ["<code>\nimport torch\n\ndef select_columns(A_log, B):\n    # Convert A_log to a boolean tensor\n    A_bool = A_log.bool()\n\n    # Create a masked tensor from B using A_bool as the mask\n    masked_B = torch.masked_select(B, A_bool)\n\n    # Reshape the masked tensor to the original shape of B\n    selected_B = masked_B.reshape(B.shape)\n\n    return selected_B\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 941, "library_problem_id": 9, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 942, "code": ["<code>\nimport torch\n\n# Define the tensors\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nA_logical = torch.ByteTensor([1, 0, 1]) # the logical index\n\n# Perform logical indexing on the columns\nC = B[:, A_logical]\nprint(C)\n\n# Perform logical indexing on the rows\nA_logical = torch.ByteTensor([[1, 0, 1]]) # the logical index\nC = B[A_logical, :]\nprint(C)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 942, "library_problem_id": 10, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 943, "code": ["<code>\nimport torch\n\ndef slice_tensor_by_logical_index(tensor, logical_index):\n    if tensor.dim() == 1:\n        tensor = tensor.unsqueeze(0)\n    if logical_index.dim() == 1:\n        logical_index = logical_index.unsqueeze(0)\n\n    if tensor.size()[0] != logical_index.size()[0]:\n        raise ValueError(\"The dimensions of the index and tensor are not the same.\")\n\n    if tensor.is_cuda and not logical_index.is_cuda:\n        logical_index = logical_index.cuda()\n    elif not tensor.is_cuda and logical_index.is_cuda:\n        tensor = tensor.cuda()\n\n    if tensor.dtype != torch.uint8 and logical_index.dtype != torch.bool:\n        raise TypeError(\"The tensor and logical index should be of type LongTensor or ByteTensor.\")\n\n    return tensor.masked_select(logical_index)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 943, "library_problem_id": 11, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 944, "code": ["<code>\nimport torch\n\ndef slice_tensor_based_on_logical_index(tensor, logical_index):\n    if tensor.dim() not in [1, 2]:\n        raise ValueError(\"The tensor should be a 1D or 2D tensor.\")\n    if logical_index.dim() != 1:\n        raise ValueError(\"The logical index should be a 1D tensor.\")\n    if tensor.size(1) != logical_index.size(0):\n        raise ValueError(\"The tensor and the logical index should have the same size in the dimension being sliced.\")\n    if not isinstance(logical_index, torch.ByteTensor):\n        raise ValueError(\"The logical index should be a ByteTensor.\")\n    if not isinstance(tensor, torch.LongTensor):\n        raise ValueError(\"The tensor should be a LongTensor.\")\n\n    logical_index = logical_index.nonzero().squeeze()\n    return tensor.index_select(1, logical_index)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 944, "library_problem_id": 12, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 945, "code": ["<code>\nimport torch\n\ndef slice_tensor(B, A_log):\n    try:\n        # Convert ByteTensor to LongTensor\n        A_log = A_log.long()\n\n        # Get the indices where A_log is 1\n        indices = torch.nonzero(A_log).squeeze()\n\n        # Slice tensor B using the indices\n        C = B[:, indices]\n\n        return C\n\n    except TypeError:\n        print(\"Error: A_log should be of type ByteTensor.\")\n        return None\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 945, "library_problem_id": 13, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 946, "code": ["<code>\nimport torch\n\ndef select_columns(tensor, index):\n    if len(index.shape) == 1:\n        index = index.unsqueeze(0)\n    elif len(index.shape) > 2:\n        raise ValueError(\"Index tensor should have 1 or 2 dimensions\")\n\n    if len(tensor.shape) == 1:\n        tensor = tensor.unsqueeze(0)\n    elif len(tensor.shape) > 2:\n        raise ValueError(\"Tensor should have 1 or 2 dimensions\")\n\n    if tensor.shape[0] != index.shape[0] and index.shape[1] != tensor.shape[1]:\n        raise ValueError(\"Tensor and index tensor should have the same number of rows\")\n\n    if index.dtype != torch.bool:\n        raise ValueError(\"Index tensor should be a boolean tensor\")\n\n    selected_columns = torch.masked_select(tensor, index)\n    return selected_columns.reshape(tensor.shape[0], index.shape[1])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 946, "library_problem_id": 14, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 947, "code": ["<code>\nprint(B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 947, "library_problem_id": 15, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 948, "code": ["<code>\nThe given code is incomplete and does not provide the necessary information to complete it. The function `load_data()` is not defined in the code. Please provide the necessary details or the complete code for the function `load_data()`.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 948, "library_problem_id": 16, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 949, "code": ["<code>\nimport numpy as np\nimport torch\n\ndef convert_to_tensor(array):\n    if isinstance(array, np.ndarray):\n        if array.dtype == object:\n            return torch.tensor([convert_to_tensor(item) for item in array])\n        else:\n            return torch.tensor(array)\n    else:\n        return array\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 949, "library_problem_id": 17, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 950, "code": ["<code>\nimport numpy as np\nimport torch\n\ndef convert_to_tensor(numpy_array):\n    tensor = torch.tensor(numpy_array, dtype=torch.float16)\n    return tensor\n\nnumpy_array = np.array([\n   np.array([0.5, 1.0, 2.0], dtype=np.float16),\n   np.array([4.0, 6.0, 8.0], dtype=np.float16)\n], dtype=object)\n\ntensor = convert_to_tensor(numpy_array)\nprint(tensor)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 950, "library_problem_id": 18, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 951, "code": ["<code>\nimport torch\n\ndef create_mask(sentence_lengths):\n    max_len = max(sentence_lengths)\n    mask = torch.zeros(len(sentence_lengths), max_len, dtype=torch.long)\n    for i, length in enumerate(sentence_lengths):\n        mask[i, :length] = 1\n    return mask\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 951, "library_problem_id": 19, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 952, "code": ["<code>\ndef create_binary_masks(sentence_lengths):\n    max_length = sentence_lengths.max().item()\n    device = sentence_lengths.device\n    masks = torch.arange(max_length, device=device).expand(len(sentence_lengths), max_length) < sentence_lengths.unsqueeze(1)\n    return masks.long()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 952, "library_problem_id": 20, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 953, "code": ["<code>\nI'm sorry, but your question is not clear. You didn't provide the function `load_data()`, and you didn't specify what you want to do with the data that is loaded. \n\nPlease provide more details so I can assist you better.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 953, "library_problem_id": 21, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 954, "code": ["<code>\nimport torch\n\ndef create_mask(sentence_lengths):\n    max_len = max(sentence_lengths)\n    mask = torch.zeros(len(sentence_lengths), max_len, dtype=torch.long)\n    for i, length in enumerate(sentence_lengths):\n        mask[i, :length] = 1\n    return mask\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 954, "library_problem_id": 22, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 955, "code": ["<code>\nimport torch\n\ndef load_data():\n    # Load your data here.\n    # For example, let's create a 2D tensor with random values.\n    Tensor_2D = torch.rand(5, 3)  # 5 rows and 3 columns\n    return Tensor_2D\n\nTensor_2D = load_data()\nprint(Tensor_2D)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 955, "library_problem_id": 23, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 956, "code": ["<code>\nimport torch\nTensor_2D = load_data()\ndef Convert(t):\n    # return the solution in this function\n    result = t.flatten()\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 956, "library_problem_id": 24, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 23}}
{"id": 957, "code": ["<code>\nimport torch\n\ndef stack_or_cat(a, b):\n    if a.shape[1] != b.shape[1]:\n        return torch.cat((a, b), dim=1)\n\n    if a.shape[0] != b.shape[0]:\n        return torch.cat((a, b), dim=0)\n\n    return torch.stack((a, b), dim=0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 957, "library_problem_id": 25, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 958, "code": ["<code>\nimport torch\n\ndef stack_tensors(a, b):\n    return torch.stack((a, b))\n\na, b = load_data()\nab = stack_tensors(a, b)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 958, "library_problem_id": 26, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 959, "code": ["<code>\nimport torch\n\ndef stack_tensors(a, b):\n    if a.shape[0] != b.shape[0]:\n        if a.shape[0] == 2 and b.shape[0] == 1:\n            b = torch.cat([b]*2, dim=0)\n        elif a.shape[0] == 1 and b.shape[0] == 2:\n            a = torch.cat([a]*2, dim=0)\n        else:\n            raise ValueError(\"Tensors a and b must have the same number of rows or one must be twice the size of the other\")\n    ab = torch.stack((a, b), 0)\n    return ab\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 959, "library_problem_id": 27, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 960, "code": ["<code>\ndef modify_tensor(a, lengths):\n    for i in range(a.shape[0]):  # for each sentence in the batch\n        a[i, lengths[i]:, :] = 0\n    return a\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 960, "library_problem_id": 28, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 961, "code": ["<code>\nimport torch\n\ndef modify_tensor(a, lengths):\n    max_length = a.size(1)\n    fill_value = 2333\n\n    for i, length in enumerate(lengths):\n        if length < max_length:\n            a[i, length:, :] = fill_value\n\n    return a\n\n# Test the function\na = torch.randn(10, 1000, 96)\nlengths = torch.randint(0, 1000, (10,))\n\nmodified_a = modify_tensor(a, lengths)\nprint(modified_a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 961, "library_problem_id": 29, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 28}}
{"id": 962, "code": ["<code>\nimport torch\n\ndef modify_tensor(a, lengths):\n    max_length = a.size(1)\n    for i, length in enumerate(lengths):\n        if length < max_length:\n            a[i, length:, :] = 0\n    return a\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 962, "library_problem_id": 30, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 963, "code": ["<code>\nimport torch\n\ndef fill_tensor(a, lengths):\n    max_length = a.size(1)\n    fill_value = 2333\n    for i, length in enumerate(lengths):\n        if length < max_length:\n            a[i, length:, :] = fill_value\n    return a\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 963, "library_problem_id": 31, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 28}}
{"id": 964, "code": ["<code>\nimport torch\n\ndef load_data():\n    # Implement this function to load your data\n    # For example, you can return a list of 1D tensors\n    return [torch.tensor([1, 2, 3]), torch.tensor([4, 5, 6]), torch.tensor([7, 8, 9])]\n\ndef convert_to_tensor():\n    data = load_data()\n    return torch.stack(data)\n\n# Test the function\nprint(convert_to_tensor())\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 964, "library_problem_id": 32, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 965, "code": ["<code>\nimport torch\n\ndef list_to_tensor(lst):\n    # Convert the list to a PyTorch tensor\n    return torch.stack([torch.tensor(x) for x in lst])\n\ndef tensor_to_list(tensor):\n    # Convert the PyTorch tensor back to a list of tensors\n    return [x.numpy() for x in torch.unbind(tensor)]\n\n# Test the functions\nlst = [torch.tensor([1, 2, 3]), torch.tensor([4, 5, 6]), torch.tensor([7, 8, 9])]\ntensor = list_to_tensor(lst)\nprint(tensor)\n\nlst_back = tensor_to_list(tensor)\nprint(lst_back)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 965, "library_problem_id": 33, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 966, "code": ["<code>\nimport torch\n\ndef list_to_tensor(tensor_list):\n    # Check if the list is empty\n    if not tensor_list:\n        return torch.tensor([])\n\n    # Check if all elements in the list are tensors\n    if not all(isinstance(t, torch.Tensor) for t in tensor_list):\n        raise ValueError(\"All elements in the list must be PyTorch tensors.\")\n\n    # Concatenate all tensors in the list along the first dimension\n    return torch.cat(tensor_list, dim=0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 966, "library_problem_id": 34, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 967, "code": ["<code>\nimport torch\n\ndef convert_to_tensor_of_tensors(list_of_tensors):\n    tensor_of_tensors = torch.stack(list_of_tensors)\n    return tensor_of_tensors\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 967, "library_problem_id": 35, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 968, "code": ["<code>\nimport numpy as np\nimport torch\n\ndef select_elements(tensor, indices):\n    # Convert the NumPy array to a PyTorch tensor\n    indices_tensor = torch.tensor(indices)\n    \n    # Use torch.gather to select the elements at the specified indices\n    selected_elements = torch.gather(tensor, 1, indices_tensor.view(-1, 1))[:, 0]\n    \n    return selected_elements\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 968, "library_problem_id": 36, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 969, "code": ["<code>\nimport torch\nimport numpy as np\n\ndef select_from_tensor(tensor, indices):\n    assert isinstance(tensor, torch.Tensor), \"Input tensor should be a PyTorch tensor\"\n    assert isinstance(indices, np.ndarray), \"Input indices should be a NumPy array\"\n    assert tensor.shape[0] == indices.shape[0], \"The first dimension of the tensor and indices should be the same\"\n    assert tensor.shape[1] == 2, \"The second dimension of the tensor should be 2\"\n    assert np.all(indices >= 0) and np.all(indices <= 1), \"Indices should be either 0 or 1\"\n\n    return tensor[:, indices.astype(int)][:, 1]\n\n# Test the function\ntensor = torch.tensor([[-22.2,  33.3], [-55.5,  11.1], [-44.4,  22.2]])\nindices = np.array([1, 1, 0])\nprint(select_from_tensor(tensor, indices))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 969, "library_problem_id": 37, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 970, "code": ["<code>\nassert type(t) == torch.Tensor\nassert type(idx) == np.ndarray\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 970, "library_problem_id": 38, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 971, "code": ["<code>\nimport torch\n\ndef select_slices(ids, x):\n    return x[ids]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 971, "library_problem_id": 39, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 972, "code": ["<code>\nimport torch\n\ndef select_slices(ids, x):\n    # Use PyTorch's gather function to select the slices\n    result = x.gather(dim=1, index=ids)\n\n    # Return the resulting tensor\n    return result[:, 0, :]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 972, "library_problem_id": 40, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 973, "code": ["<code>\nimport torch\n\ndef select_slices(ids, x):\n    # Use torch.gather to select the slices\n    result = torch.gather(x, 1, ids.unsqueeze(-1).expand(-1, -1, 2))\n    # Return only the selected slices\n    return result[:, :, 0]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 973, "library_problem_id": 41, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 974, "code": ["<code>\nimport torch\n\ndef get_max_indices(tensor):\n    # Apply torch.max to find the index of the maximum value in each row of the input tensor\n    max_indices = torch.max(tensor, dim=1)[1]\n    \n    # Reshape the tensor to have shape (n, 1)\n    max_indices = max_indices.unsqueeze(1)\n    \n    return max_indices\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 974, "library_problem_id": 42, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 975, "code": ["<code>\nimport torch\n\ndef find_function_name(softmax_output):\n    # Find the index of the maximum value in the softmax output\n    max_index = torch.argmax(softmax_output)\n\n    # Return the function name corresponding to the maximum index\n    return function_names[max_index]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 975, "library_problem_id": 43, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 976, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\ndef load_data():\n    # Load data from a CSV file\n    data = pd.read_csv('data.csv')\n\n    # Convert the data to a numpy array\n    data_array = data.values\n\n    # Convert the numpy array to a torch tensor\n    data_tensor = torch.from_numpy(data_array)\n\n    return data_tensor\n\nsoftmax_output = load_data()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 976, "library_problem_id": 44, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 977, "code": ["<code>\nimport torch\n\ndef solve(softmax_output):\n    # The function torch.argmax returns the indices of the maximum values of all elements in the input tensor.\n    # The argument dim=1 specifies that the maximum values should be found along the first dimension (i.e., for each row).\n    return torch.argmax(softmax_output, dim=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 977, "library_problem_id": 45, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 978, "code": ["<code>\nimport torch\n\ndef solve(softmax_output):\n    # Find the index of the maximum value in the softmax output\n    return torch.argmax(softmax_output)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 978, "library_problem_id": 46, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 979, "code": ["<code>\nlog_p = log_p.view(-1, c)\n    # target: (n*w*z,)\n    target = target.view(-1)\n    # loss: (n*w*z,)\n    loss = F.nll_loss(log_p, target, weight=weight, reduction='sum')\n    if size_average:\n        loss /= (n * w * z)\n    return loss\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 979, "library_problem_id": 47, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 980, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\nA, B = load_data()\n\n# Check if the arrays are equal\nare_equal = np.array_equal(A, B)\n\n# Count the number of equal elements\ncnt_equal = np.sum(A == B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 980, "library_problem_id": 48, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 981, "code": ["<code>\nimport tensorflow as tf\n\n# Load data\ntensor1, tensor2 = load_data()\n\n# Ensure tensors are 1D\ntensor1 = tf.squeeze(tensor1)\ntensor2 = tf.squeeze(tensor2)\n\n# Check if tensors are of the same length\nassert tensor1.shape[0] == tensor2.shape[0] == 11, \"Tensors are not 1D or have different lengths\"\n\n# Count equal elements\ncnt_equal = tf.reduce_sum(tf.cast(tensor1 == tensor2, tf.int32))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 981, "library_problem_id": 49, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 982, "code": ["<code>\nimport torch\n\ndef load_data():\n    # This function is not provided in the question.\n    # You need to implement this function to load your data.\n    pass\n\ndef count_differing_elements():\n    # Load the data\n    tensor1, tensor2 = load_data()\n\n    # Check if the tensors have the same size\n    if tensor1.shape != tensor2.shape:\n        raise ValueError(\"Tensors must have the same shape\")\n\n    # Calculate the number of differing elements\n    cnt_not_equal = torch.sum(tensor1 != tensor2)\n\n    return cnt_not_equal\n\n# Test the function\ntensor1 = torch.rand(1000, 1)\ntensor2 = torch.rand(1000, 1)\n\ndef load_data():\n    return tensor1, tensor2\n\nprint(count_differing_elements())\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 982, "library_problem_id": 50, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 48}}
{"id": 983, "code": ["<code>\ncnt_equal = (A == B).sum()\n    return cnt_equal\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 983, "library_problem_id": 51, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 984, "code": ["<code>\nimport torch\n\ndef count_equal_elements(A, B):\n    # Check if the tensors have the same shape\n    if A.shape != B.shape:\n        raise ValueError(\"Tensors must have the same shape\")\n\n    # Compare the tensors and count the number of equal elements\n    cnt_equal = (A == B).sum().item()\n\n    return cnt_equal\n\n# Create two tensors of shape (2*x, 1)\nA = torch.tensor([[1], [2], [3], [4], [5], [6], [7], [8]])\nB = torch.tensor([[1], [2], [3], [4], [5], [6], [7], [9]])\n\n# Call the function with the two tensors\ncnt_equal = count_equal_elements(A, B)\n\n# Print the result\nprint(cnt_equal)  # Output: 6\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 984, "library_problem_id": 52, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 985, "code": ["<code>\nI'm sorry, but your question is not clear. The function `load_data()` is not defined in the given code. Please provide more details about the function and the expected output, so I can assist you better.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 985, "library_problem_id": 53, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 986, "code": ["<code>\nimport torch\n\ndef split_tensor(tensor, dim):\n    # Get the size of the specified dimension\n    size = tensor.size(dim)\n    \n    # Initialize an empty list to store the smaller tensors\n    tensors = []\n    \n    # Loop over the specified dimension\n    for i in range(size - 1):\n        # Slice the tensor along the specified dimension\n        sliced = tensor.narrow(dim, i, 10)\n        \n        # Append the sliced tensor to the list\n        tensors.append(sliced)\n    \n    return tensors\n\n# Test the function\ntensor = torch.randn(1, 3, 10, 40, 1)\ntensors = split_tensor(tensor, 3)\n\n# Print the shapes of the tensors\nfor i, tensor in enumerate(tensors):\n    print(f\"Shape of tensor_{i+1} : {tensor.shape}\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 986, "library_problem_id": 54, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 987, "code": ["<code>\nimport torch\n\ndef split_tensor(tensor, chunk_dim):\n    # Get the size of the chunk dimension\n    chunk_size = tensor.size(chunk_dim)\n    \n    # Create a list to store the chunks\n    chunks = []\n    \n    # Iterate over the chunk dimension\n    for i in range(chunk_size):\n        # Get the chunk\n        chunk = tensor.select(chunk_dim, i)\n        \n        # Append the chunk to the list\n        chunks.append(chunk)\n        \n    return chunks\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 987, "library_problem_id": 55, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 988, "code": ["<code>\nimport torch\n\ndef modify_output_spectrogram(mask, clean_input_spectrogram, output):\n    # Create a mask tensor with the same shape as the output tensor\n    mask_tensor = mask.expand_as(output)\n\n    # Use the mask tensor to select elements from the clean input spectrogram\n    clean_input_spectrogram_selected = torch.masked_select(clean_input_spectrogram, mask_tensor)\n\n    # Reshape the clean input spectrogram selected tensor to match the output tensor\n    clean_input_spectrogram_selected = clean_input_spectrogram_selected.reshape(output.shape)\n\n    # Replace the elements in the output tensor with the selected elements from the clean input spectrogram\n    output = torch.where(mask_tensor, clean_input_spectrogram_selected, output)\n\n    return output\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 988, "library_problem_id": 56, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 989, "code": ["<code>\nimport torch\n\ndef load_data():\n    # Loading mask\n    mask = torch.load('mask.pt')\n    \n    # Loading clean input spectrogram\n    clean_input_spectrogram = torch.load('clean_input_spectrogram.pt')\n    \n    # Loading output\n    output = torch.load('output.pt')\n    \n    return mask, clean_input_spectrogram, output\n\nmask, clean_input_spectrogram, output = load_data()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 989, "library_problem_id": 57, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 990, "code": ["<code>\nimport torch\n\ndef find_signed_min(x, y):\n    # Check if x and y are of the same shape\n    assert x.shape == y.shape, \"x and y must have the same shape\"\n    \n    # Check if x and y are 1D tensors\n    assert x.ndim == 1 and y.ndim == 1, \"x and y must be 1D tensors\"\n    \n    # Check if x and y are on the same device\n    assert x.device == y.device, \"x and y must be on the same device\"\n    \n    # Find the signed minimum\n    signed_min = torch.min(x, y) * torch.sign(x - y)\n    \n    return signed_min\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 990, "library_problem_id": 58, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 991, "code": ["<code>\ndef find_signed_max(x, y):\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    max_abs = torch.max(torch.abs(x), torch.abs(y))\n\n    # If the absolute value of x is greater than y's absolute value, return x's sign and max_abs\n    # If the absolute value of y is greater than x's absolute value, return y's sign and max_abs\n    # If the absolute value of x and y are equal, return x's sign and max_abs\n    condition = torch.abs(x) > torch.abs(y)\n    signed_max = condition * sign_x * max_abs + (1 - condition) * sign_y * max_abs\n    return signed_max\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 991, "library_problem_id": 59, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 992, "code": ["<code>\nimport torch\n\ndef solve(x, y):\n    abs_x = torch.abs(x)\n    abs_y = torch.abs(y)\n    min_abs_x = torch.min(abs_x)\n    min_abs_y = torch.min(abs_y)\n\n    if min_abs_x < min_abs_y:\n        min_abs = min_abs_x\n        mask = x == min_abs_x\n    else:\n        min_abs = min_abs_y\n        mask = y == min_abs_y\n\n    signed_min = torch.where(mask, x, y)\n    return signed_min\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 992, "library_problem_id": 60, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 993, "code": ["<code>\nMyNet = torch.nn.Sequential(torch.nn.Linear(4, 15),\n                            torch.nn.Sigmoid(),\n                            torch.nn.Linear(15, 3),\n                            torch.nn.Softmax(dim=1))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 993, "library_problem_id": 61, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 994, "code": ["<code>\nimport torch\n\ndef combine_tensors(tensor1, tensor2):\n    # Check if the tensors have a single column\n    assert tensor1.shape[1] == tensor2.shape[1] == 1\n\n    # Calculate the overlap length\n    overlap_len = tensor1.shape[0]\n\n    # Concatenate the tensors\n    combined = torch.cat([tensor1, tensor2], dim=0)\n\n    # Calculate the average of the overlapping columns\n    avg_overlap = torch.mean(combined[:overlap_len] + combined[-overlap_len:])\n\n    # Replace the overlapping columns with the average\n    combined[overlap_len-1:-overlap_len+1] = avg_overlap\n\n    return combined\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 994, "library_problem_id": 62, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 995, "code": ["<code>\nimport torch\n\ndef combine_tensors(tensor1, tensor2):\n    # Check if the tensors have the same number of columns\n    if tensor1.size(1) != tensor2.size(1):\n        raise ValueError(\"The tensors must have the same number of columns\")\n\n    # Check if the tensors have the same number of rows\n    if tensor1.size(0) != tensor2.size(0):\n        raise ValueError(\"The tensors must have the same number of rows\")\n\n    # Create a mask for the overlapping columns\n    mask = torch.eye(tensor1.size(1), dtype=torch.bool)\n\n    # Compute the average of the overlapping columns\n    avg = (tensor1[:, mask] + tensor2[:, mask]) / 2\n\n    # Replace the overlapping columns in the first tensor with the average\n    tensor1[:, mask] = avg\n\n    # Return the resulting tensor\n    return tensor1\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 995, "library_problem_id": 63, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 62}}
{"id": 996, "code": ["<code>\nimport torch\n\ndef add_border(tensor):\n    # Get the dimensions of the tensor\n    _, channels, height, width = tensor.size()\n\n    # Create a new tensor with zeros\n    new_tensor = torch.zeros(channels, height + 2, width + 2)\n\n    # Copy the original tensor to the center of the new tensor\n    new_tensor[:, 1:-1, 1:-1] = tensor\n\n    return new_tensor\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 996, "library_problem_id": 64, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 997, "code": ["<code>\nimport torch\n\ndef pad_tensor(tensor):\n    # Create a 4x4 tensor filled with zeros\n    padded_tensor = torch.zeros(4, 4)\n    \n    # Place the original tensor in the center of the new tensor\n    padded_tensor[1:3, 1:3] = tensor\n    \n    return padded_tensor\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 997, "library_problem_id": 65, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 998, "code": ["<code>\nimport numpy as np\n\ndef add_border(tensor):\n    # Get the shape of the input tensor\n    rows, cols = tensor.shape\n\n    # Create a new tensor with the same shape as the input tensor but with a border of -1 values\n    new_tensor = np.full((rows + 2, cols + 2), -1)\n\n    # Insert the original tensor into the center of the new tensor\n    new_tensor[1:-1, 1:-1] = tensor\n\n    return new_tensor\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 998, "library_problem_id": 66, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 999, "code": ["<code>\nimport torch\n\ndef dot_product(data, weight):\n    # Reshape the weight vector to (1, 1, hid_dim)\n    weight = weight.view(1, 1, -1)\n    \n    # Perform the dot product using torch.bmm()\n    result = torch.bmm(data, weight)\n    \n    # Use torch.squeeze() to remove the unnecessary dimension\n    result = torch.squeeze(result)\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 999, "library_problem_id": 67, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
