{"id": 0, "code": ["<code>\nresult = df.reindex(List)\n```\nEND SOLUTION\n</code>\nThe reindex function in pandas allows you to reorder the rows of a DataFrame based on a new index. In this case, the new index is the permutation of the original index.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 0, "library_problem_id": 0, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 1, "code": ["<code>\n# Shuffle the DataFrame\ndf = df.reindex(List)\n\n# Count the number of rows with different Type\ndifferent_type_count = np.sum(df['Type'].ne(df['Type'].shift()))\n\nresult = (df, different_type_count)\nresult\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 1, "library_problem_id": 1, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 2, "code": ["<code>\nfor col in df.columns:\n    mask = df[col].map(df[col].value_counts()) >= 2\n    df[col] = df[col].where(mask, 'other')\nresult = df\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 2, "library_problem_id": 2, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 3, "code": ["<code>\nfor col in df.columns:\n    mask = df[col].map(df[col].value_counts()) >= 3\n    df[col] = df[col].where(mask, 'other')\nresult = df\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 3, "library_problem_id": 3, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 4, "code": ["<code>\n```python\n    for col in df.columns:\n        mask = df[col].map(df[col].value_counts()) < 2\n        df.loc[mask, col] = 'other'\n    return df\n\nresult = f(example_df)\nprint(result)\n### END SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 4, "library_problem_id": 4, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 5, "code": ["<code>\nfor col in ['Qu1', 'Qu2', 'Qu3']:\n    mask = df[col].map(df[col].value_counts()) < 2\n    df[col] = df[col].where(~mask, 'other')\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 5, "library_problem_id": 5, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 6, "code": ["<code>\nfor col in ['Qu1', 'Qu2', 'Qu3']:\n    mask = df[col].map(df[col].value_counts()) < 2\n    df.loc[mask, col] = 'other'\n\nresult = df\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 6, "library_problem_id": 6, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 7, "code": ["<code>\ndf['id'] = range(1, len(df) + 1)\ndf = df.sort_values(by=['url', 'keep_if_dup'], ascending=[True, False])\ndf = df.drop_duplicates(subset='url', keep='first')\nresult = df\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 7, "library_problem_id": 7, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 8, "code": ["<code>\ndf['id'] = range(1, len(df) + 1)\ndf = df.sort_values(by=['url', 'drop_if_dup'], ascending=[True, False])\ndf = df.drop_duplicates(subset='url', keep='first')\nresult = df\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 8, "library_problem_id": 8, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 7}}
{"id": 9, "code": ["<code>\ndf['id'] = range(1, len(df) + 1)\ndf = df.sort_values(['url', 'keep_if_dup'], ascending=[True, False])\ndf = df.drop_duplicates(subset='url', keep='first')\nresult = df\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 9, "library_problem_id": 9, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 7}}
{"id": 10, "code": ["<code>\nresult = (df.groupby('name')\n          .apply(lambda x: {k: dict(v.values) for k, v in x[['v1', 'v2']].T.to_dict().items()})\n          .to_dict())\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 10, "library_problem_id": 10, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 11, "code": ["<code>\ndf['datetime'] = df['datetime'].dt.tz_localize(None)\n```\nEND SOLUTION\n</code>\nThis will remove the timezone information from the datetime column.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 11, "library_problem_id": 11, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 12, "code": ["<code>\n```python\n    df['datetime'] = df['datetime'].dt.tz_convert(None)\n    return df\n\nresult = f(example_df)\nprint(result)\n### END SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 12, "library_problem_id": 12, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 13, "code": ["<code>\ndf['datetime'] = df['datetime'].dt.tz_localize(None)\ndf['datetime'] = df['datetime'].dt.strftime('%d-%b-%Y %H:%M:%S')\ndf = df.sort_values('datetime')\n```\nEND SOLUTION\n</code>\nThe first line removes the timezone information. The second line converts the datetime object to the desired string format. The last line sorts the dataframe by the datetime column.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 13, "library_problem_id": 13, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 14, "code": ["<code>\ndf['datetime'] = df['datetime'].dt.tz_localize(None)\n```\nEND SOLUTION\n</code>\nThis will remove the timezone information from the datetime objects in the 'datetime' column of your dataframe.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 14, "library_problem_id": 14, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 15, "code": ["<code>\n# First, we need to parse the 'message' column into a list of dictionaries\ndf['message'] = df['message'].apply(lambda x: x.strip('[]').split(', '))\ndf['message'] = df['message'].apply(lambda x: [i.split(': ') for i in x])\n\n# Then, we need to convert this list of dictionaries into a single dictionary\ndf['message'] = df['message'].apply(dict)\n\n# Finally, we can convert this dictionary into a DataFrame\nresult = pd.concat([df.drop('message', axis=1), df['message'].apply(pd.Series)], axis=1)\n\n# Fill NaN values with 'none'\nresult.fillna('none', inplace=True)\n\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 15, "library_problem_id": 15, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 16, "code": ["<code>\ndf.loc[df['product'].isin(products), 'score'] *= 10\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 16, "library_problem_id": 16, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 17, "code": ["<code>\ndf['score'] = df.apply(lambda row: row['score']*10 if row['product'] not in products else row['score'], axis=1)\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 17, "library_problem_id": 17, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 18, "code": ["<code>\nfor product_range in products:\n    df.loc[df['product'].between(product_range[0], product_range[1]), 'score'] *= 10\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 18, "library_problem_id": 18, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 19, "code": ["<code>\ndf['score'] = df.apply(lambda row: (row['score'] - df[df['product'].isin(products)]['score'].min()) / (df[df['product'].isin(products)]['score'].max() - df[df['product'].isin(products)]['score'].min()) if row['product'] in products else row['score'], axis=1)\ndf\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 19, "library_problem_id": 19, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 20, "code": ["<code>\ndf['category'] = df.apply(lambda row: [col for col, val in row.items() if val == 1][0], axis=1)\ndf\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 20, "library_problem_id": 20, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 21, "code": ["<code>\ndf['category'] = df.apply(lambda row: 'A' if row['A']==0 else ('B' if row['B']==0 else ('C' if row['C']==0 else 'D')), axis=1)\ndf\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 21, "library_problem_id": 21, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 22, "code": ["<code>\ndf['category'] = df.apply(lambda row: [col for col in df.columns if row[col]], axis=1)\ndf\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 22, "library_problem_id": 22, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 20}}
{"id": 23, "code": ["<code>\ndf['Date'] = df['Date'].dt.strftime('%b-%Y')\n```\nEND SOLUTION\n</code>\nThe solution uses the strftime function to format the date as desired. The '%b' is a shorthand for the full month name, and '%Y' is a shorthand for the full 4-digit year.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 23, "library_problem_id": 23, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 24, "code": ["<code>\ndf['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 24, "library_problem_id": 24, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 25, "code": ["<code>\ndf['Date'] = df['Date'].dt.strftime('%d-%b-%Y %A')\ndf = df[df['Date'].between('17-Jan-2019', '20-Feb-2019')]\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 25, "library_problem_id": 25, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 23}}
{"id": 26, "code": ["<code>\ndf = df.shift(1).iloc[-1:].append(df.shift(-1).iloc[:1])\ndf\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 26, "library_problem_id": 26, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 27, "code": ["<code>\ndf = df.shift(-1).iloc[-1:].append(df.shift(1).iloc[:1])\ndf\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 27, "library_problem_id": 27, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 28, "code": ["<code>\ndf1 = df.shift(1, freq='D')\ndf1.iloc[-1] = df.iloc[0]\n\ndf2 = df.shift(-1, freq='D')\ndf2.iloc[0] = df.iloc[-1]\n\ndf = pd.concat([df1, df2], axis=1)\ndf.columns = ['#1', '#2']\ndf\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 28, "library_problem_id": 28, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 29, "code": ["<code>\ndf = df.shift(1).iloc[-1:].append(df.shift(-1).iloc[:1])\ndf\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 29, "library_problem_id": 29, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 30, "code": ["<code>\ndf.columns = [col+'X' for col in df.columns]\n```\nEND SOLUTION\n</code>\nThis will add 'X' to the end of all column names in the dataframe. It does this by iterating over each column name in the dataframe, appending 'X' to the end, and then assigning the new list of column names back to the dataframe.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 30, "library_problem_id": 30, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 31, "code": ["<code>\ndf.columns = ['X' + col for col in df.columns]\n```\nEND SOLUTION\n</code>\nThis will rename all columns in the dataframe df by adding \"X\" in the beginning of each column name.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 31, "library_problem_id": 31, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 30}}
{"id": 32, "code": ["<code>\ndf.columns = ['X' + col if col != 'HeaderX' else col for col in df.columns]\ndf</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 32, "library_problem_id": 32, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 30}}
{"id": 33, "code": ["<code>\n# First, we identify the columns that contain 'val' in their names\nval_columns = [col for col in df.columns if 'val' in col]\n\n# Then, we use the agg function with a dictionary where the keys are the column names and the values are 'mean'\nresult = df.groupby('group').agg({col: 'mean' for col in val_columns})\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 33, "library_problem_id": 33, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 34, "code": ["<code>\n# First, we identify the columns that contain 'val' in their names\nval_columns = [col for col in df.columns if 'val' in col]\n\n# Then, we use the agg function with a dictionary where the keys are the column names and the values are 'sum'\nresult = df.groupby('group').agg({col: 'sum' for col in val_columns})\nresult['group_color'] = df.groupby('group')['group_color'].first()\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 34, "library_problem_id": 34, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 33}}
{"id": 35, "code": ["<code>\n# First, we need to identify the columns that end with '2' and the ones that don't\ncols_mean2 = [col for col in df.columns if col.endswith('2')]\ncols_other = [col for col in df.columns if col not in cols_mean2 and col not in ['group', 'group_color']]\n\n# Then, we create the aggregation dictionary\nagg_dict = {'group_color': 'first'}\nagg_dict.update({col: 'mean' for col in cols_mean2})\nagg_dict.update({col: 'sum' for col in cols_other})\n\n# Finally, we apply the groupby and agg functions\nresult = df.groupby('group').agg(agg_dict)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 35, "library_problem_id": 35, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 33}}
{"id": 36, "code": ["<code>\nresult = df.loc[row_list, column_list].mean(axis=0)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 36, "library_problem_id": 36, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 37, "code": ["<code>\nresult = df.loc[row_list, column_list].sum()\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 37, "library_problem_id": 37, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 38, "code": ["<code>\n</code>\nEND SOLUTION\n\nThis will give you the sum of the specified rows and columns.\n\nIf you want to delete the column with the largest sum, you can do:\n\n<code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 38, "library_problem_id": 38, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 39, "code": ["<code>\nresult = df.apply(pd.Series.value_counts).fillna(0)\nresult = result.unstack().fillna(0).astype(int)\nresult = result.T.unstack()\nresult = result.reset_index()\nresult.columns = ['Column', 'Value', 'Count']\nresult = result.pivot(index='Column', columns='Value', values='Count').fillna(0)\nresult = result.stack()\nresult = result.astype(int)\nresult</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 39, "library_problem_id": 39, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 40, "code": ["<code>\nresult = df.isnull().sum()\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 40, "library_problem_id": 40, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 41, "code": ["<code>\nresult = \"\"\nfor col in df.columns:\n    result += f\"---- {col} ---\\n{df[col].value_counts()}\\n\\n\"\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 41, "library_problem_id": 41, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 39}}
{"id": 42, "code": ["<code>\n# Rename the first row\ndf.columns = df.loc[0].values\n\n# Drop the first row\ndf = df.drop(df.index[0])\n\n# Reset the index\ndf = df.reset_index(drop=True)\n\nresult = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 42, "library_problem_id": 42, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 43, "code": ["<code>\n# Rename the first row\ndf.columns = df.loc[0].values\n\n# Drop the first row\ndf = df.drop(df.index[0])\n\n# Reset the index\ndf = df.reset_index(drop=True)\n\nresult = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 43, "library_problem_id": 43, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 44, "code": ["<code>\nresult = df.apply(lambda x: pd.Series(x.dropna().values.tolist() + x[x.isnull()].values.tolist()), axis=1)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 44, "library_problem_id": 44, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 45, "code": ["<code>\nresult = df.apply(lambda x: x.ffill(), axis=1)\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 45, "library_problem_id": 45, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 44}}
{"id": 46, "code": ["<code>\nresult = df.apply(lambda x: pd.Series(x.dropna().values.tolist() + x[x.isnull()].index.tolist()), axis=1)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 46, "library_problem_id": 46, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 44}}
{"id": 47, "code": ["<code>\n# First, we select the rows where value is less than the threshold\nsmall_values = df.loc[df['value'] < thresh]\n\n# Then, we sum these values\ntotal_value = small_values['value'].sum()\n\n# We drop these rows from the original dataframe\ndf = df.loc[df['value'] >= thresh]\n\n# And finally, we add a new row with the sum of the values\ndf.loc['X'] = total_value\nresult = df\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 47, "library_problem_id": 47, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 48, "code": ["<code>\n# First, we filter the dataframe to get rows with value greater than threshold\nabove_thresh = df[df['value'] > thresh]\n\n# Then, we calculate the average of these values\navg_value = above_thresh['value'].mean()\n\n# Finally, we create a new dataframe with the average value and add it to the original dataframe\nnew_row = pd.DataFrame({'value': [avg_value]}, index=['X'])\nresult = pd.concat([df[df['value'] <= thresh], new_row])\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 48, "library_problem_id": 48, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 47}}
{"id": 49, "code": ["<code>\n# First, we filter the dataframe to only include rows whose value is not in the given section\ndf = df[~df['value'].between(section_left, section_right)]\n\n# Then, we create a new row with the average value of the rows that were filtered out\naverage_value = df['value'].mean()\nnew_row = pd.DataFrame({'value': [average_value]}, index=['X'])\n\n# Finally, we append the new row to the original dataframe\nresult = df.append(new_row)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 49, "library_problem_id": 49, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 47}}
{"id": 50, "code": ["<code>\nresult = pd.concat([df, 1/df.add_prefix('inv_')], axis=1)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 50, "library_problem_id": 50, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 51, "code": ["<code>\nimport numpy as np\n\nexp_df = df.applymap(np.exp)\nexp_df.columns = ['exp_' + col for col in df.columns]\n\nresult = pd.concat([df, exp_df], axis=1)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 51, "library_problem_id": 51, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 52, "code": ["<code>\nresult = pd.concat([df, 1/df], axis=1)\nresult.columns = ['A', 'B', 'inv_A', 'inv_B']\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 52, "library_problem_id": 52, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 53, "code": ["<code>\nimport numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nfor col in df.columns:\n    df['sigmoid_'+col] = df[col].apply(sigmoid)\n\nresult = df\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 53, "library_problem_id": 53, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 54, "code": ["<code>\nmin_idx = df.idxmin()\nmask = df.index.to_series().lt(min_idx).T\nresult = df.where(mask).idxmax()\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 54, "library_problem_id": 54, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 55, "code": ["<code>\n# Create a mask where the minimum value in each column is True, and the rest is False\nmask = df.eq(df.min())\n\n# Create a mask where the maximum value in each column is True, and the rest is False\nmax_mask = df.eq(df.max())\n\n# Combine the two masks\ncombined_mask = mask | max_mask\n\n# Get the first occurrence of True in each column\nfirst_occurrence = df[combined_mask].idxmin()\n\nresult = first_occurrence\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 55, "library_problem_id": 55, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 56, "code": ["<code>\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf['val'] = df['val'].replace({pd.np.nan: 0}).astype(int)\nresult = df\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 56, "library_problem_id": 56, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 57, "code": ["<code>\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.groupby(['user', 'dt']).first().reset_index()\ndf['val'] = df['val'].replace({pd.np.nan: 0}).astype(int)\nresult = df\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 57, "library_problem_id": 57, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 56}}
{"id": 58, "code": ["<code>\ndf = df.set_index('dt').groupby('user').apply(lambda x: x.set_index(pd.date_range(start=x.index.min(), end=x.index.max()))).reset_index().fillna(233)\nresult = df\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 58, "library_problem_id": 58, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 59, "code": ["<code>\ndf = df.set_index(['user', 'dt']).unstack(fill_value=0).stack().reset_index()\ndf = df.groupby(['user', pd.Grouper(key='dt', freq='D')])['val'].max().reset_index()\ndf = df.sort_values(['user', 'dt'])\nresult = df\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 59, "library_problem_id": 59, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 60, "code": ["<code>\ndf.set_index('dt').groupby('user')['val'].resample('D').max().reset_index().fillna(method='ffill').reset_index(drop=True)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 60, "library_problem_id": 60, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 61, "code": ["<code>\nresult = df.replace({'name': {'Aaron': 1, 'Brave': 2, 'David': 3}})\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 61, "library_problem_id": 61, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 62, "code": ["<code>\nresult = df.groupby('name').ngroup() + 1\nresult = pd.concat([result, df.drop('name', axis=1)], axis=1)\nresult.columns = ['a', 'name', 'b', 'c']\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 62, "library_problem_id": 62, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 61}}
{"id": 63, "code": ["<code>\ndf['name'] = df['name'].astype('category').cat.codes\n    return df\n    ### END SOLUTION\n\nprint(f(example_df))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 63, "library_problem_id": 63, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 64, "code": ["<code>\ndf['ID'] = df.groupby('name')['a'].cumcount() + 1\nresult = df[['ID', 'b', 'c']]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 64, "library_problem_id": 64, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 61}}
{"id": 65, "code": ["<code>\ndf = df.melt(id_vars=['user', 'someBool'], var_name='date', value_name='value')\ndf\nEND SOLUTION\n</code>\n\nThis code will reshape the dataframe from wide format to long format. The 'user' and 'someBool' columns will remain the same, and the remaining columns (which are the dates) will be unpivoted into two columns: 'date' and 'value'.\n\nThe `melt()` function is a pandas function that is used to unpivot a DataFrame from wide format to long format, optionally leaving identifiers set.\n\nThe `id_vars` parameter is a list of identifiers to use to preserve the dataframe's structure.\n\nThe `var_name` parameter is the name to use for the column that will store the original column names.\n\nThe `value_name` parameter is the name to use for the column that will store the values from the original dataframe.\n\nThis code should work with Python 3.4 and above, as well as pandas versions 0.20.0 and above.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 65, "library_problem_id": 65, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 66, "code": ["<code>\ndf = df.melt(id_vars=['user', '01/12/15'], value_vars=['02/12/15', 'someBool'], var_name='others', value_name='value')\ndf\n```\n</code>\nEND SOLUTION\n\nThis code uses the melt function from pandas, which is used to unpivot a DataFrame from wide format to long format, optionally leaving identifiers set.\n\nIn this case, 'user' and '01/12/15' are the identifiers that will remain the same, and '02/12/15' and 'someBool' are the variables that will be unpivoted to the 'others' and 'value' columns.\n\nThe 'var_name' parameter is used to rename the column that contains the original column names, and 'value_name' is used to rename the column that contains the values.\n\nThis code should work in Python 3.4 and later versions.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 66, "library_problem_id": 66, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 65}}
{"id": 67, "code": ["<code>\ndf = df.melt(id_vars=['user', 'someBool'], var_name='date', value_name='value')\ndf = df.dropna()\ndf\n```\n</code>\nEND SOLUTION\n\nThis code will use the melt function from pandas to reshape the dataframe. The id_vars parameter specifies the columns that should remain the same, the var_name parameter specifies the name of the new column that will contain the original column names, and the value_name parameter specifies the name of the new column that will contain the original values.\n\nThe dropna function is used to remove rows with missing values.\n\nThis code should work with Python 3.4 and later versions.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 67, "library_problem_id": 67, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 65}}
{"id": 68, "code": ["<code>\n</code>\nEND SOLUTION\n\nThis will give you a DataFrame with only the rows where 'c' is greater than 0.5 and only the columns 'b' and 'e'.\n\nTo convert it to a numpy array, you can use:\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 68, "library_problem_id": 68, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 69, "code": ["<code>\n</code>\nEND SOLUTION\n\nThis will give you the desired DataFrame subset without the need for additional memory copies.\n\nTo convert it to a numpy array, you can use:\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 69, "library_problem_id": 69, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 70, "code": ["<code>\nreturn df.loc[df['c'] > 0.5, columns]\n### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 70, "library_problem_id": 70, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 71, "code": ["<code>\ndef f(df, columns=['b', 'e']):\n    df = df[df['c'] > 0.5][['a', 'd']]\n    df['sum'] = df.sum(axis=1)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 71, "library_problem_id": 71, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 72, "code": ["<code>\nreturn df.loc[df['c'] > 0.5, columns]\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 72, "library_problem_id": 72, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 73, "code": ["<code>\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values(by='date')\ndf = df.drop_duplicates(subset='date', keep='first')\ndf = df[df['date'].apply(lambda x: (x - df['date']).abs().min() > pd.Timedelta(days=X))]\nresult = df\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 73, "library_problem_id": 73, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 74, "code": ["<code>\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values(by='date')\ndf = df.drop_duplicates(subset='date', keep='first')\ndf = df[df['date'].apply(lambda x: (x - df['date']).abs().min() > pd.Timedelta(weeks=X))]\nresult = df\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 74, "library_problem_id": 74, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 73}}
{"id": 75, "code": ["<code>\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values(by='date')\ndf = df.reset_index(drop=True)\n\nfilter_dates = []\nfor i in range(len(df)):\n    if i == 0:\n        continue\n    if (df.loc[i, 'date'] - df.loc[i-1, 'date']).days <= X:\n        filter_dates.append(i)\n\ndf = df.drop(filter_dates)\ndf = df.reset_index(drop=True)\n\ndf['date'] = df['date'].dt.strftime('%d-%b-%Y')\n\nresult = df\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 75, "library_problem_id": 75, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 73}}
{"id": 76, "code": ["<code>\nresult = df.groupby(df.index // 3).mean()\nresult.index = result.index * 3\nresult = result.reindex(df.index)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 76, "library_problem_id": 76, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 77, "code": ["<code>\nresult = df.groupby(df.index // 3).sum()\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 77, "library_problem_id": 77, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 78, "code": ["<code>\nresult = df.groupby(df.index // 4).sum()\nresult</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 78, "library_problem_id": 78, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 79, "code": ["<code>\nresult = df.rolling(3).mean().iloc[-1]\nresult\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 79, "library_problem_id": 79, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 80, "code": ["<code>\n# Create a new column to identify the group\ndf['group'] = list(itertools.chain(*[[i]*x for i, x in enumerate([3, 2]*((len(df)+2)//5))]))\n\n# Calculate the sum and average for each group\nresult = df.groupby('group').apply(lambda x: x.sum() if len(x)==3 else x.mean()).reset_index(drop=True)\n\n# Drop the 'group' column\nresult = result.drop(columns='group')\n\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 80, "library_problem_id": 80, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 81, "code": ["<code>\ndef custom_agg(df):\n    agg_dict = {'col1': ['sum', 'mean']}\n    for i in range(len(df)//5):\n        if i % 2 == 0:\n            agg_dict['col1'].append('sum')\n        else:\n            agg_dict['col1'].append('mean')\n    return df.groupby(df.index // 5).agg(agg_dict)['col1']\n\nresult = custom_agg(df)\nprint(result)\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 81, "library_problem_id": 81, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 82, "code": ["<code>\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].ffill(downcast='infer')\ndf['A'] = df['A'].astype(int)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 82, "library_problem_id": 82, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 83, "code": ["<code>\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].ffill()\ndf['A'] = df['A'].astype(int)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 83, "library_problem_id": 83, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 82}}
{"id": 84, "code": ["<code>\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].fillna(method='ffill').combine(df['A'].fillna(method='bfill'), max)\ndf['A'] = df['A'].fillna(0).astype(int)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 84, "library_problem_id": 84, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 85, "code": ["<code>\ndf['number'] = df.duration.str.extract('(\\d+)').astype(int)\ndf['time'] = df.duration.str.extract('([a-zA-Z]+)')\ntime_day_dict = {'year': 365, 'day': 1, 'week': 7, 'month': 30}\ndf['time_day'] = df['time'].map(time_day_dict)\ndf\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 85, "library_problem_id": 85, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 86, "code": ["<code>\ndf['time'] = df['duration'].str.split().str[0]\ndf['number'] = df['duration'].str.split().str[1].astype(int)\ntime_to_days = {'year': 365, 'day': 1, 'week': 7, 'month': 30}\ndf['time_day'] = df['time'].map(time_to_days)\ndf\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 86, "library_problem_id": 86, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 87, "code": ["<code>\ndf['number'] = df['duration'].str.extract('(\\d+)').astype(int)\n    df['time'] = df['duration'].str.extract('([a-zA-Z]+)')\n    df['time_days'] = df['time'].replace({'year': 365, 'week': 7, 'day': 1, 'month': 30})\n    return df\n\nresult = f(example_df)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 87, "library_problem_id": 87, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 88, "code": ["<code>\ndf['number'] = df['duration'].str.extract('(\\d+)').astype(int)\ndf['time'] = df['duration'].str.extract('([a-zA-Z]+)')\ntime_to_days = {'year': 365, 'day': 1, 'week': 7, 'month': 30}\ndf['time_day'] = df['time'].map(time_to_days) * df['number']\ndf\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 88, "library_problem_id": 88, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 89, "code": ["<code>\nresult = (df1[columns_check_list] != df2[columns_check_list]).any(axis=1)\nresult = result.tolist()\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 89, "library_problem_id": 89, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 90, "code": ["<code>\nresult = (df1[columns_check_list] == df2[columns_check_list]).all(axis=1).tolist()\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 90, "library_problem_id": 90, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 89}}
{"id": 91, "code": ["<code>\ndf.index = df.index.set_levels(pd.to_datetime(df.index.levels[1]), level=1)\nreturn df\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 91, "library_problem_id": 91, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 92, "code": ["<code>\ndf.index = df.index.set_levels(pd.to_datetime(df.index.levels[1]), level=1)\n```\nEND SOLUTION\n<code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 92, "library_problem_id": 92, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 93, "code": ["<code>\ndf.index = pd.to_datetime(df.index.get_level_values(1))\n    df = df.reset_index()\n    df = df.drop(columns=['id', 'date'])\n    return df.values\n    ### END SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 93, "library_problem_id": 93, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 94, "code": ["<code>\ndf.index = df.index.set_levels([pd.to_datetime(df.index.levels[0]), df.index.levels[1]])\n    df = df.swaplevel(0,1)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 94, "library_problem_id": 94, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 95, "code": ["<code>\ndf_melt = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year')\ndf_melt['year'] = df_melt['year'].astype(int)\ndf_pivot = df_melt.pivot_table(index=['Country', 'year'], columns='Variable', values='value').reset_index()\ndf_pivot.columns.name = None\ndf = df_pivot\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 95, "library_problem_id": 95, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 96, "code": ["<code>\ndf = df.melt(id_vars=['Country', 'Variable'], var_name='year')\ndf['year'] = df['year'].astype(int)\ndf = df.sort_values(by=['Country', 'Variable', 'year'], ascending=[True, True, False])\ndf = df.pivot_table(index=['Country', 'year'], columns='Variable', values='value').reset_index()\ndf.columns.name = None\ndf = df[['Country', 'year', 'var1', 'var2']]\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 96, "library_problem_id": 96, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 97, "code": ["<code>\ndf = df[df.filter(like='Value').abs().max(axis=1) < 1]\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 97, "library_problem_id": 97, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 98, "code": ["<code>\ndf = df[df.filter(like='Value').abs().max(axis=1) > 1]\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 98, "library_problem_id": 98, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 97}}
{"id": 99, "code": ["<code>\n# First, we filter the dataframe based on the absolute value of each column\ndf = df[df.filter(like='Value').abs().max(axis=1) <= 1]\n\n# Then, we remove the 'Value_' prefix from the column names\ndf.columns = df.columns.str.replace('Value_', '')\n\ndf\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 99, "library_problem_id": 99, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 100, "code": ["<code>\ndf.replace('&AMP;', '&', regex=True)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 100, "library_problem_id": 100, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 101, "code": ["<code>\ndf = df.replace('&LT;', '<', regex=True)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 101, "library_problem_id": 101, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 102, "code": ["<code>\nfor col in df.columns:\n        df[col] = df[col].str.replace('&AMP;', '&')\n    return df\n\nresult = f(example_df)\nprint(result)\n### END SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 102, "library_problem_id": 102, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 100}}
{"id": 103, "code": ["<code>\ndf = df.replace(['&AMP;', '&LT;', '&GT;'], ['&', '<', '>'], regex=True)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 103, "library_problem_id": 103, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 104, "code": ["<code>\ndf = df.replace('&AMP;', '&', regex=True)\ndf\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 104, "library_problem_id": 104, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 105, "code": ["<code>\ndf[['first_name', 'last_name']] = df['name'].apply(lambda x: pd.Series(x.split(' ', 1)) if ' ' in x else pd.Series([x, np.nan])).fillna(np.nan)\ndf\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 105, "library_problem_id": 105, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 106, "code": ["<code>\ndf[['1_name', '2_name']] = df['name'].apply(lambda x: pd.Series(x.split(' ', 1)) if ' ' in x else pd.Series([x, np.nan])).fillna(np.nan)\ndf.columns = ['1_name', '2_name']\ndf\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 106, "library_problem_id": 106, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 105}}
{"id": 107, "code": ["<code>\nimport re\n\ndef split_name(name):\n    parts = name.split()\n    if len(parts) == 1:\n        return pd.Series({'first_name': parts[0], 'middle_name': pd.np.nan, 'last_name': pd.np.nan})\n    elif len(parts) == 2:\n        return pd.Series({'first_name': parts[0], 'middle_name': pd.np.nan, 'last_name': parts[1]})\n    else:\n        return pd.Series({'first_name': parts[0], 'middle_name': ' '.join(parts[1:-1]), 'last_name': parts[-1]})\n\ndf = df['name'].apply(split_name)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 107, "library_problem_id": 107, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 105}}
{"id": 108, "code": ["<code>\nresult = pd.merge_asof(df2.sort_values('Timestamp'), df1.sort_values('Timestamp'), on='Timestamp', direction='nearest')\nresult = result[['Timestamp', 'stuff', 'data']]\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 108, "library_problem_id": 108, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 109, "code": ["<code>\nresult = pd.merge_asof(df1.sort_values('Timestamp'), df2.sort_values('Timestamp'), on='Timestamp', direction='nearest')\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 109, "library_problem_id": 109, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 108}}
{"id": 110, "code": ["<code>\ndf['state'] = df.apply(lambda row: row['col1'] if row['col2'] <= 50 and row['col3'] <= 50 else max(row['col1'], row['col2'], row['col3']), axis=1)\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 110, "library_problem_id": 110, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 111, "code": ["<code>\ndf['state'] = df.apply(lambda row: row['col1'] if row['col2'] > 50 and row['col3'] > 50 else row['col1']+row['col2']+row['col3'], axis=1)\ndf\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 111, "library_problem_id": 111, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 110}}
{"id": 112, "code": ["<code>\nerrors = [val for i, val in enumerate(df['Field1']) if not isinstance(val, int)]\ndf = pd.DataFrame(errors, columns=['Field1'])\ndf\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 112, "library_problem_id": 112, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 113, "code": ["<code>\ndf['Field1'] = df['Field1'].apply(lambda x: int(x) if pd.notnull(x) and x == int(x) else None)\nint_values = df['Field1'].dropna().tolist()\nint_values\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 113, "library_problem_id": 113, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 112}}
{"id": 114, "code": ["<code>\n```python\n    result = []\n    for value in df['Field1']:\n        if not isinstance(value, int):\n            result.append(value)\n    return result\n\nprint(f(example_df))\n### END SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 114, "library_problem_id": 114, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 112}}
{"id": 115, "code": ["<code>\ndf.set_index('cat', inplace=True)\ndf = df.div(df.sum(axis=1), axis=0)\ndf.reset_index(inplace=True)\ndf\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 115, "library_problem_id": 115, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 116, "code": ["<code>\ndf = df.set_index('cat').div(df.set_index('cat').sum(axis=1), axis=0).reset_index()\ndf\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 116, "library_problem_id": 116, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 115}}
{"id": 117, "code": ["<code>\nresult = df.loc[test]\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 117, "library_problem_id": 117, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 118, "code": ["<code>\nresult = df.loc[test]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 118, "library_problem_id": 118, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 117}}
{"id": 119, "code": ["<code>\nresult = df.drop(test)\nresult\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 119, "library_problem_id": 119, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 117}}
{"id": 120, "code": ["<code>\nresult = df.loc[test]\n    result = result.drop_duplicates()\n    return result\n    ### END SOLUTION\n\ndf = pd.DataFrame({\n    'alleles': ['A/C', 'A/T', 'T/A', 'C/A', 'C/T'],\n    'chrom': [0, 0, 0, 0, 0],\n    'pos': [3, 7, 12, 15, 18],\n    'strand': ['+', '+', '+', '+', '+'],\n    'assembly#': [None, None, None, None, None],\n    'center': [None, None, None, None, None],\n    'protLSID': [None, None, None, None, None],\n    'assayLSID': [None, None, None, None, None]\n}, index=['TP3', 'TP7', 'TP12', 'TP15', 'TP18'])\n\ntest = ['TP3','TP12','TP18', 'TP3']\n\nprint(f(df, test))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 120, "library_problem_id": 120, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 117}}
{"id": 121, "code": ["<code>\nimport numpy as np\n\ndef euclidean_distance(x1, y1, x2, y2):\n    return np.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n\ndf['coords'] = list(zip(df.x, df.y))\ndf = df.sort_values(['time', 'car'])\ndf['nearest_neighbour'] = df.groupby('time').apply(lambda g: g.coords.apply(lambda x: g.coords[g.coords != x].apply(lambda y: euclidean_distance(x[0], x[1], y[0], y[1])).idxmin())).reset_index()['coords']\ndf['euclidean_distance'] = df.groupby('time').apply(lambda g: g.coords.apply(lambda x: g.coords[g.coords != x].apply(lambda y: euclidean_distance(x[0], x[1], y[0], y[1])).min())).reset_index()['coords']\ndf = df.drop(columns=['coords'])\ndf\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 121, "library_problem_id": 121, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 122, "code": ["<code>\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\n# calculate pairwise distances\ndistances = pdist(df[['x', 'y']].values, metric='euclidean')\ndistances = squareform(distances)\n\n# convert to dataframe\ndist_df = pd.DataFrame(distances, columns=df['car'], index=df['car'])\n\n# find the maximum distance for each car\nmax_distances = dist_df.max()\n\n# find the car with the maximum distance for each car\nfarmost_neighbours = dist_df.idxmax()\n\n# create the final dataframe\ndf2 = pd.DataFrame({\n    'time': df['time'],\n    'car': df['car'],\n    'farmost_neighbour': farmost_neighbours,\n    'euclidean_distance': max_distances\n})\n\ndf2.reset_index(drop=True, inplace=True)\n\ndf2\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 122, "library_problem_id": 122, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 123, "code": ["<code>\ndf['keywords_all'] = df.apply(lambda row: ','.join(row.dropna().values), axis=1)\ndf\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 123, "library_problem_id": 123, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 124, "code": ["<code>\ndf['keywords_all'] = df.apply(lambda row: '-'.join(row.dropna().values), axis=1)\ndf\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 124, "library_problem_id": 124, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 125, "code": ["<code>\ndf['keywords_all'] = df.apply(lambda row: '-'.join(row[['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']].dropna().values), axis=1)\ndf\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 125, "library_problem_id": 125, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 126, "code": ["<code>\ncols = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\ndf['keywords_all'] = df[cols].apply(lambda row: '-'.join(row.dropna().values[::-1]), axis=1)\ndf\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 126, "library_problem_id": 126, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 127, "code": ["<code>\ndf_sample = df.sample(frac=0.2, random_state=0)\ndf.loc[df_sample.index, 'Quantity'] = 0\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 127, "library_problem_id": 127, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 128, "code": ["<code>\n# Calculate the number of rows to be changed\nn_rows = int(0.2 * len(df))\n\n# Randomly select 20% of the rows\nsample = df.sample(n=n_rows, random_state=0)\n\n# Change the ProductId to 0\ndf.loc[sample.index, 'ProductId'] = 0\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 128, "library_problem_id": 128, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 127}}
{"id": 129, "code": ["<code>\ndf = df.set_index('UserId').groupby(level=0).apply(lambda x: x.sample(frac=0.2, random_state=0)).reset_index()\ndf.loc[df.index % 2 == 0, 'Quantity'] = 0\ndf\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 129, "library_problem_id": 129, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 127}}
{"id": 130, "code": ["<code>\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\ndf['index_original'] = df.duplicated(subset=['col1','col2'], keep=False)\nduplicate = df.loc[duplicate_bool == True]\nduplicate = duplicate.reset_index(drop=True)\nresult = duplicate\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 130, "library_problem_id": 130, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 131, "code": ["<code>\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\ndf['index_original'] = df.index\nduplicate = df.loc[duplicate_bool == True]\nresult = duplicate[['col1', 'col2', 'index_original']]\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 131, "library_problem_id": 131, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 130}}
{"id": 132, "code": ["<code>\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='first')\n    duplicate = df[duplicate_bool]\n    duplicate['index_original'] = duplicate.index\n    duplicate.reset_index(drop=True, inplace=True)\n    return duplicate\n\nresult = f(example_df)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 132, "library_problem_id": 132, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 130}}
{"id": 133, "code": ["<code>\nduplicate_bool = df.duplicated(subset=['col1','col2', '3col'], keep='first')\ndf['index_original'] = df.duplicated(subset=['col1','col2', '3col'], keep=False)\nduplicate = df.loc[duplicate_bool == True]\nduplicate['index_original'] = duplicate['index_original'].apply(lambda x: not x)\nduplicate = duplicate.loc[duplicate['index_original'] == True]\nduplicate.reset_index(drop=True, inplace=True)\nresult = duplicate\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 133, "library_problem_id": 133, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 134, "code": ["<code>\nduplicate_bool = df.duplicated(subset=['col1','col2'], keep='last')\ndf['index_original'] = df[duplicate_bool].index\nduplicate = df.loc[duplicate_bool == True]\n```\nEND SOLUTION\nresult = duplicate\nresult\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 134, "library_problem_id": 134, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 135, "code": ["<code>\nresult = df[df.groupby(['Sp','Mt'])['count'].transform(max) == df['count']]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 135, "library_problem_id": 135, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 136, "code": ["<code>\nresult = df[df.groupby(['Sp','Mt'])['count'].transform(max) == df['count']]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 136, "library_problem_id": 136, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 135}}
{"id": 137, "code": ["<code>\nresult = df[df.groupby(['Sp','Mt'])['count'].transform(min) == df['count']]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 137, "library_problem_id": 137, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 135}}
{"id": 138, "code": ["<code>\nresult = df.loc[df.groupby(['Sp','Value'])['count'].idxmax()]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 138, "library_problem_id": 138, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 135}}
{"id": 139, "code": ["<code>\nresult = df[df['Category'].isin(filter_list)]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 139, "library_problem_id": 139, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 140, "code": ["<code>\ndf.query(\"Category not in @filter_list\")\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 140, "library_problem_id": 140, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 139}}
{"id": 141, "code": ["<code>\n# First, we need to flatten the multi-index columns into a list of tuples\nvalue_vars = [tuple(x) for x in df.columns.to_flat_index()]\n\n# Then, we can use pd.melt with the list of tuples as value_vars\nresult = pd.melt(df.reset_index(), value_vars=value_vars)\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 141, "library_problem_id": 141, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 142, "code": ["<code>\n# First, we need to reshape the dataframe to have a multi-index columns\ndf.columns = pd.MultiIndex.from_tuples(df.columns.to_series().apply(tuple))\n\n# Then, we can use the melt function\nresult = df.reset_index().melt(id_vars='index')\n\n# Finally, we split the 'variable' column into 'variable_0', 'variable_1', and 'variable_2'\nresult[['variable_0', 'variable_1', 'variable_2']] = pd.DataFrame(result.variable.to_list(), index=result.index)\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 142, "library_problem_id": 142, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 141}}
{"id": 143, "code": ["<code>\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 143, "library_problem_id": 143, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 144, "code": ["<code>\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 144, "library_problem_id": 144, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 145, "code": ["<code>\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 145, "library_problem_id": 145, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 146, "code": ["<code>\ndf['cummax'] = df.groupby('id')['val'].cummax()\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 146, "library_problem_id": 146, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 147, "code": ["<code>\ndf['cumsum'] = df.groupby('id')['val'].cumsum().where(lambda x : x >= 0, 0)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 147, "library_problem_id": 147, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 143}}
{"id": 148, "code": ["<code>\nresult = df.groupby('l')['v'].sum(min_count=1)\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 148, "library_problem_id": 148, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 149, "code": ["<code>\nresult = df.groupby('r')['v'].sum()\nresult[result.isna()] = np.nan\nresult</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 149, "library_problem_id": 149, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 148}}
{"id": 150, "code": ["<code>\nresult = df.groupby('l')['v'].apply(lambda x: np.nansum(x)).reset_index()\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 150, "library_problem_id": 150, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 151, "code": ["<code>\ndef relationship(df):\n    result = []\n    columns = df.columns.tolist()\n    for i in range(len(columns)):\n        for j in range(i+1, len(columns)):\n            if df[columns[i]].nunique() == df[columns[j]].count():\n                result.append(f\"{columns[i]} {columns[j]} one-to-one\")\n            elif df[columns[i]].nunique() < df[columns[j]].count():\n                result.append(f\"{columns[i]} {columns[j]} one-to-many\")\n            elif df[columns[i]].nunique() > df[columns[j]].count():\n                result.append(f\"{columns[i]} {columns[j]} many-to-one\")\n            else:\n                result.append(f\"{columns[i]} {columns[j]} many-to-many\")\n    return result\n\nresult = relationship(df)\nprint(result)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 151, "library_problem_id": 151, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 152, "code": ["<code>\ndef relationship(df):\n    result = []\n    columns = df.columns.tolist()\n    for i in range(len(columns)):\n        for j in range(i+1, len(columns)):\n            col1 = df[columns[i]].nunique()\n            col2 = df[columns[j]].nunique()\n            if col1 == df.shape[0] and col2 == df.shape[0]:\n                result.append(f'{columns[i]} {columns[j]} one-2-one')\n            elif col1 == df.shape[0] and col2 < df.shape[0]:\n                result.append(f'{columns[i]} {columns[j]} one-2-many')\n            elif col1 < df.shape[0] and col2 == df.shape[0]:\n                result.append(f'{columns[i]} {columns[j]} many-2-one')\n            else:\n                result.append(f'{columns[i]} {columns[j]} many-2-many')\n    return result\n\nresult = relationship(df)\nprint(result)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 152, "library_problem_id": 152, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 153, "code": ["<code>\ndef relationship(x, y):\n    if x.is_unique and y.is_unique:\n        return 'one-to-one'\n    elif x.is_unique and not y.is_unique:\n        return 'one-to-many'\n    elif not x.is_unique and y.is_unique:\n        return 'many-to-one'\n    else:\n        return 'many-to-many'\n\nresult = pd.DataFrame(index=df.columns, columns=df.columns)\n\nfor column1 in df.columns:\n    for column2 in df.columns:\n        if column1 != column2:\n            result.loc[column1, column2] = relationship(df[column1], df[column2])\n\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 153, "library_problem_id": 153, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 154, "code": ["<code>\ndef relationship(x, y):\n    if x.nunique() == y.nunique() == 1:\n        return 'one-2-one'\n    elif x.nunique() == 1:\n        return 'one-2-many'\n    elif y.nunique() == 1:\n        return 'many-2-one'\n    else:\n        return 'many-2-many'\n\npairs = [(c1, c2) for c1 in df.columns for c2 in df.columns if c1 != c2]\n\nresult = pd.DataFrame(index=df.columns, columns=df.columns)\n\nfor c1, c2 in pairs:\n    result.loc[c1, c2] = relationship(df[c1], df[c2])\n\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 154, "library_problem_id": 154, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 155, "code": ["<code>\n# get the index of unique values, based on firstname, lastname, email\n# convert to lower and remove white space first\nuniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])\n.applymap(lambda s:s.lower() if type(s) == str else s)\n.applymap(lambda x: x.replace(\" \", \"\") if type(x)==str else x)\n.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index\n\n# save unique records\ndf_uniq = df.loc[uniq_indx]\n\n# filter out the records that have no bank account\nresult = df_uniq[df_uniq['bank'].notna()]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 155, "library_problem_id": 155, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 155}}
{"id": 156, "code": ["<code>\nresult = pd.to_numeric(s.str.replace(',', ''), errors='coerce')\nprint(result)\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 156, "library_problem_id": 156, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 156}}
{"id": 157, "code": ["<code>\ndf['Family'] = df.apply(lambda row: 'Has Family' if row['SibSp'] > 0 or row['Parch'] > 0 else 'No Family', axis=1)\nresult = df.groupby('Family')['Survived'].mean()\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 157, "library_problem_id": 157, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 158, "code": ["<code>\ndf['Group'] = np.where((df['Survived'] > 0) | (df['Parch'] > 0), 'Has Family', 'No Family')\nresult = df.groupby('Group')['SibSp'].mean()\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 158, "library_problem_id": 158, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 157}}
{"id": 159, "code": ["<code>\ndf['Group'] = df.apply(lambda row: 'Has Family' if row['SibSp']==1 and row['Parch']==1 else \n                      ('New Family' if row['SibSp']==0 and row['Parch']==1 else \n                      ('No Family' if row['SibSp']==0 and row['Parch']==0 else 'Old Family')), axis=1)\n\nresult = df.groupby('Group')['Survived'].mean()\nresult\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 159, "library_problem_id": 159, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 157}}
{"id": 160, "code": ["<code>\nresult = df.sort_values(['cokey', 'A']).reset_index(drop=True)\nresult.index = result.index.repeat(result.groupby('cokey').cumcount()+1)\nresult = result.reset_index(drop=True)\nresult</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 160, "library_problem_id": 160, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 161, "code": ["<code>\nresult = df.sort_values(['cokey', 'A']).reset_index(drop=True).groupby('cokey').apply(lambda x: x.reset_index(drop=True)).reset_index()\nresult.index = result['cokey'].apply(lambda x: str(len(x)-1) + ' ' + str(x))\nresult = result.set_index('cokey').drop(columns='index')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 161, "library_problem_id": 161, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 160}}
{"id": 162, "code": ["<code>\ndf.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Lower'])\ndf\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 162, "library_problem_id": 162, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 162}}
{"id": 163, "code": ["<code>\ndf.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Middle', 'Lower'])\ndf\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 163, "library_problem_id": 163, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 162}}
{"id": 164, "code": ["<code>\ndf.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Middle', 'Lower'])\ndf\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 164, "library_problem_id": 164, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 162}}
{"id": 165, "code": ["<code>\nresult = pd.DataFrame(someTuple).T\nresult.columns = ['birdType', 'birdCount']\nresult.reset_index(drop=True, inplace=True)\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 165, "library_problem_id": 165, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 166, "code": ["<code>\nimport numpy as np\n\ndef stdMeann(x):\n    return np.std(np.mean(x))\n\nresult = df.groupby('a').b.apply(stdMeann)\nresult = pd.DataFrame(result, columns=['std'])\nresult.index.name = 'a'\nresult['mean'] = df.groupby('a').b.mean()\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 166, "library_problem_id": 166, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 166}}
{"id": 167, "code": ["<code>\nimport numpy as np\n\ndef stdMeann(x):\n    return np.std(np.mean(x))\n\nresult = df.groupby('b').a.apply(stdMeann)\nprint(result)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 167, "library_problem_id": 167, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 166}}
{"id": 168, "code": ["<code>\nimport numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum()\n\ndef min_max(x):\n    return (x - np.min(x)) / (np.max(x) - np.min(x))\n\ndf['softmax'] = df.groupby('a')['b'].transform(softmax)\ndf['min-max'] = df.groupby('a')['b'].transform(min_max)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 168, "library_problem_id": 168, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 166}}
{"id": 169, "code": ["<code>\nresult = df.loc[:, (df != 0).any(axis=0)]\nresult = result.loc[(result != 0).any(axis=1)]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 169, "library_problem_id": 169, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 169}}
{"id": 170, "code": ["<code>\nresult = df.loc[:, (df.sum() != 0)]\nresult = result.loc[(result.sum(axis=1) != 0), :]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 170, "library_problem_id": 170, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 169}}
{"id": 171, "code": ["<code>\nresult = df[(df.max(axis=1) < 2) & (df.max(axis=0) < 2)]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 171, "library_problem_id": 171, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 169}}
{"id": 172, "code": ["<code>\nresult = df.apply(lambda x: [0 if i==2 else i for i in x], axis=0)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 172, "library_problem_id": 172, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 169}}
{"id": 173, "code": ["<code>\nresult = s.sort_values(ascending=(True, True))\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 173, "library_problem_id": 173, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 174, "code": ["<code>\ndf = s.reset_index().sort_values(by=['index', 0]).reset_index(drop=True)\ndf.columns = ['index', 1]\ndf\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 174, "library_problem_id": 174, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 173}}
{"id": 175, "code": ["<code>\nresult = df[df['A'].apply(lambda x: isinstance(x, int) or isinstance(x, (float, np.float64, np.float32)))]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 175, "library_problem_id": 175, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 175}}
{"id": 176, "code": ["<code>\nresult = df[df['A'].apply(lambda x: isinstance(x, str))]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 176, "library_problem_id": 176, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 175}}
{"id": 177, "code": ["<code>\nresult = df[df.groupby(['Sp','Mt'])['count'].transform(max) == df['count']]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 177, "library_problem_id": 177, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 177}}
{"id": 178, "code": ["<code>\nresult = df.loc[df.groupby(['Sp','Mt'])['count'].idxmax()]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 178, "library_problem_id": 178, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 177}}
{"id": 179, "code": ["<code>\nresult = df[df.groupby(['Sp','Mt'])['count'].transform(min) == df['count']]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 179, "library_problem_id": 179, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 180, "code": ["<code>\nresult = df.loc[df.groupby(['Sp','Value'])['count'].idxmax()]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 180, "library_problem_id": 180, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 181, "code": ["<code>\ndf['Date'] = df['Member'].map(dict)\ndf['Date'].fillna(df['Member'], inplace=True)\ndf\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 181, "library_problem_id": 181, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 181}}
{"id": 182, "code": ["<code>\ndf['Date'] = df['Member'].map(dict).fillna('17/8/1926')\ndf\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 182, "library_problem_id": 182, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 181}}
{"id": 183, "code": ["<code>\n```python\n    df['Date'] = df['Member'].map(dict)\n    return df\n\nresult = f(example_dict, example_df)\nprint(result)\n### END SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 183, "library_problem_id": 183, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 181}}
{"id": 184, "code": ["<code>\ndf['Date'] = df['Member'].map(dict).fillna('17/8/1926')\ndf['Date'] = pd.to_datetime(df['Date']).dt.strftime('%d-%b-%Y')\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 184, "library_problem_id": 184, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 181}}
{"id": 185, "code": ["<code>\ndf['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Count_d'] = df.Date.map(df.groupby('Date').size())\ndf['year-month'] = df['Date'].dt.to_period('M')\ndf['Count_m'] = df.groupby('year-month')['Date'].transform('count')\ndf['year'] = df['Date'].dt.to_period('Y')\ndf['Count_y'] = df.groupby('year')['Date'].transform('count')\ndf = df.drop(columns=['year','year-month'])\ndf\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 185, "library_problem_id": 185, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 186, "code": ["<code>\ndf['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date','Val']).Date.transform('count')\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month, 'Val']).Date.transform('count')\ndf['Count_y'] = df.groupby([df['Date'].dt.year, 'Val']).Date.transform('count')\ndf['Count_Val'] = df.groupby('Val').Val.transform('count')\ndf\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 186, "library_problem_id": 186, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 185}}
{"id": 187, "code": ["<code>\ndf['Date'] = pd.to_datetime(df['Date'], format= '%d/%m/%y')\ndf['Count_d'] = df.groupby(['Date','Val']).Date.transform('count')\ndf['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month, 'Val']).Date.transform('count')\ndf['Count_y'] = df.groupby([df['Date'].dt.year, 'Val']).Date.transform('count')\ndf['Count_w'] = df.groupby([df['Date'].dt.year, df['Date'].dt.week, 'Val']).Date.transform('count')\ndf['Count_Val'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month, df['Date'].dt.week, 'Val']).Date.transform('count')\ndf\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 187, "library_problem_id": 187, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 185}}
{"id": 188, "code": ["<code>\ndf['Date'] = pd.to_datetime(df['Date'])\ndf_zero = df.groupby('Date').apply(lambda x: (x==0).sum())\ndf_non_zero = df.groupby('Date').apply(lambda x: (x!=0).sum())\nresult1 = df_zero.reset_index()\nresult2 = df_non_zero.reset_index()\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 188, "library_problem_id": 188, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 189, "code": ["<code>\ndf['Date'] = pd.to_datetime(df['Date'])\ndf_even = df.groupby('Date').apply(lambda x: (x % 2 == 0).sum())\ndf_odd = df.groupby('Date').apply(lambda x: (x % 2 == 1).sum())\nresult1 = df_even.reset_index(level=0, drop=True)\nresult2 = df_odd.reset_index(level=0, drop=True)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 189, "library_problem_id": 189, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 188}}
{"id": 190, "code": ["<code>\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.sum, 'E': np.mean})\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 190, "library_problem_id": 190, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 190}}
{"id": 191, "code": ["<code>\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.sum, 'E': np.mean})\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 191, "library_problem_id": 191, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 192, "code": ["<code>\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.sum, 'E': np.mean})\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 192, "library_problem_id": 192, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 193, "code": ["<code>\nresult = pd.pivot_table(df, values=['D','E'], index=['B'], aggfunc={'D': np.max, 'E': np.min})\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 193, "library_problem_id": 193, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 190}}
{"id": 194, "code": ["<code>\nimport dask.dataframe as dd\n\n# Convert pandas dataframe to dask dataframe\nddf = dd.from_pandas(df, npartitions=2)\n\n# Split 'var2' column by ',' and explode it\nddf = ddf.assign(var2=ddf.var2.str.split(','))\nddf = ddf.explode('var2')\n\n# Convert dask dataframe back to pandas dataframe\nresult = ddf.compute()\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 194, "library_problem_id": 194, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 195, "code": ["<code>\nimport dask.dataframe as dd\n\n# Convert pandas dataframe to dask dataframe\nddf = dd.from_pandas(df, npartitions=2)\n\n# Split 'var2' column by ',' and explode it\nddf = ddf.assign(var2=ddf.var2.str.split(','))\nddf = ddf.explode('var2')\n\n# Convert dask dataframe back to pandas dataframe\nresult = ddf.compute()\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 195, "library_problem_id": 195, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 194}}
{"id": 196, "code": ["<code>\nimport dask.dataframe as dd\n\n# Convert pandas dataframe to dask dataframe\nddf = dd.from_pandas(df, npartitions=2)\n\n# Define function to split 'var2' column\ndef split_column(df):\n    return df.assign(var2=df['var2'].str.split('-')).explode('var2')\n\n# Apply function to dask dataframe\nresult = ddf.map_partitions(split_column).compute()\n\n# Convert result back to pandas dataframe\nresult = result.reset_index(drop=True)\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 196, "library_problem_id": 196, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 194}}
{"id": 197, "code": ["<code>\ndef count_special_char(string):\n    special_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha() or string[i].isdigit() or string[i].isspace()):\n            continue\n        else:\n            special_char = special_char + 1\n    return special_char\n\ndf[\"new\"] = df[\"str\"].apply(count_special_char)\nprint(df)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 197, "library_problem_id": 197, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 197}}
{"id": 198, "code": ["<code>\ndef count_alpha_char(string):\n    alpha_char = 0\n    for i in range(len(string)):\n        if(string[i].isalpha()):\n            alpha_char = alpha_char + 1\n    return alpha_char\n\ndf[\"new\"] = df[\"str\"].apply(count_alpha_char)\nprint(df)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 198, "library_problem_id": 198, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 197}}
{"id": 199, "code": ["<code>\ndf[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\ndf\n```\nEND SOLUTION\n</code>\nThe str.split() function is used to split the 'row' column into two separate columns, based on the space character. The '1' argument specifies that we want to split the string only once, and the 'expand=True' argument ensures that the split results are returned as separate columns.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 199, "library_problem_id": 199, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 199}}
{"id": 200, "code": ["<code>\ndf[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\ndf\n```\nEND SOLUTION\n</code>\nThe function str.split() is used to split the string in the 'row' column at the first occurrence of the space character and create a new dataframe with the split strings. The parameter '1' indicates that only the first occurrence of the space character is used for splitting. The parameter 'expand=True' expands the split strings into separate columns.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 200, "library_problem_id": 200, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 199}}
{"id": 201, "code": ["<code>\ndf[['fips', 'medi']] = df['row'].str.split(' ', 1, expand=True)\ndf\n```\nEND SOLUTUION\n</code>\nThe split function is used to split the 'row' column into two new columns 'fips' and 'medi' based on the space character. The '1' in the split function indicates that we want to split the string into two parts at the first space. The 'expand=True' argument is used to create new columns instead of having the split strings in a single column.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 201, "library_problem_id": 201, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 199}}
{"id": 202, "code": ["<code>\ndf.set_index('Name', inplace=True)\ndf = df.replace(0, np.nan)\ndf = df.cumsum(axis=1)\ndf = df.fillna(method='ffill', axis=1)\ndf = df.div(df.count(axis=1), axis=0)\ndf = df.replace(np.nan, 0)\ndf = df.reset_index()\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 202, "library_problem_id": 202, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 203, "code": ["<code>\ndf.set_index('Name', inplace=True)\ndf = df.replace(0, pd.np.nan)\ndf = df.iloc[:, ::-1].expanding(axis=1).mean().iloc[:, ::-1]\ndf = df.fillna(0).replace(pd.np.nan, 0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 203, "library_problem_id": 203, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 202}}
{"id": 204, "code": ["<code>\ndf.set_index('Name', inplace=True)\n    df = df.replace(0, np.nan)\n    df = df.cumsum(axis=1)\n    count = df.count(axis=1)\n    result = df.div(count, axis=0)\n    result = result.fillna(0)\n    return result.reset_index()\n\n# Test the function\nprint(f(example_df))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 204, "library_problem_id": 204, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 202}}
{"id": 205, "code": ["<code>\ndf = df.set_index('Name')\ndf = df.replace(0, np.nan)\ndf = df.iloc[:, ::-1].expanding(axis=1).mean().iloc[:, ::-1]\ndf = df.fillna(0)\ndf = df.round(2)\ndf\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 205, "library_problem_id": 205, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 206, "code": ["<code>\ndf['DateTime'] = pd.to_datetime(df['DateTime'])\ndf.set_index('DateTime', inplace=True)\ndf['label'] = (df['Close'] - df['Close'].shift(1) > 0).astype(int)\ndf.reset_index(inplace=True)\ndf.loc[0, 'label'] = 1\ndf\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 206, "library_problem_id": 206, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 206}}
{"id": 207, "code": ["<code>\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else -1 if x < 0 else 0)\ndf.loc[0, 'label'] = 1\ndf\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 207, "library_problem_id": 207, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 206}}
{"id": 208, "code": ["<code>\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\ndf['label'] = [1] + df['Close'].diff().apply(lambda x: -1 if x<0 else (1 if x>0 else 0)).tolist()\ndf\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 208, "library_problem_id": 208, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 206}}
{"id": 209, "code": ["<code>\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'], errors='coerce')\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df.groupby('id')['departure_time'].shift() - df['arrival_time']\ndf\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 209, "library_problem_id": 209, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 210, "code": ["<code>\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'], errors='coerce')\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\ndf['Duration'] = df.groupby('id')['departure_time'].shift() - df['arrival_time']\ndf['Duration'] = df['Duration'].dt.total_seconds()\ndf\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 210, "library_problem_id": 210, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 209}}
{"id": 211, "code": ["<code>\ndf['arrival_time'] = df['arrival_time'].replace('0', pd.NaT)\ndf['departure_time'] = df['departure_time'].replace('0', pd.NaT)\ndf[['arrival_time', 'departure_time']] = df[['arrival_time', 'departure_time']].apply(pd.to_datetime)\ndf['Duration'] = df['departure_time'].sub(df.groupby('id')['arrival_time'].shift())\ndf['Duration'] = df['Duration'].dt.total_seconds()\ndf[['arrival_time', 'departure_time']] = df[['arrival_time', 'departure_time']].dt.strftime('%d-%b-%Y %H:%M:%S')\ndf\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 211, "library_problem_id": 211, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 209}}
{"id": 212, "code": ["<code>\nresult = df[df['key2'] == 'one'].groupby('key1').size().reset_index(name='count')\nresult = result.reindex(index=['a', 'b', 'c']).fillna(0)\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 212, "library_problem_id": 212, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 212}}
{"id": 213, "code": ["<code>\nresult = df[df['key2'] == 'two'].groupby('key1').size().reset_index(name='count')\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 213, "library_problem_id": 213, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 212}}
{"id": 214, "code": ["<code>\nresult = df[df['key2'].str.endswith('e')].groupby('key1').size().reset_index(name='count')\nresult = result.set_index('key1').reindex(df['key1'].unique(), fill_value=0).reset_index()\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 214, "library_problem_id": 214, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 212}}
{"id": 215, "code": ["<code>\nmax_result = df.index.max()\nmin_result = df.index.min()\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 215, "library_problem_id": 215, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 216, "code": ["<code>\n# First, we need to convert the index to datetime\ndf.index = pd.to_datetime(df.index)\n\n# Then, we can calculate the mode and median\nmode_result = df.index.mode()[0]\nmedian_result = df.index.median()\n\nmode_result, median_result\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 216, "library_problem_id": 216, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 215}}
{"id": 217, "code": ["<code>\nresult = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 217, "library_problem_id": 217, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 217}}
{"id": 218, "code": ["<code>\nresult = df[~((df['closing_price'] >= 99) & (df['closing_price'] <= 101))]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 218, "library_problem_id": 218, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 217}}
{"id": 219, "code": ["<code>\ndf1 = df.groupby(\"item\", as_index=False).agg({\"diff\": \"min\", \"otherstuff\": \"first\"})\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 219, "library_problem_id": 219, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 219}}
{"id": 220, "code": ["<code>\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].apply(lambda x: x.split('_')[0] if '_' in x else x)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 220, "library_problem_id": 220, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 220}}
{"id": 221, "code": ["<code>\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].apply(lambda x: x.split('_')[-1] if '_' in x else x)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 221, "library_problem_id": 221, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 220}}
{"id": 222, "code": ["<code>\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].apply(lambda x: x.split('_')[0] if '_' in x else x)\n    return df\n\n# Call the function\nresult = f()\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 222, "library_problem_id": 222, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 220}}
{"id": 223, "code": ["<code>\n# Calculate the number of NaN values\nnum_nan = df['Column_x'].isna().sum()\n\n# Calculate the number of NaN values to fill with 0 and 1\nnum_to_fill_0 = num_nan // 2\nnum_to_fill_1 = num_nan - num_to_fill_0\n\n# Fill the NaN values with 0 and 1\ndf['Column_x'] = df['Column_x'].fillna(0, limit=num_to_fill_0)\ndf['Column_x'] = df['Column_x'].fillna(1, limit=num_to_fill_1)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 223, "library_problem_id": 223, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 223}}
{"id": 224, "code": ["<code>\n# First, we calculate the number of NaN values in the column\nnum_nan = df['Column_x'].isna().sum()\n\n# Then we calculate the number of NaN values for each category\nnum_0 = int(num_nan * 0.3)\nnum_05 = int(num_nan * 0.3)\nnum_1 = num_nan - num_0 - num_05\n\n# We replace the NaN values with the corresponding values\ndf['Column_x'] = df['Column_x'].fillna(0, inplace=True)\ndf['Column_x'] = df['Column_x'].fillna(0.5, inplace=True)\ndf['Column_x'] = df['Column_x'].fillna(1, inplace=True)\n\ndf\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 224, "library_problem_id": 224, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 223}}
{"id": 225, "code": ["<code>\n# First, we calculate the number of NaN values\nnum_nan = df['Column_x'].isna().sum()\n\n# Then, we calculate the number of NaN values to be filled with 0 and 1\nnum_zeros = num_nan // 2\nnum_ones = num_nan // 2\n\n# We fill the NaN values with 0 and 1 alternatively\ndf['Column_x'] = df['Column_x'].fillna(0, limit=num_zeros)\ndf['Column_x'] = df['Column_x'].fillna(1, limit=num_ones)\n\n# We shuffle the values to ensure that 0s come first and then 1s\ndf = df.reindex(np.random.permutation(df.index))\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 225, "library_problem_id": 225, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 223}}
{"id": 226, "code": ["<code>\nresult = pd.concat([a, b], axis=1).groupby(level=0, axis=1).apply(lambda x: x.values.tolist())\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 226, "library_problem_id": 226, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 226}}
{"id": 227, "code": ["<code>\nresult = pd.concat([a, b, c], axis=1).groupby(level=0, axis=1).apply(lambda x: x.values.tolist())\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 227, "library_problem_id": 227, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 226}}
{"id": 228, "code": ["<code>\n# First, we need to align the dataframes by repeating the last value of the shorter one.\na = a.append(a.iloc[-1].repeat(len(b)-len(a)).to_frame().T, ignore_index=True)\nb = b.append(b.iloc[-1].repeat(len(a)-len(b)).to_frame().T, ignore_index=True)\n\n# Then, we create the new dataframe by zipping the corresponding columns of a and b.\na_b = pd.DataFrame(list(zip(a.values, b.values)), columns=['one', 'two'])\na_b\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 228, "library_problem_id": 228, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 226}}
{"id": 229, "code": ["<code>\nresult = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack()\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 229, "library_problem_id": 229, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 229}}
{"id": 230, "code": ["<code>\nresult = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack()\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 230, "library_problem_id": 230, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 231, "code": ["<code>\nresult = df.groupby('username').views.apply(lambda x: pd.cut(x, bins=bins).value_counts()).unstack()\nresult\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 231, "library_problem_id": 231, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 232, "code": ["<code>\nresult = df['text'].str.cat(sep=', ')\nresult = pd.DataFrame({'text': [result]})\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 232, "library_problem_id": 232, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 232}}
{"id": 233, "code": ["<code>\nresult = df['text'].str.cat(sep='-')\nresult = pd.DataFrame({'text': [result]})\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 233, "library_problem_id": 233, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 234, "code": ["<code>\nresult = df['text'].str.cat(sep=', ')\nresult = pd.DataFrame({'text': [result]})\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 234, "library_problem_id": 234, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 235, "code": ["<code>\nresult = df['text'].str.cat(sep=', ')\nresult = pd.Series([result], name='text')\n```\nEND SOLUTION\n</code>\nThe `str.cat()` function is used to concatenate the 'text' column of the dataframe. The `sep` parameter is used to specify the separator between the elements. In this case, we use a comma and a space as the separator. The result is then converted to a pandas Series with a name 'text'.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 235, "library_problem_id": 235, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 236, "code": ["<code>\nresult = df['text'].str.cat(sep='-')\nresult = pd.Series([result], name='text')\n```\nEND SOLUTION\n</code>\nThe function `str.cat()` is used to concatenate all the values in the 'text' column of the dataframe df with a '-' as the separator. The result is then put into a pandas Series.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 236, "library_problem_id": 236, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 232}}
{"id": 237, "code": ["<code>\n# First, we need to merge df1 and df2 on 'id' and 'date' columns\nmerged_df = pd.merge(df1, df2, on=['id', 'date'], how='outer')\n\n# Then, we need to fill NaN values in 'city' and 'district' columns with corresponding values from df1\nmerged_df['city'] = merged_df['city_x'].combine(merged_df['city_y'], lambda x, y: x if pd.notnull(x) else y)\nmerged_df['district'] = merged_df['district_x'].combine(merged_df['district_y'], lambda x, y: x if pd.notnull(x) else y)\n\n# Finally, we drop 'city_x', 'city_y', 'district_x', 'district_y' columns as they are not needed anymore\nmerged_df.drop(columns=['city_x', 'city_y', 'district_x', 'district_y'], inplace=True)\n\nresult = merged_df\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 237, "library_problem_id": 237, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 237}}
{"id": 238, "code": ["<code>\n# First, we convert the date column to datetime format\ndf1['date'] = pd.to_datetime(df1['date'])\ndf2['date'] = pd.to_datetime(df2['date'])\n\n# Then, we format the date column to the desired format\ndf1['date'] = df1['date'].dt.strftime('%d-%b-%Y')\ndf2['date'] = df2['date'].dt.strftime('%d-%b-%Y')\n\n# We sort the dataframes by id and date\ndf1.sort_values(['id', 'date'], inplace=True)\ndf2.sort_values(['id', 'date'], inplace=True)\n\n# We concatenate the dataframes along the rows\ndf = pd.concat([df1, df2], axis=0, ignore_index=True)\n\n# We fill the NaN values in the city and district columns with the corresponding values from df1\ndf['city'].fillna(df.groupby('id')['city'].transform('first'), inplace=True)\ndf['district'].fillna(df.groupby('id')['district'].transform('first'), inplace=True)\n\nresult = df\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 238, "library_problem_id": 238, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 239, "code": ["<code>\n# First, we need to fill the city and district from df1 to df2\ndf2 = df2.merge(df1[['id', 'city', 'district']], how='left', on='id')\n\n# Then, we concatenate df1 and df2\ndf = pd.concat([df1, df2], ignore_index=True)\n\n# Finally, we sort the dataframe by id and date\ndf = df.sort_values(by=['id', 'date'])\n\nresult = df\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 239, "library_problem_id": 239, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 240, "code": ["<code>\nresult = pd.concat([C, D]).drop_duplicates(subset='A', keep='last').reset_index(drop=True)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 240, "library_problem_id": 240, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 240}}
{"id": 241, "code": ["<code>\nresult = pd.concat([C, D]).drop_duplicates(subset='A', keep='first').reset_index(drop=True)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 241, "library_problem_id": 241, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 240}}
{"id": 242, "code": ["<code>\n# First, we merge C and D on column 'A' with an outer join. This will create NaN for the values in D that are not present in C.\nmerge_result = pd.merge(C, D, on='A', how='outer')\n\n# Then, we replace NaN values in column 'B_y' with the corresponding values in column 'B_x'.\nmerge_result['B'] = merge_result['B_x'].combine_first(merge_result['B_y'])\n\n# We drop the columns 'B_x' and 'B_y' as they are no longer needed.\nmerge_result.drop(columns=['B_x', 'B_y'], inplace=True)\n\n# We create a new column 'duplicated' that is True if the value in column 'A' is present in both C and D, and False otherwise.\nmerge_result['duplicated'] = merge_result['A'].apply(lambda x: x in C['A'].values and x in D['A'].values)\n\n# We reset the index to maintain the original order of rows.\nmerge_result.reset_index(drop=True, inplace=True)\n\nresult = merge_result\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 242, "library_problem_id": 242, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 240}}
{"id": 243, "code": ["<code>\ndf['pair'] = list(zip(df.time, df.amount))\nresult = df.groupby('user')['pair'].apply(list)\nresult = result.apply(lambda x: [i for i in x])\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 243, "library_problem_id": 243, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 243}}
{"id": 244, "code": ["<code>\ndf['amount-time'] = list(zip(df.time, df.amount))\nresult = df.groupby('user')['amount-time'].apply(list).reset_index()\nresult = result.rename(columns={'amount-time':'amount-time-tuple'})\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 244, "library_problem_id": 244, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 243}}
{"id": 245, "code": ["<code>\ndf['amount-time'] = list(zip(df.time, df.amount))\nresult = df.groupby('user')['amount-time'].apply(list).reset_index()\nresult = result.rename(columns={'amount-time':'amount-time-tuple'})\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 245, "library_problem_id": 245, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 243}}
{"id": 246, "code": ["<code>\ndf = series.apply(lambda x: pd.Series(x)).T\ndf.index = series.index\ndf\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 246, "library_problem_id": 246, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 246}}
{"id": 247, "code": ["<code>\ndf = series.reset_index().rename(columns={'index':'name'})\ndf = pd.DataFrame(df['file1'].tolist(), index=df['name']).reset_index().rename(columns={'index':'name'})\ndf.columns = df.loc[0].values\ndf = df.iloc[1:].reset_index(drop=True)\ndf\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 247, "library_problem_id": 247, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 246}}
{"id": 248, "code": ["<code>\nresult = [col for col in df.columns if s in col]\n```\nEND SOLUTION\n</code>\nThis will return a list of all column names that contain the string 'spike' anywhere in the column name.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 248, "library_problem_id": 248, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 248}}
{"id": 249, "code": ["<code>\nresult = df.columns[df.columns.str.contains(s)].tolist()\n```\nEND SOLUTION\n</code>\nThis will return a list of all column names that contain the string 'spike'. You can then use these column names to access the columns in your dataframe.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 249, "library_problem_id": 249, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 248}}
{"id": 250, "code": ["<code>\n# First, we find the columns that contain the string 'spike'\nmatching_columns = [col for col in df.columns if s in col]\n\n# Then, we rename these columns with the pattern 'spike1', 'spike2', etc.\nfor i, col in enumerate(matching_columns, 1):\n    df.rename(columns={col: f'spike{i}'}, inplace=True)\n\nresult = df\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 250, "library_problem_id": 250, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 248}}
{"id": 251, "code": ["<code>\nresult = pd.DataFrame(df['codes'].to_list(), index=df.index).add_prefix('code_')\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 251, "library_problem_id": 251, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 251}}
{"id": 252, "code": ["<code>\nresult = pd.DataFrame(df['codes'].to_list(), index=df.index).add_prefix('code_')\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 252, "library_problem_id": 252, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 251}}
{"id": 253, "code": ["<code>\nresult = pd.DataFrame(df['codes'].to_list(), index=df.index).add_prefix('code_')\nresult.fillna(method='ffill', inplace=True)\nresult.fillna(method='bfill', inplace=True)\nresult = result.fillna(method='ffill', axis=1)\nresult = result.astype(int)\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 253, "library_problem_id": 253, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 251}}
{"id": 254, "code": ["<code>\nresult = [item for sublist in df['col1'].values.tolist() for item in sublist]\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 254, "library_problem_id": 254, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 254}}
{"id": 255, "code": ["<code>\ndf['col1'] = df['col1'].apply(lambda x: ','.join(map(str, x[::-1])))\nresult = df['col1'].str.cat(sep=',')\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 255, "library_problem_id": 255, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 256, "code": ["<code>\nresult = ','.join(str(i) for sub in df['col1'] for i in sub)\nresult\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 256, "library_problem_id": 256, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 257, "code": ["<code>\ndf.set_index('Time', inplace=True)\ndf = df.resample('2min').mean()\ndf.reset_index(inplace=True)\ndf\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 257, "library_problem_id": 257, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 257}}
{"id": 258, "code": ["<code>\ndf.set_index('Time', inplace=True)\ndf = df.resample('3T').sum()\ndf.reset_index(inplace=True)\ndf\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 258, "library_problem_id": 258, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 257}}
{"id": 259, "code": ["<code>\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method=\"min\")\ndf\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 259, "library_problem_id": 259, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 259}}
{"id": 260, "code": ["<code>\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method=\"min\", ascending=False)\ndf\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 260, "library_problem_id": 260, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 259}}
{"id": 261, "code": ["<code>\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method=\"min\", ascending=False)\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %a %H:%M:%S')\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 261, "library_problem_id": 261, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 259}}
{"id": 262, "code": ["<code>\nresult = df.loc[filt]\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 262, "library_problem_id": 262, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 262}}
{"id": 263, "code": ["<code>\nresult = df.loc[filt, :]\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 263, "library_problem_id": 263, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 262}}
{"id": 264, "code": ["<code>\ndef equalp(x, y):\n    return (x == y) or (np.isnan(x) and np.isnan(y))\n\ndef diff_cols(df, row1=0, row2=8):\n    return df.columns[(df.iloc[row1] != df.iloc[row2]) | (df.iloc[row1].apply(np.isnan) & df.iloc[row2].apply(np.isnan))]\n\nresult = diff_cols(df)\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 264, "library_problem_id": 264, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 264}}
{"id": 265, "code": ["<code>\ndef equalp(x, y):\n    return (x == y) or (np.isnan(x) and np.isnan(y))\n\ndef same_columns(df):\n    return [col for col in df.columns if df.loc[0, col] == df.loc[8, col]]\n\nresult = same_columns(df)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 265, "library_problem_id": 265, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 266, "code": ["<code>\ndef equalp(x, y):\n    return (x == y) or (np.isnan(x) and np.isnan(y))\n\ndef different_columns(df):\n    return [col for col in df.columns if not df.loc[0].equals(df.loc[8])]\n\nresult = different_columns(df)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 266, "library_problem_id": 266, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 267, "code": ["<code>\nresult = [(x, y) for x, y in zip(df.iloc[0], df.iloc[8]) if x != y]\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 267, "library_problem_id": 267, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 264}}
{"id": 268, "code": ["<code>\nts = pd.Series(df['Value'].values, index=df['Date'])\n```\nEND SOLUTION\nprint(ts)\n</code>\nIn your code, you are trying to create a Series from a DataFrame, but you are not specifying the index of the Series. The index of the Series is the 'Date' column of the DataFrame, and the values of the Series are the 'Value' column of the DataFrame. The 'Value' column of the DataFrame is accessed with df['Value'].values, which returns an array of the values in the 'Value' column.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 268, "library_problem_id": 268, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 268}}
{"id": 269, "code": ["<code>\ndf = df.T.reset_index().T\ndf.columns = [f\"{col}_{i+1}\" if i > 0 else f\"{col}_1\" for i, col in enumerate(df.columns)]\ndf = df.iloc[1:].reset_index(drop=True)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 269, "library_problem_id": 269, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 269}}
{"id": 270, "code": ["<code>\ndf = df.T.reset_index().T\ndf.columns = [f\"{col}_{i}\" if i != 0 else col for i, col in enumerate(df.columns.tolist())]\ndf = df.iloc[1:].reset_index(drop=True)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 270, "library_problem_id": 270, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 269}}
{"id": 271, "code": ["<code>\ndf['dogs'] = df['dogs'].apply(lambda x: round(x, 2) if pd.notna(x) else x)\n```\nEND SOLUTION\ndf\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 271, "library_problem_id": 271, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 271}}
{"id": 272, "code": ["<code>\ndf['dogs'] = df['dogs'].apply(lambda x: round(x, 2) if pd.notna(x) else x)\ndf['cats'] = df['cats'].apply(lambda x: round(x, 2) if pd.notna(x) else x)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 272, "library_problem_id": 272, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 271}}
{"id": 273, "code": ["<code>\ndf['Sum'] = df[list_of_my_columns].sum(axis=1)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 273, "library_problem_id": 273, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 273}}
{"id": 274, "code": ["<code>\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 274, "library_problem_id": 274, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 273}}
{"id": 275, "code": ["<code>\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\ndf['Min'] = df[list_of_my_columns].min(axis=1)\ndf['Max'] = df[list_of_my_columns].max(axis=1)\ndf['Median'] = df[list_of_my_columns].median(axis=1)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 275, "library_problem_id": 275, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 273}}
{"id": 276, "code": ["<code>\nresult = df.sort_values(by='time')\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 276, "library_problem_id": 276, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 276}}
{"id": 277, "code": ["<code>\nresult = df.sort_values(by=['VIM', 'time'])\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 277, "library_problem_id": 277, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 276}}
{"id": 278, "code": ["<code>\nresult = df[~df.index.isin(['2020-02-17', '2020-02-18'])]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 278, "library_problem_id": 278, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 278}}
{"id": 279, "code": ["<code>\n# Convert the Date column to datetime format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Set the Date column as the index\ndf.set_index('Date', inplace=True)\n\n# Filter out the dates\ndf = df[~((df.index.date == pd.to_datetime('2020-02-17').date()) | (df.index.date == pd.to_datetime('2020-02-18').date()))]\n\n# Add a new column for the day of the week\ndf['Day of Week'] = df.index.day_name()\n\n# Reset the index\ndf.reset_index(inplace=True)\n\n# Format the Date column\ndf['Date'] = df['Date'].dt.strftime('%d-%b-%Y %A')\n\nresult = df\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 279, "library_problem_id": 279, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 278}}
{"id": 280, "code": ["<code>\nresult = corr.stack().reset_index()\nresult.columns = ['Col1', 'Col2', 'Pearson Correlation Coefficient']\nresult = result[result['Pearson Correlation Coefficient'].abs() > 0.3]\nresult = result[result['Col1'] != result['Col2']]\nresult.set_index(['Col1', 'Col2'], inplace=True)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 280, "library_problem_id": 280, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 280}}
{"id": 281, "code": ["<code>\nresult = corr.stack()[corr.stack() > 0.3]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 281, "library_problem_id": 281, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 280}}
{"id": 282, "code": ["<code>\ndf.columns = ['A', 'B', 'Test']\nresult = df\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 282, "library_problem_id": 282, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 282}}
{"id": 283, "code": ["<code>\nresult = df.rename(columns={df.columns[0]: 'Test'})\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 283, "library_problem_id": 283, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 282}}
{"id": 284, "code": ["<code>\ndf['frequent'] = df.mode(axis=1)[0]\ndf['freq_count'] = df.apply(lambda row: list(row).count(row['frequent']), axis=1)\ndf\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 284, "library_problem_id": 284, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 284}}
{"id": 285, "code": ["<code>\ndf['frequent'] = df.mode(axis=1)[0]\ndf['freq_count'] = df.apply(lambda row: list(row).count(row['frequent']), axis=1)\ndf\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 285, "library_problem_id": 285, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 284}}
{"id": 286, "code": ["<code>\ndf['frequent'] = df.apply(lambda row: list(row.value_counts().index[row.value_counts() == row.value_counts().max()]), axis=1)\ndf['freq_count'] = df['frequent'].apply(lambda x: len(x))\ndf\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 286, "library_problem_id": 286, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 284}}
{"id": 287, "code": ["<code>\ndf['bar'] = df['bar'].replace('NULL', pd.np.nan).astype(float)\nresult = df.groupby([\"id1\",\"id2\"])[\"foo\",\"bar\"].mean().reset_index()\nresult\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 287, "library_problem_id": 287, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 287}}
{"id": 288, "code": ["<code>\ndf['bar'] = df['bar'].replace('NULL', 0)\ndf['bar'] = df['bar'].astype(int)\nres = df.groupby([\"id1\",\"id2\"])[\"foo\",\"bar\"].mean().reset_index()\nres\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 288, "library_problem_id": 288, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 287}}
{"id": 289, "code": ["<code>\nresult = pd.merge(df_a, df_b[['EntityNum', 'a_col']], on='EntityNum')\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 289, "library_problem_id": 289, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 289}}
{"id": 290, "code": ["<code>\nresult = pd.merge(df_a, df_b[['EntityNum','b_col']], on='EntityNum')\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 290, "library_problem_id": 290, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 289}}
{"id": 291, "code": ["<code>\nresult = a.shape\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 291, "library_problem_id": 0, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 292, "code": ["<code>\nx = x[~np.isnan(x)]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 292, "library_problem_id": 1, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 293, "code": ["<code>\nx = np.nan_to_num(x, nan=np.inf)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 293, "library_problem_id": 2, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 294, "code": ["<code>\nresult = [[val for val in sublist if not np.isnan(val)] for sublist in x]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 294, "library_problem_id": 3, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 1}}
{"id": 295, "code": ["<code>\nb = np.eye(a.max() + 1)[a]\n```\n</code>\nEND SOLUTION\nThis solution uses numpy's eye function to create an identity matrix of size a.max() + 1, then indexing to create the one-hot encoding. The eye function creates an identity matrix, and indexing with a is used to select the rows corresponding to the elements in a.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 295, "library_problem_id": 4, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 296, "code": ["<code>\nb = np.eye(max(a)+1)[a]\n```\nEND SOLUTION\n</code>\nThe np.eye(max(a)+1) creates an identity matrix of size max(a)+1. The [a] then selects the rows of the identity matrix corresponding to the elements of a. This results in a one-hot encoding of a.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 296, "library_problem_id": 5, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 297, "code": ["<code>\nb = np.eye(a.max() + 1)[a]\n```\nEND SOLUTION\n</code>\nThe solution uses numpy's eye function to create an identity matrix of size `a.max() + 1`, then indexing to create the one-hot encoding. The `a.max() + 1` is used to ensure the size of the identity matrix is large enough to accommodate all the unique values in `a`. The indexing operation `[a]` then selects the rows of the identity matrix corresponding to the values in `a`.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 297, "library_problem_id": 6, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 298, "code": ["<code>\nb = np.eye(len(a))[np.argsort(a)]\n```\n</code>\nEND SOLUTION\nThe solution uses the np.eye function to create an identity matrix of the same size as `a`, then np.argsort to sort the indices of `a`. The result is then used to index into the identity matrix, effectively creating the one-hot encoding.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 298, "library_problem_id": 7, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 299, "code": ["<code>\nb = np.zeros((a.size, a.max()+1), dtype=int)\nnp.put_along_axis(b, a[:,:,None], 1, axis=-1)\nb = np.flip(b, axis=-1)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 299, "library_problem_id": 8, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 300, "code": ["<code>\nresult = np.percentile(a, p)\n```\nEND SOLUTION\n</code>\nThe function np.percentile(a, p) calculates the p-th percentile of the array a. For example, np.percentile(a, 25) calculates the 25th percentile of the array a.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 300, "library_problem_id": 9, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 301, "code": ["<code>\nB = np.reshape(A, (A.size // ncol, ncol))\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 301, "library_problem_id": 10, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 302, "code": ["<code>\nB = np.reshape(A, (nrow, -1))\n```\nEND SOLUTION\n</code>\nThe `-1` in the reshape function tells numpy to calculate the number of columns automatically based on the original array and the number of rows. This way, you can easily convert a 1D array into a 2D array with a specified number of rows.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 302, "library_problem_id": 11, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 303, "code": ["<code>\ndef vec2matrix(A, ncol):\n    return A[:len(A)//ncol*ncol].reshape(-1, ncol)\n\nB = vec2matrix(A, ncol)\nprint(B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 303, "library_problem_id": 12, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 304, "code": ["<code>\ndef vec2matrix(A, ncol):\n    # Reverse the array\n    A = A[::-1]\n    # Calculate the number of rows\n    nrow = len(A) // ncol\n    # Reshape the array\n    B = A[:nrow*ncol].reshape(nrow, ncol)\n    return B\n\nB = vec2matrix(A, ncol)\nprint(B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 304, "library_problem_id": 13, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 305, "code": ["<code>\ndef shift(a, shift):\n    if shift > 0:\n        return np.pad(a, (shift, 0), 'constant', constant_values=np.nan)[:-shift]\n    elif shift < 0:\n        return np.pad(a, (0, -shift), 'constant', constant_values=np.nan)[-shift:]\n    else:\n        return a\n\nresult = shift(a, shift)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 305, "library_problem_id": 14, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 306, "code": ["<code>\ndef shift(array, shift):\n    if shift == 0:\n        return array\n    elif shift > 0:\n        return np.pad(array, ((0, 0), (shift, 0)), 'constant', constant_values=np.nan)[:, :-shift]\n    else:\n        return np.pad(array, ((0, 0), (0, -shift)), 'constant', constant_values=np.nan)[:, -shift:]\n\nresult = shift(a, shift)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 306, "library_problem_id": 15, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 307, "code": ["<code>\ndef shift_2d(arr, shifts):\n    shifted_arr = np.zeros_like(arr)\n    for i in range(arr.shape[0]):\n        shifted_arr[i] = np.roll(arr[i], shifts[i])\n    return shifted_arr\n\nresult = shift_2d(a, shift)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 307, "library_problem_id": 16, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 14}}
{"id": 308, "code": ["<code>\nnp.random.seed(0)\nr_old = np.random.randint(3, size=(100, 2000)) - 1\nr_new = np.random.randint(3, size=(100, 2000)) - 1\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 308, "library_problem_id": 17, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 309, "code": ["<code>\nresult = np.unravel_index(np.argmax(a, axis=None), a.shape, order='C')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 309, "library_problem_id": 18, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 310, "code": ["<code>\nresult = np.unravel_index(np.argmin(a, axis=None), a.shape, order='C')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 310, "library_problem_id": 19, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 311, "code": ["<code>\nresult = np.unravel_index(np.argmax(a, axis=None), a.shape, order='F')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 311, "library_problem_id": 20, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 312, "code": ["<code>\nresult = np.unravel_index(np.argmax(a, axis=None), a.shape)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 312, "library_problem_id": 21, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 313, "code": ["<code>\nresult = np.unravel_index(np.argmax(a, axis=None), a.shape, order='C')\n    return result\n### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 313, "library_problem_id": 22, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 18}}
{"id": 314, "code": ["<code>\nimport numpy as np\n\na = np.array([[10,50,30],[60,20,40]])\n\n# Flatten the array and get the second largest value\nsecond_largest = np.sort(a, axis=None)[-2]\n\n# Get the indices of the second largest value\nresult = np.unravel_index(np.argmax(a == second_largest), a.shape)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 314, "library_problem_id": 23, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 18}}
{"id": 315, "code": ["<code>\nimport numpy as np\n\na = np.array([[np.nan, 2., 3., np.nan],\n              [1., 2., 3., 9]])\n\nmask = ~np.isnan(a).any(axis=0)\na = a[:, mask]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 315, "library_problem_id": 24, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 316, "code": ["<code>\na = a[~np.isnan(a).any(axis=1)]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 316, "library_problem_id": 25, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 24}}
{"id": 317, "code": ["<code>\nresult = np.array(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 317, "library_problem_id": 26, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 318, "code": ["<code>\na = np.transpose(a[:, permutation])\na = np.transpose(a)\n```\nEND SOLUTION\n</code>\nThe first `np.transpose` changes the order of the columns, and the second `np.transpose` changes the order of the rows back to the original.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 318, "library_problem_id": 27, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 319, "code": ["<code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 319, "library_problem_id": 28, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 27}}
{"id": 320, "code": ["<code>\nresult = np.unravel_index(a.argmin(), a.shape)\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 320, "library_problem_id": 29, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 321, "code": ["<code>\nresult = np.unravel_index(a.argmax(), a.shape)\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 321, "library_problem_id": 30, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 29}}
{"id": 322, "code": ["<code>\nresult = np.argwhere(a == np.min(a))\n```\nEND SOLUTION\n</code>\nThe np.argwhere function returns the indices of non-zero elements in the array. The np.min function is used to find the minimum value in the array. The result will be a 2D array where each row is a pair of indices.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 322, "library_problem_id": 31, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 29}}
{"id": 323, "code": ["<code>\nresult = np.sin(np.radians(degree))\n```\n</code>\nEND SOLUTION\nThe numpy.sin() function requires the input to be in radians, not degrees. Therefore, we need to convert the degree to radians using numpy.radians() before passing it to the sin function.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 323, "library_problem_id": 32, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 324, "code": ["<code>\nresult = np.cos(np.radians(degree))\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 324, "library_problem_id": 33, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 325, "code": ["<code>\nimport numpy as np\n\nnumber = np.random.randint(0, 360)\n\n# The np.sin() function returns the sine of the input number.\n# If the sine value is greater when the number is regarded as degree, then it is degree.\n# Otherwise, it is radian.\n\nif np.sin(np.radians(number)) > np.sin(number):\n    result = 0  # degree\nelse:\n    result = 1  # radian\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 325, "library_problem_id": 34, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 326, "code": ["<code>\nresult = np.arcsin(value) * (180 / np.pi)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 326, "library_problem_id": 35, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 327, "code": ["<code>\ndef pad(A, length):\n    if len(A) >= length:\n        return A[:length]\n    return np.pad(A, (0, length-len(A)), 'constant', constant_values=0)\n\nA = np.array([1,2,3,4,5])\nlength = 8\nresult = pad(A, length)\nprint(result)  # expected : [1,2,3,4,5,0,0,0]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 327, "library_problem_id": 36, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 328, "code": ["<code>\ndef pad(A, length):\n    if len(A) >= length:\n        return A[:length]\n    else:\n        return np.pad(A, (0, length-len(A)), 'constant', constant_values=0)\n\nA = np.array([1,2,3,4,5])\nlength = 8\nresult = pad(A, length)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 328, "library_problem_id": 37, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 329, "code": ["<code>\na = np.power(a, power)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 329, "library_problem_id": 38, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 330, "code": ["<code>\nresult = np.power(a, power)\n    return result\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 330, "library_problem_id": 39, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 331, "code": ["<code>\nfrom fractions import Fraction\n\ndef reduce_fraction(numerator, denominator):\n    fraction = Fraction(numerator, denominator)\n    return (fraction.numerator, fraction.denominator)\n\nresult = reduce_fraction(numerator, denominator)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 331, "library_problem_id": 40, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 332, "code": ["<code>\nfrom fractions import Fraction\n\ndef f(numerator = 98, denominator = 42):\n    # Use the Fraction class to reduce the fraction\n    fraction = Fraction(numerator, denominator)\n    # Return the reduced fraction as a tuple\n    return (fraction.numerator, fraction.denominator)\n\n# Test the function\nprint(f(98, 42))  # Output: (7, 3)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 332, "library_problem_id": 41, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 40}}
{"id": 333, "code": ["<code>\nfrom fractions import Fraction\n\ndef reduce_fraction(numerator, denominator):\n    if denominator == 0:\n        return (float('NaN'), float('NaN'))\n    else:\n        fraction = Fraction(numerator, denominator)\n        return (fraction.numerator, fraction.denominator)\n\nresult = reduce_fraction(numerator, denominator)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 333, "library_problem_id": 42, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 40}}
{"id": 334, "code": ["<code>\nresult = np.mean(np.array([a, b, c]), axis=0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 334, "library_problem_id": 43, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 43}}
{"id": 335, "code": ["<code>\nresult = np.maximum(np.maximum(a, b), c)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 335, "library_problem_id": 44, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 43}}
{"id": 336, "code": ["<code>\nresult = np.fliplr(a).diagonal()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 336, "library_problem_id": 45, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 337, "code": ["<code>\ndef get_diagonal_from_top_right(a):\n    rows, cols = a.shape\n    start_row = min(rows, cols)\n    indices = np.arange(start_row)\n    diagonal = np.diag_indices_from(a[::-1, :])\n    return a[diagonal[0][::-1], diagonal[1]]\n\nresult = get_diagonal_from_top_right(a)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 337, "library_problem_id": 46, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 338, "code": ["<code>\nresult = np.fliplr(a).diagonal()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 338, "library_problem_id": 47, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 339, "code": ["<code>\nresult = np.fliplr(a).diagonal()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 339, "library_problem_id": 48, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 45}}
{"id": 340, "code": ["<code>\nresult = [item for sublist in X for item in sublist]\n```\nEND SOLUTION\n</code>\nThis solution uses list comprehension to flatten the 2D array into a 1D list. It iterates over each sublist in X (which are the rows of X), then iterates over each item in the sublist. The item is then added to the result list.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 340, "library_problem_id": 49, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 341, "code": ["<code>\nresult = []\nfor row in X:\n    for cell in row:\n        result.append(cell)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 341, "library_problem_id": 50, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 49}}
{"id": 342, "code": ["<code>\nresult = []\n    for row in X:\n        for cell in row:\n            result.append(cell)\n    return result\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 342, "library_problem_id": 51, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 343, "code": ["<code>\nresult = []\nfor row in np.nditer(X, order='F'):\n    result.append(row)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 343, "library_problem_id": 52, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 49}}
{"id": 344, "code": ["<code>\nresult = np.array([int(i) for i in mystr])\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 344, "library_problem_id": 53, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 53}}
{"id": 345, "code": ["<code>\nresult = np.cumsum(a[:, col] * multiply_number)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 345, "library_problem_id": 54, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 346, "code": ["<code>\nresult = np.cumsum(a[row] * multiply_number)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 346, "library_problem_id": 55, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 347, "code": ["<code>\n# Divide the row-th row of the array by the number\na[row, :] = a[row, :] / divide_number\n\n# Calculate the multiplication of the numbers in that row\nresult = np.prod(a[row, :])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 347, "library_problem_id": 56, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 54}}
{"id": 348, "code": ["<code>\nimport numpy as np\n\ndef get_maximal_set(a):\n    # Compute the rank of the matrix\n    rank = np.linalg.matrix_rank(a)\n    \n    # Return the first rank columns of the matrix\n    return a[:, :rank]\n\na = np.array([[0,1,0,0], [0,0,1,0], [0,1,1,0], [1,0,0,1]])\nresult = get_maximal_set(a)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 348, "library_problem_id": 57, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 349, "code": ["<code>\nresult = a.shape[1]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 349, "library_problem_id": 58, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 350, "code": ["<code>\nimport scipy.stats as stats\n\n# Calculate the mean and standard deviation of the two samples\nmean_a = np.mean(a)\nstd_a = np.std(a)\nmean_b = np.mean(b)\nstd_b = np.std(b)\n\n# Calculate the pooled standard deviation and degrees of freedom\nsp = np.sqrt(((len(a)-1)*std_a**2 + (len(b)-1)*std_b**2) / (len(a)+len(b)-2))\ndf = len(a) + len(b) - 2\n\n# Calculate the t-statistic\nt_stat = (mean_a - mean_b) / (sp * np.sqrt(1/len(a) + 1/len(b)))\n\n# Calculate the p-value\np_value = 2 * (1 - stats.t.cdf(abs(t_stat), df))\n\np_value\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 350, "library_problem_id": 59, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 351, "code": ["<code>\n# First, we need to remove the NaNs from the arrays\na = a[~np.isnan(a)]\nb = b[~np.isnan(b)]\n\n# Then, we calculate the means and standard deviations of the samples\nmean_a = np.mean(a)\nmean_b = np.mean(b)\nstd_a = np.std(a)\nstd_b = np.std(b)\n\n# The degrees of freedom is the total number of observations minus 2\n# (since we're doing a two-tailed t-test)\ndf = len(a) + len(b) - 2\n\n# The pooled standard deviation is the square root of the sum of the squares of the standard deviations divided by the number of observations\nsp = np.sqrt(((len(a)-1)*std_a**2 + (len(b)-1)*std_b**2) / df)\n\n# The t-statistic is the difference in means divided by the pooled standard deviation\nt_stat = (mean_a - mean_b) / sp\n\n# The p-value is the two-tailed t-distribution probability at the absolute value of the t-statistic\np_value = 2 * (1 - scipy.stats.t.cdf(abs(t_stat), df))\n\np_value\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 351, "library_problem_id": 60, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 352, "code": ["<code>\n# Calculate the pooled variance and degrees of freedom\nsp2 = ((anobs - 1) * avar + (bnobs - 1) * bvar) / (anobs + bnobs - 2)\ndf = anobs + bnobs - 2\n\n# Calculate the t-statistic\nt_stat = (amean - bmean) / np.sqrt(sp2 * (1/anobs + 1/bnobs))\n\n# Calculate the p-value\np_value = 2 * (1 - scipy.stats.t.cdf(t_stat, df))\n\nprint(\"The p-value is:\", p_value)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 352, "library_problem_id": 61, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 353, "code": ["<code>\nimport numpy as np\n\nA=np.asarray([[1,1,1], [1,1,2], [1,1,3], [1,1,4]])\nB=np.asarray([[0,0,0], [1,0,2], [1,0,3], [1,0,4], [1,1,0], [1,1,1], [1,1,4]])\n\n# Convert the arrays to a hashable format\nA_hashable = [tuple(x) for x in A]\nB_hashable = [tuple(x) for x in B]\n\n# Use set difference to get the elements in A that are not in B\noutput = np.array([x for x in A_hashable if x not in B_hashable])\n\n# Convert the output back to a numpy array\noutput = np.array(output)\n\nprint(output)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 353, "library_problem_id": 62, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 354, "code": ["<code>\n# Convert the 2D arrays to 1D arrays\nA_flat = A.reshape(-1, A.shape[-1])\nB_flat = B.reshape(-1, B.shape[-1])\n\n# Find the unique elements in each array\nA_unique = np.unique(A_flat, axis=0)\nB_unique = np.unique(B_flat, axis=0)\n\n# Find the elements in A that are not in B\nA_not_B = np.setdiff1d(A_unique, B_unique)\n\n# Find the elements in B that are not in A\nB_not_A = np.setdiff1d(B_unique, A_unique)\n\n# Combine the two sets of unique elements\noutput = np.concatenate((A_not_B, B_not_A))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 354, "library_problem_id": 63, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 62}}
{"id": 355, "code": ["<code>\nimport numpy as np\n\na = np.array([[[ 1.,  1.,  1.],\n               [ 1.,  1.,  1.],\n               [ 1.,  1.,  1.]],\n              [[ 3.,  3.,  3.],\n               [ 3.,  2.,  3.],\n               [ 3.,  3.,  3.]],\n              [[ 2.,  2.,  2.],\n               [ 2.,  3.,  2.],\n               [ 2.,  2.,  2.]]])\n\nb = np.arange(3*3*3).reshape((3, 3, 3))\n\nsort_indices = np.argsort(a, axis=0)\nc = np.zeros_like(b)\n\nfor i in range(a.shape[1]):\n    for j in range(a.shape[2]):\n        c[:, i, j] = b[sort_indices[:, i, j], i, j]\n\nprint(c)\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 355, "library_problem_id": 64, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 356, "code": ["<code>\nimport numpy as np\n\na = np.array([[[ 1.,  1.,  1.],\n               [ 1.,  1.,  1.],\n               [ 1.,  1.,  1.]],\n\n              [[ 3.,  3.,  3.],\n               [ 3.,  3.,  3.],\n               [ 3.,  3.,  3.]],\n\n              [[ 2.,  2.,  2.],\n               [ 2.,  2.,  2.],\n               [ 2.,  2.,  2.]]])\n\nb = np.arange(3*3*3).reshape((3, 3, 3))\n\nsort_indices = np.argsort(a, axis=0)\nc = b[sort_indices]\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 356, "library_problem_id": 65, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 357, "code": ["<code>\nsort_indices = np.argsort(a, axis=0)[::-1]\nc = b[sort_indices]\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 357, "library_problem_id": 66, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 358, "code": ["<code>\n# Calculate the sum along the first axis\nsum_a = a.sum(axis=(1,2))\n\n# Get the indices that would sort sum_a\nindices = np.argsort(sum_a)\n\n# Use these indices to sort b\nresult = b[indices]\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 358, "library_problem_id": 67, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 64}}
{"id": 359, "code": ["<code>\na = np.delete(a, 2, axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 359, "library_problem_id": 68, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 360, "code": ["<code>\nimport numpy as np\n\na = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\na = np.delete(a, 2, axis=0)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 360, "library_problem_id": 69, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 361, "code": ["<code>\na = np.delete(a, [0, 2], axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 361, "library_problem_id": 70, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 362, "code": ["<code>\nresult = np.delete(a, del_col-1, axis=1)\nresult = result[:, ~np.isin(np.arange(result.shape[1]), del_col-1)]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 362, "library_problem_id": 71, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 363, "code": ["<code>\na = np.insert(a, pos, element)\nprint(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 363, "library_problem_id": 72, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 364, "code": ["<code>\na = np.insert(a, pos, element, axis=0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 364, "library_problem_id": 73, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 365, "code": ["<code>\na_l = a.tolist()\n    a_l.insert(pos, element)\n    return np.asarray(a_l)\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 365, "library_problem_id": 74, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 72}}
{"id": 366, "code": ["<code>\n# Insert the new rows at the specified positions\nfor i in range(len(pos)):\n    a = np.insert(a, pos[i], element[i], axis=0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 366, "library_problem_id": 75, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 72}}
{"id": 367, "code": ["<code>\n</code>\nEND SOLUTION\nThe np.copy() function is used to create a deep copy of the array. This means that changes to the original array will not affect the copied array, and vice versa.\n\nTo interact with each element in this array of different sized arrays, you can use a for loop. Here's an example:\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 367, "library_problem_id": 76, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 368, "code": ["<code>\nresult = np.all(a[0] == a)\n```\n</code>\nEND SOLUTION\nThis solution uses numpy's broadcasting feature to compare all elements in the first row with all rows in the array. The result is a boolean array where True means that the corresponding element in the row is equal to the first row, and False means it's not. The np.all function then checks if all elements in the resulting array are True, which means that all rows are equal.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 368, "library_problem_id": 77, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 369, "code": ["<code>\nresult = np.all(a[:, 0] == a[:, 1]) and np.all(a[:, 1] == a[:, 2])\n```\n</code>\nEND SOLUTION\nThis solution works by comparing the first column of the array to the second and third columns. If all elements in the first column are equal to the corresponding elements in the second and third columns, the np.all function will return True. Otherwise, it will return False.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 369, "library_problem_id": 78, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Semantic", "perturbation_origin_id": 77}}
{"id": 370, "code": ["<code>\nresult = np.all(np.diff(a, axis=0) == 0)\n    return result\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 370, "library_problem_id": 79, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Surface", "perturbation_origin_id": 77}}
{"id": 371, "code": ["<code>\nimport scipy.integrate as spi\n\n# Define the function\ndef f(x, y):\n    return (np.cos(x))**4 + (np.sin(y))**2\n\n# Perform the integration\nresult = spi.simps(spi.simps(f(x, y), y), x)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 371, "library_problem_id": 80, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 372, "code": ["<code>\nfrom scipy import integrate\n\ndef f(x=example_x, y=example_y):\n    # Define the function\n    def func(x, y):\n        return (np.cos(x))**4 + (np.sin(y))**2\n\n    # Initialize the result\n    result = 0\n\n    # Perform the integration\n    for i in range(len(x)-1):\n        for j in range(len(y)-1):\n            # Use Simpson's rule for each small rectangle\n            result += (integrate.simps(integrate.simps(func(x[i:i+2], y[:2]), y[:2]), x[i:i+2]) +\n                       integrate.simps(integrate.simps(func(x[i:i+2], y[-2:]), y[-2:]), x[i:i+2]) +\n                       integrate.simps(integrate.simps(func(x[:2], y[i:i+2]), x[:2]), y[i:i+2]) +\n                       integrate.simps(integrate.simps(func(x[-2:], y[i:i+2]), x[-2:]), y[i:i+2])) / 4\n\n    return result\n\n# Test the function\nprint(f(example_x, example_y))\n### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 372, "library_problem_id": 81, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 373, "code": ["<code>\nimport numpy as np\n\ndef ecdf(data):\n    # sort the data\n    x = np.sort(data)\n    # calculate the proportional values of samples\n    n = x.size\n    # calculate the cumulative sum, which is the empirical distribution function\n    y = np.arange(1, n+1) / n\n    return x, y\n\ngrades = np.array((93.5,93,60.8,94.5,82,87.5,91.5,99.5,86,93.5,92.5,78,76,69,94.5,\n          89.5,92.8,78,65.5,98,98.5,92.3,95.5,76,91,95,61))\n\nx, y = ecdf(grades)\nresult = (x, y)\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 373, "library_problem_id": 82, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 374, "code": ["<code>\ndef ecdf(data):\n    # sort the data\n    x = np.sort(data)\n    # calculate the normalized cumulative sum\n    n = len(x)\n    y = np.arange(1, n+1) / n\n    return x, y\n\n# calculate the empirical cumulative distribution function\ngrades_ecdf = ecdf(grades)\n\n# evaluate the ecdf at the eval array\nresult = np.interp(eval, grades_ecdf[0], grades_ecdf[1])\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 374, "library_problem_id": 83, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 375, "code": ["<code>\n# Sort the grades\ngrades_sorted = np.sort(grades)\n\n# Compute the ECDF\necdf = np.cumsum(grades_sorted) / np.sum(grades_sorted)\n\n# Find the longest interval that satisfies ECDF < threshold\nlow = np.min(grades_sorted[ecdf < threshold])\nhigh = np.max(grades_sorted[ecdf < threshold])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 375, "library_problem_id": 84, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 376, "code": ["<code>\nnums = np.hstack((np.ones(int(one_ratio*size), dtype=int), np.zeros(size - int(one_ratio*size), dtype=int)))\nnp.random.shuffle(nums)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 376, "library_problem_id": 85, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 377, "code": ["<code>\na_np = a.numpy()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 377, "library_problem_id": 86, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 378, "code": ["<code>\na_pt = torch.from_numpy(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 378, "library_problem_id": 87, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 86}}
{"id": 379, "code": ["<code>\na_np = a.numpy()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 379, "library_problem_id": 88, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 380, "code": ["<code>\na_tf = tf.convert_to_tensor(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 380, "library_problem_id": 89, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 88}}
{"id": 381, "code": ["<code>\nresult = np.argsort(a)[::-1]\n```\n</code>\nEND SOLUTION\nThe numpy function `argsort` returns the indices that would sort the array in ascending order. By using slicing `[::-1]`, we reverse the order to get the indices that would sort the array in descending order.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 381, "library_problem_id": 90, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 382, "code": ["<code>\nresult = np.argsort(a)\n```\n</code>\nEND SOLUTION\nThe numpy function `argsort` returns the indices that would sort the array in ascending order. In your case, it would return the indices of the elements in the array `a` in increasing order.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 382, "library_problem_id": 91, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 383, "code": ["<code>\nresult = np.argsort(a)[-N:][::-1]\n```\n</code>\nEND SOLUTION\nThe `np.argsort()` function returns the indices that would sort the array in ascending order. By using `[-N:]` we get the last N elements, which are the N biggest elements. The `[::-1]` is used to reverse the array, so we get the indices of the N biggest elements in decreasing order.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 383, "library_problem_id": 92, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 90}}
{"id": 384, "code": ["<code>\nresult = np.power(A, n)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 384, "library_problem_id": 93, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 93}}
{"id": 385, "code": ["<code>\ndef extract_patches(a, patch_size):\n    nrows, ncols = a.shape\n    nrows_patch, ncols_patch = patch_size\n    patches = []\n    for i in range(0, nrows - nrows_patch + 1, nrows_patch):\n        for j in range(0, ncols - ncols_patch + 1, ncols_patch):\n            patch = a[i:i+nrows_patch, j:j+ncols_patch]\n            patches.append(patch)\n    return patches\n\nresult = extract_patches(a, (2, 2))\nresult = np.array(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 385, "library_problem_id": 94, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 386, "code": ["<code>\nimport numpy as np\n\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\n\nresult = []\n\nfor i in range(a.shape[0]-1):\n    for j in range(a.shape[1]-1):\n        patch = a[i:i+2, j:j+2]\n        result.append(patch)\n\nresult = np.array(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 386, "library_problem_id": 95, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 387, "code": ["<code>\nimport numpy as np\n\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\n\n# Get the shape of the array\nshape = a.shape\n\n# Calculate the new shape\nnew_shape = (shape[0]//2, 2, shape[1]//2, 2)\n\n# Reshape the array\nb = a.reshape(new_shape)\n\n# Swap the axes to get the patches\nresult = np.swapaxes(b, 1, 2).reshape(-1, 2, 2)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 387, "library_problem_id": 96, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 388, "code": ["<code>\ndef extract_patches(a, patch_size):\n    nrows, ncols = a.shape\n    patches = []\n    for i in range(0, nrows, patch_size):\n        for j in range(0, ncols, patch_size):\n            patch = a[i:i+patch_size, j:j+patch_size]\n            if patch.shape == (patch_size, patch_size):\n                patches.append(patch)\n    return patches\n\nresult = extract_patches(a, patch_size)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 388, "library_problem_id": 97, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 389, "code": ["<code>\nresult = a.reshape(h, w)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 389, "library_problem_id": 98, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 390, "code": ["<code>\ndef extract_patches(a, patch_size):\n    n_rows, n_cols = a.shape\n    patches = []\n    for i in range(0, n_rows, patch_size):\n        for j in range(0, n_cols, patch_size):\n            patch = a[i:i+patch_size, j:j+patch_size]\n            if patch.shape == (patch_size, patch_size):\n                patches.append(patch)\n    return patches\n\nresult = extract_patches(a, patch_size)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 390, "library_problem_id": 99, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 391, "code": ["<code>\nresult = a[:, low:high]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 391, "library_problem_id": 100, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 392, "code": ["<code>\nresult = a[low:high+1]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 392, "library_problem_id": 101, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 393, "code": ["<code>\nresult = a[:, low:high]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 393, "library_problem_id": 102, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 394, "code": ["<code>\na = np.array([[0.5544, 0.4456], [0.8811, 0.1189]])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 394, "library_problem_id": 103, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 395, "code": ["<code>\ndef loguni(n, min, max, base=np.e):\n    return base**(np.random.uniform(np.log(min), np.log(max), n))\n\nresult = loguni(n, min, max)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 395, "library_problem_id": 104, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 396, "code": ["<code>\ndef log_uniform(n, min_val, max_val, base=np.exp(1)):\n    return base**np.random.uniform(min_val, max_val, n)\n\nresult = log_uniform(n, np.log(min), np.log(max))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 396, "library_problem_id": 105, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 104}}
{"id": 397, "code": ["<code>\n# Convert min and max to the log scale\n    min_log = np.log(min)\n    max_log = np.log(max)\n\n    # Generate n samples from a uniform distribution between min_log and max_log\n    samples_log = np.random.uniform(min_log, max_log, n)\n\n    # Convert the log-scale samples back to the original scale\n    samples = np.exp(samples_log)\n\n    return samples\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 397, "library_problem_id": 106, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 398, "code": ["<code>\ndef calculate_series_B(A, a, b):\n    B = a * A + b * A.shift(1)\n    B = B.fillna(0)  # fill the NaN values with 0\n    return B\n\nB = calculate_series_B(A, a, b)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 398, "library_problem_id": 107, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 399, "code": ["<code>\ndef compute_B(A, a, b, c):\n    B = pd.Series(index=A.index)\n    B[0] = a * A[0]\n    B[1] = a * A[1] + b * B[0]\n    for t in range(2, len(A)):\n        B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n    return B\n\nB = compute_B(A, a, b, c)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 399, "library_problem_id": 108, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 107}}
{"id": 400, "code": ["<code>\nresult = np.array([])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 400, "library_problem_id": 109, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 401, "code": ["<code>\nresult = np.empty((3,0))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 401, "library_problem_id": 110, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 109}}
{"id": 402, "code": ["<code>\nimport numpy as np\n\ndef sub2ind_fortran(dims, indices):\n    # Calculate the strides for each dimension\n    strides = np.array(dims[::-1])\n    strides[:-1] *= np.cumprod(dims[::-1])[1:]\n    # Calculate the linear index\n    linear_index = np.dot(indices, strides) - 1\n    return linear_index\n\ndims = (3, 4, 2)\nindex = (1, 0, 1)\n\nresult = sub2ind_fortran(dims, index)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 402, "library_problem_id": 111, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 403, "code": ["<code>\nresult = np.ravel_multi_index(index, dims, mode='C')\nprint(result)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 403, "library_problem_id": 112, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 404, "code": ["<code>\nvalues = np.zeros((2,3), dtype=[('', np.int32), ('', np.float32)])\ndf = pd.DataFrame(values, index=index, columns=columns)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 404, "library_problem_id": 113, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 405, "code": ["<code>\nresult = np.add.reduceat(a, np.append(0, np.where(np.diff(accmap))[0] + 1))\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 405, "library_problem_id": 114, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 406, "code": ["<code>\nunique_indices = np.unique(index)\nresult = np.max(a[index == np.max(unique_indices)])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 406, "library_problem_id": 115, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 407, "code": ["<code>\nresult = np.bincount(accmap, weights=a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 407, "library_problem_id": 116, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 408, "code": ["<code>\nunique_index = np.unique(index)\nresult = np.minimum.reduce(a[index==i])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 408, "library_problem_id": 117, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 114}}
{"id": 409, "code": ["<code>\nx = np.array(x)\ny = np.array(y)\nz = np.add(x, y)\n```\n</code>\nEND SOLUTION\nThe numpy library in python provides a very efficient way to perform operations on arrays. The np.add function is used to add two arrays element-wise. This is more efficient than using a for loop to iterate over the arrays and add the elements.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 409, "library_problem_id": 118, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 410, "code": ["<code>\nresult = np.random.choice(range(len(lista_elegir)), samples, p=probabilit)\nresult = [lista_elegir[i] for i in result]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 410, "library_problem_id": 119, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 411, "code": ["<code>\nresult = np.pad(a, ((1, 1), (1, 1)), 'constant')[1 + low_index : 3 + high_index, 1 + low_index : 3 + high_index]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 411, "library_problem_id": 120, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 412, "code": ["<code>\nresult = x[x >= 0]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 412, "library_problem_id": 121, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 413, "code": ["<code>\nresult = x[np.iscomplex(x)]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 413, "library_problem_id": 122, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 414, "code": ["<code>\ndef bin_and_mean(data, bin_size):\n    # Calculate the number of bins\n    num_bins = len(data) // bin_size\n    # Create the bins\n    bins = np.split(data[:num_bins*bin_size], num_bins)\n    # Calculate the mean of each bin\n    bin_data_mean = [np.mean(bin) for bin in bins]\n    return bin_data_mean\n\nbin_data_mean = bin_and_mean(data, bin_size)\nprint(bin_data_mean)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 414, "library_problem_id": 123, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 415, "code": ["<code>\ndef max_bin_data(data, bin_size):\n    # Calculate the number of bins\n    num_bins = len(data) // bin_size\n    # Create a view of the data array with the correct shape\n    bin_data = data[:num_bins*bin_size].reshape(num_bins, bin_size)\n    # Calculate the maximum of each bin\n    bin_data_max = np.max(bin_data, axis=1)\n    return bin_data_max\n\nbin_data_max = max_bin_data(data, bin_size)\nprint(bin_data_max)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 415, "library_problem_id": 124, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 416, "code": ["<code>\n# Solution\n# Calculate the number of bins\nnum_bins = data.shape[1] // bin_size\n\n# Reshape the data into bins\nbin_data = data[:, :num_bins*bin_size].reshape(data.shape[0], num_bins, bin_size)\n\n# Calculate the mean of each bin\nbin_data_mean = bin_data.mean(axis=2)\n\nbin_data_mean\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 416, "library_problem_id": 125, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 417, "code": ["<code>\ndef bin_and_mean(data, bin_size):\n    # Calculate the number of bins\n    n_bins = len(data) // bin_size\n    # Create an array of indices for the bins\n    bin_indices = np.arange(n_bins) * bin_size\n    # Reverse the array of indices\n    bin_indices = bin_indices[::-1]\n    # Create the bins\n    bins = np.split(data, bin_indices)[::-1]\n    # Calculate the mean of each bin\n    bin_data_mean = [np.mean(bin) for bin in bins]\n    return bin_data_mean\n\nbin_data_mean = bin_and_mean(data, bin_size)\nprint(bin_data_mean)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 417, "library_problem_id": 126, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 418, "code": ["<code>\n# Reverse the data\ndata = data[:, ::-1]\n\n# Calculate the number of bins\nn_bins = data.shape[1] // bin_size\n\n# Create the bins\nbins = np.array_split(data, n_bins, axis=1)\n\n# Calculate the mean of each bin\nbin_data_mean = np.array([[np.mean(bin) for bin in row] for row in bins])\n\n# Reverse the bin_data_mean to get the correct order\nbin_data_mean = bin_data_mean[:, ::-1]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 418, "library_problem_id": 127, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 419, "code": ["<code>\n# Calculate the number of bins\nnum_bins = data.shape[1] // bin_size\n\n# Create a mask to select the correct number of elements\nmask = np.arange(num_bins * bin_size).reshape(1, -1)\n\n# Apply the mask to the data\nbin_data = data[:, mask]\n\n# Calculate the mean of each bin\nbin_data_mean = bin_data.mean(axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 419, "library_problem_id": 128, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 420, "code": ["<code>\ndef smoothclamp(x, min_val, max_val):\n    t = (x-min_val) / (max_val-min_val) # Normalize x to range [0, 1]\n    t = max(0, min(1, t)) # Clamp t to [0, 1]\n    t = t*t*(3 - 2*t) # Smooth t\n    return t * (max_val - min_val) + min_val # Scale and shift to original range\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 420, "library_problem_id": 129, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 421, "code": ["<code>\ndef smoothclamp(x, x_min, x_max, N):\n    t = np.clip((x - x_min)/(x_max - x_min), 0.0, 1.0)\n    return x_min + (x_max - x_min) * (6*t**5 - 15*t**4 + 10*t**3) if N == 5 else x_min + (x_max - x_min) * t\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 421, "library_problem_id": 130, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 129}}
{"id": 422, "code": ["<code>\ndef circular_correlation(a, b):\n    # Pad the shorter array with zeros\n    max_len = max(len(a), len(b))\n    a = np.pad(a, (0, max_len - len(a)))\n    b = np.pad(b, (0, max_len - len(b)))\n\n    # Perform cross-correlation\n    result = np.correlate(a, b, mode='full')\n\n    # Return the circular part of the result\n    return result[max_len - 1:]\n\n# Test the function\na = np.array([1,2,3,4])\nb = np.array([5, 4, 3, 2])\nresult = circular_correlation(a, b)\nprint(result)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 422, "library_problem_id": 131, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 423, "code": ["<code>\nresult = df.groupby('major').apply(lambda x: x.values.reshape(1,4,5)).values.reshape(4,15,5)\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 423, "library_problem_id": 132, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 424, "code": ["<code>\nresult = df.groupby('major').apply(lambda x: x.values.reshape(1,4,5)).values\nresult = np.concatenate(result)\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 424, "library_problem_id": 133, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 132}}
{"id": 425, "code": ["<code>\nresult = np.array([list(np.binary_repr(num, width=m)) for num in a]).astype(int)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 425, "library_problem_id": 134, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 426, "code": ["<code>\nresult = np.array([list(np.binary_repr(num, width=m)) for num in a]).astype(int)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 426, "library_problem_id": 135, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 427, "code": ["<code>\nresult = np.array([list(np.binary_repr(num, width=m)) for num in a]).astype(int)\nresult = np.bitwise_xor.reduce(result, axis=0)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 427, "library_problem_id": 136, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 428, "code": ["<code>\nmean = np.mean(a)\nstd_dev = np.std(a)\nresult = (mean - 3*std_dev, mean + 3*std_dev)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 428, "library_problem_id": 137, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 429, "code": ["<code>\nmean = np.mean(a)\nstd_dev = np.std(a)\nresult = (mean - 2*std_dev, mean + 2*std_dev)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 429, "library_problem_id": 138, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 137}}
{"id": 430, "code": ["<code>\nmean = np.mean(a)\n    std_dev = np.std(a)\n    lower_bound = mean - 3 * std_dev\n    upper_bound = mean + 3 * std_dev\n    return (lower_bound, upper_bound)\n\nresult = f(example_a)\nprint(result)\n### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 430, "library_problem_id": 139, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 137}}
{"id": 431, "code": ["<code>\nmean = np.mean(a)\nstd_dev = np.std(a)\nlower_bound = mean - 2*std_dev\nupper_bound = mean + 2*std_dev\nresult = (a < lower_bound) | (a > upper_bound)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 431, "library_problem_id": 140, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 137}}
{"id": 432, "code": ["<code>\nimport numpy as np\nimport numpy.ma as ma\n\nDataArray = np.arange(-5.5, 10.5)\nmasked_data = ma.masked_less(DataArray, 0)\nprob = np.percentile(masked_data.compressed(), percentile)\nprint(prob)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 432, "library_problem_id": 141, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 433, "code": ["<code>\na[zero_rows, :] = 0\na[:, zero_cols] = 0\n```\nEND SOLUTION\n</code>\nThis will zero out the specified row and column.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 433, "library_problem_id": 142, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 142}}
{"id": 434, "code": ["<code>\na[zero_rows, :] = 0\na[:, zero_cols] = 0\n```\nEND SOLUTION\n</code>\nThis code will zero out the rows and columns specified by the `zero_rows` and `zero_cols` lists. The `:` operator is used to select all elements in a row or column.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 434, "library_problem_id": 143, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 435, "code": ["<code>\na[1:, 0] = 0  # zero out the first column\na[0, :] = 0  # zero out the first row\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 435, "library_problem_id": 144, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 436, "code": ["<code>\nmask = np.equal.outer(a, np.amax(a, axis=1))\n```\nEND SOLUTION\n</code>\nThe outer function creates a 2D array where each element is compared with the max value along the given axis. The result is a boolean mask where True indicates the max value along the axis and False elsewhere.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 436, "library_problem_id": 145, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 145}}
{"id": 437, "code": ["<code>\nmask = np.argmin(a, axis=1) == np.arange(a.shape[0])[:, None]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 437, "library_problem_id": 146, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 145}}
{"id": 438, "code": ["<code>\nimport scipy.stats\n\npost = [2, 5, 6, 10]\ndistance = [50, 100, 500, 1000]\n\nresult, _ = scipy.stats.pearsonr(post, distance)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 438, "library_problem_id": 147, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 439, "code": ["<code>\nresult = np.array([np.outer(X[:, i], X[:, i]) for i in range(X.shape[1])]).reshape(X.shape[1], X.shape[0], X.shape[0])\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 439, "library_problem_id": 148, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 440, "code": ["<code>\nU, S, VT = np.linalg.svd(Y, full_matrices=False)\nX = U.dot(np.diag(S)).dot(VT)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 440, "library_problem_id": 149, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 441, "code": ["<code>\nis_contained = number in a\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 441, "library_problem_id": 150, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 442, "code": ["<code>\nC = np.setdiff1d(A, B)\n```\n</code>\nEND SOLUTION\nThe np.setdiff1d function in numpy is used to find the set difference of two arrays. It returns the sorted, unique values in array1 that are not in array2. This function is faster than a for loop and is more efficient for large arrays.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 442, "library_problem_id": 151, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 443, "code": ["<code>\nimport numpy as np\n\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,2,8])\n\nC = A[np.in1d(A, B)]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 443, "library_problem_id": 152, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 444, "code": ["<code>\nC = A[np.in1d(A, B)]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 444, "library_problem_id": 153, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 445, "code": ["<code>\nresult = len(a) + 1 - rankdata(a).astype(int)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 445, "library_problem_id": 154, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 446, "code": ["<code>\nresult = len(a) - rankdata(a).astype(int)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 446, "library_problem_id": 155, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 154}}
{"id": 447, "code": ["<code>\nresult = len(a) + 1 - rankdata(a).astype(int)\n    return result\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 447, "library_problem_id": 156, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 154}}
{"id": 448, "code": ["<code>\ndists = np.dstack((x_dists, y_dists))\n```\nEND SOLUTION\n</code>\nThe np.dstack function is used to stack arrays in sequence depth wise (along third axis). It takes the arrays as input and returns a new array of the same type, with the arrays joined along the third axis. In this case, it's used to join the x_dists and y_dists arrays along the third axis, resulting in the desired output.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 448, "library_problem_id": 157, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 449, "code": ["<code>\ndists = np.dstack((x_dists, y_dists))\n```\nEND SOLUTION\n</code>\nThe np.dstack function is used to stack arrays in sequence depth wise (along third axis). It takes the arrays as input and returns a new array of the same type, with the arrays joined along the third axis. In this case, it's used to join the x_dists and y_dists arrays along the third axis, resulting in the desired output.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 449, "library_problem_id": 158, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 157}}
{"id": 450, "code": ["<code>\nresult = a[:, second, third]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 450, "library_problem_id": 159, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 159}}
{"id": 451, "code": ["<code>\narr = np.zeros((20,10,10,2))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 451, "library_problem_id": 160, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 452, "code": ["<code>\nresult = X / np.abs(X).sum(axis=1, keepdims=True)\n```\nEND SOLUTION\n</code>\nThe `np.abs(X).sum(axis=1, keepdims=True)` part calculates the sum of absolute values of each row, and `X / np.abs(X).sum(axis=1, keepdims=True)` divides each element of `X` by the corresponding element in the sum, effectively normalizing each row. The `keepdims=True` option ensures that the sum is kept as a 2D array, not a 1D array, so that the division operation can be performed element-wise.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 452, "library_problem_id": 161, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 161}}
{"id": 453, "code": ["<code>\nresult = X / np.linalg.norm(X, ord=2, axis=1, keepdims=True)\n```\n</code>\nEND SOLUTION\nThis solution uses numpy's broadcasting feature to divide each row of X by its L2 norm. The 'axis' parameter is set to 1 to divide along each row, and 'keepdims' is set to True to ensure the result has the same number of dimensions as X.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 453, "library_problem_id": 162, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 454, "code": ["<code>\nresult = np.divide(X, np.abs(X).max(axis=1).reshape(-1, 1))\n```\n</code>\nEND SOLUTION\nThis solution uses numpy's divide function to divide each element of the array by the corresponding element in the reshaped version of the array of the maximum absolute values of the rows. The reshape function is used to make the array the same shape as X, so that we can divide each row by the corresponding element in the array of maximum absolute values.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 454, "library_problem_id": 163, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 455, "code": ["<code>\nconditions = [df['a'].astype(str).str.contains(target)]\nresult = np.select(conditions, choices, default=np.nan)\ndf['result'] = result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 455, "library_problem_id": 164, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 164}}
{"id": 456, "code": ["<code>\nfrom scipy.spatial.distance import cdist\nresult = cdist(a, a)\n```\n</code>\nEND SOLUTION\nThe cdist function from scipy.spatial.distance module calculates the distance between each pair of the two collections of inputs. It returns a matrix where the entry at i,j is the distance between the i-th and j-th inputs.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 456, "library_problem_id": 165, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 457, "code": ["<code>\nfrom scipy.spatial.distance import cdist\nresult = cdist(a, a)\n```\nEND SOLUTION\n</code>\nThe cdist function from scipy.spatial.distance module calculates the distance between each pair of the two collections of inputs. It returns a matrix where the entry at i,j is the distance between the i-th and j-th inputs.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 457, "library_problem_id": 166, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 165}}
{"id": 458, "code": ["<code>\nfrom scipy.spatial.distance import cdist\nresult = cdist(a, a)\n```\nEND SOLUTION\n</code>\nThe cdist function from scipy.spatial.distance module calculates the distance between each pair of the two collections of inputs. It returns a matrix where the entry at row i and column j is the distance between the i-th and j-th inputs.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 458, "library_problem_id": 167, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 165}}
{"id": 459, "code": ["<code>\n# Convert the strings to floats\nNA = NA.astype(float)\n\n# Calculate the mean\nAVG = np.mean(NA)\n\nprint(AVG)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 459, "library_problem_id": 168, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 168}}
{"id": 460, "code": ["<code>\n# Convert the string elements to float and 'inf' to np.inf\nNA = np.array([np.inf if a == 'inf' else float(a) for a in A])\n\n# Calculate the mean\nAVG = np.mean(NA)\n\nprint(AVG)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 460, "library_problem_id": 169, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 168}}
{"id": 461, "code": ["<code>\nimport numpy as np\nA = ['np.inf', '33.33', '33.33', '33.37']\nNA = np.array([np.inf if x == 'np.inf' else float(x) for x in A])\nAVG = np.mean(NA)\nprint(AVG)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 461, "library_problem_id": 170, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 168}}
{"id": 462, "code": ["<code>\nresult = np.diff(a[a!=0])\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 462, "library_problem_id": 171, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 171}}
{"id": 463, "code": ["<code>\nresult = np.where(a[:-1] != a[1:], a, 0)\nresult = result[result.flatten() != 0]\nresult = result.reshape(-1, 1)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 463, "library_problem_id": 172, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 171}}
{"id": 464, "code": ["<code>\ndf = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\ndf.reset_index(drop=True, inplace=True)\ndf\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 464, "library_problem_id": 173, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 465, "code": ["<code>\ndf = pd.DataFrame({\n        'lat': lat.flatten(),\n        'lon': lon.flatten(),\n        'val': val.flatten()\n    })\n    return df\n\ndf = f(example_lat, example_lon, example_val)\nprint(df)\n### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 465, "library_problem_id": 174, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 173}}
{"id": 466, "code": ["<code>\n# Flatten the arrays\nlat_flat = lat.flatten()\nlon_flat = lon.flatten()\nval_flat = val.flatten()\n\n# Create a dataframe\ndf = pd.DataFrame({'lat': lat_flat, 'lon': lon_flat, 'val': val_flat})\n\n# Add a column 'maximum' with the maximum value of each row\ndf['maximum'] = df.max(axis=1)\n\ndf\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 466, "library_problem_id": 175, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 173}}
{"id": 467, "code": ["<code>\ndef moving_window_2D(arr, window_size):\n    h, w = arr.shape\n    window_h, window_w = window_size\n    pad_h = window_h // 2\n    pad_w = window_w // 2\n    padded_arr = np.pad(arr, ((pad_h, pad_h), (pad_w, pad_w)))\n    result = []\n    for i in range(pad_h, h + pad_h):\n        for j in range(pad_w, w + pad_w):\n            if i - pad_h < pad_h or i + pad_h >= h or j - pad_w < pad_w or j + pad_w >= w:\n                result.append(padded_arr[i - pad_h:i + pad_h + 1, j - pad_w:j + pad_w + 1])\n    return result\n\nresult = moving_window_2D(a, size)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 467, "library_problem_id": 176, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 176}}
{"id": 468, "code": ["<code>\ndef moving_window_2D(arr, window_size):\n    h, w = arr.shape\n    window_h, window_w = window_size\n    pad_h = window_h // 2\n    pad_w = window_w // 2\n    padded_arr = np.pad(arr, ((pad_h, pad_h), (pad_w, pad_w)))\n    result = []\n    for i in range(pad_h, h + pad_h):\n        for j in range(pad_w, w + pad_w):\n            if i - pad_h < pad_h or i + pad_h >= h or j - pad_w < pad_w or j + pad_w >= w:\n                result.append(padded_arr[i - pad_h:i + pad_h + 1, j - pad_w:j + pad_w + 1])\n    return result\n\nresult = moving_window_2D(a, size)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 468, "library_problem_id": 177, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 176}}
{"id": 469, "code": ["<code>\nresult = np.nanmean(a, axis=0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 469, "library_problem_id": 178, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 178}}
{"id": 470, "code": ["<code>\ndef f(a = np.array([1 + 0j, 2 + 3j, np.inf + 0j])):\n    # Convert the array to a list of real numbers\n    a = a.tolist()\n    # Replace the complex infinity with a large real number\n    a = [x.real if np.isinf(x) else x for x in a]\n    # Compute the mean\n    result = np.mean(a)\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 470, "library_problem_id": 179, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 178}}
{"id": 471, "code": ["<code>\nresult = Z[..., -1:]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 471, "library_problem_id": 180, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 180}}
{"id": 472, "code": ["<code>\nresult = a[-1:]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 472, "library_problem_id": 181, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 180}}
{"id": 473, "code": ["<code>\n</code>\nEND SOLUTION\n\nThis will return True if `c` is in `CNTS` and False otherwise. The `np.array_equal` function is used to compare the numpy arrays element-wise.\n\nFor removing an array from a list, you can use the list comprehension with the `remove` method:\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 473, "library_problem_id": 182, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 182}}
{"id": 474, "code": ["<code>\ndef is_member(c, CNTS):\n    for i in CNTS:\n        if np.array_equal(c, i):\n            return True\n    return False\n\nresult = is_member(c, CNTS)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 474, "library_problem_id": 183, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 182}}
{"id": 475, "code": ["<code>\nx = np.linspace(0, 1, 2)\ny = np.linspace(0, 1, 2)\nf = intp.interp2d(x, y, a, kind='linear')\nresult = f(x_new, y_new)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 475, "library_problem_id": 184, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 184}}
{"id": 476, "code": ["<code>\ndf['Q_cum'] = df.groupby('D')['Q'].cumsum()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 476, "library_problem_id": 185, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 477, "code": ["<code>\ni = np.diag(i)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 477, "library_problem_id": 186, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 186}}
{"id": 478, "code": ["<code>\nimport numpy as np\n\na = np.array([[1,0,2,3],[0,5,3,4],[2,3,2,10],[3,4, 10, 7]])\n\nmask = np.ones_like(a, dtype=bool)\nnp.fill_diagonal(mask, 0)\na[mask] = 0\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 478, "library_problem_id": 187, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 187}}
{"id": 479, "code": ["<code>\nimport numpy as np\nimport pandas as pd\n\nstart = \"23-FEB-2015 23:09:19.445506\"\nend = \"24-FEB-2015 01:09:22.404973\"\nn = 50\n\nt0 = pd.to_datetime(start)\ntf = pd.to_datetime(end)\n\nseries = pd.date_range(start=t0, end=tf, periods=n)\n\nresult = series\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 479, "library_problem_id": 188, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 480, "code": ["<code>\nresult = np.where((x == a) & (y == b))[0]\nif result.size > 0:\n    result = result[0]\nelse:\n    result = -1\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 480, "library_problem_id": 189, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 189}}
{"id": 481, "code": ["<code>\nresult = np.where((x == a) & (y == b))[0]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 481, "library_problem_id": 190, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 189}}
{"id": 482, "code": ["<code>\n# We can use numpy's polyfit function to solve this.\n# The degree of the polynomial is len(x) - 1, which is 3 in this case.\ncoeff = np.polyfit(x, y, len(x) - 1)\n\n# The coefficients are returned in reverse order of their degree,\n# so we reverse them to get the result in the order [a, b, c].\nresult = coeff[::-1]\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 482, "library_problem_id": 191, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 191}}
{"id": 483, "code": ["<code>\n# Use numpy's polyfit function to find the coefficients\ncoefficients = np.polyfit(x, y, degree)\n\n# The coefficients are returned in reverse order of their degree\nresult = coefficients[::-1]\n```\nEND SOLUTION\n</code>\nThe result will be an array of coefficients, from highest order to lowest order. The polyfit function uses the method of least squares to fit a polynomial of a given degree to a set of data.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 483, "library_problem_id": 192, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 191}}
{"id": 484, "code": ["<code>\ndf = df.sub(a, axis=0)\n```\nEND SOLUTION\n</code>\nThe solution uses the sub() function of pandas DataFrame which subtracts the corresponding elements of the DataFrame from the elements of the series. The axis=0 argument ensures that the subtraction is done column-wise.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 484, "library_problem_id": 193, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 193}}
{"id": 485, "code": ["<code>\nresult = np.einsum('ijk,kl->ijl', A, B)\n```\n</code>\nEND SOLUTION\nThis solution uses np.einsum with the proper indices to perform the matrix multiplication. The 'ijk,kl->ijl' specifies that the first two dimensions of A (i, j) are the same as the first two dimensions of the result (i, j), the third dimension of A (k) is the same as the first dimension of B (k), and the last dimension of A (l) is the same as the second dimension of B (l).\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 485, "library_problem_id": 194, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 486, "code": ["<code>\n# Flatten the array first\na = a.flatten()\n\n# Initialize MinMaxScaler\nscaler = MinMaxScaler()\n\n# Fit and transform the array\nresult = scaler.fit_transform(a.reshape(-1, 1))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 486, "library_problem_id": 195, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 195}}
{"id": 487, "code": ["<code>\n# Initialize MinMaxScaler\nscaler = MinMaxScaler()\n\n# Fit and transform the data\nresult = scaler.fit_transform(arr)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 487, "library_problem_id": 196, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 488, "code": ["<code>\n# Flatten the 3D array to 2D\nb = a.reshape(-1, a.shape[-1])\n\n# Initialize MinMaxScaler\nscaler = MinMaxScaler()\n\n# Fit and transform the data\nb_scaled = scaler.fit_transform(b)\n\n# Reshape the data back to 3D\nresult = b_scaled.reshape(a.shape[0], a.shape[1], a.shape[2])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 488, "library_problem_id": 197, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 489, "code": ["<code>\narr_temp = arr.copy()\nmask1 = arr_temp < -10\nmask2 = arr_temp >= 15\n\n# Combine the masks\nmask3 = np.logical_or(mask1, mask2)\n\n# Apply the masks\narr[mask1] = 0\narr[mask2] = 30\narr[~mask3] = arr_temp[~mask3] + 5\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 489, "library_problem_id": 198, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 198}}
{"id": 490, "code": ["<code>\nimport numpy as np\n\narr = (np.random.rand(5, 50)-0.5) * 50\nn1 = [1,2,3,4,5]\nn2 = [6,7,8,9,10]\n\nfor i in range(arr.shape[0]):\n    arr_temp = arr[i].copy()\n    mask1 = arr_temp < n1[i]\n    mask2 = arr_temp >= n2[i]\n    arr[i][mask1] = 0\n    arr[i][~mask1 & ~mask2] = arr_temp[~mask1 & ~mask2] + 5\n    arr[i][mask2] = 30\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 490, "library_problem_id": 199, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 198}}
{"id": 491, "code": ["<code>\nresult = np.sum(np.abs(s1 - s2) > 1e-10)\n```\n</code>\nEND SOLUTION\nThis solution uses the absolute difference between the two sums and checks if it is greater than a very small tolerance (1e-10). If it is, it counts it as a difference. This should give you the number of elements where the sums are different, even if they are very close.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 491, "library_problem_id": 200, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 200}}
{"id": 492, "code": ["<code>\nresult = np.sum(~np.isclose(s1, s2))\n```\n</code>\nEND SOLUTION\nThis solution uses the np.isclose function to compare the elements of s1 and s2. The ~ operator is used to invert the boolean result of np.isclose, so that True values are now False and vice versa. Finally, np.sum is used to count the number of True values, which represent the number of truly different elements in s1 and s2.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 492, "library_problem_id": 201, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 200}}
{"id": 493, "code": ["<code>\nresult = all(np.array_equal(a[0], arr) for arr in a)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 493, "library_problem_id": 202, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 494, "code": ["<code>\nresult = all(np.isnan(arr).all() for arr in a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 494, "library_problem_id": 203, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 495, "code": ["<code>\nresult = np.pad(a, ((0, shape[0] - a.shape[0]), (0, shape[1] - a.shape[1])))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 495, "library_problem_id": 204, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 204}}
{"id": 496, "code": ["<code>\nresult = np.pad(a, ((0, shape[0] - a.shape[0]), (0, shape[1] - a.shape[1])))\n```\n</code>\nEND SOLUTION\nThis will pad the array `a` with zeros to match the shape `(93, 13)`. The padding is added to the right and bottom of the original array. The number of rows and columns to pad is calculated by subtracting the original shape from the desired shape.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 496, "library_problem_id": 205, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 497, "code": ["<code>\nresult = np.pad(a, ((0, shape[0] - a.shape[0]), (0, shape[1] - a.shape[1])), mode='constant', constant_values=element)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 497, "library_problem_id": 206, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 204}}
{"id": 498, "code": ["<code>\nresult = np.pad(arr, ((0, shape[0] - arr.shape[0]), (0, shape[1] - arr.shape[1])), mode='constant')\n    return result\n### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 498, "library_problem_id": 207, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 499, "code": ["<code>\n# Calculate the padding widths\npad_width = [(0, max(shape[0] - a.shape[0], 0)), (0, max(shape[1] - a.shape[1], 0))]\n\n# Pad the array\nresult = np.pad(a, pad_width, mode='constant')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 499, "library_problem_id": 208, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 204}}
{"id": 500, "code": ["<code>\nimport numpy as np\n\n# Create a numpy array\na = np.arange(3 * 12)\n\n# Reshape the array\na = a.reshape(4, 3)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 500, "library_problem_id": 209, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 501, "code": ["<code>\nresult = np.take_along_axis(a, np.expand_dims(b, axis=-1), axis=-1).squeeze(-1)\nprint(result)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 501, "library_problem_id": 210, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 210}}
{"id": 502, "code": ["<code>\nresult = np.take_along_axis(a, np.expand_dims(b, axis=-1), axis=-1).squeeze(-1)\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 502, "library_problem_id": 211, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 210}}
{"id": 503, "code": ["<code>\nresult = np.take_along_axis(a, np.expand_dims(b, axis=-1), axis=-1).squeeze(-1)\nprint(result)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 503, "library_problem_id": 212, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 210}}
{"id": 504, "code": ["<code>\nresult = np.sum(a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1])[None, :, None], b[None, :, :]])\nprint(result)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 504, "library_problem_id": 213, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 505, "code": ["<code>\n# Create a mask of the same shape as a using np.arange to create a 3D array of indices\nmask = np.arange(a.shape[2])[None, None, :]\n\n# Use the mask to index a and sum along the third dimension\nresult = np.sum(a[np.arange(a.shape[0])[:, None, None], np.arange(a.shape[1])[None, :, None], b[None, :, :]])\n\n# Print the result\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 505, "library_problem_id": 214, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 506, "code": ["<code>\nresult = df.loc[df['a'].between(2, 4), 'b']\nresult = result.reset_index(drop=True)\nresult</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 506, "library_problem_id": 215, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 507, "code": ["<code>\ndef process_image(im):\n    # Find the indices of the first and last non-zero elements in each row\n    row_indices = np.where(np.any(im, axis=1))[0]\n    first_row, last_row = row_indices[0], row_indices[-1]\n\n    # Find the indices of the first and last non-zero elements in each column\n    col_indices = np.where(np.any(im, axis=0))[0]\n    first_col, last_col = col_indices[0], col_indices[-1]\n\n    # Return the subarray of the image within these indices\n    return im[first_row:last_row+1, first_col:last_col+1]\n\nresult = process_image(im)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 507, "library_problem_id": 216, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 216}}
{"id": 508, "code": ["<code>\ndef trim_zeros(A):\n    nonzero_rows = np.any(A, axis=1)\n    nonzero_cols = np.any(A, axis=0)\n    return A[np.ix_(nonzero_rows, nonzero_cols)]\n\nresult = trim_zeros(A)\nprint(result)\n```\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 508, "library_problem_id": 217, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 216}}
{"id": 509, "code": ["<code>\ndef process_image(im):\n    # Find the first and last non-zero indices in each row\n    row_indices = np.argwhere(im > 0)\n    row_min = row_indices[:,1].min()\n    row_max = row_indices[:,1].max()\n\n    # Find the first and last non-zero indices in each column\n    col_indices = np.argwhere(im.T > 0)\n    col_min = col_indices[:,1].min()\n    col_max = col_indices[:,1].max()\n\n    # Return the subarray within the indices\n    return im[row_min:row_max+1, col_min:col_max+1]\n\nresult = process_image(im)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 509, "library_problem_id": 218, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 510, "code": ["<code>\ndef process_image(im):\n    rows = np.any(im, axis=1)\n    cols = np.any(im, axis=0)\n    rmin, rmax = np.where(rows)[0].reshape(2,1)\n    cmin, cmax = np.where(cols)[0].reshape(2,1)\n    return im[rmin:rmax+1, cmin:cmax+1]\n\nresult = process_image(im)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 510, "library_problem_id": 219, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 511, "code": ["<code>\nplt.figure(figsize=(8, 6))\nplt.scatter(x, y, label='x-y')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 511, "library_problem_id": 0, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 512, "code": ["<code>\nplt.minorticks_y()\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 512, "library_problem_id": 1, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 513, "code": ["<code>\nplt.minorticks_on()\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 513, "library_problem_id": 2, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 514, "code": ["<code>\nplt.minorticks_x()\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 514, "library_problem_id": 3, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 515, "code": ["<code>\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 515, "library_problem_id": 4, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 516, "code": ["<code>\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 516, "library_problem_id": 5, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 517, "code": ["<code>\nplt.plot(x, y, marker='d', linestyle='-', linewidth=1)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 517, "library_problem_id": 6, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 518, "code": ["<code>\nplt.plot(x, y, marker='D', markersize=10, linestyle='-')\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 518, "library_problem_id": 7, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 519, "code": ["<code>\nax.set_ylim(0, 40)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 519, "library_problem_id": 8, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 8}}
{"id": 520, "code": ["<code>\nplt.axvspan(2, 4, color='red')\n\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 520, "library_problem_id": 9, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 521, "code": ["<code>\nplt.plot([0, 1], [0, 2])\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 521, "library_problem_id": 10, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 522, "code": ["<code>\nplt.plot([0, 1], [0, 2])\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 522, "library_problem_id": 11, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 523, "code": ["<code>\nseaborn.relplot(x=\"Height (cm)\", y=\"Weight (kg)\", hue=\"Gender\", data=df, kind=\"scatter\")\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 523, "library_problem_id": 12, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 12}}
{"id": 524, "code": ["<code>\n# SOLUTION END\nplt.figure(figsize=(10, 6))\nsns.set(style=\"whitegrid\")\nsns.regplot(x=x, y=y)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 524, "library_problem_id": 13, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 525, "code": ["<code>\n# Convert the arrays to pandas DataFrame\ndf = pd.DataFrame({'x': x, 'y': y})\n\n# Use seaborn to plot the data\nsns.lineplot(x='x', y='y', data=df)\n\n# Display the plot\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 525, "library_problem_id": 14, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 526, "code": ["<code>\nplt.plot(x, y, marker='+', markersize=7)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 526, "library_problem_id": 15, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 527, "code": ["<code>\nplt.legend(fontsize=20)\n\n# show the plot\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 527, "library_problem_id": 16, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 528, "code": ["<code>\nplt.plot(x, y, label='xyz')\nplt.legend(title='Legend', title_fontsize='20')\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 528, "library_problem_id": 17, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 529, "code": ["<code>\nl.set_color('b')\nl.set_alpha(0.2)\n\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 529, "library_problem_id": 18, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 530, "code": ["<code>\nl.set_color('black')\nl.set_linewidth(5)\n# SOLUTION END\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 530, "library_problem_id": 19, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 531, "code": ["<code>\nl.set_color('r')\nl.set_markerfacecolor('r')\n# SOLUTION END\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 531, "library_problem_id": 20, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 532, "code": ["<code>\nplt.xticks(rotation=45)\n# SOLUTION END\n\n# add a legend\n# SOLUTION START\nplt.legend()\n# SOLUTION END\n\n# show the plot\n# SOLUTION START\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 532, "library_problem_id": 21, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 21}}
{"id": 533, "code": ["<code>\nplt.xticks(rotation=-45)\n# SOLUTION END\n\n# add a legend\n# SOLUTION START\nplt.legend()\n# SOLUTION END\n\n# show the plot\n# SOLUTION START\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 533, "library_problem_id": 22, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 534, "code": ["<code>\nplt.xticks(np.arange(0, max(x)+1, 2*np.pi/5))\n\n# put a y axis ticklabels at -1, 0, 1...\n# SOLUTION START\nplt.yticks(np.arange(-1, 1.5, 0.5))\n\n# add a legend\nplt.legend()\n\n# show the plot\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 534, "library_problem_id": 23, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 535, "code": ["<code>\nplt.legend(loc='upper right')\n# SOLUTION END\n\n# show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 535, "library_problem_id": 24, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 536, "code": ["<code>\nplt.imshow(H, cmap='hot', interpolation='nearest')\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 536, "library_problem_id": 25, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 537, "code": ["<code>\nplt.imshow(H, cmap='gray')\nplt.colorbar()\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 537, "library_problem_id": 26, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 538, "code": ["<code>\nplt.xlabel(\"X\")\nplt.xticks(rotation=90)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 538, "library_problem_id": 27, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 539, "code": ["<code>\ng.set_xticklabels(g.get_xticklabels(), rotation=90)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 539, "library_problem_id": 28, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 540, "code": ["<code>\n# SOLUTION END\n\nplt.title(myTitle)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 540, "library_problem_id": 29, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 541, "code": ["<code>\n# SOLUTION END\nplt.plot(x, y)\nplt.gca().invert_yaxis()\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 541, "library_problem_id": 30, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 542, "code": ["<code>\nplt.xticks([0, 1.5])\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 542, "library_problem_id": 31, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 31}}
{"id": 543, "code": ["<code>\nplt.yticks([-1, 1])\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 543, "library_problem_id": 32, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 31}}
{"id": 544, "code": ["<code>\nplt.figure(figsize=(10,5))\n\n# Plotting x\nplt.subplot(1, 3, 1)\nplt.scatter(range(10), x, color='r')\nplt.title('x')\n\n# Plotting y\nplt.subplot(1, 3, 2)\nplt.scatter(range(10), y, color='g')\nplt.title('y')\n\n# Plotting z\nplt.subplot(1, 3, 3)\nplt.scatter(range(10), z, color='b')\nplt.title('z')\n\nplt.tight_layout()\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 544, "library_problem_id": 33, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 545, "code": ["<code>\nplt.scatter(x, y, facecolor='blue', edgecolor='black')\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 545, "library_problem_id": 34, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 546, "code": ["<code>\nfig, ax = plt.subplots()\n\nax.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\nax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n\n# SOLUTION END\n\nplt.plot(x, y, 'o-')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 546, "library_problem_id": 35, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 547, "code": ["<code>\nplt.gca().yaxis.get_major_formatter().labelOnlyBase = False\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 547, "library_problem_id": 36, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 548, "code": ["<code>\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 548, "library_problem_id": 37, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 549, "code": ["<code>\nfig, axs = plt.subplots(2, sharex=True)\naxs[0].plot(x, y1)\naxs[1].plot(x, y2)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 549, "library_problem_id": 38, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 550, "code": ["<code>\nfig, axs = plt.subplots(2)\n\n# Plot x vs y1 in the first subplot\naxs[0].plot(x, y1)\naxs[0].set_title('Sine Function')\naxs[0].spines['top'].set_color('none')\naxs[0].spines['bottom'].set_color('none')\naxs[0].spines['left'].set_color('none')\naxs[0].spines['right'].set_color('none')\n\n# Plot x vs y2 in the second subplot\naxs[1].plot(x, y2)\naxs[1].set_title('Cosine Function')\naxs[1].spines['top'].set_color('none')\naxs[1].spines['bottom'].set_color('none')\naxs[1].spines['left'].set_color('none')\naxs[1].spines['right'].set_color('none')\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 550, "library_problem_id": 39, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 551, "code": ["<code>\nplt.xlabel('')\n# SOLUTION END\n\n# remove y axis label\n# SOLUTION START\nplt.ylabel('')\n# SOLUTION END\n\n# remove title\n# SOLUTION START\nplt.title('')\n# SOLUTION END\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 551, "library_problem_id": 40, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 552, "code": ["<code>\nplt.xticks([])\n# SOLUTION END\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 552, "library_problem_id": 41, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 40}}
{"id": 553, "code": ["<code>\nplt.xticks([3, 4])\nplt.grid(axis='x')\n\n# show the plot\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 553, "library_problem_id": 42, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 554, "code": ["<code>\nplt.yticks([3, 4])\nplt.grid(axis='y')\n\n# SOLUTION END\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 554, "library_problem_id": 43, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 555, "code": ["<code>\nplt.yticks([3, 4])\nplt.grid(axis='y')\nplt.xticks([1, 2])\nplt.grid(axis='x')\n\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 555, "library_problem_id": 44, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 556, "code": ["<code>\nplt.grid(True)\n# SOLUTION END\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 556, "library_problem_id": 45, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 557, "code": ["<code>\nplt.legend(loc='lower right')\n\n# SOLUTION END\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 557, "library_problem_id": 46, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 46}}
{"id": 558, "code": ["<code>\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6))\naxes = axes.flatten()\n\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\n\n# Adjust the subplot padding to have enough space to display axis labels\nplt.tight_layout()\n\nplt.show()\nplt.clf()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 558, "library_problem_id": 47, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 559, "code": ["<code>\nplt.plot(x, y, label='Y')\nplt.plot(x, z, label='Z')\nplt.legend()\n\n# SOLUTION END\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 559, "library_problem_id": 48, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 560, "code": ["<code>\n# SOLUTION END\nax.xaxis.tick_top()\nax.xaxis.set_label_position('top')\n\n# Set the x-axis and y-axis labels\nax.set_xticks(np.arange(data.shape[0]) + 0.5, minor=False)\nax.set_yticks(np.arange(data.shape[1]) + 0.5, minor=False)\nax.set_xticklabels(row_labels, minor=False)\nax.set_yticklabels(column_labels, minor=False)\n\n# Rotate the x-axis labels\nplt.xticks(rotation=90)\n\n# Set the heatmap title\nax.set_title('Heatmap')\n\n# Set the x-axis and y-axis limits\nax.set_xlim(0, data.shape[0])\nax.set_ylim(0, data.shape[1])\n\n# Add the colorbar\nfig.colorbar(heatmap)\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 560, "library_problem_id": 49, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 561, "code": ["<code>\nplt.plot(x, y)\nplt.xlabel('X', labelpad=20)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 561, "library_problem_id": 50, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 562, "code": ["<code>\nplt.plot(x, y)\nplt.xticks([])\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 562, "library_problem_id": 51, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 563, "code": ["<code>\nplt.plot(x, y)\nplt.yticks(ticks=range(0, max(y)+1, 1), labels=range(0, max(y)+1, 1))\nplt.yticks(rotation=90)\nplt.ylabel('Y-axis')\nplt.xlabel('X-axis')\nplt.title('Plot of Y over X')\nplt.grid(True)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 563, "library_problem_id": 52, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 564, "code": ["<code>\nplt.plot(x, y)\nplt.ylabel('Y')\nplt.yticks(y)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 564, "library_problem_id": 53, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 52}}
{"id": 565, "code": ["<code>\njoint_reg_plot = sns.jointplot(x='total_bill', y='tip', data=tips, kind='reg', line_kws={'color':'green'}, scatter_kws={'color':'green'}, color='blue')\njoint_reg_plot.ax_marg_x.hist(tips['total_bill'], color='blue', alpha=.6, bins=20)\njoint_reg_plot.ax_marg_y.hist(tips['tip'], color='blue', alpha=.6, bins=20, orientation=\"horizontal\")\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 565, "library_problem_id": 54, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 566, "code": ["<code>\njoint_reg_plot = sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='reg', line_kws={'color': 'green'})\njoint_reg_plot.ax_marg_x.hist(tips[\"total_bill\"], color=\"blue\", alpha=.6, bins=10)\njoint_reg_plot.ax_marg_y.hist(tips[\"tip\"], color=\"blue\", alpha=.6, bins=10, orientation=\"horizontal\")\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 566, "library_problem_id": 55, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 567, "code": ["<code>\njoint_reg_plot = sns.jointplot(x='total_bill', y='tip', data=tips, kind='reg')\n\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 567, "library_problem_id": 56, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 568, "code": ["<code>\nfig, ax = plt.subplots()\n\nax.bar(df['celltype'], df['s1'], label='s1')\nax.bar(df['celltype'], df['s2'], label='s2')\n\nax.set_xlabel('celltype')\nax.legend()\n\nplt.xticks(rotation=90)\n\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 568, "library_problem_id": 57, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 569, "code": ["<code>\nfig, ax = plt.subplots()\n\nax.bar(df['celltype'], df['s1'], label='s1')\nax.bar(df['celltype'], df['s2'], label='s2')\n\nax.set_xlabel('celltype')\nax.legend()\n\nplt.xticks(rotation=45)\n\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 569, "library_problem_id": 58, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 57}}
{"id": 570, "code": ["<code>\nplt.plot(x, y)\nplt.xlabel('X', color='red')\nplt.xticks(color='red')\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 570, "library_problem_id": 59, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 571, "code": ["<code>\nplt.plot(x, y, color='red')\nplt.xlabel('X')\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 571, "library_problem_id": 60, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 59}}
{"id": 572, "code": ["<code>\nplt.plot(x, y)\nplt.xticks(rotation='vertical')\nplt.tick_params(axis='x', which='major', labelsize=10)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 572, "library_problem_id": 61, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 573, "code": ["<code>\nimport matplotlib.pyplot as plt\n\ndef draw_vertical_lines(x_positions):\n    for x in x_positions:\n        plt.axvline(x, color='r')\n\n# draw vertical lines at [0.22058956, 0.33088437, 2.20589566]\nx_positions = [0.22058956, 0.33088437, 2.20589566]\ndraw_vertical_lines(x_positions)\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 573, "library_problem_id": 62, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 574, "code": ["<code>\nimport seaborn as sns\nimport pandas as pd\n\n# Convert the numpy array to pandas DataFrame\ndf = pd.DataFrame(rand_mat, columns=xlabels, index=ylabels)\n\n# Create a heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(df, annot=True, cmap='viridis')\n\n# Set the x-axis tick labels on top\nplt.xticks(rotation=45, ha='right')\n\n# Invert the order of the y-axis labels\nplt.gca().invert_yaxis()\n\n# Show the plot\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 574, "library_problem_id": 63, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 575, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\n\nrc(\"mathtext\", default=\"regular\")\n\ntime = np.arange(10)\ntemp = np.random.random(10) * 30\nSwdown = np.random.random(10) * 100 - 10\nRn = np.random.random(10) * 100 - 10\n\nfig, ax = plt.subplots(2, 1)\n\nax[0].plot(time, Swdown, \"-\", label=\"Swdown\")\nax[0].plot(time, Rn, \"-\", label=\"Rn\")\nax[0].legend(loc=0)\nax[0].grid()\nax[0].set_xlabel(\"Time (h)\")\nax[0].set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax[0].set_ylim(-20, 100)\n\nax2 = ax[1].twinx()\nax2.plot(time, temp, \"-r\", label=\"temp\")\nax2.legend(loc=0)\nax2.grid()\nax2.set_xlabel(\"Time (h)\")\nax2.set_ylabel(r\"Temperature ($^\\circ$C)\")\nax2.set_ylim(0, 35)\n\nplt.show()\nplt.clf()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 575, "library_problem_id": 64, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 576, "code": ["<code>\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\naxs[0].plot(x, y)\naxs[0].set_title('Y')\n\naxs[1].plot(x, y)\naxs[1].set_title('Y')\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 576, "library_problem_id": 65, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 577, "code": ["<code>\nsns.scatterplot(data=df, x=\"bill_length_mm\", y=\"bill_depth_mm\", s=30)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 577, "library_problem_id": 66, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 578, "code": ["<code>\n# Create a scatter plot\nplt.scatter(b, a)\n\n# Annotate each data point with correspond numbers in c\nfor i, txt in enumerate(c):\n    plt.annotate(txt, (b[i], a[i]))\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 578, "library_problem_id": 67, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
{"id": 579, "code": ["<code>\nplt.plot(x, y, label=\"y over x\")\nplt.legend(title=\"Legend\")\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 579, "library_problem_id": 68, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 580, "code": ["<code>\nplt.plot(x, y, label=\"y over x\")\nplt.legend(title=\"Legend\", title_fontsize='large', title_fontweight='bold')\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 580, "library_problem_id": 69, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 581, "code": ["<code>\nplt.hist(x, edgecolor=\"black\", linewidth=1.2)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 581, "library_problem_id": 70, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 582, "code": ["<code>\nfig, axs = plt.subplots(1, 2, gridspec_kw={'width_ratios':[3, 1]})\naxs[0].plot(x, y)\naxs[1].plot(y, x)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 582, "library_problem_id": 71, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 583, "code": ["<code>\n# Plotting histograms\nplt.hist(x, bins=bins, alpha=0.5, label='x', color='r')\nplt.hist(y, bins=bins, alpha=0.5, label='y', color='b')\n\n# Adding legend\nplt.legend(loc='upper right')\n\n# Showing the plot\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 583, "library_problem_id": 72, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 584, "code": ["<code>\nplt.figure(figsize=(10, 6))\n\n# Create a histogram for x\nplt.hist(x, bins=5, color='b', label='x', alpha=0.7)\n\n# Create a histogram for y\nplt.hist(y, bins=5, color='r', label='y', alpha=0.7)\n\n# Add legend\nplt.legend()\n\n# Show the plot\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 584, "library_problem_id": 73, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 585, "code": ["<code>\nplt.plot([a, c], [b, d])\nplt.xlim(0, 5)\nplt.ylim(0, 5)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 585, "library_problem_id": 74, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 586, "code": ["<code>\nfig, axs = plt.subplots(2, 1)\n\np1 = axs[0].imshow(x, cmap='viridis')\np2 = axs[1].imshow(y, cmap='viridis')\n\nfig.colorbar(p1, ax=axs[0])\nfig.colorbar(p2, ax=axs[1])\n\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 586, "library_problem_id": 75, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 587, "code": ["<code>\nplt.figure()\nplt.plot(x[:, 0], label='a')\nplt.plot(x[:, 1], label='b')\nplt.legend()\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 587, "library_problem_id": 76, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 588, "code": ["<code>\nfig, axs = plt.subplots(2)\nfig.suptitle('Y and Z')\naxs[0].plot(x, y)\naxs[1].plot(a, z)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 588, "library_problem_id": 77, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 589, "code": ["<code>\n# SOLUTION END\nplt.figure(figsize=(10, 6))\nplt.plot(*zip(*points), marker='o')\nplt.yscale('log')\nplt.grid(True)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 589, "library_problem_id": 78, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 78}}
{"id": 590, "code": ["<code>\nplt.plot(x, y)\nplt.title('Title', fontsize=20)\nplt.xlabel('X Label', fontsize=18)\nplt.ylabel('Y Label', fontsize=16)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 590, "library_problem_id": 79, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 79}}
{"id": 591, "code": ["<code>\nax.plot(x, y)\nax.set_xticks(np.arange(1, 11))\nax.set_yticks(np.arange(1, 11))\n\n# SOLUTION END\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 591, "library_problem_id": 80, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 592, "code": ["<code>\nfor line, color in zip(lines, c):\n    for start, end in line:\n        plt.plot([start[0], end[0]], [start[1], end[1]], color=color[:3])\n\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 592, "library_problem_id": 81, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 593, "code": ["<code>\nfig, ax = plt.subplots()\nax.loglog(x, y, basex=10, basey=10)\nax.set_xticks([1,10,100,1000])\nax.set_yticks([1,10,100,1000])\nax.set_xticklabels(['1','10','100','1000'])\nax.set_yticklabels(['1','10','100','1000'])\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 593, "library_problem_id": 82, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 594, "code": ["<code>\nfig, ax = plt.subplots()\n\nax.plot(df.index, df['A'], 'r-', label='A')\nax.plot(df.index, df['B'], 'g-', label='B')\nax.plot(df.index, df['C'], 'b-', label='C')\nax.plot(df.index, df['D'], 'y-', label='D')\n\nax.set_xlabel('Date')\nax.set_ylabel('Value')\nax.set_title('Line plot of data')\nax.legend()\n\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 594, "library_problem_id": 83, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 595, "code": ["<code>\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 595, "library_problem_id": 84, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 84}}
{"id": 596, "code": ["<code>\nplt.plot(x, y, marker='o', markersize=5, markerfacecolor='none', markeredgecolor='blue', markeredgewidth=2, alpha=0.5)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 596, "library_problem_id": 85, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 597, "code": ["<code>\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\naxs[0].plot(x, y, label='y')\naxs[0].legend(loc='upper right')\n\naxs[1].plot(z, a, label='a')\naxs[1].legend(loc='upper right')\n\n# Adding a legend to the entire figure\nfig.legend(labels=['y', 'a'], loc='upper right')\n\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 597, "library_problem_id": 86, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 598, "code": ["<code>\nfig, axs = plt.subplots(2, figsize=(10,10))\n\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=axs[0])\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=axs[1])\n\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 598, "library_problem_id": 87, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 87}}
{"id": 599, "code": ["<code>\nax2 = ax.twiny()\nax2.set_xlim(ax.get_xlim())\nax2.set_xticks(ax.get_xticks())\nax2.set_xticklabels([\"first\", \"second\", \"third\", \"fourth\", \"fifth\", \"sixth\", \"seventh\", \"eighth\", \"ninth\"])\nax2.set_xticklabels(ax2.get_xticklabels(), rotation=45)\n\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 599, "library_problem_id": 88, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 600, "code": ["<code>\nplt.plot(x, y, label='$\\lambda$')\nplt.legend()\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 600, "library_problem_id": 89, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 601, "code": ["<code>\n# Add extra ticks [2.1, 3, 7.6] to existing xticks\nplt.xticks(range(0, 10, 2) + [2.1, 3, 7.6])\n\n# SOLUTION END\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 601, "library_problem_id": 90, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 602, "code": ["<code>\nplt.xticks(rotation=-60, ha='left')\n\n# SOLUTION END\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 602, "library_problem_id": 91, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 603, "code": ["<code>\nplt.yticks(rotation=-60)\nplt.gca().xaxis.tick_top()\n\n# SOLUTION END\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 603, "library_problem_id": 92, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 604, "code": ["<code>\nfor label in plt.gca().get_xticklabels():\n    label.set_alpha(0.5)\n\n# SOLUTION END\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 604, "library_problem_id": 93, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 605, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\nplt.plot(x, y)\n\n# Remove the margin before the first xtick but use greater than zero margin for the yaxis\n# SOLUTION START\nplt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\nplt.gca().yaxis.set_tick_params(which='major', pad=10)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 605, "library_problem_id": 94, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 606, "code": ["<code>\nplt.yticks(np.arange(1,10))\nplt.xmargin=0.05\nplt.ymargin=0.1\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 606, "library_problem_id": 95, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 607, "code": ["<code>\nfig, axs = plt.subplots(1, 2)\nfig.suptitle('Figure')\n\naxs[0].plot(x, y)\naxs[1].plot(x, y)\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 607, "library_problem_id": 96, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 96}}
{"id": 608, "code": ["<code>\ndf.plot(kind='line')\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 608, "library_problem_id": 97, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 609, "code": ["<code>\nplt.scatter(x, y, marker='|', hatch='|')\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 609, "library_problem_id": 98, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 610, "code": ["<code>\nplt.scatter(x, y, marker='|', edgecolors='none')\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 610, "library_problem_id": 99, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 611, "code": ["<code>\nplt.scatter(x, y, marker='*')\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 611, "library_problem_id": 100, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 612, "code": ["<code>\nplt.scatter(x, y, s=100, marker='*|')\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 612, "library_problem_id": 101, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 613, "code": ["<code>\n# SOLUTION END\nplt.xlim(1, 5)\nplt.ylim(1, 4)\nplt.pcolormesh(data)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 613, "library_problem_id": 102, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 614, "code": ["<code>\nplt.stem(y, x, orientation='horizontal')\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 614, "library_problem_id": 103, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 615, "code": ["<code>\nplt.bar(d.keys(), d.values(), color=[c[key] for key in d.keys()])\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 615, "library_problem_id": 104, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 616, "code": ["<code>\nimport matplotlib.pyplot as plt\n\ndef plot_line():\n    plt.axvline(x=3, color='r', label='cutoff')\n    plt.legend()\n    plt.show()\n\nplot_line()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 616, "library_problem_id": 105, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 617, "code": ["<code>\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.bar(labels, height)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 617, "library_problem_id": 106, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 106}}
{"id": 618, "code": ["<code>\nplt.pie(data, labels=l, wedgeprops={'width': 0.4})\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 618, "library_problem_id": 107, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 619, "code": ["<code>\nplt.plot(x, y, 'b--')\nplt.grid(True)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 619, "library_problem_id": 108, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 620, "code": ["<code>\nplt.plot(x, y)\nplt.minorticks_on()\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='gray')\nplt.grid(which='major', linestyle='', linewidth='0')\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 620, "library_problem_id": 109, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 621, "code": ["<code>\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140, textprops={'fontweight': 'bold'})\nplt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 621, "library_problem_id": 110, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 622, "code": ["<code>\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140, textprops={'fontweight': 'bold'})\nplt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 622, "library_problem_id": 111, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 623, "code": ["<code>\nplt.plot(x, y, marker='o', markersize=10, markerfacecolor='none', markeredgecolor='blue')\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 623, "library_problem_id": 112, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 624, "code": ["<code>\nplt.axvline(x=55, color='green')\n# SOLUTION END\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 624, "library_problem_id": 113, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 625, "code": ["<code>\n# Specify the values of blue bars (height)\nblue_bar = (23, 25, 17)\n# Specify the values of orange bars (height)\norange_bar = (19, 18, 14)\n\n# Create a figure and a set of subplots\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\n# Create a bar plot for blue bars\naxs[0].bar(range(len(blue_bar)), blue_bar, color='b')\naxs[0].set_title('Blue Bars')\n\n# Create a bar plot for orange bars\naxs[1].bar(range(len(orange_bar)), orange_bar, color='orange')\naxs[1].set_title('Orange Bars')\n\n# Display the plot\nplt.tight_layout()\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 625, "library_problem_id": 114, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 626, "code": ["<code>\nfig, ax1 = plt.subplots()\n\n# Plot y over x\nline1 = ax1.plot(x, y, 'r-', label='Line y over x')\n# Plot z over a\nax2 = ax1.twinx()\nline2 = ax2.plot(a, z, 'b-', label='Line z over a')\n\n# Label each line chart\nax1.set_xlabel('x')\nax1.set_ylabel('y', color='r')\nax2.set_ylabel('z', color='b')\n\n# Put them into a single legend on the first subplot\nlines = line1 + line2\nlabels = [l.get_label() for l in lines]\nax1.legend(lines, labels, loc='upper left')\n\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 626, "library_problem_id": 115, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 627, "code": ["<code>\nplt.scatter(x, y, c=y, cmap='Spectral')\nplt.colorbar(label='y-value')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Scatter plot of y over x with color based on y-value')\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 627, "library_problem_id": 116, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 116}}
{"id": 628, "code": ["<code>\nplt.plot(x, y)\nplt.xticks(np.arange(0, max(x)+1, 1))\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 628, "library_problem_id": 117, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 629, "code": ["<code>\n# SOLUTION END\nsns.catplot(data=df, x=\"species\", y=\"bill_length_mm\", col=\"sex\", kind=\"bar\", sharey=False)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 629, "library_problem_id": 118, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 630, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\ndef draw_circle():\n    fig, ax = plt.subplots()\n\n    circle = patches.Circle((0.5, 0.5), 0.2, edgecolor='b', facecolor='none')\n    ax.add_patch(circle)\n\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n\n    plt.show()\n\n# Call the function\ndraw_circle()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 630, "library_problem_id": 119, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 631, "code": ["<code>\nplt.plot(x, y)\nplt.title(r'$\\boldsymbol{\\phi}$')\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 631, "library_problem_id": 120, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 632, "code": ["<code>\nplt.plot(x, y, label=\"Line\")\nplt.legend(labelspacing=0.1)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 632, "library_problem_id": 121, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 633, "code": ["<code>\nplt.plot(x, y, label=\"Line\", marker='o')\nplt.legend(scatterpoints=1)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 633, "library_problem_id": 122, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 634, "code": ["<code>\nplt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.05), ncol=2, fancybox=True)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 634, "library_problem_id": 123, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 635, "code": ["<code>\nplt.legend()\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 635, "library_problem_id": 124, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 636, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.random((10, 10))\n\n# plot the 2d matrix data with a colorbar\nplt.imshow(data, cmap='viridis')\nplt.colorbar()\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 636, "library_problem_id": 125, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 125}}
{"id": 637, "code": ["<code>\nplt.plot(x, y)\nplt.title(r'$\\bf{' + 'Figure' + '}' + ' 1')\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 637, "library_problem_id": 126, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 126}}
{"id": 638, "code": ["<code>\nsns.pairplot(df, x_vars='x', y_vars='y', hue='id', legend=False)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 638, "library_problem_id": 127, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 639, "code": ["<code>\nplt.plot(x, y)\nplt.gca().invert_xaxis()\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 639, "library_problem_id": 128, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 128}}
{"id": 640, "code": ["<code>\nplt.scatter(x, y)\nplt.gca().set_clip_on(False)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 640, "library_problem_id": 129, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 641, "code": ["<code>\nplt.scatter(x, y, c='red', edgecolors='black')\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 641, "library_problem_id": 130, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 642, "code": ["<code>\nfig, axs = plt.subplots(2, 2, figsize=(15, 15))\nfor ax in axs.flatten():\n    ax.plot(x, y)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 642, "library_problem_id": 131, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 643, "code": ["<code>\nplt.hist(x, bins=np.linspace(0, 10, 5), rwidth=0.8)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 643, "library_problem_id": 132, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 644, "code": ["<code>\nplt.figure(figsize=(10, 5))\nplt.errorbar(x, y, yerr=error, fmt='o', color='b', label='Data with error')\nplt.fill_between(x, y - error, y + error, color='b', alpha=0.2, label='Error region')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.legend(loc='best')\nplt.grid(True)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 644, "library_problem_id": 133, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 133}}
{"id": 645, "code": ["<code>\nplt.axhline(0, color='white')\nplt.axvline(0, color='white')\n\n# SOLUTION END\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 645, "library_problem_id": 134, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 646, "code": ["<code>\nax.errorbar(box_position, box_height, yerr=box_errors, fmt='o', color=c)\n# SOLUTION END\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 646, "library_problem_id": 135, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 647, "code": ["<code>\nfig, axs = plt.subplots(2)\n\n# Plot y over x\naxs[0].plot(x, y)\naxs[0].set_title('Y')\n\n# Plot z over a\naxs[1].plot(a, z)\naxs[1].set_title('Z')\n\n# Raise the title of the second subplot to be higher than the first one\naxs[1].title.set_position([0.5,1.05])\n\nplt.tight_layout()\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 647, "library_problem_id": 136, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 136}}
{"id": 648, "code": ["<code>\nfig, axs = plt.subplots(4, 4, figsize=(5, 5))\nfor i in range(4):\n    for j in range(4):\n        axs[i, j].plot(x, y)\n        axs[i, j].tick_params(axis='both', which='major', labelsize=5)\nplt.tight_layout()\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 648, "library_problem_id": 137, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 649, "code": ["<code>\nplt.figure(figsize=(8, 8))\nplt.matshow(d, fignum=1)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 649, "library_problem_id": 138, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 138}}
{"id": 650, "code": ["<code>\n```python\nimport matplotlib.table as mplt_table\n\nfig, ax = plt.subplots(1, 1)\nax.axis('tight')\nax.axis('off')\n\ntable_data = df.values\ntable_cols = df.columns\n\nthe_table = mplt_table.Table(ax, bbox=[0, 0, 1, 1])\n\n# Adding the column labels\nthe_table.add_cell(0, 0, text=table_cols[0], loc='center', bbox=[0, 0, 1, 1])\nthe_table.add_cell(0, 1, text=table_cols[1], loc='center', bbox=[0, 0, 1, 1])\nthe_table.add_cell(0, 2, text=table_cols[2], loc='center', bbox=[0, 0, 1, 1])\nthe_table.add_cell(0, 3, text=table_cols[3], loc='center', bbox=[0, 0, 1, 1])\n\n# Adding the data\nfor i in range(10):\n    the_table.add_cell(i+1, 0, text=table_data[i, 0], loc='center', bbox=[0, 0, 1, 1])\n    the_table.add_cell(i+1, 1, text=table_data[i, 1], loc='center', bbox=[0, 0,\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 650, "library_problem_id": 139, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 651, "code": ["<code>\nplt.figure(figsize=(10,5))\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\n\n# Show x axis tick labels on both top and bottom of the figure\nplt.tick_params(axis='x', which='both', labeltop=True, labelbottom=True)\n\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 651, "library_problem_id": 140, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 140}}
{"id": 652, "code": ["<code>\nplt.figure(figsize=(10,5))\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.xticks(np.arange(min(x), max(x)+1, 1.0))\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 652, "library_problem_id": 141, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 653, "code": ["<code>\nplt.plot(x, y)\nplt.xticks(x)\nplt.yticks(y)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 653, "library_problem_id": 142, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 654, "code": ["<code>\n# Create a catplot\ng = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"scatter\")\n\n# Set the subplot titles\ng.set_titles(\"Group: {col_name}\")\ng.fig.subplots_adjust(top=0.8)\ng.fig.suptitle('Group: Fat and Group: No Fat')\n\n# Show the plot\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 654, "library_problem_id": 143, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 655, "code": ["<code>\n# Create a catplot\ncatplot = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"scatter\")\n\n# Change the xlabels\ncatplot.set_xlabels(\"Exercise Time\")\ncatplot.set_ylabels(\"Pulse\")\n\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 655, "library_problem_id": 144, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 656, "code": ["<code>\n# Create a catplot\ng = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"swarm\")\n\n# Loop through each subplot and remove ylabel\nfor ax in g.axes.flatten():\n    ax.set_ylabel('')\n\n# Show the plot\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 656, "library_problem_id": 145, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 657, "code": ["<code>\nplt.plot(x, y, label=\"y\")\nplt.legend(fontsize=8)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 657, "library_problem_id": 146, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 146}}
{"id": 658, "code": ["<code>\nplt.figure(figsize=(5, 5), dpi=300)\nplt.plot(x, y)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 658, "library_problem_id": 147, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 659, "code": ["<code>\nplt.plot(x, y, label=\"y\")\nplt.legend(frameon=False)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 659, "library_problem_id": 148, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 660, "code": ["<code>\nplt.figure(figsize=(8, 6))\nplt.plot(t, a, label='sin(t)')\nplt.plot(t, b, label='cos(t)')\nplt.plot(t, c, label='sin(t) + cos(t)')\nplt.legend(loc='upper right')\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 660, "library_problem_id": 149, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 149}}
{"id": 661, "code": ["<code>\nsns.stripplot(x=\"sex\", y=\"bill_length_mm\", hue=\"species\", data=df, dodge=True)\nplt.legend([],[], frameon=False)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 661, "library_problem_id": 150, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 662, "code": ["<code>\ng = sns.FacetGrid(df, col=\"b\", hue=\"b\", sharex=False, sharey=False)\ng.map(sns.pointplot, \"a\", \"c\", order=df['a'].sort_values().unique())\n\nfor ax in g.axes.flatten():\n    ax.xaxis.set_ticks(np.arange(min(df['a']), max(df['a'])+1, 1))\n    ax.set_xticklabels(ax.get_xticks(), rotation=90)\n\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 662, "library_problem_id": 151, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 663, "code": ["<code>\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x, y, z)\nax.view_init(azim=100, elev=50)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 663, "library_problem_id": 152, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 152}}
{"id": 664, "code": ["<code>\nplt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xticks([])\nplt.yticks([])\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 664, "library_problem_id": 153, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 153}}
{"id": 665, "code": ["<code>\ngs = gridspec.GridSpec(nrow, ncol)\nfor i in range(nrow):\n    for j in range(ncol):\n        ax = fig.add_subplot(gs[i, j])\n        ax.imshow(x, aspect='auto')\n        ax.axis('off')\n\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 665, "library_problem_id": 154, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 666, "code": ["<code>\n# Assign a new value to the variable\nx.assign(1)\n\n# Convert the variable to a numpy integer\nresult = x.numpy().item()\n\nprint(result)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 666, "library_problem_id": 0, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 667, "code": ["<code>\nx.assign(114514)\nresult = x.numpy()\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 667, "library_problem_id": 1, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 668, "code": ["<code>\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 668, "library_problem_id": 2, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 669, "code": ["<code>\nresult = tf.one_hot(labels, depth=10, dtype=tf.int32)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 669, "library_problem_id": 3, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 670, "code": ["<code>\nresult = tf.one_hot(9 - tf.constant(labels), depth=10, dtype=tf.int32)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 670, "library_problem_id": 4, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 671, "code": ["<code>\nnum_classes = 10\n    labels = tf.constant(labels, dtype=tf.int32)\n    one_hot_labels = tf.one_hot(labels, depth=num_classes, dtype=tf.int32)\n    return one_hot_labels\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 671, "library_problem_id": 5, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 672, "code": ["<code>\nresult = tf.one_hot(9 - tf.constant(labels), depth=10, dtype=tf.int32)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 672, "library_problem_id": 6, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 673, "code": ["<code>\ndef my_map_func(i):\n  return [i, i+1, i+2]\n\nds = tf.data.Dataset.from_tensor_slices(input)\nds = ds.map(map_func=lambda input: tf.py_function(\n  func=my_map_func, inp=[input], Tout=(tf.int32, tf.int32, tf.int32)\n))\nds = ds.unbatch()\n\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 673, "library_problem_id": 7, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 674, "code": ["<code>\ndef my_map_func(i):\n  return [i, i+1, i+2]\n\nds = tf.data.Dataset.from_tensor_slices(example_input)\nds = ds.map(map_func=lambda input: tf.py_function(\n  func=my_map_func, inp=[input], Tout=(tf.int32, tf.int32, tf.int32)\n))\nds = ds.unbatch()\n\nelement = tf.compat.v1.data.make_one_shot_iterator(ds).get_next()\nresult = []\nwith tf.compat.v1.Session() as sess:\n  for _ in range(9):\n    result.append(sess.run(element))\nprint(result)\n### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 674, "library_problem_id": 8, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 7}}
{"id": 675, "code": ["<code>\nmax_length = tf.constant(8)\nlengths_tensor = tf.constant(lengths)\nmask = tf.sequence_mask(lengths_tensor, max_length, dtype=tf.int32)\nresult = tf.cast(mask, tf.int32)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 675, "library_problem_id": 9, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 676, "code": ["<code>\nmax_length = tf.constant(8)\nlengths_tensor = tf.constant(lengths)\nmask = tf.sequence_mask(lengths_tensor, max_length, dtype=tf.int32)\nresult = tf.cast(mask, tf.int32)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 676, "library_problem_id": 10, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 677, "code": ["<code>\nmax_length = tf.constant(8)\nlengths_tensor = tf.constant(lengths)\nmask = tf.sequence_mask(lengths_tensor, max_length, dtype=tf.float32)\nresult = mask\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 677, "library_problem_id": 11, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 678, "code": ["<code>\nmax_length = tf.constant(8)\n    mask = tf.sequence_mask(lengths, max_length, dtype=tf.int32)\n    return mask\n\nresult = f(example_lengths)\nprint(result)\n### END SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 678, "library_problem_id": 12, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 679, "code": ["<code>\nmax_length = tf.constant(8)\nmask = tf.sequence_mask(lengths, max_length, dtype=tf.float32)\nresult = tf.cast(mask, tf.float32)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 679, "library_problem_id": 13, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 680, "code": ["<code>\nresult = tf.meshgrid(a, b, indexing='ij')\nresult = tf.stack(result, axis=-1)\nresult = tf.reshape(result, [-1, 2])\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 680, "library_problem_id": 14, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 681, "code": ["<code>\n# Create a tensor of indices for the first dimension of `a` and `b`\n    a_indices, b_indices = tf.meshgrid(\n        tf.range(tf.shape(a)[0]),\n        tf.range(tf.shape(b)[0]),\n        indexing='ij',\n    )\n    # Reshape the indices into 2D tensors and combine them with `a` and `b`\n    result = tf.stack([tf.reshape(a_indices, [-1]), tf.reshape(b_indices, [-1])], axis=1)\n    result = tf.gather(a, result[:, 0]), tf.gather(b, result[:, 1])\n    return result\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 681, "library_problem_id": 15, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 682, "code": ["<code>\nresult = tf.squeeze(a, [2])\n```\n</code>\nEND SOLUTION\nThe `tf.squeeze` function is used to remove dimensions of size 1 from the shape of a tensor. In this case, we are removing the dimension of size 1 along the 2nd axis (index 2).\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 682, "library_problem_id": 16, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 683, "code": ["<code>\nresult = tf.expand_dims(a, axis=2)\n```\n</code>\nEND SOLUTION\nThe `tf.expand_dims` function is used to add a new dimension to the tensor at the specified position. In this case, we want to add a new dimension at the 2nd position (index starts from 0), so we use `axis=2`.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 683, "library_problem_id": 17, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 684, "code": ["<code>\nresult = tf.expand_dims(a, axis=[0, 3])\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 684, "library_problem_id": 18, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 685, "code": ["<code>\nresult = tf.reduce_sum(A, axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 685, "library_problem_id": 19, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 686, "code": ["<code>\nresult = tf.reduce_prod(A, axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 686, "library_problem_id": 20, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 687, "code": ["<code>\nimport tensorflow as tf\n\nA = tf.constant([-0.5, -0.1, 0, 0.1, 0.5, 2], dtype=tf.float32)\nresult = 1 / A\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 687, "library_problem_id": 21, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 19}}
{"id": 688, "code": ["<code>\nresult = tf.reduce_sum(tf.square(a - b), axis=1)\n```\nEND SOLUTION\n</code>\nThe `axis=1` argument tells `tf.reduce_sum` to sum along the rows.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 688, "library_problem_id": 22, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 689, "code": ["<code>\nresult = tf.reduce_sum(tf.square(a - b), axis=0)\n```\nEND SOLUTION\n</code>\nThe `axis=0` argument tells TensorFlow to reduce the sum along the columns.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 689, "library_problem_id": 23, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 22}}
{"id": 690, "code": ["<code>\nresult = tf.reduce_sum(tf.square(A - B), axis=1)\n    return result\n\nresult = f(example_a, example_b)\nprint(result)\n### END SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 690, "library_problem_id": 24, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 691, "code": ["<code>\nresult = tf.gather_nd(x, tf.stack([y, z], axis=1))\n```\n</code>\nEND SOLUTION\nThe `tf.gather_nd` function is used to gather slices from `x` according to `y` and `z`. It's equivalent to advanced indexing in numpy. The `tf.stack` function is used to create a tensor from `y` and `z`.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 691, "library_problem_id": 25, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 692, "code": ["<code>\nresult = tf.gather_nd(x, tf.stack([row, col], axis=1))\n```\n</code>\nEND SOLUTION\nThe `tf.gather_nd` function is used to gather slices from `x` according to `indices`. The `indices` are used to specify the position of the elements to gather. The `tf.stack` function is used to create a tensor by stacking a list of tensors. The `axis=1` argument specifies that the tensors should be stacked along the second dimension.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 692, "library_problem_id": 26, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 693, "code": ["<code>\nresult = tf.gather_nd(x, tf.stack([y, z], axis=1))\n### END SOLUTION\nreturn result\n\nprint(f())\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 693, "library_problem_id": 27, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 694, "code": ["<code>\nresult = tf.einsum('ijk,ilk->ijl', A, B)\n```\nEND SOLUTION\n</code>\nThe einsum function in TensorFlow allows you to perform operations on tensors using the Einstein summation convention. The string 'ijk,ilk->ijl' specifies that the first tensor should have dimensions 'ijk' and the second tensor should have dimensions 'ilk'. The result will have dimensions 'ijl'. This will give you the desired output.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 694, "library_problem_id": 28, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 695, "code": ["<code>\nresult = tf.einsum('ijk,ilk->ijl', A, B)\n```\nEND SOLUTION\n</code>\nThe function tf.einsum is a powerful tool in TensorFlow that allows you to perform advanced tensor operations using a string that describes the contraction. In this case, 'ijk,ilk->ijl' means that the first tensor has dimensions 'ijk' and the second tensor has dimensions 'ilk'. The result will have dimensions 'ijl'. This is equivalent to the operation you described.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 695, "library_problem_id": 29, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 696, "code": ["<code>\nresult = [i.decode('utf-8') for i in x]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 696, "library_problem_id": 30, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 697, "code": ["<code>\nresult = [s.decode('utf-8') for s in x]\n    return result\n    ### END SOLUTION\n\nprint(f(example_x))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 697, "library_problem_id": 31, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 698, "code": ["<code>\n# Create a mask where non-zero values are 1 and zero values are 0\nmask = tf.cast(tf.not_equal(x, 0), tf.float32)\n\n# Count the number of non-zero values along the second to last dimension\ncount = tf.reduce_sum(mask, axis=-2, keepdims=True)\n\n# Average the values along the second to last dimension\navg = tf.reduce_sum(x * mask, axis=-2, keepdims=True) / count\n\n# Replace the zero values in the original tensor with the average values\nresult = tf.where(tf.equal(x, 0), avg, x)\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 698, "library_problem_id": 32, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 699, "code": ["<code>\n# Create a mask where non-zero values are True\nmask = tf.not_equal(x, 0)\n\n# Count the number of non-zero values along the last dimension\ncount = tf.math.count_nonzero(mask, axis=-1, dtype=tf.float32, keepdims=True)\n\n# Compute the mean of the non-zero values along the last dimension\nmean = tf.math.reduce_sum(x * mask, axis=-1, keepdims=True) / count\n\n# Compute the squared differences from the mean\nsq_diff = tf.square(x - mean)\n\n# Compute the variance by taking the mean of the squared differences\nvariance = tf.math.reduce_sum(sq_diff * mask, axis=-1, keepdims=True) / count\n\n# Compute the standard deviation by taking the square root of the variance\nstd_dev = tf.sqrt(variance)\n\n# Compute the result by dividing the standard deviation by the mean\nresult = std_dev / mean\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 699, "library_problem_id": 33, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 700, "code": ["<code>\nmask = x!=0\n    sum_x = tf.reduce_sum(x, axis=-2, keepdims=True)\n    count_x = tf.reduce_sum(tf.cast(mask, tf.float32), axis=-2, keepdims=True)\n    result = sum_x / count_x\n    return result\n\nresult = f(example_x)\nprint(result)\n### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 700, "library_problem_id": 34, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 701, "code": ["<code>\ntf.random.set_seed(10)\nA = tf.random.normal([100,100])\nB = tf.random.normal([100,100])\nresult = tf.reduce_sum(tf.matmul(A,B))\n```\n</code>\nEND SOLUTION\n\nIn TensorFlow 2.x, the Session class has been removed and the tf.function decorator is used to convert a Python function to a TensorFlow graph. This means that the code inside the session is no longer executed eagerly, but instead it is compiled into a graph. This can lead to significant speed improvements, especially for complex models.\n\nThe reason why the use of Session slows down TensorFlow 1.16 by 6000x is likely due to the fact that TensorFlow 1.x uses a more flexible and dynamic execution model, which can be slower than TensorFlow 2.x's static graph execution model. This is because TensorFlow 1.x is designed to be more flexible and user-friendly, allowing for more dynamic changes to the graph during runtime.\n\nHowever, it's important to note that TensorFlow 2.x is generally faster than TensorFlow 1.x, especially for complex models. The speed difference you're seeing might be due to the specifics of your code and hardware, and it's possible that the performance difference is not as significant in TensorFlow 2.x.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 701, "library_problem_id": 35, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 702, "code": ["<code>\nresult = tf.argmax(a, axis=1)\n```\n</code>\nEND SOLUTION\nThe `tf.argmax` function returns the indices of the maximum values along an axis. In this case, we're interested in the maximum values along the columns (axis=1), so we pass that in. The result will be a tensor of shape [3] containing the indices of the maximum values in each row of the input tensor.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 702, "library_problem_id": 36, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 703, "code": ["<code>\nresult = tf.argmax(a, axis=1)\n```\n</code>\nEND SOLUTION\nThe `tf.argmax` function returns the indices of the maximum values along an axis. In this case, we're interested in the maximum values along the columns (axis=1), so we use 1. The result will be a tensor with the shape [100] and the values will be the indices of the maximum values in each row of the input tensor.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 703, "library_problem_id": 37, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 704, "code": ["<code>\n```python\n    result = tf.argmax(a, axis=1)\n    return result\n\nprint(f(example_a))\n### END SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 704, "library_problem_id": 38, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 705, "code": ["<code>\nresult = tf.argmin(a, axis=0)\n```\n</code>\nEND SOLUTION\nThe `tf.argmin` function returns the indices of the minimum values along an axis. By setting `axis=0`, it will find the minimum value in each column. The result will be a 1D tensor that contains the indices of the minimum values in each column of the input tensor.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 705, "library_problem_id": 39, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 706, "code": ["<code>\nEND SOLUTION\n\n#Training the model\nhistory = model.fit(x=Xtrain,y=ytrain,validation_data=(Xtest,ytest),batch_size=32,epochs=epochs)\n\n#Save the model in \"export/1\"\nmodel.save(\"export/1\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 706, "library_problem_id": 40, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 707, "code": ["<code>\ntf.random.set_seed(seed_x)\nresult = tf.random.uniform(shape=[10], minval=1, maxval=5, dtype=tf.int32)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 707, "library_problem_id": 41, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 708, "code": ["<code>\ntf.random.set_seed(seed_x)\nresult = tf.random.uniform(shape=[114], minval=2, maxval=6, dtype=tf.int32)\n```\n</code>\nEND SOLUTION\nIn the above code, `tf.random.uniform` is used to generate random integers. The `minval` and `maxval` parameters are set to 2 and 6 respectively to get values in the range {2, 3, 4, 5}. The `shape` parameter is set to [114] to generate 114 random integers. The `dtype` parameter is set to `tf.int32` to ensure that the output is an integer tensor.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 708, "library_problem_id": 42, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 709, "code": ["<code>\ntf.random.set_seed(seed_x)\n    result = tf.random.uniform(shape=[10], minval=1, maxval=5, dtype=tf.int32)\n    return result\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 709, "library_problem_id": 43, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 710, "code": ["<code>\nimport tensorflow as tf\n\nresult = tf.__version__\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 710, "library_problem_id": 44, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 711, "code": ["<code>\n# Convert x to log scale\nx_log = np.log(x)\n\n# Fit a polynomial to the log-transformed x\ncoeffs = np.polyfit(x_log, y, 1)\n\n# The coefficients are [B, A] in the original equation y = A*log(x) + B\nresult = np.array([coeffs[1], coeffs[0]])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 711, "library_problem_id": 0, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 712, "code": ["<code>\n# Fit a line to the data using polyfit\ncoeffs = np.polyfit(x, y, 1)\n\n# The coefficients are [slope, intercept], so we reverse them to [A, B]\nresult = coeffs[::-1]\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 712, "library_problem_id": 1, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 713, "code": ["<code>\ndef func(x, a, b, c):\n    return a * np.exp(b * x) + c\n\nresult, _ = scipy.optimize.curve_fit(func, x, y, p0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 713, "library_problem_id": 2, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 714, "code": ["<code>\nfrom scipy import stats\nimport numpy as np\nnp.random.seed(42)\nx = np.random.normal(0, 1, 1000)\ny = np.random.normal(0, 1, 1000)\n\nstatistic, p_value = stats.ks_2samp(x, y)\nprint('K-S statistic:', statistic)\nprint('P-value:', p_value)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 714, "library_problem_id": 3, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 3}}
{"id": 715, "code": ["<code>\ntest_stat, p_value = stats.ks_2samp(x, y)\nresult = p_value < alpha\n```\n</code>\nEND SOLUTION\nThe p-value is the probability that the two samples were drawn from the same distribution. If the p-value is less than your significance level (alpha), you can reject the null hypothesis that the distributions are the same. In this case, if p_value < alpha, result will be True, otherwise it will be False.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 715, "library_problem_id": 4, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 3}}
{"id": 716, "code": ["<code>\ndef f(x):\n    a, b, c = x\n    return ((a+b-c)-2)**2 + ((3*a-b-c))**2 + sin(b) + cos(b) + 4\n\nresult = optimize.minimize(f, initial_guess)\nprint(result.x)\n```\n</code>\nEND SOLUTION\nThe function `f` takes a list of three variables `x`, which are unpacked into `a`, `b`, and `c`. The function returns the value of the objective function at `x`. The `minimize` function is then called with `f` as the objective function and `initial_guess` as the initial guess for the variables. The result is a `OptimizeResult` object, which contains the result of the optimization. The minimum value of the objective function is in `result.fun`, and the values of the variables that minimize the function are in `result.x`.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 716, "library_problem_id": 5, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 5}}
{"id": 717, "code": ["<code>\np_values = scipy.stats.norm.sf(z_scores)\n```\n</code>\nEND SOLUTION\nThe function `scipy.stats.norm.sf` computes the survival function (1 - CDF) of the standard normal distribution at the given points, which corresponds to the p-values for left-tailed tests.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 717, "library_problem_id": 6, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 718, "code": ["<code>\np_values = scipy.stats.norm.sf(z_scores, loc=mu, scale=sigma)\n</code>\nEND SOLUTION\nThe function scipy.stats.norm.sf calculates the survival function (1 - CDF) of the standard normal distribution, which gives the probability that a random variable from the distribution is greater than the given value. The parameters loc and scale are the mean and standard deviation of the distribution, respectively.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 718, "library_problem_id": 7, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 719, "code": ["<code>\nz_scores = scipy.stats.norm.ppf(1-np.array(p_values))\n</code>\nEND SOLUTION\nThe ppf (percent point function) is the inverse of the cdf (cumulative distribution function). It returns the argument x such that P(X <= x) = q, where X is a random variable distributed as the given distribution. In this case, we are using the standard normal distribution, so the ppf is the same as the z-score.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 719, "library_problem_id": 8, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 720, "code": ["<code>\ndist = stats.lognorm([stddev],loc=0,scale=np.exp(mu))\nresult = dist.cdf(x)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 720, "library_problem_id": 9, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 721, "code": ["<code>\n# Calculate the shape parameter (s) from standard deviation\ns = stddev / np.sqrt(np.log(10))\n\n# Calculate the scale parameter (scale) from mu\nscale = np.exp(mu)\n\n# Create a lognormal distribution with the calculated parameters\ndist = stats.lognorm([s], scale=scale)\n\n# Calculate the expected value (mean) and median\nexpected_value = dist.mean()\nmedian = dist.median()\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 721, "library_problem_id": 10, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 722, "code": ["<code>\nresult = sa.multiply(sb)\n```\nEND SOLUTION\n</code>\nThe `multiply` function in scipy sparse CSR matrix performs element-wise multiplication of two matrices. It is equivalent to the numpy's element-wise multiplication operation.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 722, "library_problem_id": 11, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 723, "code": ["<code>\nresult = sA.multiply(sB)\n    return result\n### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 723, "library_problem_id": 12, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 724, "code": ["<code>\ninterp = scipy.interpolate.LinearNDInterpolator(points, V)\nresult = interp(request)\nprint(result)\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 724, "library_problem_id": 13, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 725, "code": ["<code>\n# Create a LinearNDInterpolator object\ninterp = scipy.interpolate.LinearNDInterpolator(points, V)\n\n# Use the interpolator to get the values at the requested points\nresult = interp(request)\n\nresult\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 725, "library_problem_id": 14, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 726, "code": ["<code>\nimport numpy as np\nfrom scipy import misc\nfrom scipy.ndimage import rotate\n\n# Load the image\ndata_orig = misc.face()\nx0, y0 = 580, 300  # left eye; (xrot,yrot) should point there\n\n# Generate a random rotation angle\nangle = np.random.randint(1, 360)\n\n# Rotate the image\ndata_rot = rotate(data_orig, angle)\n\n# Create a rotation matrix\ntheta = np.radians(angle)\nc, s = np.cos(theta), np.sin(theta)\nrotation_matrix = np.array(((c, -s), (s, c)))\n\n# Create a translation matrix\ntranslation_matrix = np.array([x0, y0])\n\n# Apply the rotation and translation to the original coordinates\nxrot, yrot = np.dot(rotation_matrix, [x0, y0]) + translation_matrix\n\n# Convert to integer\nxrot, yrot = int(xrot), int(yrot)\n\n# Print the rotated coordinates\nprint(f\"The rotated coordinates are ({xrot}, {yrot})\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 726, "library_problem_id": 15, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 727, "code": ["<code>\nresult = M.diagonal()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 727, "library_problem_id": 16, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 728, "code": ["<code>\n# First, we need to convert the times to probabilities, which are uniform in the range 0 to T.\n# We do this by dividing each time by T.\ntimes_prob = np.array(times) / T\n\n# Then, we can use the kstest function from scipy.stats to test the uniformity of these probabilities.\n# We use the 'uniform' distribution as the reference distribution.\nresult = stats.kstest(times_prob, 'uniform')\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 728, "library_problem_id": 17, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 729, "code": ["<code>\n# The cumulative distribution function of the uniform distribution is given by\n    # F(x) = 1 for x >= 1\n    # F(x) = 0 for x < 0\n    # So, we need to transform the times to the range [0, 1] and then apply the cumulative distribution function of the uniform distribution.\n    times_normalized = np.array(times) / T\n    cdf_times = stats.uniform.cdf(times_normalized, loc=0, scale=1)\n\n    # The Kolmogorov-Smirnov test statistic is the maximum absolute difference between the empirical CDF and the cumulative distribution function of the uniform distribution.\n    ks_test_statistic = np.max(np.abs(np.sort(cdf_times) - stats.uniform.cdf(np.sort(times_normalized), loc=0, scale=1)))\n\n    # The p-value of the KS test is the probability that the observed data could have been drawn from the uniform distribution.\n    # We use the Kolmogorov-Smirnov test to compare the empirical distribution function of the times with the uniform distribution.\n    # The null hypothesis of the KS test is that the times are drawn from the uniform distribution.\n    # We use the scipy function 'stats.kstest' to perform the KS test.\n    # The p-value is the second element of the result.\n    p_value = stats.kstest(cdf_times, 'uniform').pvalue\n\n    return ks_test_statistic, p_value\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 729, "library_problem_id": 18, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 730, "code": ["<code>\n# We need to convert the times to a numpy array because kstest from scipy works with numpy arrays\ntimes_array = np.array(times)\n\n# We subtract 1 from T because the last time is T, and we don't want to include it in the range\n# We divide by rate because the rate is the lambda parameter in the exponential distribution\n# We add 1 to the result because the kstest function returns the D statistic, which is 1 minus the cumulative distribution function\nresult = stats.kstest(times_array, 'uniform', args=(0, T/rate)).pvalue\n\n# If the p-value is less than 0.05 (or 5%), we reject the null hypothesis that the data is uniformly distributed\n# In this case, we interpret it as a boolean value: True if the data is uniformly distributed, False otherwise\nresult < 0.05\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 730, "library_problem_id": 19, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 17}}
{"id": 731, "code": ["<code>\nFeature = sparse.hstack((c1, c2))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 731, "library_problem_id": 20, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 732, "code": ["<code>\nfrom scipy.sparse import hstack\nFeature = hstack((c1, c2))\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 732, "library_problem_id": 21, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 733, "code": ["<code>\nfrom scipy.sparse import vstack\n\nFeature = vstack([c1, c2])\n```\nEND SOLUTION\n</code>\nThe vstack function from scipy.sparse module can be used to stack 1-D arrays as columns into a 2-D array. It concatenates arrays along the second axis. The resulting array will have the same type as the input arrays.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 733, "library_problem_id": 22, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 734, "code": ["<code>\n# Create a distance matrix\nD = scipy.spatial.distance.cdist(points1, points2)\n# Solve the linear sum assignment problem\nrow_ind, col_ind = scipy.optimize.linear_sum_assignment(D)\n# Get the matching points\nresult = col_ind\n```\nEND SOLUTION\n</code>\nThe result is the indices of the matching points in points2 to points1.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 734, "library_problem_id": 23, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 735, "code": ["<code>\n# Create a distance matrix\nD = scipy.spatial.distance.cdist(points1, points2)\n# Solve the linear sum assignment problem\nrow_ind, col_ind = scipy.optimize.linear_sum_assignment(D)\n# Get the matching points\nresult = col_ind\nreturn result\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 735, "library_problem_id": 24, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 736, "code": ["<code>\nb = sparse.csr_matrix(np.diag(np.diagonal(b)))\n```\n</code>\nEND SOLUTION\nThis will create a new csr_matrix with only the diagonal elements. The diagonal elements are extracted using `np.diagonal(b)` and then a new square matrix is created with these diagonal elements using `np.diag`.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 736, "library_problem_id": 25, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 737, "code": ["<code>\n# Use the label function from scipy.ndimage to label the different regions in the image\nlabels, num_labels = ndimage.label(img > threshold)\nresult = num_labels\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 737, "library_problem_id": 26, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 738, "code": ["<code>\n# Use the label function from scipy.ndimage to label the different regions in the image\nlabels, num_labels = ndimage.label(img > threshold)\nresult = num_labels\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 738, "library_problem_id": 27, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 739, "code": ["<code>\n# Use the label function from scipy.ndimage to label the different regions in the image\n    labeled_img, num_labels = ndimage.label(img > threshold)\n    return num_labels\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 739, "library_problem_id": 28, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 740, "code": ["<code>\n# Create a binary mask where the values exceed the threshold\nmask = img > threshold\n\n# Label the connected regions in the mask\nlabels, num_labels = ndimage.label(mask)\n\n# Calculate the center of mass for each region\ncoms = ndimage.measurements.center_of_mass(img, labels, range(1, num_labels+1))\n\n# Calculate the distance between the center of mass and the top left corner\ndistances = [np.sqrt((com[0]-0)**2 + (com[1]-0)**2) for com in coms]\n\n# Put the distances in a list\nresult = list(distances)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 740, "library_problem_id": 29, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 741, "code": ["<code>\nM = (M + M.T)/2\nM = M + M.T - np.diag(M.diagonal())\nM = sparse.csr_matrix(M)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 741, "library_problem_id": 30, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 742, "code": ["<code>\nsA = (sA + sA.T)/2\n    upper = sA.tocsr()\n    sA = upper.T.conjugate().copy()\n    sA[np.arange(sA.shape[0]), np.arange(sA.shape[0])] = upper.diagonal()\n    return sA\n    ### END SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 742, "library_problem_id": 31, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 743, "code": ["<code>\nimport scipy.ndimage\n\ndef remove_isolated_cells(arr):\n    # Create a binary structure of 1s of the same shape as arr\n    structure = np.ones((arr.shape[0], arr.shape[1]), dtype=np.int8)\n\n    # Use scipy.ndimage.label to find all connected regions in arr\n    labeled_arr, num_features = scipy.ndimage.label(arr, structure)\n\n    # Create a list to store the sizes of all connected regions\n    sizes = []\n\n    # Iterate over all features in labeled_arr\n    for feature in range(1, num_features+1):\n        # Get the indices of all pixels in labeled_arr that belong to the current feature\n        indices = np.where(labeled_arr == feature)\n\n        # Get the size of the current feature\n        size = len(indices[0])\n\n        # Append the size to the sizes list\n        sizes.append(size)\n\n    # Create a new array to store the result\n    result = np.zeros_like(arr)\n\n    # Iterate over all features in labeled_arr\n    for feature in range(1, num_features+1):\n        # Get the indices of all pixels in labeled_arr that belong to the current feature\n        indices = np.where(labeled_arr == feature)\n\n        # Get the size of the current feature\n        size = len(indices[0])\n\n        # If the size of the current feature is greater than 1, set all pixels in labeled_arr that belong to the current feature to 1 in the result array\n        if size\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 743, "library_problem_id": 32, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 744, "code": ["<code>\ndef remove_isolated_cells(image):\n    # Create a binary mask of the image\n    mask = image > 0\n\n    # Use scipy.ndimage.label to identify connected components\n    labels, num_labels = scipy.ndimage.label(mask)\n\n    # Create a list to store the sizes of the connected components\n    sizes = []\n\n    # Iterate over the labels\n    for i in range(1, num_labels+1):\n        # Get the indices of the pixels in the current label\n        indices = np.where(labels == i)\n\n        # Get the size of the current label\n        size = len(indices[0])\n\n        # Append the size to the list\n        sizes.append(size)\n\n    # Create a new image to store the result\n    result = np.zeros_like(image)\n\n    # Iterate over the labels\n    for i in range(1, num_labels+1):\n        # Get the indices of the pixels in the current label\n        indices = np.where(labels == i)\n\n        # Get the size of the current label\n        size = len(indices[0])\n\n        # If the size of the current label is greater than 1, set the pixels in the current label to 0\n        if size > 1:\n            result[indices] = image[indices]\n\n    return result\n\nsquare = remove_isolated_cells(square)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 744, "library_problem_id": 33, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 745, "code": ["<code>\nmean = np.mean(col.data)\nstandard_deviation = np.std(col.data)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 745, "library_problem_id": 34, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 746, "code": ["<code>\nMax = col.max()\nMin = col.min()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 746, "library_problem_id": 35, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 34}}
{"id": 747, "code": ["<code>\nfrom scipy import stats\n\n# Convert the sparse column to a 1D numpy array\ncol_array = np.array(col.toarray()).flatten()\n\n# Calculate the median\nMedian = np.median(col_array)\n\n# Calculate the mode\nMode = stats.mode(col_array)[0][0]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 747, "library_problem_id": 36, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 34}}
{"id": 748, "code": ["<code>\ndef fourier(x, *coeffs):\n    return sum(coeffs[i] * np.cos(i * np.pi / tau * x) for i in range(1, len(coeffs))) + coeffs[0]\n\npopt, pcov = curve_fit(fourier, z, Ua, p0=[1] + [0]*degree)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 748, "library_problem_id": 37, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 749, "code": ["<code>\nfrom scipy.spatial.distance import cdist\nfrom scipy import ndimage\n\n# Get unique IDs\nlabels, num_labels = ndimage.label(example_array)\nids = np.unique(labels)[1:]  # Exclude background label\n\n# Get coordinates of each unique ID\ncoords = np.array(np.where(np.isin(labels, ids))).T\n\n# Calculate pairwise distances\ndistances = cdist(coords, coords)\n\n# Convert distances from cells to metres (assuming resolution is 1m)\ndistances *= 1\n\n# Create a 2D array to store the results\nresult = np.zeros((num_labels, num_labels))\n\n# Fill in the result array\nfor i, id1 in enumerate(ids):\n    for j, id2 in enumerate(ids):\n        if id1 != id2:\n            result[id1, id2] = distances[i, j]\n\n# Print the result array\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 749, "library_problem_id": 38, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 750, "code": ["<code>\nfrom scipy.spatial.distance import cdist\nfrom scipy.ndimage import label\n\n# Find unique IDs\nunique_ids = np.unique(example_array)[1:]  # Ignore the 0 (background)\n\n# Create a mask for each unique ID\nmasks = example_array[np.newaxis, :, :] == unique_ids[:, np.newaxis, np.newaxis]\n\n# Calculate centroids for each unique ID\ncentroids = np.array([np.argwhere(mask)[0] for mask in masks])\n\n# Calculate pairwise Manhattan distances\ndistances = cdist(centroids, centroids, metric='cityblock')\n\n# Create a result array\nresult = np.zeros((len(unique_ids), len(unique_ids)), dtype=int)\nresult[np.triu_indices(len(unique_ids), k=1)] = distances.flatten()\n\n# Fill the lower triangle with the transpose\nresult = result + result.T - np.diag(result.diagonal())\n\n# Convert to a DataFrame for better readability\nresult = pd.DataFrame(result, index=unique_ids, columns=unique_ids)\n\nresult\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 750, "library_problem_id": 39, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 751, "code": ["<code>\nfrom scipy.spatial.distance import cdist\nfrom scipy import ndimage\n\ndef f(example_array):\n    # Get unique labels\n    labels, num_labels = ndimage.label(example_array)\n\n    # Initialize result\n    result = []\n\n    # Loop over all unique labels\n    for i in range(1, num_labels+1):\n        # Get coordinates of current label\n        coords = np.argwhere(labels == i)\n\n        # Calculate pairwise distances\n        distances = cdist(coords, coords)\n\n        # Add to result\n        for j in range(len(coords)):\n            for k in range(j+1, len(coords)):\n                result.append((i, i, 0))  # distance 0 for same region\n                result.append((i, j+1, distances[j, k]))  # add distance to result\n\n    return result\n\n# Test the function\nresult = f(example_array)\n\n# Print the result\nfor r in result:\n    print(f\"From region {r[0]}, to region {r[1]}, distance: {r[2]}\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 751, "library_problem_id": 40, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 752, "code": ["<code>\nresult = np.zeros((5, 100))\nfor i in range(5):\n    tck = interpolate.splrep(x[:, i], y[:, i], k=2, s=4)\n    result[i] = interpolate.splev(x_val, tck, der=0)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 752, "library_problem_id": 41, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 753, "code": ["<code>\n# Combine all datasets into a list\ndata = [x1, x2, x3, x4]\n\n# Use scipy.stats.anderson_ksamp to perform the Anderson-Darling K-sample test\nstatistic, critical_values, significance_level = ss.anderson_ksamp(data)\n\nprint('Statistic: ', statistic)\nprint('Critical values: ', critical_values)\nprint('Significance level: ', significance_level)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 753, "library_problem_id": 42, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 754, "code": ["<code>\n# Convert the lists to numpy arrays\nx1 = np.array(x1)\nx2 = np.array(x2)\n\n# Create a list of the two arrays\ndata_list = [x1, x2]\n\n# Use the anderson_ksamp function\nresult = ss.anderson_ksamp(data_list)\n\n# The critical values for the Anderson-Darling test are given in the second element of the result.\n# If the p-value is less than 0.05, we can reject the null hypothesis that the data are drawn from the same distribution.\n# In this case, we are testing against the alternative that the data are not drawn from the same distribution.\n# So, if the test statistic is larger than the critical value at the 5% significance level, we reject the null hypothesis.\n\nsame_distribution = result.statistic < result.critical_values[4]\nprint(same_distribution)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 754, "library_problem_id": 43, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 755, "code": ["<code>\ndef kendall_tau(df, window):\n    # Create a copy of the dataframe to avoid modifying the original dataframe\n    df = df.copy()\n    \n    # Define a function to calculate Kendall's tau correlation\n    def kendall_tau_func(x):\n        x1, x2 = x.values[0], x.values[1]\n        return stats.kendalltau(x1, x2)[0]\n    \n    # Create a list to store the new columns\n    new_cols = []\n    \n    # Iterate over each pair of columns\n    for col1, col2 in df.columns.to_list():\n        # Calculate the rolling Kendall's tau correlation\n        new_col = df[[col1, col2]].rolling(window=window).apply(kendall_tau_func)\n        # Add the new column to the list\n        new_cols.append(new_col)\n    \n    # Concatenate all the new columns into a single dataframe\n    df_new = pd.concat(new_cols, axis=1)\n    \n    return df_new\n\ndf = kendall_tau(df, 3)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 755, "library_problem_id": 44, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 756, "code": ["<code>\nresult = (sa.data == 0).all()\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 756, "library_problem_id": 45, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 757, "code": ["<code>\nresult = (sa == 0).all()\n```\n</code>\nEND SOLUTION\nThis will return True if all elements in the lil matrix are zero, and False otherwise. It's more efficient because it checks all elements in one go, rather than calling nonzero() and checking the length of the result.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 757, "library_problem_id": 46, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 758, "code": ["<code>\nresult = block_diag(*a)\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 758, "library_problem_id": 47, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 759, "code": ["<code>\nresult = stats.ranksums(pre_course_scores, during_course_scores)\np_value = result.pvalue\nprint(p_value)\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 759, "library_problem_id": 48, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 760, "code": ["<code>\nresult = stats.ranksums(pre_course_scores, during_course_scores)\n    p_value = result.pvalue\n    return p_value\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 760, "library_problem_id": 49, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 761, "code": ["<code>\ndef kurtosis(data):\n    mean = np.mean(data)\n    var = np.var(data)\n    n = len(data)\n    return n * np.sum((data - mean)**4) / ((n - 1) * (n - 2) * (n - 3) * var**2)\n\nkurtosis_result = kurtosis(a)\nprint(kurtosis_result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 761, "library_problem_id": 50, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 762, "code": ["<code>\nkurtosis_result = scipy.stats.kurtosis(a, fisher=True, bias=False)\nprint(kurtosis_result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 762, "library_problem_id": 51, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 763, "code": ["<code>\n# Create a 2D interpolation function\nf = scipy.interpolate.interp2d(s, t, z, kind='cubic')\n\n# Generate new points to interpolate\ns_new = np.linspace(-1, 1, 100)\nt_new = np.linspace(-2, 0, 100)\n\n# Interpolate the new points\nz_new = f(s_new, t_new)\n\n# Reshape the new points to match the original shape\nresult = z_new.reshape(-1)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 763, "library_problem_id": 52, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 764, "code": ["<code>\n# Create a 2D interpolation function\n    interp_func = scipy.interpolate.interp2d(s, t, z, kind='cubic')\n\n    # Generate new s and t values\n    new_s = np.linspace(-1, 1, 100)\n    new_t = np.linspace(-2, 0, 100)\n\n    # Interpolate the values\n    result = interp_func(new_s, new_t)\n    ### END SOLUTION\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 764, "library_problem_id": 53, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 52}}
{"id": 765, "code": ["<code>\nimport numpy as np\n\n# Create an empty list to store the region indices for each extra point\nregion_indices = []\n\n# Loop over each extra point\nfor extra_point in extraPoints:\n    # Find the nearest point in the Voronoi diagram to the extra point\n    nearest_point_index = np.argmin([np.linalg.norm(np.array(extra_point)-point) for point in points])\n    \n    # Get the region index of the nearest point\n    region_index = vor.point_region[nearest_point_index]\n    \n    # Append the region index to the list\n    region_indices.append(region_index)\n\n# Convert the list to a numpy array\nresult = np.array(region_indices)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 765, "library_problem_id": 54, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 766, "code": ["<code>\nimport numpy as np\n\n# Create an empty list to store the region indices for each extra point\nresult = []\n\n# Loop over each extra point\nfor extra_point in extraPoints:\n    # Find the nearest point in the Voronoi diagram to the extra point\n    nearest_point_index = np.argmin([np.linalg.norm(np.array(extra_point)-point) for point in points])\n    \n    # Get the region indices for the nearest point\n    region_indices = vor.regions[vor.point_region[nearest_point_index]]\n    \n    # Check if the extra point is inside the region of the nearest point\n    if vor.points[nearest_point_index] in vor.vertices[region_indices]:\n        result.append(nearest_point_index)\n    else:\n        # If the extra point is not inside the region of the nearest point, find the region that contains the extra point\n        for region_index in region_indices:\n            if vor.points[region_index] in vor.vertices[region_indices]:\n                result.append(region_index)\n                break\n\n# Convert the result list to a numpy array\nresult = np.array(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 766, "library_problem_id": 55, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 767, "code": ["<code>\n# First, we need to find the maximum size in the list\nmax_size = max(len(vec) for vec in vectors)\n\n# Then, we create a new list where each vector is padded with zeros to the maximum size\npadded_vectors = [np.pad(vec, (0, max_size - len(vec)), 'constant') for vec in vectors]\n\n# Finally, we convert the list of padded vectors into a sparse matrix\nsparse_matrix = sparse.csc_matrix(padded_vectors)\n\nresult = sparse_matrix\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 767, "library_problem_id": 56, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 768, "code": ["<code>\nb = scipy.ndimage.median_filter(a, size=3, mode='constant', origin=[0, 1])\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 768, "library_problem_id": 57, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 769, "code": ["<code>\nresult = M[row, column]\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 769, "library_problem_id": 58, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 770, "code": ["<code>\nresult = [M[i, j] for i, j in zip(row, column)]\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 770, "library_problem_id": 59, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 771, "code": ["<code>\nnew_array = np.array([scipy.interpolate.interp1d(x, array[:, i, j])(x_new) for i in range(array.shape[1]) for j in range(array.shape[2])]).reshape(array.shape[1], array.shape[2], -1)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 771, "library_problem_id": 60, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 60}}
{"id": 772, "code": ["<code>\ndef NormalDistro(u, o2, x):\n    dev = abs((x-u)/o2)\n    P_inner, _ = scipy.integrate.quad(NDfx, -dev, dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return P\n\nprob = NormalDistro(u, o2, x)\nprint(prob)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 772, "library_problem_id": 61, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 773, "code": ["<code>\nimport scipy.integrate\nimport math\nimport numpy as np\n\ndef NDfx(x):\n    return((1/math.sqrt((2*math.pi)))*(math.e**((-.5)*(x**2))))\n\ndef NormalDistro(u, o2, x):\n    dev = abs((x-u)/o2)\n    P_inner, _ = scipy.integrate.quad(NDfx, -dev, dev)\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer/2\n    return P\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 773, "library_problem_id": 62, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 774, "code": ["<code>\n# Create a 1D DCT matrix\ndct_1d = sf.dct(np.ones(N))\n\n# Create a 2D DCT matrix by taking the outer product of the 1D DCT matrix with itself\ndct_2d = np.outer(dct_1d, dct_1d)\n\n# Normalize the DCT matrix\ndct_2d = dct_2d / np.sqrt(N**2)\n\nresult = dct_2d\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 774, "library_problem_id": 63, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 775, "code": ["<code>\nv1 = [3*i**2 +(i/2) for i in range(1, 6)]\nv2 = [-(6*i**2 - 1) for i in range(1, 6)]\nv3 = [3*i**2 -(i/2) for i in range(1, 6)]\nmatrix = np.array([v1, v2, v3])\n\ndiag_offsets = [-1, 0, 1]\nresult = sparse.diags(matrix, diag_offsets).toarray()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 775, "library_problem_id": 64, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 776, "code": ["<code>\nimport scipy.special\n\ndef binomial_distribution_matrix(N, p):\n    M = scipy.special.comb(np.arange(N+1), np.arange(N+1)[:, None], exact=True)\n    M = M * p**np.arange(N+1)[:, None] * (1-p)**(np.arange(N+1)[:, None] - np.arange(N+1))\n    M[np.arange(N+1)[:, None], np.arange(N+1)] = M[np.arange(N+1)[:, None], np.arange(N+1)] * (1-p)\n    return M\n\nresult = binomial_distribution_matrix(N, p)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 776, "library_problem_id": 65, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 777, "code": ["<code>\nresult = df.apply(lambda x: stats.zscore(x), axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 777, "library_problem_id": 66, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 778, "code": ["<code>\nresult = df.apply(lambda x: stats.zscore(x), axis=0)\nresult\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 778, "library_problem_id": 67, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 66}}
{"id": 779, "code": ["<code>\n# Calculate z-scores\nz_scores = df.apply(lambda x: stats.zscore(x), axis=1)\n\n# Create a new dataframe to store the results\nresult = pd.DataFrame(index=df.index)\n\n# Add the original data to the result dataframe\nresult['data'] = df\n\n# Add the z-scores to the result dataframe\nresult['zscore'] = z_scores\n\n# Print the result\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 779, "library_problem_id": 68, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 780, "code": ["<code>\n# Calculate z-scores\nz_scores = df.apply(lambda x: stats.zscore(x), axis=0)\n\n# Create a new dataframe to store the results\nresult = pd.DataFrame(index=df.index)\n\n# Add the original data to the result dataframe\nresult['data'] = df\n\n# Add the z-scores to the result dataframe\nresult['zscore'] = z_scores\n\n# Round the values to 3 decimal places\nresult = result.applymap(lambda x: round(x, 3))\n\nresult\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 780, "library_problem_id": 69, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 781, "code": ["<code>\nresult = scipy.optimize.line_search(test_func, test_grad, np.array(starting_point), np.array(direction))\nprint(result[0])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 781, "library_problem_id": 70, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 782, "code": ["<code>\nimport numpy as np\n\ndef get_distance_2(shape):\n    y, x = np.ogrid[:shape[0], :shape[1]]\n    cy, cx = np.array(shape) // 2  # center\n    y -= cy\n    x -= cx\n    return np.sqrt(y**2 + x**2)\n\nshape = (6, 6)\nresult = get_distance_2(shape)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 782, "library_problem_id": 71, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 783, "code": ["<code>\nimport numpy as np\n\ndef get_distance_2(shape):\n    y, x = np.ogrid[:shape[0], :shape[1]]\n    cy, cx = np.array(shape) // 2  # center\n    return np.abs(y - cy) + np.abs(x - cx)\n\nshape = (6, 6)\nresult = get_distance_2(shape)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 783, "library_problem_id": 72, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 71}}
{"id": 784, "code": ["<code>\nimport numpy as np\n\ndef f(shape=(6, 6)):\n    y, x = np.ogrid[:shape[0], :shape[1]]\n    cy, cx = np.array(shape) // 2  # center\n\n    # calculate euclidean distance\n    distances = np.sqrt((x-cx)**2 + (y-cy)**2)\n    return distances\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 784, "library_problem_id": 73, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 785, "code": ["<code>\n# Calculate the zoom factors\nzoom_factors = np.array(shape) / np.array(x.shape)\n\n# Resample the array\nresult = scipy.ndimage.zoom(x, zoom_factors, order=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 785, "library_problem_id": 74, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 786, "code": ["<code>\ndef func(x, a):\n    return a.dot(x ** 2)\n\ndef residual(x, a, y):\n    return (y - func(x, a)) ** 2\n\nout = scipy.optimize.minimize(residual, x0, args=(a, y))\nprint(out.x)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 786, "library_problem_id": 75, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 787, "code": ["<code>\ndef func(x, a):\n    return a.dot(x ** 2)\n\ndef residual(x, a, y):\n    return (y - func(x, a)) ** 2\n\ndef main():\n    out = scipy.optimize.minimize(residual, x0, args=(a, y), method='L-BFGS-B', bounds=scipy.optimize.Bounds(x_lower_bounds, np.inf))\n    print(out.x)\n\nif __name__ == '__main__':\n    main()\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 787, "library_problem_id": 76, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 75}}
{"id": 788, "code": ["<code>\ndef dN1_dt_time_varying(t, N1):\n    return -100 * N1 + np.sin(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_time_varying, t_span=time_span, y0=[N0,])\nresult = sol.y\n```\n</code>\nEND SOLUTION\nIn this solution, the function `dN1_dt_time_varying` is defined to return the derivative of `N1` at time `t`, which is a time-varying function of `N1` and `t`. The `solve_ivp` function is then used to solve this ODE with the given initial condition `y0=[N0]` and time span `t_span`. The solution values are stored in `sol.y`.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 788, "library_problem_id": 77, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 789, "code": ["<code>\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 + (t - np.sin(t) if 0 < t < 2*np.pi else 2*np.pi)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n```\nEND SOLUTION\n</code>\nIn this solution, the function `dN1_dt_simple` is modified to include the time-varying input `t - np.sin(t) if 0 < t < 2*np.pi else 2*np.pi`. The `solve_ivp` function is then called with this modified function and the initial condition `N0`. The solution is stored in `sol.y`.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 789, "library_problem_id": 78, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 790, "code": ["<code>\ndef dN1_dt_simple(t, N1):\n    return -100 * N1 - np.cos(t)\n\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_simple, t_span=time_span, y0=[N0,])\nresult = sol.y\n```\n</code>\nEND SOLUTION\nIn this solution, the function `dN1_dt_simple` is modified to include the time-varying input `-np.cos(t)`. The `solve_ivp` function is then called with this modified function and the result is stored in `result`.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 790, "library_problem_id": 79, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 791, "code": ["<code>\nfor t in range (4):\n    def const(x):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': const})\n\nout=minimize(function, x0, method=\"SLSQP\", constraints=cons)\nx=out[\"x\"]\n```\n</code>\nEND SOLUTION\n\nThe problem is that the constraints are not properly defined for each decision variable. The function `const` is defined for each decision variable `x[t]`, but it only returns the value of `x[t]`. This means that the constraint is that the decision variable `x[t]` must be equal to `x[t]`, which is always true.\n\nTo fix this, you need to define the constraint for each decision variable `x[t]` such that it is non-negative. Here is the corrected code:\n\n<code>\nfor t in range (4):\n    def const(x, t=t):    \n        y=x[t]\n        return y\n    cons.append({'type':'ineq', 'fun': lambda x, t=t: const(x, t)})\n</code>\n\nIn this corrected code, the function `const` is defined with an additional parameter `t`. This `t` is a default argument that is set when the function is defined. The lambda function that is appended to `cons` takes the same `x` as the `const` function, but also takes the `t` from the lambda function. This ensures that the `t` is passed to the `const` function when it is called by the scipy minimize function.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 791, "library_problem_id": 80, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 792, "code": ["<code>\nresult = sparse.vstack((sa, sb))\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 792, "library_problem_id": 81, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 793, "code": ["<code>\nresult = sparse.hstack((sa, sb), format='csr')\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 793, "library_problem_id": 82, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 81}}
{"id": 794, "code": ["<code>\nimport numpy as np\nfrom scipy import integrate\n\nfn = 'cooltemp.dat'\nc = np.loadtxt(fn,unpack=True,usecols=[1])\nI=[]\n\nfor n in range(len(c)):\n    # equation\n    eqn = lambda x: 2*x*c[n]\n    # integrate \n    result,error = integrate.quad(eqn,0,1)\n    I.append(result)\n\nI = np.array(I)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 794, "library_problem_id": 83, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 795, "code": ["<code>\nimport numpy as np\nfrom scipy import integrate\n\ndef f(c_values, low=0, high=1):\n    I = []\n    for c in c_values:\n        eqn = lambda x: 2*c*x\n        result, error = integrate.quad(eqn, low, high)\n        I.append(result)\n    return np.array(I)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 795, "library_problem_id": 84, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 83}}
{"id": 796, "code": ["<code>\nV = V.multiply(x)\n```\n</code>\nEND SOLUTION\nThe `multiply` function in SciPy's sparse matrices will multiply each element of the matrix by the scalar `x`. This will only affect the non-zero elements of the matrix, as desired.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 796, "library_problem_id": 85, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 797, "code": ["<code>\nV = sparse.coo_matrix((V.data + x, (V.row, V.col)), shape=V.shape)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 797, "library_problem_id": 86, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 798, "code": ["<code>\n# Convert V to a coo_matrix\nV_coo = V.tocoo()\n\n# Create a new coo_matrix with the same shape as V, but all values set to x\nA_coo = sparse.coo_matrix((np.full(V.nnz, x), (V_coo.row, V_coo.col)), shape=V.shape)\n\n# Add A_coo and y\nB_coo = A_coo + y\n\n# Convert B_coo back to a coo_matrix\nB = B_coo.tocoo()\n\nV = B\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 798, "library_problem_id": 87, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 799, "code": ["<code>\n# Convert the sparse matrix to a dense matrix\ndense_matrix = sa.toarray()\n\n# Iterate through columns\nfor col in range(sa.shape[1]):\n    column = dense_matrix[:, col]\n    list_ = [x**2 for x in column]\n    length = math.sqrt(sum(list_))\n    dense_matrix[:, col] = [x/length for x in column]\n\n# Convert the dense matrix back to a sparse matrix\nsa = sparse.csc_matrix(dense_matrix)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 799, "library_problem_id": 88, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 800, "code": ["<code>\n#csr sparse matrix\nself.__WeightMatrix__ = self.__WeightMatrix__.tocsr()\n#iterate through columns\nfor Col in range(self.__WeightMatrix__.shape[1]):\n   Column = self.__WeightMatrix__[:,Col].data\n   List = [x**2 for x in Column]\n   #get the column length\n   Len = math.sqrt(sum(List))\n   #normalize the column\n   self.__WeightMatrix__[:,Col].data = np.array([x/Len for x in Column])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 800, "library_problem_id": 89, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 88}}
{"id": 801, "code": ["<code>\na = (a != 0).astype(int)\n```\n</code>\nEND SOLUTION\nThis will convert all non-zero elements to 1 and zero elements to 0. The `!=` operator is used to check if the elements in the array are not equal to 0, and the `astype(int)` function is used to convert the boolean result to an integer.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 801, "library_problem_id": 90, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 802, "code": ["<code>\na = (a > 0).astype(int)\n```\n</code>\nEND SOLUTION\nThis will convert all non-zero elements to 1 and zero elements to 0. The `>` operator is used to check if each element in the array is greater than 0, and `astype(int)` is used to convert the boolean result to an integer.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 802, "library_problem_id": 91, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 803, "code": ["<code>\ndistances = scipy.spatial.distance.cdist(data, centroids)\nresult = np.argmin(distances, axis=0)\n```\n</code>\nEND SOLUTION\nThis solution calculates the pairwise distances between each data point and each centroid, then uses `np.argmin` to find the index of the closest data point to each centroid. The result is an array of indices, where `result[i]` is the index of the closest data point to centroid `i`.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 803, "library_problem_id": 92, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 92}}
{"id": 804, "code": ["<code>\nresult = []\nfor centroid in centroids:\n    distances = scipy.spatial.distance.cdist([centroid], data).flatten()\n    closest = np.argmin(distances)\n    result.append(data[closest])\nresult = np.array(result)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 804, "library_problem_id": 93, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 92}}
{"id": 805, "code": ["<code>\nfrom scipy.spatial.distance import cdist\n\n# Calculate the distance matrix between data and centroids\ndist_matrix = cdist(data, centroids)\n\n# Get the indices of the k-th closest elements to each centroid\nresult = np.argpartition(dist_matrix, k, axis=0)[:k, :]\n\n# Get the indices of the k-th closest elements to each cluster\nresult = np.argpartition(result, k, axis=1)[:, :k]\n\n# Get the indices of the k-th closest elements to each cluster's centroid\nresult = np.argpartition(dist_matrix[result], k, axis=1)[:, :k]\n\n# Flatten the result\nresult = result.flatten()\n\nresult\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 805, "library_problem_id": 94, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 92}}
{"id": 806, "code": ["<code>\ndef solve_for_a(x, b):\n    def eqn(a):\n        return x + 2*a - b**2\n    return fsolve(eqn, 0.5)\n\nresult = [solve_for_a(x, b) for x, b in zip(xdata, bdata)]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 806, "library_problem_id": 95, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 807, "code": ["<code>\nresult = []\nfor x, a in zip(xdata, adata):\n    sol = fsolve(lambda b: eqn(x, a, b), 0.5)\n    result.append(sol)\nresult = np.array(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 807, "library_problem_id": 96, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 808, "code": ["<code>\n```python\n# First, we need to integrate the Bekkers function over the range of interest.\n# This will give us the cumulative distribution function (CDF) of the Bekkers distribution.\ndef bekkers_cdf(x, a, m, d):\n    return integrate.quad(lambda t: bekkers(t, a, m, d), range_start, x)[0]\n\n# Next, we need to normalize the Bekkers CDF so that it becomes a valid probability distribution function (PDF).\n# This is done by dividing the Bekkers CDF by the integral of the Bekkers PDF over the range of interest.\ndef bekkers_pdf(x, a, m, d):\n    return bekkers(x, a, m, d) / integrate.quad(lambda t: bekkers(t, a, m, d), range_start, range_end)[0]\n\n# Finally, we can perform the K-S test.\n# We use the scipy.stats.kstest function, which takes as input the data and the CDF of the null distribution.\n# We use the scipy.stats.rv_continuous.ppf method to generate the null distribution.\nnull_distribution = stats.rv_continuous(a=0, b=1)\nnull_cdf = lambda x: null_distribution.ppf(x)\n\n# We use the scipy.stats.kstest function to perform the K-S test.\n# We use the scipy.stats.rv_continuous.ppf method to generate the null distribution.\nresult = stats.kstest(sample_data, lambda x: be\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 808, "library_problem_id": 97, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 809, "code": ["<code>\n# First, we need to create a cumulative distribution function (CDF) from the probability density function (PDF)\ndef bekkers_cdf(x, a, m, d):\n    integral, error = integrate.quad(bekkers, range_start, x, args=(a, m, d))\n    return integral\n\n# Then we can use scipy.stats.kstest to perform the KS test\n# We need to provide the CDF and the sample data\nkstest_result = stats.kstest(sample_data, lambda x: bekkers_cdf(x, estimated_a, estimated_m, estimated_d))\n\n# The p-value is the second element of the result\np_value = kstest_result[1]\n\n# We can reject the null hypothesis if the p-value is less than 0.05 (for a 95% confidence level)\nresult = p_value < 0.05\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 809, "library_problem_id": 98, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 810, "code": ["<code>\ndf['Time'] = pd.to_datetime(df['Time'])\ndf.set_index('Time', inplace=True)\ndf = df.resample('25S').sum()\ndf['A'] = df['A'].rolling(window=4).sum()\ndf.dropna(inplace=True)\nintegral_df = df\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 810, "library_problem_id": 99, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 811, "code": ["<code>\nimport numpy as np\n\n# Convert the tuples to numpy arrays\nx = np.array(x)\ny = np.array(y)\neval = np.array(eval)\n\n# Use griddata to interpolate the values\nresult = scipy.interpolate.griddata(x, y, eval, method='linear')\n\nprint(result)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 811, "library_problem_id": 100, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 812, "code": ["<code>\ndef multinomial_log_likelihood(weights, data):\n    return -np.sum(np.log(weights[data]))\n\ninitial_guess = np.array([1/len(a)]*len(a))\nresult = sciopt.minimize(multinomial_log_likelihood, initial_guess, args=(a['A1']), method='Nelder-Mead')\nweights = result.x\n```\n</code>\nEND SOLUTION\nThe function `multinomial_log_likelihood` calculates the negative log-likelihood of the multinomial distribution given the weights and data. The `sciopt.minimize` function is used to find the weights that minimize this function. The method 'Nelder-Mead' is used as it is simple and fast. The result is stored in the variable `weights`.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 812, "library_problem_id": 101, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 101}}
{"id": 813, "code": ["<code>\nresult = sciopt.fminbound(e, pmin, pmax, args=(x,y))\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 813, "library_problem_id": 102, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 814, "code": ["<code>\ndef find_relative_extrema(arr, n):\n    # Create a mask of elements that are less or equal to the neighbouring n elements\n    mask = np.less_equal.outer(arr, np.roll(arr, range(-n, n+1)))\n\n    # Find the indices of the elements that meet the condition\n    indices = np.where(np.all(mask, axis=0))[0]\n\n    return indices\n\nresult = find_relative_extrema(arr, n)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 814, "library_problem_id": 103, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 815, "code": ["<code>\ndef find_relative_extrema(arr, n):\n    # Initialize an empty list to store the indices of the relative extrema\n    extrema_indices = []\n\n    # Iterate over each row in the array\n    for i in range(arr.shape[0]):\n        # Apply a sliding window of size 2n+1 to the row\n        for j in range(n, arr.shape[1]-n):\n            # Check if the current element is a relative extremum\n            if all(arr[i, j-n:j] <= arr[i, j]) and all(arr[i, j+1:j+n+1] <= arr[i, j]):\n                extrema_indices.append([i, j])\n\n    return extrema_indices\n\nresult = find_relative_extrema(arr, n)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 815, "library_problem_id": 104, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 103}}
{"id": 816, "code": ["<code>\n# First, we need to separate numeric and categorical columns\nnumeric_cols = df.select_dtypes(include=[np.number]).columns\ncategorical_cols = df.select_dtypes(include=[object]).columns\n\n# Then, we calculate z-scores for numeric columns\ndf_numeric = df[numeric_cols]\ndf_numeric = df_numeric[(np.abs(stats.zscore(df_numeric)) < 3).all(axis=1)]\n\n# Then, we concatenate the numeric and categorical dataframes\ndf = pd.concat([df_numeric, df[categorical_cols]], axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 816, "library_problem_id": 105, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 817, "code": ["<code>\ndata1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 817, "library_problem_id": 0, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 818, "code": ["<code>\ndata1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 818, "library_problem_id": 1, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 819, "code": ["<code>\ndata1 = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=list(data['feature_names']) + ['target'])\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 819, "library_problem_id": 2, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 820, "code": ["<code>\ndf = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=data['feature_names'] + ['target'])\n    return df\n\ndata1 = solve(data)\nprint(data1.head())\n### END SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 820, "library_problem_id": 3, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 821, "code": ["<code>\ndf = pd.get_dummies(df['Col3'].apply(pd.Series).stack()).sum(level=0)\ndf = pd.concat([df, df_out], axis=1)\ndf_out = df.fillna(0)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 821, "library_problem_id": 4, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 822, "code": ["<code>\ndf = pd.get_dummies(df['Col3'].apply(pd.Series).stack()).sum(level=0)\ndf_out = pd.concat([df.drop('Col3', axis=1), df_out], axis=1)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 822, "library_problem_id": 5, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 823, "code": ["<code>\ndf = pd.get_dummies(df['Col4'].apply(pd.Series).stack()).sum(level=0)\ndf = pd.concat([df, df_out], axis=1)\ndf_out = df.fillna(0)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 823, "library_problem_id": 6, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 824, "code": ["<code>\n# First, we need to flatten the list of fruits in each row into a single list\nfruits = df['Col3'].apply(pd.Series).stack().reset_index(drop=True)\n\n# Then, we get the unique list of fruits\nunique_fruits = fruits.unique()\n\n# We create a new dataframe with the unique fruits as columns and 0s as initial values\ndf_out = pd.DataFrame(0, index=np.arange(len(df)), columns=unique_fruits)\n\n# We iterate over each row in the original dataframe\nfor i in range(len(df)):\n    # We iterate over each fruit in the list of fruits for the current row\n    for fruit in df.loc[i, 'Col3']:\n        # We set the corresponding cell in the new dataframe to 1\n        df_out.loc[i, fruit] = 1\n\n# We replace the original list of fruits in the original dataframe with the new dataframe\ndf = df.drop('Col3', axis=1)\ndf = pd.concat([df, df_out], axis=1)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 824, "library_problem_id": 7, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 825, "code": ["<code>\n# First, we get all unique elements from the last column\nunique_elements = set()\nfor list_ in df.iloc[:, -1]:\n    unique_elements.update(list_)\nunique_elements = list(unique_elements)\n\n# Then, we create new columns for each unique element and fill them with 0 or 1\nfor element in unique_elements:\n    df[element] = df.iloc[:, -1].apply(lambda x: 1 if element in x else 0)\n\ndf_out = df\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 825, "library_problem_id": 8, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 826, "code": ["<code>\nfrom sklearn.calibration import CalibratedClassifierCV\n\nsvmmodel=suppmach.LinearSVC(penalty='l1',C=1)\ncalibrator = CalibratedClassifierCV(svmmodel, cv=5)\ncalibrator.fit(X, y)\nproba = calibrator.predict_proba(x_test)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 826, "library_problem_id": 9, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 827, "code": ["<code>\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# Train the model\nmodel.fit(X, y)\n\n# Calibrate the model\ncalibrator = CalibratedClassifierCV(model, cv=5)\ncalibrator.fit(X, y)\n\n# Get the calibrated probabilities\nproba = calibrator.predict_proba(x_predict)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 827, "library_problem_id": 10, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 828, "code": ["<code>\n# First, convert the sparse matrix to a dense one\ndense_output = transform_output.todense()\n\n# Then convert the dense matrix to a DataFrame\ndf_output = pd.DataFrame(dense_output)\n\n# Concatenate the original dataframe with the new dataframe\ndf = pd.concat([df_origin, df_output], axis=1)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 828, "library_problem_id": 11, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 829, "code": ["<code>\n# Convert the sparse matrix to a DataFrame\ndf_sparse = pd.DataFrame.sparse.from_spmatrix(transform_output)\n\n# Merge the original DataFrame and the DataFrame with the transformed output\ndf = pd.concat([df_origin, df_sparse], axis=1)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 829, "library_problem_id": 12, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 830, "code": ["<code>\n# Convert the sparse matrix to a dense one\n    dense_output = transform_output.todense()\n\n    # Convert the dense matrix to a DataFrame\n    df_output = pd.DataFrame(dense_output)\n\n    # Concatenate the original DataFrame with the new DataFrame\n    result = pd.concat([df, df_output], axis=1)\n\n    return result\n    ### END SOLUTION\n\n# Call the function\nresult = solve(df_origin, transform_output)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 830, "library_problem_id": 13, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 831, "code": ["<code>\n</code>\nEND SOLUTION\n\nInsert a step\n<code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 831, "library_problem_id": 14, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 832, "code": ["<code>\n# Delete a step\ndel clf.steps[1]  # Deletes the PCA step\nprint(clf.named_steps)\n\n# Insert a step\nclf.steps.insert(1, ('new_step', PCA()))  # Inserts a new PCA step at position 1\nprint(clf.named_steps)\n```\nEND SOLUTION\n</code>\n\nIn the above code, we first delete the PCA step by its index in the steps list. Then we insert a new PCA step at a specific position in the steps list.\n\nPlease note that modifying the steps list directly can cause unexpected effects if not done carefully. The steps in a Pipeline are executed in the order they are listed, so deleting or inserting a step can change the order of the transformations. Also, if you delete a step that is used by another step in the pipeline, that step will not work correctly.\n\nTherefore, it's generally recommended to create a new Pipeline object with the desired steps if you want to change the order or add/remove steps.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 832, "library_problem_id": 15, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 833, "code": ["<code>\nEND SOLUTION\n\n# Inserting a new step\nclf.steps.insert(1, ('reduce_dim', PCA()))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 833, "library_problem_id": 16, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 834, "code": ["<code>\n# Inserting a step\nclf.steps.insert(1, ('new_step', PolynomialFeatures()))\nprint(clf.named_steps)\n\n# Deleting a step\ndel clf.steps[1]\nprint(clf.named_steps)\n```\n</code>\nEND SOLUTION\n\nIn the above code, we are inserting a new step 'new_step' before the second step in the pipeline. And then we are deleting the second step.\n\nPlease note that modifying the steps list directly (i.e., `clf.steps`) will not update the `clf` object. You need to reassign the modified list to `clf.steps`.\n\nAlso, be aware that the order of steps in the pipeline is important. If you delete or insert a step, you may need to adjust the order of the steps accordingly.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 834, "library_problem_id": 17, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 835, "code": ["<code>\n# Inserting a step\nclf.steps.insert(2, ('new_step', PolynomialFeatures()))\nprint(clf.named_steps)\n\n# Deleting a step\ndel clf.steps[2]\nprint(clf.named_steps)\n```\nEND SOLUTION\n</code>\n\nIn the above code, we are inserting a new step 'new_step' before the step at index 2 and deleting the step at index 2.\n\nPlease note that the index is 0-based. So, if you want to insert or delete the first step, you should use index 0.\n\nAlso, please note that modifying the steps directly in the pipeline object can lead to unexpected behavior if not done carefully. The pipeline is designed to be immutable, meaning that once you have a pipeline object, you should not modify it. Instead, you should create a new pipeline object with the desired steps.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 835, "library_problem_id": 18, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 836, "code": ["<code>\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# Create a new pipeline with the inserted step\nnew_clf = make_pipeline(StandardScaler(), ('t1919810', PCA()), ('reduce_dim', PCA()), ('pOly', PolynomialFeatures()), ('svdm', SVC()))\n\n# Replace the old pipeline with the new one\nclf.steps = new_clf.steps\n```\nEND SOLUTION\n\nIn this solution, we first create a new pipeline with the inserted step. Then we replace the steps of the old pipeline with the new ones. This way, we have inserted a new step into the pipeline without modifying the original pipeline.\n\nPlease note that the order of the steps in the pipeline is important. The new pipeline is created with the inserted step before the 'reduce_dim' step. If you want to insert the step after the 'reduce_dim' step, you would need to adjust the order of the steps accordingly.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 836, "library_problem_id": 19, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 17}}
{"id": 837, "code": ["<code>\n# Define the parameter grid\nparamGrid = {'n_estimators': [100, 500, 1000], 'max_depth': [2, 4, 6], 'learning_rate': [0.01, 0.1, 0.2]}\n\n# Define the XGBoost regressor\nmodel = xgb.XGBRegressor()\n\n# Define the evaluation set\neval_set = [(testX, testY)]\n\n# Define the fit parameters\nfit_params = {\"early_stopping_rounds\": 42, \"eval_metric\": \"mae\", \"eval_set\": eval_set}\n\n# Define the GridSearchCV\ngridsearch = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=5).get_n_splits([trainX, trainY]), n_jobs=-1)\n\n# Fit the GridSearchCV\ngridsearch.fit(trainX, trainY, **fit_params)\n\n# Get the best score\nb = gridsearch.best_score_\n\n# Get the best parameters\nprint(gridsearch.best_params_)\n\n# Make predictions\nc = gridsearch.predict(testX)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 837, "library_problem_id": 20, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 838, "code": ["<code>\n# Define the parameter grid\nparamGrid = {'n_estimators': [100, 500, 1000], 'max_depth': [2, 4, 6], 'learning_rate': [0.01, 0.1, 0.2]}\n\n# Define the XGBoost regressor model\nmodel = xgb.XGBRegressor()\n\n# Define the fit parameters\nfit_params={\"early_stopping_rounds\":42,\n            \"eval_metric\" : \"mae\",\n            \"eval_set\" : [[testX, testY]]}\n\n# Define the GridSearchCV\ngridsearch = GridSearchCV(model, paramGrid, verbose=1, cv=TimeSeriesSplit(n_splits=3).get_n_splits([trainX, trainY]), n_jobs=-1, fit_params=fit_params)\n\n# Fit the GridSearchCV\ngridsearch.fit(trainX, trainY)\n\n# Get the best score\nb = gridsearch.best_score_\n\n# Get the best parameters\nprint(gridsearch.best_params_)\n\n# Predict on the test set\nc = gridsearch.predict(testX)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 838, "library_problem_id": 21, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 839, "code": ["<code>\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\n\nproba = np.concatenate(proba)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 839, "library_problem_id": 22, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 840, "code": ["<code>\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\n\nproba = np.concatenate(proba)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 840, "library_problem_id": 23, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 841, "code": ["<code>\ninversed = scaler.inverse_transform(scaled)\n```\n</code>\nEND SOLUTION\n\nYes, you can use the `inverse_transform` method of the `StandardScaler` to get back the original values of the 't' feature.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 841, "library_problem_id": 24, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 842, "code": ["<code>\ninversed = scaler.inverse_transform(scaled)\n    return inversed\n### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 842, "library_problem_id": 25, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 24}}
{"id": 843, "code": ["<code>\nmodel_name = model.__class__.__name__\n```\n</code>\nEND SOLUTION\n\nThis will give you the name of the model as a string. In this case, it will be 'LinearRegression'.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 843, "library_problem_id": 26, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 844, "code": ["<code>\nmodel_name = model.__class__.__name__\n```\n</code>\nEND SOLUTION\nIn this case, `model.__class__.__name__` will give you the name of the model without its parameters.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 844, "library_problem_id": 27, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 845, "code": ["<code>\nmodel_name = model.__class__.__name__\n```\nEND SOLUTION\n</code>\n\nThis will give you the name of the model as a string. In this case, it will be 'LinearSVC'.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 845, "library_problem_id": 28, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 846, "code": ["<code>\ntf_idf_out = pipe.fit_transform(data.test)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 846, "library_problem_id": 29, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 847, "code": ["<code>\nEND SOLUTION\n</code>\n\nThe `fit_transform` method in a pipeline applies the `fit_transform` method of each step in the pipeline to the data. This means that the output of the TfidfVectorizer will be the input to the NMF.\n\nIf you want to get the output of the TfidfVectorizer before it is passed to the NMF, you can use the `transform` method instead of `fit_transform`. However, this will not update the parameters of the TfidfVectorizer (like the vocabulary), so you might not get the expected results if you use the `transform` method on new data.\n\nIf you want to get the intermediate result, you can use the `fit_transform` method on the data and then access the output of the TfidfVectorizer by using the `named_steps` attribute of the pipeline. Here is how you can do it:\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 847, "library_problem_id": 30, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 848, "code": ["<code>\n</code>\nEND SOLUTION\n\nIn the above code, `pipe.fit_transform(data, target)` will fit the pipeline and transform the data at each step. The output of the 'select' step will be the transformed data.\n\nHowever, if you want to get the transformed data without fitting the pipeline, you can use the `fit_transform` method of the 'select' step directly.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 848, "library_problem_id": 31, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 849, "code": ["<code>\nclf = GridSearchCV(bc, param_grid, cv=5)\nclf.fit(X_train, y_train)\nproba = clf.predict_proba(X_test)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 849, "library_problem_id": 32, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 850, "code": ["<code>\n# Reshape your data to be 2D array\nX = np.array(X).reshape(-1, 1)\ny = np.array(y)\n\n# Fit the model\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=2, random_state=42)\nregressor.fit(X, y)\n\n# Predict\nX_test = np.array(X_test).reshape(-1, 1)\npredict = regressor.predict(X_test)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 850, "library_problem_id": 33, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 851, "code": ["<code>\n# Reshape your data to be 2D array\nX = np.array(X).reshape(-1, 1)\ny = np.array(y)\n\n# Fit the model\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=2, random_state=42)\nregressor.fit(X, y)\n\n# Predict\nX_test = np.array(X_test).reshape(-1, 1)\npredict = regressor.predict(X_test)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 851, "library_problem_id": 34, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 33}}
{"id": 852, "code": ["<code>\ndef preprocess(s):\n    return s.upper()\n\n# Create a TfidfVectorizer object with the preprocessor parameter set to the preprocess function\ntfidf = TfidfVectorizer(preprocessor=preprocess)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 852, "library_problem_id": 35, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 853, "code": ["<code>\ndef prePro(text):\n    return text.lower()\n\ntfidf = TfidfVectorizer(preprocessor=prePro)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 853, "library_problem_id": 36, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Semantic", "perturbation_origin_id": 35}}
{"id": 854, "code": ["<code>\nscaled_data = preprocessing.scale(data)\nscaled_df = pd.DataFrame(scaled_data, index=data.index, columns=data.columns)\ndf_out = scaled_df\n```\nEND SOLUTION\n</code>\nThe solution is to first scale the data using sklearn's preprocessing.scale function, which returns a numpy array. Then, we convert this numpy array back into a pandas DataFrame, specifying the index and column names from the original DataFrame. This way, the index and column information from the original DataFrame is preserved.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 854, "library_problem_id": 37, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 855, "code": ["<code>\nscaled_data = preprocessing.scale(data)\nscaled_df = pd.DataFrame(scaled_data, index=data.index, columns=data.columns)\ndf_out = scaled_df\n```\nEND SOLUTION\n</code>\nThe solution is to first scale the data using sklearn's preprocessing.scale function, which returns a numpy array. Then, we convert this numpy array back into a pandas DataFrame, specifying the index and column names from the original DataFrame. This way, the scaled DataFrame will retain the original index and column names.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 855, "library_problem_id": 38, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 37}}
{"id": 856, "code": ["<code>\ngrid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_\n```\nEND SOLUTION\n</code>\n\nThe best model can be accessed through the attribute `best_estimator_` of the `GridSearchCV` object. The coefficients of the model can be accessed through the `coef_` attribute of the `SGDClassifier` object.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 856, "library_problem_id": 39, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 857, "code": ["<code>\ngrid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_\n```\nEND SOLUTION\n</code>\n\nThe best model can be accessed through the attribute `best_estimator_` of the `GridSearchCV` object. The coefficients of the model can be accessed through the `coef_` attribute of the `RidgeClassifier` object.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 857, "library_problem_id": 40, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 858, "code": ["<code>\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\nmask = model.get_support() #list of booleans for selected features\nnew_features = [] # The list of your K best features\nfor bool, feature in zip(mask, X.columns):\n    if bool:\n        new_features.append(feature)\ncolumn_names = np.array(new_features)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 858, "library_problem_id": 41, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 859, "code": ["<code>\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\nmask = model.get_support() # list of booleans for selected features\nnew_feature_names = [] # The list of your column names\nfor bool, name in zip(mask, X.columns):\n    if bool:\n        new_feature_names.append(name)\ncolumn_names = new_feature_names\nprint(column_names)\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 859, "library_problem_id": 42, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 860, "code": ["<code>\n# get the mask of selected features\nmask = model.get_support()\n\n# get the column names of selected features\ncolumn_names = X.columns[mask]\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 860, "library_problem_id": 43, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 861, "code": ["<code>\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(X)\nmask = model.get_support() #list of booleans for selected features\nnew_features = [] # The list of your K best features\n\nfor bool, name in zip(mask, X.columns):\n    if bool:\n        new_features.append(name)\ncolumn_names = new_features\nprint(column_names)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 861, "library_problem_id": 44, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 862, "code": ["<code>\nkm = KMeans(n_clusters=p)\nkm.fit(X)\ndistances = km.transform(X)\nclosest_50_samples = X[np.argpartition(distances[:, p-1], 50)[:50]]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 862, "library_problem_id": 45, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 863, "code": ["<code>\nkm = km.fit(X)\ndistances = km.transform(X)\nindices = np.argsort(distances[:, p])\nclosest_50_samples = X[indices[:50]]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 863, "library_problem_id": 46, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 864, "code": ["<code>\nkm = KMeans(n_clusters=p)\nkm.fit(X)\ndistances = km.transform(X)\nclosest_100_samples = X[np.argpartition(distances[:, p-1], 100)[:100]]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 864, "library_problem_id": 47, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 865, "code": ["<code>\nkm.fit(X)\n    cluster_center = km.cluster_centers_[p]\n    distances = np.linalg.norm(X - cluster_center, axis=1)\n    closest_samples = X[np.argsort(distances)[:50]]\n    return closest_samples\n    ### END SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 865, "library_problem_id": 48, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 866, "code": ["<code>\n# Convert categorical variable to matrix and merge back with original training data use get_dummies in pandas\nX_train = pd.get_dummies(X_train)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 866, "library_problem_id": 49, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 867, "code": ["<code>\n# Convert categorical variable to matrix\nX_train_dummies = pd.get_dummies(X_train[0])\n\n# Merge back with original training data\nX_train = pd.concat([X_train.drop(0, axis=1), X_train_dummies], axis=1)\n```\nEND SOLUTION\n</code>\n\nThis code will convert the categorical variable to a matrix using pandas' get_dummies function, and then merge this back with the original training data. The categorical variable is dropped from the original dataframe before the dummy variables are added.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 867, "library_problem_id": 50, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 868, "code": ["<code>\nfrom sklearn.svm import SVR\n\n# Create a SVM regressor with a gaussian kernel\nregressor = SVR(kernel='rbf')\n\n# Fit the regressor to the data\nregressor.fit(X, y)\n\n# Predict the output for X\npredict = regressor.predict(X)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 868, "library_problem_id": 51, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 869, "code": ["<code>\nfrom sklearn.svm import SVR\n\n# Create a SVM with a gaussian kernel\nsvr = SVR(kernel='rbf')\n\n# Fit the model to the data\nsvr.fit(X, y)\n\n# Predict the output\npredict = svr.predict(X)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 869, "library_problem_id": 52, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 51}}
{"id": 870, "code": ["<code>\nfrom sklearn.svm import SVR\n\n# Create a SVM regressor with a polynomial kernel\nregressor = SVR(kernel='poly', degree=2)\n\n# Fit the regressor to the data\nregressor.fit(X, y)\n\n# Predict the output for X\npredict = regressor.predict(X)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 870, "library_problem_id": 53, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 51}}
{"id": 871, "code": ["<code>\nfrom sklearn.svm import SVR\n\n# Create a SVM Regressor with a polynomial kernel\nregressor = SVR(kernel='poly', degree=2)\n\n# Fit the model to the data\nregressor.fit(X, y)\n\n# Predict the output\npredict = regressor.predict(X)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 871, "library_problem_id": 54, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 53}}
{"id": 872, "code": ["<code>\nquery_vectors = tfidf.transform(queries)\ncosine_similarities_of_queries = np.asarray((query_vectors * tfidf_matrix.T).todense())\n```\nEND SOLUTION\n</code>\nThe solution is to transform the queries into vectors using the same TfidfVectorizer that was used to transform the documents. Then, we calculate the cosine similarity between the query vectors and the document vectors. The result is a matrix where each row corresponds to a query and each column corresponds to a document. The value at position (i, j) is the cosine similarity between the i-th query and the j-th document.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 872, "library_problem_id": 55, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 55}}
{"id": 873, "code": ["<code>\nquery_vectors = tfidf.transform(queries)\ncosine_similarities_of_queries = np.asarray((query_vectors * tfidf_matrix.T).todense())\n```\nEND SOLUTION\n</code>\n\nThe solution is to transform the queries into vectors using the same TfidfVectorizer that was used to transform the documents. Then, we calculate the cosine similarity between the query vectors and the document vectors. The result is a matrix where each row represents the cosine similarity between a query and all documents.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 873, "library_problem_id": 56, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 874, "code": ["<code>\n# Transform the queries into tf-idf vectors\n    query_vectors = tfidf.transform(queries)\n\n    # Calculate the cosine similarity between the queries and the documents\n    cosine_similarities = np.asarray((query_vectors * tfidf.idf_).sum(axis=1)).T.todense()\n\n    return cosine_similarities\n    ### END SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 874, "library_problem_id": 57, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 875, "code": ["<code>\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer()\nnew_features = mlb.fit_transform(features)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 875, "library_problem_id": 58, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 876, "code": ["<code>\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer()\nnew_f = mlb.fit_transform(f)\n```\n</code>\nEND SOLUTION\n\nThis solution uses the MultiLabelBinarizer from sklearn.preprocessing. It transforms the list of lists into a 2D numpy array where each unique item in the original list is a column in the new array. The values in the new array are boolean (1 for True, 0 for False) indicating whether the item was present in the original list for that row.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 876, "library_problem_id": 59, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 877, "code": ["<code>\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer()\nnew_features = mlb.fit_transform(features)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 877, "library_problem_id": 60, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 878, "code": ["<code>\n# First, we need to find all unique features across all samples\n    all_features = set(feature for sublist in features for feature in sublist)\n\n    # Then, we create a dataframe with these features and fill it with zeros\n    df = pd.DataFrame(0, index=range(len(features)), columns=all_features)\n\n    # For each sample, we set the corresponding feature to 1\n    for i, sample in enumerate(features):\n        df.loc[i, sample] = 1\n\n    # Finally, we convert the dataframe to a numpy array\n    new_features = df.values\n    return new_features\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 878, "library_problem_id": 61, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 879, "code": ["<code>\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer()\nnew_features = mlb.fit_transform(f)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 879, "library_problem_id": 62, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 58}}
{"id": 880, "code": ["<code>\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Create a linkage matrix and then plot the dendrogram\nlinkage_matrix = linkage(data_matrix, 'ward')\nplt.figure(figsize=(10, 5))\ndendrogram(linkage_matrix, leaf_rotation=90.)\nplt.show()\n\n# Create a clustering object\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\ncluster_labels = cluster.fit_predict(data_matrix)\ncluster_labels\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 880, "library_problem_id": 63, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 881, "code": ["<code>\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Create a linkage matrix and then plot the dendrogram\nlinkage_matrix = linkage(data_matrix, 'ward')\nplt.figure(figsize=(10, 5))\ndendrogram(linkage_matrix, leaf_rotation=90.)\nplt.show()\n\n# Create a clustering object\nclustering = AgglomerativeClustering(n_clusters=2, linkage='ward')\n\n# Fit the data\nclustering.fit(data_matrix)\n\n# Get the labels\ncluster_labels = clustering.labels_\ncluster_labels\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 881, "library_problem_id": 64, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 882, "code": ["<code>\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Create a distance matrix\ndistM = 1 - np.array(simM)\n\n# Create a linkage matrix and then convert it to a condensed distance matrix\nlinkage_matrix = linkage(distM, 'ward')\ndist_matrix = squareform(linkage_matrix[:, 2])\n\n# Perform hierarchical clustering\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\ncluster_labels = cluster.fit_predict(dist_matrix)\n\ncluster_labels\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 882, "library_problem_id": 65, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 883, "code": ["<code>\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n# Perform hierarchical/agglomerative clustering\nZ = linkage(data_matrix, 'ward')\n\n# Cut the dendrogram at a certain height (e.g., 0.5)\n# This will give two clusters\ncluster_labels = fcluster(Z, t=0.5, criterion='distance')\n\n# Print the cluster labels\nprint(cluster_labels)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 883, "library_problem_id": 66, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 884, "code": ["<code>\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n# Perform hierarchical/agglomerative clustering\nZ = linkage(data_matrix, 'ward')\n\n# Cut the dendrogram at a certain height (e.g., 0.5)\n# This will give two clusters\nfrom scipy.cluster.hierarchy import fcluster\ncluster_labels = fcluster(Z, t=0.5, criterion='distance')\n\n# Print the cluster labels\nprint(cluster_labels)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 884, "library_problem_id": 67, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 885, "code": ["<code>\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n# Perform hierarchical/agglomerative clustering\nZ = linkage(simM, 'ward')\n\n# Get the cluster labels\nfrom scipy.cluster.hierarchy import fcluster\ncluster_labels = fcluster(Z, 2, criterion='maxclust')\n\ncluster_labels\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 885, "library_problem_id": 68, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 886, "code": ["<code>\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 886, "library_problem_id": 69, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 69}}
{"id": 887, "code": ["<code>\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ncentered_scaled_data = scaler.fit_transform(data)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 887, "library_problem_id": 70, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 69}}
{"id": 888, "code": ["<code>\nfrom sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer(method='box-cox')\nbox_cox_data = pt.fit_transform(data)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 888, "library_problem_id": 71, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 889, "code": ["<code>\nfrom sklearn.preprocessing import PowerTransformer\npt = PowerTransformer(method='box-cox')\nbox_cox_data = pt.fit_transform(data)\n```\nEND SOLUTION\n</code>\n\nThe PowerTransformer class in sklearn.preprocessing can be used to perform power transformations, including the Box-Cox transformation. The 'box-cox' method is specified when creating the PowerTransformer object. The fit_transform method is then used to transform the data.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 889, "library_problem_id": 72, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 890, "code": ["<code>\nfrom sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer(method='yeo-johnson')\nyeo_johnson_data = pt.fit_transform(data)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 890, "library_problem_id": 73, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 891, "code": ["<code>\nfrom sklearn.preprocessing import PowerTransformer\n\n# Initialize PowerTransformer\npt = PowerTransformer(method='yeo-johnson')\n\n# Fit and transform the data\nyeo_johnson_data = pt.fit_transform(data)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 891, "library_problem_id": 74, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 73}}
{"id": 892, "code": ["<code>\nvectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w\\w+\\b\")\ntransformed_text = vectorizer.fit_transform([text])\n```\n</code>\nEND SOLUTION\n\nThe solution uses a regular expression to match words. The `(?u)` flag makes the pattern \"ungreedy\", which means it will match the shortest possible string. The `\\b\\w\\w+\\b` pattern matches any word defined as one or more word characters between word boundaries. This will preserve punctuation marks that are not part of a word.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 892, "library_problem_id": 75, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 893, "code": ["<code>\nfrom sklearn.model_selection import train_test_split\n\n# Split the dataset into features (x) and target (y)\nx = dataset.iloc[:, :-1]\ny = dataset.iloc[:, -1]\n\n# Split the dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 893, "library_problem_id": 76, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 894, "code": ["<code>\nfrom sklearn.model_selection import train_test_split\n\n# Assuming df is your dataframe\nX = df.iloc[:, :-1]  # all columns except the last one\ny = df.iloc[:, -1]  # the last column\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 894, "library_problem_id": 77, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 895, "code": ["<code>\nfrom sklearn.model_selection import train_test_split\n\n# Split the dataset into features (x) and target (y)\nx = dataset.iloc[:, :-1]\ny = dataset.iloc[:, -1]\n\n# Split the dataset into training and testing sets with 30% of the data for testing and 70% for training\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n\n# Further split the training set into 70% for training and 30% for validation\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.33, random_state=42)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 895, "library_problem_id": 78, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 896, "code": ["<code>\nfrom sklearn.model_selection import train_test_split\n\n    # Split the dataset into 80% training and 20% testing\n    train_set, test_set = train_test_split(dataset, test_size=0.2, random_state=42)\n\n    # Split the training set into x (all columns except the last one) and y (the last column)\n    x_train = train_set.iloc[:, :-1]\n    y_train = train_set.iloc[:, -1]\n\n    # Split the testing set into x (all columns except the last one) and y (the last column)\n    x_test = test_set.iloc[:, :-1]\n    y_test = test_set.iloc[:, -1]\n\n    return x_train, y_train, x_test, y_test\n\nx_train, y_train, x_test, y_test = solve(dataset)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 896, "library_problem_id": 79, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 897, "code": ["<code>\nmse_values = df['mse'].values.reshape(-1,1)\nkmeans = KMeans(n_clusters=2, n_init=10).fit(mse_values)\nlabels = kmeans.predict(mse_values)\n```\nEND SOLUTION\n</code>\nIn your case, you are only using one feature (mse), so you need to reshape your data to fit the input requirement of the KMeans function. The reshape function is used to change the number of rows and columns of the array. In this case, you want to change the number of columns to 1, so you use -1 as the argument for the rows. This tells numpy to calculate the number of rows automatically based on the original data and the number of columns you want.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 897, "library_problem_id": 80, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 898, "code": ["<code>\nimport numpy as np\nimport pandas as pd\n\ndf = pd.read_csv(\"file.csv\", parse_dates=[\"date\"])\nf1 = df['mse'].values.reshape(-1,1) # reshape the data to fit the KMeans model\nkmeans = KMeans(n_clusters=2, n_init=10).fit(f1)\nlabels = kmeans.predict(f1)\ncentroids = kmeans.cluster_centers_\n```\nEND SOLUTION\n</code>\nThe reshape function is used to change the dimensions of the array. In this case, we are changing the 1D array to a 2D array because KMeans requires a 2D array as input. The parameter -1 in the reshape function means that the size of that dimension is inferred.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 898, "library_problem_id": 81, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 899, "code": ["<code>\nsvc = LinearSVC(C=1.0, penalty='l1', dual=False)\nsvc.fit(X, y)\nmask = svc.coef_ != 0\nselected_indices = np.where(mask.sum(axis=0) != 0)[1]\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[selected_indices]\n```\nEND SOLUTION\n</code>\nIn this solution, we first fit a LinearSVC model with L1 penalty to the data. The coefficients of the model are stored in the `coef_` attribute. We then create a mask where the coefficients are not equal to zero. This mask indicates which features are selected by the model. We then find the indices of the selected features using `np.where`. Finally, we retrieve the names of the selected features using `np.asarray(vectorizer.get_feature_names())[selected_indices]`.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 899, "library_problem_id": 82, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 900, "code": ["<code>\nsvc = LinearSVC(C=1.0, penalty='l1', dual=False)\nsvc.fit(X, y)\ncoef = svc.coef_\nindices = np.argsort(coef)\nselected_feature_names = np.asarray(vectorizer.get_feature_names())[indices]\n```\nEND SOLUTION\n</code>\n\nIn the solution, we first fit the LinearSVC model to the data. The coefficients of the model are stored in `svc.coef_`. We then sort these coefficients to get the indices of the features that have the smallest coefficients first. We use `np.argsort` to get the indices. Finally, we use these indices to get the feature names from the vectorizer and store them in `selected_feature_names`.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 900, "library_problem_id": 83, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 901, "code": ["<code>\nsvc = LinearSVC(C=1.0, penalty='l1', dual=False)\n    svc.fit(X, y)\n    selected_features = np.where(svc.coef_ != 0)[1]\n    selected_feature_names = np.asarray(vectorizer.get_feature_names())[selected_features]\n    return selected_feature_names\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 901, "library_problem_id": 84, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 902, "code": ["<code>\nvocabulary = {'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'}\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary=vocabulary)\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names_out()\n```\nEND SOLUTION\n</code>\n\nIn the above code, the vocabulary is set in the CountVectorizer and the feature_names are obtained using get_feature_names_out() method. This will ensure that the vocabulary is in the order as defined in the set.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 902, "library_problem_id": 85, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 903, "code": ["<code>\nvocabulary = {'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'}\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary=vocabulary)\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names_out()\nX = X.toarray()\n```\nEND SOLUTION\n</code>\n\nIn the above code, the vocabulary is set in the CountVectorizer and the feature_names are obtained using get_feature_names_out() method. The output is in the same order as the vocabulary.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 903, "library_problem_id": 86, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 904, "code": ["<code>\nvocabulary = {'Jscript','.Net','TypeScript','SQL', 'NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'}\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary=vocabulary)\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names_out()\nX = X.toarray()\n\n# reorder columns to match the original vocabulary order\nX = X[:, np.argsort(vectorizer.vocabulary)]\n\n# convert to binary\nX = (X > 0).astype(int)\n\n# check the results\nprint(feature_names)\nprint(X)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 904, "library_problem_id": 87, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 905, "code": ["<code>\nvocabulary = {'Jscript','.Net','TypeScript','NodeJS','Angular','Mongo','CSS','Python','PHP','Photoshop','Oracle','Linux','C++',\"Java\",'TeamCity','Frontend','Backend','Full stack', 'UI Design', 'Web','Integration','Database design','UX'}\nvectorizer = CountVectorizer(stop_words=\"english\",binary=True,lowercase=False,vocabulary=vocabulary)\nX = vectorizer.fit_transform(corpus)\nfeature_names = vectorizer.get_feature_names_out()\nX = X.toarray()\n\n# reorder feature_names to match the order in vocabulary\nidx = np.argsort(np.array(list(vocabulary)))\nfeature_names = feature_names[idx]\nX = X[:, idx]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 905, "library_problem_id": 88, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 906, "code": ["<code>\nslopes = []\nfor col in df1.columns[1:]:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes.append(m)\nslopes = np.array(slopes)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 906, "library_problem_id": 89, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 907, "code": ["<code>\nslopes = []\nfor col in df1.columns[1:]:\n    df2 = df1[~np.isnan(df1[col])]\n    df3 = df2[['Time', col]]\n    npMatrix = np.matrix(df3)\n    X, Y = npMatrix[:,0], npMatrix[:,1]\n    slope = LinearRegression().fit(X,Y)\n    m = slope.coef_[0]\n    slopes.append(m)\nslopes = np.array(slopes)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 907, "library_problem_id": 90, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 89}}
{"id": 908, "code": ["<code>\n# Initialize LabelEncoder\nle = LabelEncoder()\n\n# Fit and transform 'Sex' column\ndf['Sex'] = le.fit_transform(df['Sex'])\n\ntransformed_df = df\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 908, "library_problem_id": 91, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 909, "code": ["<code>\n# Initialize a LabelEncoder\nle = LabelEncoder()\n\n# Fit and transform the 'Sex' column\ndf['Sex'] = le.fit_transform(df['Sex'])\n\ntransformed_df = df\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 909, "library_problem_id": 92, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 910, "code": ["<code>\ndef Transform(df):\n    le = LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 910, "library_problem_id": 93, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 911, "code": ["<code>\n# ElasticNet Regression\n\nfrom sklearn import linear_model\nimport statsmodels.api as sm\n\nElasticNet = linear_model.ElasticNet() # create a ElasticNet instance\nElasticNet.fit(X_train, y_train) # fit data\n\n# print(ElasticNet.coef_)\n# print (ElasticNet.intercept_) # print out the coefficients\n\nprint (\"R^2 for training set:\")\nprint (ElasticNet.score(X_train, y_train))\n\nprint ('-'*50)\n\nprint (\"R^2 for test set:\")\nprint (ElasticNet.score(X_test, y_test))\n\ntraining_set_score = ElasticNet.score(X_train, y_train)\ntest_set_score = ElasticNet.score(X_test, y_test)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 911, "library_problem_id": 94, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 912, "code": ["<code>\nscaler = MinMaxScaler()\nnp_array = np_array.reshape(-1, 1)  # reshape the array to 1D\nnp_array_transformed = scaler.fit_transform(np_array)\ntransformed = np_array_transformed.reshape(2, 2)  # reshape it back to 2D\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 912, "library_problem_id": 95, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 913, "code": ["<code>\nscaler = MinMaxScaler()\nnp_array_reshaped = np_array.reshape(-1,1)\nnp_array_scaled = scaler.fit_transform(np_array_reshaped)\ntransformed = np_array_scaled.reshape(np_array.shape)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 913, "library_problem_id": 96, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 914, "code": ["<code>\ndef Transform(a):\n    # reshape the array into a 1D array\n    a = a.reshape(-1)\n    # initialize MinMaxScaler\n    scaler = MinMaxScaler()\n    # fit and transform the array\n    a = scaler.fit_transform(a.reshape(-1, 1))\n    # return the transformed array\n    return a\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 914, "library_problem_id": 97, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 915, "code": ["<code>\nclose_buy1 = close.values[:-1]\nm5 = ma50.values[:-1]\nm10 = ma100.values[:-1]\nma20 = ma200.values[:-1]\nb = np.concatenate([close_buy1, m5, m10, ma20], axis=1)\npredict = clf.predict(b)\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 915, "library_problem_id": 98, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 916, "code": ["<code>\n# Convert the string data to numerical data\nle = LabelEncoder()\nnew_X = np.array(X)\nnew_X[:, 0] = le.fit_transform(new_X[:, 0])\nnew_X = new_X.astype(float)\n\n# Train the model\nclf.fit(new_X, ['2', '3'])\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 916, "library_problem_id": 99, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 917, "code": ["<code>\n# Convert the string data to numerical data\nle = LabelEncoder()\nnew_X = np.array(X)\nnew_X[:, 0] = le.fit_transform(new_X[:, 0])\nnew_X = new_X.astype(float)\n\n# Convert the labels to numerical data\nle = LabelEncoder()\ny = ['2', '3']\ny = le.fit_transform(y)\n\n# Fit the model\nclf.fit(new_X, y)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 917, "library_problem_id": 100, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 918, "code": ["<code>\n# Convert the string data to numerical data\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nnew_X = np.array(X)\nnew_X[:, 0] = le.fit_transform(new_X[:, 0])\nnew_X = new_X.astype(float)\n\n# Train the model\nclf.fit(new_X, ['4', '5'])\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 918, "library_problem_id": 101, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 919, "code": ["<code>\n# Data splt\n# Seperating the data into dependent and independent variables\nX = dataframe.iloc[:, :-1].astype(float)\ny = dataframe.iloc[:, -1]\n\nlogReg = LogisticRegression()\n\nlogReg.fit(X, y)\n\n# Predicting the class of the last row of the dataframe\npredict = logReg.predict(X.iloc[-1:])\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 919, "library_problem_id": 102, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 920, "code": ["<code>\n# Split the data into X and y\nX = dataframe.iloc[:, :-1].astype(float)\ny = dataframe.iloc[:, -1]\n\n# Initialize the logistic regression model\nlogReg = LogisticRegression()\n\n# Fit the model with the data\nlogReg.fit(X, y)\n\n# Make predictions\npredict = logReg.predict(X)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 920, "library_problem_id": 103, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 102}}
{"id": 921, "code": ["<code>\n# Split the dataframe into a feature dataframe and a target dataframe\nX = features_dataframe.drop('date', axis=1)\ny = features_dataframe['date']\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n\n# Convert the date back to dataframe and sort it\ntrain_dataframe = pd.concat([X_train, y_train], axis=1).sort_values('date')\ntest_dataframe = pd.concat([X_test, y_test], axis=1).sort_values('date')\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 921, "library_problem_id": 104, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 922, "code": ["<code>\n# Split the data into train and test sets\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, test_size=0.2, shuffle=False)\n\n# Sort the dataframes by date\ntrain_dataframe = train_dataframe.sort_values(by='date')\ntest_dataframe = test_dataframe.sort_values(by='date')\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 922, "library_problem_id": 105, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 923, "code": ["<code>\n```python\n    # First, we need to sort the dataframe by date\n    features_dataframe = features_dataframe.sort_values(by='date')\n\n    # Then, we calculate the index where the split should occur\n    split_index = int(len(features_dataframe) * 0.2)\n\n    # We split the dataframe into two parts\n    train_dataframe = features_dataframe.iloc[:split_index]\n    test_dataframe = features_dataframe.iloc[split_index:]\n\n    return train_dataframe, test_dataframe\n\ntrain_dataframe, test_dataframe = solve(features_dataframe)\n### END SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 923, "library_problem_id": 106, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 924, "code": ["<code>\ncols = ['X2', 'X3']\nfor name, group in df.groupby('Month'):\n    scaled = scaler.fit_transform(group[cols])\n    df.loc[group.index, 'X2_scale'] = scaled[:,0]\n    df.loc[group.index, 'X3_scale'] = scaled[:,1]\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 924, "library_problem_id": 107, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 925, "code": ["<code>\ncols = myData.columns[2:4]\nfor month, df in myData.groupby('Month'):\n    scaled_data = scaler.fit_transform(df[cols])\n    new_cols = ['new_' + col for col in cols]\n    df[new_cols] = scaled_data\nmyData = myData.sort_values(by='Month').reset_index(drop=True)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 925, "library_problem_id": 108, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 107}}
{"id": 926, "code": ["<code>\ncount = CountVectorizer(lowercase = False, token_pattern=r\"(?u)\\b\\w\\w+\\b\")\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 926, "library_problem_id": 109, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 927, "code": ["<code>\ncount = CountVectorizer(lowercase = False, token_pattern=r'\\b\\w\\w+\\b|#\\w+|@\\w+')\nvocabulary = count.fit_transform([words])\nfeature_names = count.get_feature_names_out()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 927, "library_problem_id": 110, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 109}}
{"id": 928, "code": ["<code>\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results\n```\n</code>\nEND SOLUTION\nThis will give you a pandas DataFrame with all the information that GridSearchCV has computed. This includes the scores of all the models, the parameters used, the rank of the models, and many more.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 928, "library_problem_id": 111, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 929, "code": ["<code>\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results = full_results.sort_values(by='mean_fit_time')\nfull_results\n```\n</code>\nEND SOLUTION\nThis will give you a pandas DataFrame sorted by mean_fit_time. The DataFrame contains all the information that GridSearchCV stores, including the parameters of each run, the score for each parameter set, and the time taken to fit each parameter set.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 929, "library_problem_id": 112, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 930, "code": ["<code>\n</code>\nTo load the model later, you can use:\n<code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 930, "library_problem_id": 113, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 931, "code": ["<code>\ntfidf_matrix = tfidf.fit_transform(df['description'])\ncosine_similarity_matrix = sklearn.metrics.pairwise.cosine_similarity(tfidf_matrix, tfidf_matrix)\n```\n</code>\nEND SOLUTION\n\nThe cosine_similarity_matrix will be a square matrix where the entry at index i,j is the cosine similarity between the i-th and j-th descriptions.\n\nTo answer the question \"What is the similarity between the items 1fgg and 2hhj\", you would look at the entry in the matrix at position (0,1) or (1,0), depending on whether you consider '1fgg' as the first or second item. In this case, it would be at position (0,1) because '1fgg' is the first item in the DataFrame. So, the similarity between '1fgg' and '2hhj' is cosine_similarity_matrix[0,1].\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 931, "library_problem_id": 114, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 932, "code": ["<code>\nfor param_group in optim.param_groups:\n    param_group['lr'] = 0.001\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 932, "library_problem_id": 0, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 933, "code": ["<code>\nfor param_group in optim.param_groups:\n    param_group['lr'] = 0.001\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 933, "library_problem_id": 1, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 934, "code": ["<code>\nfor param_group in optim.param_groups:\n    param_group['lr'] = 0.0005\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 934, "library_problem_id": 2, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 935, "code": ["<code>\n# You can change the learning rate of your optimizer by accessing its param_groups attribute and modifying the 'lr' parameter.\n\n# For example, if you have an optimizer:\noptim = torch.optim.SGD(model.parameters(), lr=0.005)\n\n# You can change the learning rate to 0.01 as follows:\nfor param_group in optim.param_groups:\n    param_group['lr'] = 0.01\n\n# Now, the learning rate of your optimizer is 0.01.\n\n# Regarding your second question, it depends on the specifics of your problem. \n# If your learning rate is too high, it may cause the model to overshoot the optimal solution and get stuck. \n# If your learning rate is too low, it may take too long to converge or get stuck in a suboptimal solution.\n# It's often a good idea to try different learning rates and see what works best for your specific problem.\n# You can use tools like learning rate finder or cyclical learning rates to help you find a good learning rate.\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 935, "library_problem_id": 3, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 936, "code": ["<code>\n# Create a PyTorch tensor from the gensim word2vec model's vectors\nweight = torch.FloatTensor(word2vec.wv.vectors)\n\n# Create a PyTorch embedding layer with the same size as the word2vec vectors\nembedding = torch.nn.Embedding.from_pretrained(weight)\n\n# Use the embedding layer to embed your input data\nembedded_input = embedding(input_Tensor)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 936, "library_problem_id": 4, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 937, "code": ["<code>\n```python\n    # Get the vocabulary size and the vector size from the word2vec model\n    vocab_size, vector_size = len(word2vec.wv.key_to_index), word2vec.wv.vector_size\n\n    # Create a PyTorch tensor with the same size as the word2vec vectors\n    embedding_weights = torch.zeros(vocab_size, vector_size)\n\n    # Load the word2vec weights into the PyTorch tensor\n    for word, i in word2vec.wv.key_to_index.items():\n        embedding_weights[i] = torch.tensor(word2vec.wv[word], dtype=torch.float32)\n\n    # Create an embedding layer with the pre-trained weights\n    embedding = torch.nn.Embedding.from_pretrained(embedding_weights)\n\n    # Embed the input tensor\n    embedded_input = embedding(input_Tensor)\n\n    return embedded_input\n    ### END SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 937, "library_problem_id": 5, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 938, "code": ["<code>\nx = torch.rand(4,4)\npx = pd.DataFrame(x.numpy())\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 938, "library_problem_id": 6, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 939, "code": ["<code>\npx = pd.DataFrame(x.detach().numpy())\n```\n</code>\nEND SOLUTION\nThe `detach()` function is used to get rid of the computation graph and only keep the tensor. Then, `numpy()` function is used to convert the tensor to a numpy array. Finally, this numpy array is used to create a pandas DataFrame.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 939, "library_problem_id": 7, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 940, "code": ["<code>\nx = torch.rand(6,6)\npx = pd.DataFrame(x.numpy())\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 940, "library_problem_id": 8, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 941, "code": ["<code>\n# Convert ByteTensor to LongTensor\nA_log = A_log.nonzero().squeeze()\n\n# Use torch.index_select to slice the tensor\nC = torch.index_select(B, 1, A_log)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 941, "library_problem_id": 9, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 942, "code": ["<code>\nC = torch.index_select(B, 1, torch.nonzero(A_logical).squeeze())\n```\nEND SOLUTION\n</code>\nIn PyTorch, you can use the `torch.index_select()` function to select the columns of a tensor that correspond to the indices where a ByteTensor is '1'. The `torch.nonzero()` function is used to get the indices where the ByteTensor is '1'. The `squeeze()` function is used to remove the 1-dimension from the output of `torch.nonzero()`.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 942, "library_problem_id": 10, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 943, "code": ["<code>\n# Convert ByteTensor to LongTensor\nA_log = A_log.nonzero().squeeze()\n\n# Use torch.index_select to slice the tensor\nC = torch.index_select(B, 1, A_log)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 943, "library_problem_id": 11, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 944, "code": ["<code>\nimport torch\n\n# Logical indexing on the columns\nA_log = torch.ByteTensor([0, 1, 0]) # the logical index\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\n\n# Use torch.masked_select to select elements using a ByteTensor mask\nC = torch.masked_select(B, A_log)\n\nprint(C)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 944, "library_problem_id": 12, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 945, "code": ["<code>\ndef solve(A_log, B):\n    # Convert ByteTensor to LongTensor\n    A_log = A_log.nonzero(as_tuple=True)[0]\n    # Use torch.index_select to slice the tensor\n    C = torch.index_select(B, 1, A_log)\n    return C\n\nA_log = torch.ByteTensor([1, 0, 1])\nB = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\nC = solve(A_log, B)\nprint(C)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 945, "library_problem_id": 13, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 946, "code": ["<code>\nC = torch.index_select(B, 1, torch.nonzero(A_log, as_tuple=True)[0])\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 946, "library_problem_id": 14, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 947, "code": ["<code>\nC = torch.index_select(B, 1, idx)\n```\nEND SOLUTION\n</code>\nThe function torch.index_select(input, dim, index) selects the elements of the input tensor at the given indices along the given dimension. In your case, you want to select the columns of B at the indices specified by idx. Therefore, you set dim=1 (0-indexed).\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 947, "library_problem_id": 15, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 948, "code": ["<code>\nx_tensor = torch.tensor(x_array.tolist())\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 948, "library_problem_id": 16, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 949, "code": ["<code>\n# Convert the numpy array to a list of lists\nx_list = x.tolist()\n\n# Convert the list of lists to a torch tensor\nx_tensor = torch.tensor(x_list)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 949, "library_problem_id": 17, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 950, "code": ["<code>\ndef Convert(a):\n    # Convert the numpy array to a list of lists\n    a_list = a.tolist()\n    # Convert the list of lists to a torch tensor\n    t = torch.tensor(a_list)\n    return t\n\n# Test the function\nx_array = np.array([\n   np.array([0.5, 1.0, 2.0], dtype=np.float16),\n   np.array([4.0, 6.0, 8.0], dtype=np.float16)\n], dtype=object)\n\nt = Convert(x_array)\nprint(t)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 950, "library_problem_id": 18, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 951, "code": ["<code>\nlens = torch.tensor([3, 5, 4])\nmax_len = lens.max().item()\nmask = torch.arange(max_len)[None, :] < lens[:, None]\nmask = mask.long()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 951, "library_problem_id": 19, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 952, "code": ["<code>\n# Convert the lengths to torch.LongTensor\nlens = torch.tensor(lens, dtype=torch.long)\n\n# Create a range tensor of the same size as lens\nrange_tensor = torch.arange(0, len(lens))\n\n# Expand dims to match the shape of lens\nrange_tensor = range_tensor.unsqueeze(1)\n\n# Compare the range tensor with lens\nmask = (range_tensor < lens.unsqueeze(1)).long()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 952, "library_problem_id": 20, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 953, "code": ["<code>\n# Convert the list to a torch tensor\nlens = torch.tensor(lens)\n\n# Create a tensor of ones with the same size as lens\nones = torch.ones_like(lens)\n\n# Create a range tensor with the same size as lens\nrange_tensor = torch.arange(lens.size(0)).unsqueeze(1)\n\n# Subtract the lens tensor from the range tensor\ndiff = range_tensor - lens.unsqueeze(1)\n\n# Create a mask by comparing the result with 0\nmask = (diff < 0).long()\n\nprint(mask)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 953, "library_problem_id": 21, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 954, "code": ["<code>\nmax_len = max(lens)\n    mask = torch.zeros(len(lens), max_len)\n    for idx, length in enumerate(lens):\n        mask[idx, :length] = 1\n    return mask.long()\n    ### END SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 954, "library_problem_id": 22, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 955, "code": ["<code>\nTensor_3D = torch.diag_embed(Tensor_2D)\n```\nEND SOLUTION\n</code>\nThe torch.diag_embed function is used to create a 3D tensor with the 2D tensor as the diagonal of the 3D tensor. This function is useful when you want to create a diagonal matrix from a 2D tensor.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 955, "library_problem_id": 23, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 956, "code": ["<code>\ndef Convert(t):\n    # Create an empty list to store the diagonal matrices\n    diag_matrices = []\n\n    # Iterate over each 1D tensor in the 2D tensor\n    for i in range(t.shape[0]):\n        # Use torch.diag to create a diagonal matrix from the 1D tensor\n        diag_matrix = torch.diag(t[i])\n        # Append the diagonal matrix to the list\n        diag_matrices.append(diag_matrix)\n\n    # Convert the list of diagonal matrices to a 3D tensor\n    result = torch.stack(diag_matrices)\n\n    return result\n\n# Test the function\nTensor_2D = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nresult = Convert(Tensor_2D)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 956, "library_problem_id": 24, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 23}}
{"id": 957, "code": ["<code>\nab = torch.cat((a, b), dim=0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 957, "library_problem_id": 25, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 958, "code": ["<code>\nab = torch.cat((a, b), dim=0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 958, "library_problem_id": 26, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 959, "code": ["<code>\ndef solve(a, b):\n    if a.size(0) < b.size(0):\n        a = torch.unsqueeze(a, 0)\n    elif a.size(0) > b.size(0):\n        b = torch.unsqueeze(b, 0)\n    ab = torch.cat((a, b), 0)\n    return ab\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 959, "library_problem_id": 27, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 960, "code": ["<code>\nfor i in range(len(lengths)):\n    a[i, lengths[i]:, :] = 0\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 960, "library_problem_id": 28, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 961, "code": ["<code>\nfor i in range(len(lengths)):\n    a[i, lengths[i]:, :] = 2333\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 961, "library_problem_id": 29, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 28}}
{"id": 962, "code": ["<code>\nmask = torch.arange(a.size(1))[None, :, None] < lengths[:, None, None]\na = a * mask.float()\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 962, "library_problem_id": 30, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 963, "code": ["<code>\nfor i in range(len(lengths)):\n    a[i, :lengths[i], :] = 2333\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 963, "library_problem_id": 31, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 28}}
{"id": 964, "code": ["<code>\ntensor_of_tensors = torch.stack([torch.tensor(t) for t in list_of_tensors])\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 964, "library_problem_id": 32, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 965, "code": ["<code>\nnew_tensors = torch.stack([torch.tensor(t) for t in list])\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 965, "library_problem_id": 33, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 966, "code": ["<code>\ntensor_of_tensors = torch.stack([torch.tensor(t) for t in lt])\n    return tensor_of_tensors\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 966, "library_problem_id": 34, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 967, "code": ["<code>\ntensor_of_tensors = torch.stack([torch.tensor(t) for t in list_of_tensors])\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 967, "library_problem_id": 35, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 968, "code": ["<code>\nresult = torch.index_select(t, 0, torch.tensor(idx))\n```\nEND SOLUTION\n</code>\nThe torch.index_select() function is used to select the elements or rows of the tensor at the given indices. The first argument is the input tensor, the second argument is the dimension along which to index, and the third argument is the tensor of indices.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 968, "library_problem_id": 36, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 969, "code": ["<code>\nresult = torch.index_select(t, 0, torch.tensor(idx))\nresult = result[:, 1]\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 969, "library_problem_id": 37, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 970, "code": ["<code>\nresult = torch.index_select(t, 0, torch.tensor(idx, dtype=torch.long))\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 970, "library_problem_id": 38, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 971, "code": ["<code>\nresult = torch.gather(x, 1, ids.unsqueeze(-1).expand(-1,-1,2)).squeeze(1)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 971, "library_problem_id": 39, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 972, "code": ["<code>\nresult = torch.gather(x, 1, ids.unsqueeze(-1).expand(-1,-1,x.size(-1))).squeeze(1)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 972, "library_problem_id": 40, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 973, "code": ["<code>\n# Convert ids and x to torch tensors\nids = torch.tensor(ids)\nx = torch.tensor(x)\n\n# Use torch.gather to gather the selected slices\nresult = torch.gather(x, 1, ids.unsqueeze(-1)).squeeze(1)\n\n# If you want to select only the maximum score for each row, you can use torch.max\n# result = torch.max(x, 1)[0]\n\nprint(result.shape)  # should be (70,2)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 973, "library_problem_id": 41, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 974, "code": ["<code>\n# The torch.max function returns the maximum value and its index for each row in the tensor.\n# We only need the indices, so we specify dim=1 to get the maximum values along each row.\n# The indices are 0-based, so we add 1 to get the class labels in {1, 2, 3}.\ny = torch.max(softmax_output, dim=1)[1].unsqueeze(1) + 1\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 974, "library_problem_id": 42, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 975, "code": ["<code>\n# The torch.max function returns the maximum value and its index for each row in the tensor.\n# We only need the index, so we discard the maximum value using the _ operator.\ny, _ = torch.max(softmax_output, dim=1)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 975, "library_problem_id": 43, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 976, "code": ["<code>\n# get the index of the minimum value along the last dimension\ny = torch.argmin(softmax_output, dim=1)\n\n# add a new dimension to the tensor\ny = y.unsqueeze(1)\n```\nEND SOLUTION\n</code>\nThe `argmin` function will return the indices of the minimum values along the last dimension. We then add a new dimension to the tensor to match the desired output shape.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 976, "library_problem_id": 44, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 977, "code": ["<code>\n```python\n    # Use torch.max to find the maximum value and its index in each row\n    max_values, max_indices = torch.max(softmax_output, dim=1)\n\n    # The indices are already in the format you need, so we just need to reshape them to match the output shape\n    y = max_indices.view(-1, 1)\n\n    return y\n### END SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 977, "library_problem_id": 45, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 978, "code": ["<code>\n# Use torch.argmin to find the index of the minimum value along the specified dimension.\n# The dimension to reduce is 1, so it will reduce along the columns (dimension 1).\n# The argument 'keepdim' is set to True to keep the reduced dimension in the output tensor.\n# The argument 'return_count' is set to True to return the minimum value and its count.\n# We only need the index of the minimum value, so we ignore the second returned value.\ny = torch.argmin(softmax_output, dim=1, keepdim=True, out=None)\n\n# Convert the tensor to a LongTensor\ny = y.long()\n\nreturn y\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 978, "library_problem_id": 46, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 979, "code": ["<code>\ndef cross_entropy2d(input, target, weight=None, size_average=True):\n    # input: (n, c, w, z), target: (n, w, z)\n    n, c, w, z = input.size()\n    # log_p: (n, c, w, z)\n    log_p = F.log_softmax(input, dim=1)\n    # log_p: (n*w*z, c)\n    log_p = log_p.permute(0, 3, 2, 1).contiguous().view(-1, c)  # make class dimension last dimension\n    # target: (n, w, z) -> (n*w*z, c)\n    target = target.view(-1, c)\n    mask = target >= 0\n    target = target[mask]\n    loss = F.nll_loss(log_p, target.view(-1), weight=weight, size_average=False)\n    if size_average:\n        loss /= mask.data.sum()\n    return loss\n\nimages = Variable(torch.randn(5, 3, 4, 4))\nlabels = Variable(torch.LongTensor(5, 4, 4).random_(3))\nloss = cross_entropy2d(images, labels)\nprint(loss)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 979, "library_problem_id": 47, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 980, "code": ["<code>\ncnt_equal = torch.sum(A == B)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 980, "library_problem_id": 48, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 981, "code": ["<code>\ncnt_equal = torch.sum(A == B)\n```\nEND SOLUTION\n</code>\n\nThis will return the number of elements in A and B that are equal. The `==` operator is used to compare the elements of the two tensors, and `torch.sum` is used to count the number of True values (which are equivalent to 1 in a binary context).\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 981, "library_problem_id": 49, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 982, "code": ["<code>\ncnt_not_equal = torch.sum(A != B)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 982, "library_problem_id": 50, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 48}}
{"id": 983, "code": ["<code>\nreturn torch.sum(A == B).item()\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 983, "library_problem_id": 51, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 984, "code": ["<code>\n# Assuming A and B are your tensors\n# Get the last x elements of each tensor\nA_last_x = A[-x:, 0]\nB_last_x = B[-x:, 0]\n\n# Compare the last x elements of each tensor\ncnt_equal = (A_last_x == B_last_x).sum().item()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 984, "library_problem_id": 52, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 985, "code": ["<code>\n# Assuming A and B are your tensors\n# We first get the last x elements of each tensor\nA_last_x = A[-x:, 0]\nB_last_x = B[-x:, 0]\n\n# We then compare these two sets of elements\nnot_equal_mask = A_last_x != B_last_x\n\n# We count the number of True values in the mask\ncnt_not_equal = not_equal_mask.sum().item()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 985, "library_problem_id": 53, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 986, "code": ["<code>\ntensors_31 = []\nfor i in range(31):\n    start = i*chunk_dim\n    end = (i+1)*chunk_dim\n    tensor_i = a[:,:,:,start:end,:]\n    tensors_31.append(tensor_i)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 986, "library_problem_id": 54, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 987, "code": ["<code>\ntensors_31 = []\nfor i in range(31):\n    start = i*10\n    end = (i+1)*10\n    tensor_i = a[:,:,start:end,:,:]\n    tensors_31.append(tensor_i)\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 987, "library_problem_id": 55, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 988, "code": ["<code>\noutput[mask.bool()] = clean_input_spectrogram[mask.bool()]\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 988, "library_problem_id": 56, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 989, "code": ["<code>\noutput[mask.bool()] = clean_input_spectrogram[mask.bool()]\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 989, "library_problem_id": 57, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 990, "code": ["<code>\n# Compute the absolute values of x and y\nabs_x = torch.abs(x)\nabs_y = torch.abs(y)\n\n# Compute the minimum of the absolute values of x and y\nmin_abs = torch.min(abs_x, abs_y)\n\n# Compute the signs of x and y\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\n\n# Compute the signs of the minimum absolute values\nsign_min_abs = torch.where(abs_x < abs_y, sign_x, sign_y)\n\n# Multiply the signs of the minimum absolute values with the minimum absolute values\nsigned_min = sign_min_abs * min_abs\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 990, "library_problem_id": 58, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 991, "code": ["<code>\n# Compute the absolute values and signs of x and y\nabs_x = torch.abs(x)\nabs_y = torch.abs(y)\nsign_x = torch.sign(x)\nsign_y = torch.sign(y)\n\n# Compute the maximum absolute values of x and y\nmax_abs_x = torch.max(abs_x)\nmax_abs_y = torch.max(abs_y)\n\n# Create a mask for x and y based on which has the maximum absolute value\nmask_x = abs_x == max_abs_x\nmask_y = abs_y == max_abs_y\n\n# Apply the mask to the signs of x and y\nsigned_max_x = mask_x * sign_x + (1 - mask_x) * sign_y\nsigned_max_y = mask_y * sign_y + (1 - mask_y) * sign_x\n\n# Multiply the maximum absolute values with the corresponding signs\nsigned_max = max_abs_x * signed_max_x + max_abs_y * signed_max_y\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 991, "library_problem_id": 59, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 992, "code": ["<code>\ndef solve(x, y):\n    # Compute the absolute values of x and y\n    abs_x = torch.abs(x)\n    abs_y = torch.abs(y)\n\n    # Compute the signs of x and y\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n\n    # Compute the minimum absolute values between x and y\n    min_abs = torch.min(abs_x, abs_y)\n\n    # Create a mask where the elements of x are smaller than the elements of y\n    mask = abs_x < abs_y\n\n    # Apply the mask to the signs of x and y\n    signed_min = mask.float() * sign_x + (1 - mask.float()) * sign_y\n\n    # Multiply the minimum absolute values with the obtained signs\n    signed_min = signed_min * min_abs\n\n    return signed_min\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 992, "library_problem_id": 60, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 993, "code": ["<code>\noutput = MyNet(input)\nconf, classes = torch.max(output.reshape(1, 3), 1)\nclass_names = '012'\nclass_index = classes.item()\n\n# Apply softmax to get confidence scores\nsoftmax = torch.nn.Softmax(dim=1)\noutput = softmax(output)\nconfidence_score = output[0, class_index]\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 993, "library_problem_id": 61, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 994, "code": ["<code>\n# Get the tensors\na = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.Tensor([[5, 6, 7], [5, 6, 7]])\n\n# Get the last column of a and the first column of b\nlast_a = a[:, -1]\nfirst_b = b[:, 0]\n\n# Calculate the average of the last column of a and the first column of b\navg = (last_a + first_b) / 2\n\n# Concatenate the first two columns of a, the average column and the last two columns of b\nresult = torch.cat([a[:, :2], avg.unsqueeze(1), b[:, 1:]], dim=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 994, "library_problem_id": 62, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 995, "code": ["<code>\ndef solve(a, b):\n    # Get the dimensions of the tensors\n    a_rows, a_cols = a.shape\n    b_rows, b_cols = b.shape\n\n    # Check if the tensors can be combined\n    assert a_rows == b_rows, \"Tensors must have the same number of rows\"\n\n    # Create a new tensor to store the result\n    result = torch.zeros((a_rows, a_cols + b_cols - 1))\n\n    # Copy the first two columns of 'a' to the result\n    result[:, :a_cols] = a\n\n    # Copy the last two columns of 'b' to the result\n    result[:, -b_cols:] = b\n\n    # Compute the average of the overlapping elements\n    avg = (a[:, -1] + b[:, 0]) / 2\n\n    # Insert the average into the middle column\n    result[:, a_cols - 1] = avg\n\n    return result\n\na = torch.Tensor([[1, 2, 3], [1, 2, 3]])\nb = torch.Tensor([[5, 6, 7], [5, 6, 7]])\n\nprint(solve(a, b))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 995, "library_problem_id": 63, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 62}}
{"id": 996, "code": ["<code>\nt = torch.arange(8).reshape(1,4,2).float()\nprint(t)\n\nnew = torch.zeros(6,4).float()\nnew[:4,1:-1] = t[0]\nprint(new)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 996, "library_problem_id": 64, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 997, "code": ["<code>\nt = torch.tensor([[1, 2], [3, 4]])\n\n# Create a new tensor with zeros\nnew = torch.zeros(4, 4)\n\n# Replace the middle part of the new tensor with the original tensor\nnew[1:3, 1:3] = t\n\nresult = new\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 997, "library_problem_id": 65, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 998, "code": ["<code>\nt = torch.arange(8).reshape(1,4,2).float()\nprint(t)\n\nnew = torch.full((6, 4), -1)\nprint(new)\n\nr = torch.cat([new[:1], t, new[1:]], dim=0)\nprint(r)\n```\n</code>\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 998, "library_problem_id": 66, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 999, "code": ["<code>\nW = torch.unsqueeze(W, 0) # add a dimension to W to match the batch size\nW = W.expand(10, hid_dim) # expand W to match the batch size\nresult = torch.bmm(data, W) # perform batch matrix multiplication\nresult = result.squeeze(2) # remove the unnecessary dimension\nresult = result.view(10, 2, 3) # reshape the result to match the desired output\n```\nEND SOLUTION\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 999, "library_problem_id": 67, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
