{"id": 0, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Col1': [1, 4, 7, 10, 13, 16],\n                   'Col2': [2, 5, 8, 11, 14, 17],\n                   'Col3': [3, 6, 9, 12, 15, 18],\n                   'Type': [1, 1, 2, 2, 3, 3]})\nList = np.random.permutation(len(df))\n\nresult = df.reindex(List)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 0, "library_problem_id": 0, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 1, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Col1': [1, 4, 7, 10, 13, 16],\n                   'Col2': [2, 5, 8, 11, 14, 17],\n                   'Col3': [3, 6, 9, 12, 15, 18],\n                   'Type': [1, 1, 2, 2, 3, 3]})\n\nList = np.random.permutation(len(df))\n\n# Reorder the DataFrame using the index list\ndf = df.reindex(List)\n\n# Count the number of rows where the 'Type' column differs from the original sorted DataFrame after reordering\nresult = np.sum(df['Type'] != df.loc[List, 'Type'])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 1, "library_problem_id": 1, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 2, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\n\ndef replace_values(df):\n    for col in df.columns:\n        freq = df[col].value_counts()\n        df[col] = df[col].apply(lambda x: x if freq[x] > 1 else 'other')\n    return df\n\nresult = replace_values(df)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 2, "library_problem_id": 2, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 3, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\n\ndef replace_values(df):\n    for col in df.columns:\n        freq = df[col].value_counts()\n        df[col] = df[col].apply(lambda x: x if freq[x] >= 3 else 'other')\n    return df\n\nresult = replace_values(df)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 3, "library_problem_id": 3, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 4, "code": ["<code>\ndef f(df=example_df):\n    for col in df.columns:\n        freq = df[col].value_counts()\n        df[col] = df[col].apply(lambda x: x if freq[x] >= 2 else 'other')\n    return df\n\nresult = f(example_df)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 4, "library_problem_id": 4, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 5, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\n\ndef modify_df(df, threshold=3):\n    for col in df.columns:\n        freq = df[col].value_counts()\n        df[col] = df[col].apply(lambda x: 'other' if freq[x] <= threshold else x)\n    return df\n\nresult = modify_df(df)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 5, "library_problem_id": 5, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 6, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],\n                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],\n                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})\n\ndef modify_values(df, threshold=3):\n    for col in df.columns:\n        freq = df[col].value_counts()\n        to_replace = freq[freq < threshold].index.tolist()\n        df[col] = df[col].replace(to_replace, 'other')\n    return df\n\nresult = modify_values(df)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 6, "library_problem_id": 6, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 7, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'url': ['A.com', 'A.com', 'A.com', 'B.com', 'B.com', 'C.com', 'B.com'],\n                   'keep_if_dup': ['Yes', 'Yes', 'No', 'No', 'No', 'No', 'Yes']})\n\n# First, we create a mask where we keep the first occurrence of each unique URL and where 'keep_if_dup' is 'Yes'\nmask = (df['keep_if_dup'] == 'Yes') | (df['url'].duplicated(keep=False))\n\n# Then, we apply this mask to the DataFrame\nresult = df[mask]\n\n# Finally, we sort the DataFrame by the 'url' column to have the first occurrences of each unique URL at the top\nresult = result.sort_values('url')\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 7, "library_problem_id": 7, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 8, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'url': ['A.com', 'A.com', 'A.com', 'B.com', 'B.com', 'C.com', 'B.com'],\n                   'drop_if_dup': ['Yes', 'Yes', 'No', 'No', 'No', 'No', 'Yes']})\n\nresult = df[df.duplicated(subset='url', keep=False) & (df['drop_if_dup'] == 'Yes')]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 8, "library_problem_id": 8, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 7}}
{"id": 9, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'url': ['A.com', 'A.com', 'A.com', 'B.com', 'B.com', 'C.com', 'B.com'],\n                   'keep_if_dup': ['Yes', 'Yes', 'No', 'No', 'No', 'No', 'Yes']})\n\n# First, we sort the DataFrame by 'url' and 'keep_if_dup' in descending order.\n# This ensures that the rows with 'Yes' in 'keep_if_dup' are prioritized.\ndf = df.sort_values(['url', 'keep_if_dup'], ascending=[True, False])\n\n# Then, we drop duplicates based on 'url'.\n# This will keep the last occurrence of a duplicate URL unless 'keep_if_dup' is 'Yes' for any occurrence of that URL.\nresult = df.drop_duplicates(subset='url', keep='last')\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 9, "library_problem_id": 9, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 7}}
{"id": 10, "code": ["<code>\ndef df_to_dict(df):\n    df_dict = df.to_dict(orient='records')\n    result = {}\n    for item in df_dict:\n        d = result\n        for key in item.keys():\n            if key not in d:\n                d[key] = {}\n            d = d[key]\n        d.update(item)\n    return result\n\nresult = df_to_dict(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 10, "library_problem_id": 10, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 11, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'datetime': ['2015-12-01 00:00:00-06:00', '2015-12-02 00:01:00-06:00', '2015-12-03 00:00:00-06:00']})\ndf['datetime'] = pd.to_datetime(df['datetime']).dt.tz_localize(None)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 11, "library_problem_id": 11, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 12, "code": ["<code>\nimport pandas as pd\n\nexample_df = pd.DataFrame({'datetime': ['2015-12-01 00:00:00-06:00', '2015-12-02 00:01:00-06:00', '2015-12-03 00:00:00-06:00']})\nexample_df['datetime'] = pd.to_datetime(example_df['datetime'])\n\ndef f(df=example_df):\n    df['datetime'] = df['datetime'].dt.tz_localize(None)\n    return df\n\nresult = f(example_df)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 12, "library_problem_id": 12, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 13, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'datetime': ['2015-12-01 00:00:00-06:00', '2015-12-02 00:01:00-06:00', '2015-12-03 00:00:00-06:00']})\ndf['datetime'] = pd.to_datetime(df['datetime'])\n\ndf['datetime'] = df['datetime'].dt.strftime('%d-%b-%Y %H:%M:%S')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 13, "library_problem_id": 13, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 14, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'datetime': ['2015-12-01 00:00:00-06:00', '2015-12-02 00:01:00-06:00', '2015-12-03 00:00:00-06:00']})\ndf['datetime'] = pd.to_datetime(df['datetime']).dt.tz_localize(None)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 14, "library_problem_id": 14, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 15, "code": ["<code>\nimport pandas as pd\nimport re\n\n# function to extract key-value pairs from the 'message' column\ndef extract_pairs(row):\n    pairs = re.findall(r'\\[(.*?)\\]', row)\n    return [{k.strip(): v.strip()} for k, v in [pair.split(':') for pair in pairs[0].split(',')]]\n\n# apply the function to the 'message' column\ndf['message'] = df['message'].apply(extract_pairs)\n\n# explode the list of dictionaries into separate rows\ndf = df.explode('message')\n\n# convert the dictionary to a DataFrame\ndf = pd.concat([df, df['message'].apply(pd.Series)], axis=1)\n\n# drop the 'message' column\ndf = df.drop('message', axis=1)\n\n# fill missing values with 'none'\ndf = df.fillna('none')\n\n# reset the index\ndf = df.reset_index(drop=True)\n\nresult = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 15, "library_problem_id": 15, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 16, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'product': [1179160, 1066490, 1148126, 1069104, 1069105, 1160330, 1069098, 1077784, 1193369, 1179741],\n                   'score': [0.424654, 0.424509, 0.422207, 0.420455, 0.414603, 0.168784, 0.168749, 0.168738, 0.168703, 0.168684]})\nproducts = [1066490, 1077784]\n\n# Multiply 'score' by 2 for products in the list\ndf.loc[df['product'].isin(products), 'score'] *= 2\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 16, "library_problem_id": 16, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 17, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'product': [1179160, 1066490, 1148126, 1069104, 1069105, 1160330, 1069098, 1077784, 1193369, 1179741],\n                   'score': [0.424654, 0.424509, 0.422207, 0.420455, 0.414603, 0.168784, 0.168749, 0.168738, 0.168703, 0.168684]})\nproducts = [1066490, 1077784]\n\ndf.loc[df['product'].isin(products), 'score'] *= 10\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 17, "library_problem_id": 17, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 18, "code": ["<code>\nimport pandas as pd\n\ndef multiply_scores(df, products, factor=10):\n    for product_range in products:\n        mask = df['product'].between(product_range[0], product_range[1])\n        df.loc[mask, 'score'] *= factor\n\ndf = pd.DataFrame({'product': [1179160, 1066490, 1148126, 1069104, 1069105, 1160330, 1069098, 1077784, 1193369, 1179741],\n                   'score': [0.424654, 0.424509, 0.422207, 0.420455, 0.414603, 0.168784, 0.168749, 0.168738, 0.168703, 0.168684]})\nproducts = [[1069104, 1069105], [1066489, 1066491]]\n\nmultiply_scores(df, products)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 18, "library_problem_id": 18, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 19, "code": ["<code>\nHere is the completed function:\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({'product': [1179160, 1066490, 1148126, 1069104, 1069105, 1160330, 1069098, 1077784, 1193369, 1179741],\n                   'score': [0.424654, 0.424509, 0.422207, 0.420455, 0.414603, 0.168784, 0.168749, 0.168738, 0.168703, 0.168684]})\nproducts = [1066490, 1077784, 1179741]\n\ndef min_max_normalize(df, products):\n    for product in products:\n        product_rows = df[df['product'] == product]['score']\n        min_val = product_rows.min()\n        max_val = product_rows.max()\n        df.loc[df['product'] == product, 'score'] = (df[df['product'] == product]['score'] - min_val) / (max_val - min_val)\n    return df\n\ndf = min_max_normalize(df, products)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 19, "library_problem_id": 19, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 20, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'A': [1, 0, 0, 0, 1, 0],\n                   'B': [0, 1, 0, 0, 0, 1],\n                   'C': [0, 0, 1, 0, 0, 0],\n                   'D': [0, 0, 0, 1, 0, 0]})\n\ndef binary_to_categorical(df):\n    df['category'] = df.apply(lambda row: ','.join(df.columns[row==1]), axis=1)\n    return df\n\ndf = binary_to_categorical(df)\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 20, "library_problem_id": 20, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 21, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'A': [0, 1, 1, 1, 0, 1],\n                   'B': [1, 0, 1, 1, 1, 0],\n                   'C': [1, 1, 0, 1, 1, 1],\n                   'D': [1, 1, 1, 0, 1, 1]})\n\ndf['E'] = df.apply(lambda row: 'A' if row['A'] == 0 else \n                   ('B' if row['B'] == 0 else \n                    ('C' if row['C'] == 0 else 'D')), axis=1)\n\ndf = df.drop(columns=['A', 'B', 'C', 'D'])\ndf\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 21, "library_problem_id": 21, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 22, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'A': [1, 0, 0, 0, 1, 0],\n                   'B': [0, 1, 0, 0, 1, 1],\n                   'C': [1, 1, 1, 0, 1, 0],\n                   'D': [0, 0, 0, 1, 1, 0]})\n\ndef binary_to_list(df):\n    df['list'] = df.apply(lambda row: [col for col in df.columns if row[col] == 1], axis=1)\n    return df\n\ndf = binary_to_list(df)\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 22, "library_problem_id": 22, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 20}}
{"id": 23, "code": ["<code>\ndf['Date'] = df['Date'].dt.strftime('%b-%Y')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 23, "library_problem_id": 23, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 24, "code": ["<code>\ndf['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 24, "library_problem_id": 24, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 25, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Date':['2019-01-01','2019-02-08','2019-02-08', '2019-03-08']})\ndf['Date'] = pd.to_datetime(df['Date'])\nList = ['2019-01-17', '2019-02-20']\n\nstart_date, end_date = pd.to_datetime(List)\nmask = (df['Date'] >= start_date) & (df['Date'] <= end_date)\n\ndf = df.loc[mask]\ndf['Date'] = df['Date'].dt.strftime('%d-%b-%Y %A')\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 25, "library_problem_id": 25, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 23}}
{"id": 26, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'#1': [11.6985, 43.6431, 54.9089, 63.1225, 72.4399],\n                   '#2': [126.0, 134.0, 130.0, 126.0, 120.0]},\n                  index=['1980-01-01', '1980-01-02', '1980-01-03', '1980-01-04', '1980-01-05'])\n\n# Shift the first value of the first column to the second row\ndf.loc['1980-01-02', '#1'] = df.loc['1980-01-01', '#1']\n\n# Move the last value of the first column to the first row\ndf.loc['1980-01-01', '#1'] = df.loc['1980-01-05', '#1']\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 26, "library_problem_id": 26, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 27, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'#1': [11.6985, 43.6431, 54.9089, 63.1225, 72.4399],\n                   '#2': [126.0, 134.0, 130.0, 126.0, 120.0]},\n                  index=['1980-01-01', '1980-01-02', '1980-01-03', '1980-01-04', '1980-01-05'])\n\n# Shift the last row to the second row\ndf = df.shift(1).iloc[-1:, :]\n\n# Move the first row to the last row\ndf = df.append(df.iloc[0])\n\ndf\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 27, "library_problem_id": 27, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 28, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'#1': [11.6985, 43.6431, 54.9089, 63.1225, 72.4399],\n                   '#2': [126.0, 134.0, 130.0, 126.0, 120.0]},\n                  index=['1980-01-01', '1980-01-02', '1980-01-03', '1980-01-04', '1980-01-05'])\n\n# Shift the first column down by one position\ndf['#1'] = df['#1'].shift(1)\n\n# Shift the second column up by one position\ndf['#2'] = df['#2'].shift(-1)\n\n# Fill the NaN values with the last valid observation\ndf = df.fillna(method='pad')\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 28, "library_problem_id": 28, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 29, "code": ["<code>\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import r2_score\n\ndf = pd.DataFrame({'#1': [11.6985, 43.6431, 54.9089, 63.1225, 72.4399],\n                   '#2': [126.0, 134.0, 130.0, 126.0, 120.0]},\n                  index=['1980-01-01', '1980-01-02', '1980-01-03', '1980-01-04', '1980-01-05'])\n\nmin_r2 = float('inf')\nmin_df = df\n\nfor i in range(len(df)):\n    df_shifted = df.shift(-i)\n    df_shifted.index = df.index\n    r2 = r2_score(df['#1'], df_shifted['#2'])\n    if r2 < min_r2:\n        min_r2 = r2\n        min_df = df_shifted\n\ndf = min_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 29, "library_problem_id": 29, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 30, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame(\n    {'HeaderA': [476],\n     'HeaderB': [4365],\n     'HeaderC': [457]})\n\ndef rename_columns(df, suffix=\"X\"):\n    return df.rename(columns=lambda x: x + suffix)\n\ndf = rename_columns(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 30, "library_problem_id": 30, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 31, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame(\n    {'HeaderA': [476],\n     'HeaderB': [4365],\n     'HeaderC': [457]})\n\ndef rename_columns(df, prefix):\n    df.columns = [prefix + col for col in df.columns]\n    return df\n\ndf = rename_columns(df, 'Prefix_')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 31, "library_problem_id": 31, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 30}}
{"id": 32, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame(\n    {'HeaderA': [476],\n     'HeaderB': [4365],\n     'HeaderC': [457],\n     \"HeaderX\": [345]})\n\ndf.columns = [\"X\" + col if not col.endswith(\"X\") else col for col in df.columns]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 32, "library_problem_id": 32, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 30}}
{"id": 33, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({ 'group': ['A', 'A', 'A', 'B', 'B'], 'group_color' : ['green', 'green', 'green', 'blue', 'blue'], 'val1': [5, 2, 3, 4, 5], 'val2' : [4, 2, 8, 5, 7],'val3':[1,1,4,5,1] })\n\ndef calculate_mean(df):\n    value_columns = [col for col in df.columns if 'val' in col]\n    result = df.groupby('group')[value_columns].mean().reset_index()\n    return result\n\nresult = calculate_mean(df)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 33, "library_problem_id": 33, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 34, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({ 'group': ['A', 'A', 'A', 'B', 'B'], 'group_color' : ['green', 'green', 'green', 'blue', 'blue'], 'val1': [5, 2, 3, 4, 5], 'val2' : [4, 2, 8, 5, 7],'val3':[1,1,4,5,1] })\n\ndef calculate_sum(df):\n    value_columns = [col for col in df.columns if 'val' in col]\n    result = df.groupby('group')[value_columns].sum()\n    return result\n\nresult = calculate_sum(df)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 34, "library_problem_id": 34, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 33}}
{"id": 35, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({ 'group': ['A', 'A', 'A', 'B', 'B'], 'group_color' : ['green', 'green', 'green', 'blue', 'blue'], 'val1': [5, 2, 3, 4, 5], 'val2' : [4, 2, 8, 5, 7],'val42':[1,1,4,5,1] })\n\ndef calculate_grouped_stats(df, group_by_column):\n    grouped_df = df.groupby(group_by_column)\n    grouped_mean_df = df.groupby(group_by_column).mean()\n    grouped_sum_df = df.groupby(group_by_column).sum()\n\n    for col in df.columns:\n        if col.endswith('2'):\n            df[col] = grouped_mean_df[col]\n        else:\n            df[col] = grouped_sum_df[col]\n\n    return df\n\nresult = calculate_grouped_stats(df, 'group')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 35, "library_problem_id": 35, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 33}}
{"id": 36, "code": ["<code>\nimport pandas as pd\n\ndef calculate_mean(df, row_list, column_list):\n    return df.loc[row_list, column_list].mean()\n\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\n\nresult = calculate_mean(df, row_list, column_list)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 36, "library_problem_id": 36, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 37, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\n\ndef sum_intersections(df, row_list, column_list):\n    return df.loc[row_list, column_list].sum().sum()\n\nresult = sum_intersections(df, row_list, column_list)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 37, "library_problem_id": 37, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 38, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'a':[1,1,1,1],'b':[2,2,1,0],'c':[3,3,1,0],'d':[0,4,6,0],'q':[5,5,1,0]})\nrow_list = [0,2,3]\ncolumn_list = ['a','b','d']\n\n# Calculate the sum of the specified rows and columns\nsum_df = df.loc[row_list, column_list].sum()\n\n# Remove the largest sum from the resulting series\nresult = sum_df.drop(sum_df.idxmax())\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 38, "library_problem_id": 38, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 39, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame(data=[[34, 'null', 'mark'], [22, 'null', 'mark'], [34, 'null', 'mark']], columns=['id', 'temp', 'name'], index=[1, 2, 3])\n\nresult = df.apply(pd.Series.nunique)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 39, "library_problem_id": 39, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 40, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame(data=[[34, 'null', 'null'], [22, 'null', 'mark'], [34, 'null', 'mark']], columns=['id', 'temp', 'name'], index=[1, 2, 3])\n\nresult = df.isnull().sum()\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 40, "library_problem_id": 40, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 41, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame(data=[[34, 'null', 'mark'], [22, 'null', 'mark'], [34, 'null', 'mark']], columns=['id', 'temp', 'name'], index=[1, 2, 3])\n\ndef value_counts_df(df):\n    for col in df.columns:\n        if df[col].nunique() > 1 or df[col].isnull().any():\n            print(df[col].value_counts())\n        else:\n            print(f\"Column '{col}' is either all nulls or has a single unique value.\")\n\nvalue_counts_df(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 41, "library_problem_id": 41, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 39}}
{"id": 42, "code": ["<code>\nHere is the completed function:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Nanonose': ['Sample type','Water','Water','Water','Water'],\n                   'Unnamed: 1': ['Concentration',9200,9200,9200,4600],\n                   'A': [np.nan,95.5,94.5,92.0,53.0,],\n                   'B': [np.nan,21.0,17.0,16.0,7.5],\n                   'C': [np.nan,6.0,5.0,3.0,2.5],\n                   'D': [np.nan,11.942308,5.484615,11.057692,3.538462],\n                   'E': [np.nan,64.134615,63.205769,62.586538,35.163462],\n                   'F': [np.nan,21.498560,19.658560,19.813120,6.876207],\n                   'G': [np.nan,5.567840,4.968000,5.192480,1.641724],\n                   'H': [np.nan,1.174135,1\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 42, "library_problem_id": 42, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 43, "code": ["<code>\nHere is the completed function:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Nanonose': ['Sample type','Water','Water','Water','Water'],\n                   'Unnamed: 1': ['Concentration',9200,9200,9200,4600],\n                   'A': [np.nan,95.5,94.5,92.0,53.0,],\n                   'B': [np.nan,21.0,17.0,16.0,7.5],\n                   'C': [np.nan,6.0,5.0,3.0,2.5],\n                   'D': [np.nan,11.942308,5.484615,11.057692,3.538462],\n                   'E': [np.nan,64.134615,63.205769,62.586538,35.163462],\n                   'F': [np.nan,21.498560,19.658560,19.813120,6.876207],\n                   'G': [np.nan,5.567840,4.968000,5.192480,1.641724],\n                   'H': [np.nan,1.174135,1\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 43, "library_problem_id": 43, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 44, "code": ["<code>\ndef shift_missing_values(df):\n    return df.apply(lambda row: row.dropna().append(row[row.isna()]), axis=1)\n\nresult = shift_missing_values(df)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 44, "library_problem_id": 44, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 45, "code": ["<code>\ndef rearrange_rows(df):\n    return df.apply(lambda row: pd.Series(row.dropna().append(row[row.isna()])), axis=1)\n\nresult = rearrange_rows(df)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 45, "library_problem_id": 45, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 44}}
{"id": 46, "code": ["<code>\ndef move_nan_to_top(df):\n    return df.sort_values(by=list(df.columns), ascending=False).reset_index(drop=True)\n\nresult = move_nan_to_top(df)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 46, "library_problem_id": 46, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 44}}
{"id": 47, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'lab':['A', 'B', 'C', 'D', 'E', 'F'], 'value':[50, 35, 8, 5, 1, 1]})\ndf = df.set_index('lab')\nthresh = 6\n\n# Filter rows where 'value' is below the threshold\nbelow_thresh = df[df['value'] < thresh]\n\n# If there are rows below the threshold\nif not below_thresh.empty:\n    # Replace these rows with a single row whose 'value' is the sum of the aggregated rows\n    df = df[df['value'] >= thresh]\n    df.loc['X'] = below_thresh['value'].sum()\n\nresult = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 47, "library_problem_id": 47, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 48, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'lab':['A', 'B', 'C', 'D', 'E', 'F'], 'value':[50, 35, 8, 5, 1, 1]})\ndf = df.set_index('lab')\nthresh = 6\n\ndef aggregate_and_replace(df, thresh):\n    above_thresh = df[df['value'] > thresh]\n    replaced = df[df['value'] <= thresh]\n    for index, row in above_thresh.iterrows():\n        replaced.loc[index, 'value'] = row['value']\n    replaced['value'] = replaced['value'].mean()\n    return replaced\n\nresult = aggregate_and_replace(df, thresh)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 48, "library_problem_id": 48, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 47}}
{"id": 49, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'lab':['A', 'B', 'C', 'D', 'E', 'F'], 'value':[50, 35, 8, 5, 1, 1]})\ndf = df.set_index('lab')\nsection_left = 4\nsection_right = 38\n\nmask = (df['value'] >= section_left) & (df['value'] <= section_right)\n\nif mask.sum() > 1:\n    df = df[~mask]\n    df.loc['X'] = df['value'].mean()\nelse:\n    df = df[~mask]\n\nresult = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 49, "library_problem_id": 49, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 47}}
{"id": 50, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\n# Add inverse columns to the DataFrame\ndf['inv_A'] = 1/df['A']\ndf['inv_B'] = 1/df['B']\n\nresult = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 50, "library_problem_id": 50, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 51, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\ndf = df.rename(columns=lambda x: 'exp_' + x)\ndf = df.applymap(lambda x: math.exp(x))\n\nresult = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 51, "library_problem_id": 51, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 52, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({\"A\": [1, 0, 3], \"B\": [4, 5, 6]})\n\ndef add_inverse_columns(df):\n    for col in df.columns:\n        df['inv_' + col] = 1 / df[col].replace(0, 1)\n    return df\n\nresult = add_inverse_columns(df)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 52, "library_problem_id": 52, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 53, "code": ["<code>\nimport numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nresult = df.copy()\nfor col in df.columns:\n    result[\"sigmoid_\" + col] = df[col].apply(sigmoid)\n\nresult\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 53, "library_problem_id": 53, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 54, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\na = np.array([[ 1. ,  0.9,  1. ],\n              [ 0.9,  0.9,  1. ],\n              [ 0.8,  1. ,  0.5],\n              [ 1. ,  0.3,  0.2],\n              [ 1. ,  0.2,  0.1],\n              [ 0.9,  1. ,  1. ],\n              [ 1. ,  0.9,  1. ],\n              [ 0.6,  0.9,  0.7],\n              [ 1. ,  0.9,  0.8],\n              [ 1. ,  0.8,  0.9]])\nidx = pd.date_range('2017', periods=a.shape[0])\ndf = pd.DataFrame(a, index=idx, columns=list('abc'))\n\n# Create a mask where each element is True if the index is less than the column-wise minimum\nmask = df.index < df.idxmin(axis=0)\n\n# Use the mask to find the last occurrence of the maximum value in each column\nresult = df[mask].idxmax(axis=0)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 54, "library_problem_id": 54, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 55, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\na = np.array([[ 1. ,  0.9,  1. ],\n              [ 0.9,  0.9,  1. ],\n              [ 0.8,  1. ,  0.5],\n              [ 1. ,  0.3,  0.2],\n              [ 1. ,  0.2,  0.1],\n              [ 0.9,  1. ,  1. ],\n              [ 1. ,  0.9,  1. ],\n              [ 0.6,  0.9,  0.7],\n              [ 1. ,  0.9,  0.8],\n              [ 1. ,  0.8,  0.9]])\n\nidx = pd.date_range('2017', periods=a.shape[0])\ndf = pd.DataFrame(a, index=idx, columns=list('abc'))\n\n# Find the column-wise maximum and minimum\nmax_cols = df.idxmax(axis=0)\nmin_cols = df.idxmin(axis=0)\n\n# Find the first occurrence of the column-wise maximum value that precedes the column-wise minimum value\nresult = max_cols[max_cols < min_cols].index[0]\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 55, "library_problem_id": 55, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 56, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\n\n# Create a new dataframe with all dates for each user\nall_dates = pd.DataFrame({'dt': pd.date_range(start=df['dt'].min(), end=df['dt'].max())})\n\n# Merge the original dataframe and the new dataframe\ndf = pd.merge(all_dates, df, on='dt', how='left').fillna(0)\n\n# Set the index to 'dt'\ndf.set_index('dt', inplace=True)\n\n# Sort the dataframe by index\ndf.sort_index(inplace=True)\n\n# Reset the index\ndf.reset_index(inplace=True)\n\n# Rename the 'index' column to 'dt'\ndf.rename(columns={'index': 'dt'}, inplace=True)\n\nresult = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 56, "library_problem_id": 56, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 57, "code": ["<code>\nimport pandas as pd\n\ndf= pd.DataFrame({'user': ['abc','abc','efg','efg'], 'dt': ['2022-01-01','2022-01-02', '2022-01-05','2022-01-06'], 'val': [1,14,51,4]})\ndf['dt'] = pd.to_datetime(df['dt'])\n\n# Create a new dataframe with all possible dates for each user\nall_dates = pd.DataFrame({'dt': pd.date_range(start=df['dt'].min(), end=df['dt'].max())})\n\n# Merge the original dataframe with the new dataframe\ndf = pd.merge(all_dates, df, on='dt', how='left')\n\n# Fill NaN values with 0\ndf['val'].fillna(0, inplace=True)\n\nresult = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 57, "library_problem_id": 57, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 56}}
{"id": 58, "code": ["<code>\nimport pandas as pd\n\ndf= pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\n\n# Create a new dataframe with all dates for each user\nall_dates = pd.DataFrame({'dt': pd.date_range(start=df['dt'].min(), end=df['dt'].max())})\n\n# Merge the original dataframe and the new dataframe\ndf = pd.merge(all_dates, df, on='dt', how='left')\n\n# Fill missing 'val' values with 233\ndf['val'].fillna(233, inplace=True)\n\nresult = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 58, "library_problem_id": 58, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 59, "code": ["<code>\nSure, here is the completed function:\n\n```python\nimport pandas as pd\n\ndf= pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\n\n# Create a new dataframe with the maximum value for each user\nmax_df = df.groupby('user')['val'].max().reset_index()\n\n# Create a new dataframe with the minimum and maximum date for each user\ndate_df = df.groupby('user')['dt'].agg(['min', 'max']).reset_index()\n\n# Merge the two dataframes\nresult = pd.merge(date_df, max_df, on='user')\n\n# Create a new dataframe with all dates between the minimum and maximum dates for each user\nresult = result.set_index(['user', 'dt']).reindex(pd.date_range(result['min'].min(), result['max'].max()).to_frame().reset_index().set_index('user'))\n\n# Fill the 'val' column with the maximum value for that user during the expanded date range\nresult['val'] = result.groupby('user')['val'].transform('max')\n\n# Reset the index\nresult.reset_index(inplace=True)\n\n# Drop the 'min' and 'max' columns\nresult.drop(['min', 'max\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 59, "library_problem_id": 59, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 60, "code": ["<code>\nimport pandas as pd\n\ndf= pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\ndf['dt'] = pd.to_datetime(df['dt'])\n\n# Create a new dataframe with the maximum value for each user\nmax_df = df.groupby('user')['val'].max().reset_index()\n\n# Create a new dataframe with the minimum and maximum date for each user\ndate_df = df.groupby('user').agg({'dt': ['min', 'max']}).reset_index()\ndate_df.columns = ['user', 'min_dt', 'max_dt']\n\n# Merge the two dataframes\ndf = pd.merge(date_df, max_df, on='user')\n\n# Create a new dataframe with all dates between the minimum and maximum dates for each user\ndf = df.set_index(['user', 'min_dt']).apply(lambda x: pd.date_range(x['min_dt'], x['max_dt'])).reset_index()\ndf = df.set_index(['user', 0]).apply(lambda x: x['val']).reset_index()\ndf.columns = ['user', 'dt', 'val']\n\nresult = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 60, "library_problem_id": 60, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 61, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\n\n# Use pandas.factorize to convert unique names to unique integers\ndf['name'], _ = pd.factorize(df['name'])\n\nresult = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 61, "library_problem_id": 61, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 62, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\n\n# Create a dictionary where keys are unique names and values are unique integer IDs\nname_to_id = {name: id for id, name in enumerate(df['name'].unique(), 1)}\n\n# Replace 'a' column values with corresponding IDs\ndf['a'] = df['name'].map(name_to_id)\n\nresult = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 62, "library_problem_id": 62, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 61}}
{"id": 63, "code": ["<code>\ndef f(df=example_df):\n    df['name'] = df['name'].astype('category').cat.codes\n    return df\n\nresult = f(example_df)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 63, "library_problem_id": 63, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 64, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'name': ['Aaron', 'Aaron', 'Aaron', 'Brave', 'Brave', 'David'],\n                   'a': [3, 3, 3, 4, 3, 5],\n                   'b': [5, 6, 6, 6, 6, 1],\n                   'c': [7, 9, 10, 0, 1, 4]})\n\ndf['ID'] = df['name'] + '_' + df['a'].astype(str)\ndf = df.drop(columns=['name', 'a'])\n\nresult = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 64, "library_problem_id": 64, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 61}}
{"id": 65, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'user': ['u1', 'u2', 'u3'],\n                   '01/12/15': [100, 200, -50],\n                   '02/12/15': [300, -100, 200],\n                   'someBool': [True, False, True]})\n\ndf = df.melt(id_vars='user', value_vars=['01/12/15', '02/12/15'], var_name='date', value_name='value')\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 65, "library_problem_id": 65, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 66, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'user': ['u1', 'u2', 'u3'],\n                   '01/12/15': [100, 200, -50],\n                   '02/12/15': [300, -100, 200],\n                   'someBool': [True, False, True]})\n\n# First, we transpose the dataframe to have the columns as rows\ndf = df.set_index('user').T\n\n# Then, we reset the index to get a dataframe with a single column\ndf = df.reset_index()\n\n# We rename the 'index' column to 'others'\ndf.columns.values[0] = 'others'\n\n# We pivot the dataframe to get the 'value' column\ndf = df.melt(id_vars='others', var_name='user', value_name='value')\n\n# We sort the dataframe by 'user' and 'others'\ndf = df.sort_values(['user', 'others'])\n\n# We reset the index\ndf = df.reset_index(drop=True)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 66, "library_problem_id": 66, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 65}}
{"id": 67, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'user': ['u1', 'u2', 'u3'],\n                   '01/12/15': [100, 200, None],\n                   '02/12/15': [None, -100, 200],\n                   'someBool': [True, False, True]})\n\ndf = df.melt(id_vars='user', var_name='date', value_name='value')\n\ndf\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 67, "library_problem_id": 67, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 65}}
{"id": 68, "code": ["<code>\n# Filter rows based on condition in column 'a' and select columns 'b' and 'e'\nsubset = df[df['a'] > 0.5][['b', 'e']]\n\n# Convert the result to a NumPy array\nresult = subset.values\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 68, "library_problem_id": 68, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 69, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.rand(4,5), columns = list('abcde'))\ncolumns = ['a','b','e']\n\n# Filter rows where column 'a' is greater than 0.5 and select columns 'a', 'b', 'e'\nfiltered_df = df[df['a'] > 0.5][columns]\n\n# Convert the filtered DataFrame to a NumPy array\nresult = filtered_df.values\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 69, "library_problem_id": 69, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 70, "code": ["<code>\nimport pandas as pd\n\ndef f(df, columns=['b', 'e'], threshold=0):\n    # Select rows where column 'a' exceeds the threshold\n    df = df[df['a'] > threshold]\n    # Select only the specified columns\n    df = df[columns]\n    # Convert the result to a NumPy array\n    result = df.values\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 70, "library_problem_id": 70, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 71, "code": ["<code>\nimport pandas as pd\n\ndef f(df, columns=['b', 'e']):\n    # Select the subset of rows and columns\n    subset = df[columns]\n\n    # Add a new column representing the sum of two selected columns\n    subset['sum'] = subset.sum(axis=1)\n\n    return subset\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 71, "library_problem_id": 71, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 72, "code": ["<code>\ndef f(df, columns=['b', 'e']):\n    # return the solution in this function\n    # result = f(df, columns)\n    ### BEGIN SOLUTION\n    result = df.loc[df['a'] > 10, columns]\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 72, "library_problem_id": 72, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 73, "code": ["<code>\nHere is the completed function:\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '1/14/9', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 514.14, 145.99, 146.73, 171.10]})\n\ndf['date'] = pd.to_datetime(df['date'])\nX = 120\n\n# Sort the dataframe by date\ndf = df.sort_values('date')\n\n# Reset the index\ndf = df.reset_index(drop=True)\n\n# Initialize a new dataframe to store the result\nresult = pd.DataFrame(columns=df.columns)\n\n# Loop through the dataframe\nfor i in range(len(df)):\n    # If it's the first row, add it to the result\n    if i == 0:\n        result = result.append(df.loc[i])\n    else:\n        # If the date of the current row is more than X days after the date of the last row in the\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 73, "library_problem_id": 73, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 74, "code": ["<code>\nHere is the completed function:\n\n```python\nimport pandas as pd\nimport datetime as dt\n\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '1/14/9', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 514.14, 145.99, 146.73, 171.10]})\n\n# Convert the 'date' column to datetime\ndf['date'] = pd.to_datetime(df['date'])\n\n# Sort the DataFrame by 'date'\ndf = df.sort_values('date')\n\n# Initialize an empty DataFrame to store the result\nresult = pd.DataFrame()\n\n# Iterate over the DataFrame\nfor i in range(len(df)):\n    # If it's the first row, add it to the result\n    if i == 0:\n        result = result.append(df.iloc[i])\n    else:\n        # Calculate the difference in weeks between the current row and the previous row\n        diff = (df.iloc[i]['date'] - df.iloc[\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 74, "library_problem_id": 74, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 73}}
{"id": 75, "code": ["<code>\nHere is the completed function:\n\n```python\nimport pandas as pd\nimport datetime as dt\n\ndf = pd.DataFrame({'ID': [1, 2, 3, 4, 5, 6, 7, 8],\n                   'date': ['09/15/07', '06/01/08', '10/25/08', '1/14/9', '05/13/09', '11/07/09', '11/15/09', '07/03/11'],\n                   'close': [123.45, 130.13, 132.01, 118.34, 514.14, 145.99, 146.73, 171.10]})\n\n# Convert 'date' column to datetime\ndf['date'] = pd.to_datetime(df['date'])\n\n# Sort the dataframe by 'date'\ndf = df.sort_values('date')\n\n# Reset the index\ndf = df.reset_index(drop=True)\n\n# Initialize an empty list to store the indices of rows to be removed\nremove = []\n\n# Loop through the dataframe\nfor i in range(len(df) - 1):\n    # If the difference between the 'date' of the current row and the next row is within X weeks\n    if (df.loc[i+1, 'date'] - df.loc[i, 'date']).days <= 7*X:\n        # Add\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 75, "library_problem_id": 75, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 73}}
{"id": 76, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0]})\n\nresult = df.groupby(df.index // 3).mean()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 76, "library_problem_id": 76, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 77, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'col1':[1, 1, 4, 5, 1]})\n\nresult = df.groupby(df.index // 3).sum()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 77, "library_problem_id": 77, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 78, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'col1':[1, 1, 4, 5, 1, 4]})\n\ndef bin_df(df):\n    return df.groupby(df.index // 4).sum().reset_index(drop=True)\n\nresult = bin_df(df)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 78, "library_problem_id": 78, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 79, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0]})\n\nresult = df.iloc[::-1].groupby(df.index // 3).mean()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 79, "library_problem_id": 79, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 80, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0, 2, 1, 3, 1]})\n\n# Calculate the sum of every three consecutive rows\ndf['three_sum'] = df['col1'].rolling(window=3).sum().shift(-1)\n\n# Calculate the average of every two consecutive rows\ndf['two_avg'] = df['col1'].rolling(window=2).mean().shift(-0.5)\n\n# Drop the NaN values\ndf = df.dropna()\n\n# Create a new DataFrame with the alternating sums and averages\nresult = pd.DataFrame({\n    'three_sum': df['three_sum'],\n    'two_avg': df['two_avg']\n})\n\nresult\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 80, "library_problem_id": 80, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 81, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'col1':[2, 1, 3, 1, 0, 2, 1, 3, 1]})\n\ndef process_df(df):\n    sums = []\n    averages = []\n    for i in range(len(df)):\n        if i % 2 == 0:\n            if i+2 < len(df):\n                sums.append(df.iloc[i:i+3]['col1'].sum())\n            else:\n                sums.append(df.iloc[i:]['col1'].sum())\n        else:\n            if i+1 < len(df):\n                averages.append(df.iloc[i:i+2]['col1'].mean())\n            else:\n                averages.append(df.iloc[i:]['col1'].mean())\n    result = pd.DataFrame({'sums': sums, 'averages': averages})\n    return result\n\nresult = process_df(df)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 81, "library_problem_id": 81, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 82, "code": ["<code>\nimport pandas as pd\n\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns = ['A'])\n\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].ffill(downcast='infer')\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 82, "library_problem_id": 82, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 83, "code": ["<code>\nimport pandas as pd\n\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns = ['A'])\n\ndf['A'] = df['A'].mask(df['A'] == 0)\ndf['A'] = df['A'].ffill()\ndf['A'] = df['A'].astype(int)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 83, "library_problem_id": 83, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 82}}
{"id": 84, "code": ["<code>\nimport pandas as pd\n\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns = ['A'])\n\ndef fill_zeros(df):\n    mask = df['A'] == 0\n    df.loc[mask, 'A'] = df['A'].mask(mask).diff().abs().cumsum()\n    df.loc[mask, 'A'] = df['A'].mask(mask).diff(-1).abs().cumsum()\n    return df\n\ndf = fill_zeros(df)\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 84, "library_problem_id": 84, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 85, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'duration': ['7 year', '2day', '4 week', '8 month']},\n                  index=list(range(1,5)))\n\n# Split the 'duration' column into 'number' and 'time'\ndf[['number', 'time']] = df['duration'].str.split(' ', expand=True)\n\n# Convert 'number' column to integer\ndf['number'] = df['number'].astype(int)\n\n# Create a dictionary to map time units to their respective number of days\ntime_to_days = {'year': 365, 'day': 1, 'week': 7, 'month': 30}\n\n# Create a new column 'time_days' by mapping 'time' column to the dictionary\ndf['time_days'] = df['time'].map(time_to_days)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 85, "library_problem_id": 85, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 86, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'duration': ['year 7', 'day2', 'week 4', 'month 8']},\n                  index=list(range(1,5)))\n\n# Split the 'duration' column into two new columns 'time' and 'number'\ndf[['time', 'number']] = df['duration'].str.split(' ', expand=True)\n\n# Convert 'number' column to integer type\ndf['number'] = df['number'].astype(int)\n\n# Create a dictionary to map time units to their corresponding number of days\ntime_to_days = {'year': 365, 'day': 1, 'week': 7, 'month': 30}\n\n# Create a new column 'time_day' by mapping 'time' column to the dictionary\ndf['time_day'] = df['time'].map(time_to_days)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 86, "library_problem_id": 86, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 87, "code": ["<code>\ndef f(df=example_df):\n        # Split the 'duration' column into two new columns 'number' and 'time'\n        df[['number', 'time']] = df['duration'].str.split(' ', expand=True)\n\n        # Convert the 'number' column to integer type\n        df['number'] = df['number'].astype(int)\n\n        # Create a dictionary to map the time units to their equivalent number of days\n        time_to_days = {'year': 365, 'day': 1, 'week': 7, 'month': 30}\n\n        # Create a new column 'time_days' by mapping the 'time' column to the dictionary\n        df['time_days'] = df['time'].map(time_to_days)\n\n        return df\n\n    result = f(example_df)\n    print(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 87, "library_problem_id": 87, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 88, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'duration': ['year 7', 'day 2', 'week 4', 'month 8']},\n                  index=list(range(1,5)))\n\n# Split the 'duration' column into two new columns 'time' and 'number'\ndf[['time', 'number']] = df['duration'].str.split(' ', expand=True)\n\n# Convert the 'number' column to integer type\ndf['number'] = df['number'].astype(int)\n\n# Create a dictionary to map time units to days\ntime_to_days = {'year': 365, 'day': 1, 'week': 7, 'month': 30}\n\n# Create a new column 'time_day' by mapping 'time' column to the dictionary\ndf['time_day'] = df['time'].map(time_to_days) * df['number']\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 88, "library_problem_id": 88, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 89, "code": ["<code>\nimport pandas as pd\n\ndf1 = pd.DataFrame({'A': [1, 1, 1],\n                   'B': [2, 2, 2],\n                   'C': [3, 3, 3],\n                   'D': [4, 4, 4],\n                   'E': [5, 5, 5],\n                   'F': [6, 6, 6],\n                   'Postset': ['yes', 'no', 'yes']})\ndf2 = pd.DataFrame({'A': [1, 1, 1],\n                   'B': [2, 2, 2],\n                   'C': [3, 3, 3],\n                   'D': [4, 4, 4],\n                   'E': [5, 5, 5],\n                   'F': [6, 4, 6],\n                   'Preset': ['yes', 'yes', 'yes']})\ncolumns_check_list = ['A','B','C','D','E','F']\n\nresult = df1[columns_check_list] == df2[columns_check_list]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 89, "library_problem_id": 89, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 90, "code": ["<code>\nimport pandas as pd\n\ndf1 = pd.DataFrame({'A': [1, 1, 1],\n                   'B': [2, 2, 2],\n                   'C': [3, 3, 3],\n                   'D': [4, 4, 4],\n                   'E': [5, 5, 5],\n                   'F': [6, 6, 6],\n                   'Postset': ['yes', 'no', 'yes']})\n\n\ndf2 = pd.DataFrame({'A': [1, 1, 1],\n                   'B': [2, 2, 2],\n                   'C': [3, 3, 3],\n                   'D': [4, 4, 4],\n                   'E': [5, 5, 5],\n                   'F': [6, 4, 6],\n                   'Preset': ['yes', 'yes', 'yes']})\n\n\ncolumns_check_list = ['A','B','C','D','E','F']\n\nresult = df1[columns_check_list].eq(df2[columns_check_list]).all(axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 90, "library_problem_id": 90, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 89}}
{"id": 91, "code": ["<code>\nimport pandas as pd\n\nindex = pd.MultiIndex.from_tuples([('abc', '3/1/1994'), ('abc', '9/1/1994'), ('abc', '3/1/1995')],\n                                 names=('id', 'date'))\ndf = pd.DataFrame({'x': [100, 90, 80], 'y':[7, 8, 9]}, index=index)\n\ndf.index = df.index.set_levels(pd.to_datetime(df.index.levels[1]), level=1)\ndf\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 91, "library_problem_id": 91, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 92, "code": ["<code>\ndf.index = df.index.set_levels(pd.to_datetime(df.index.levels[1]), level=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 92, "library_problem_id": 92, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 93, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef f(df):\n    # Convert date string to datetime object\n    df['date'] = pd.to_datetime(df['date'])\n\n    # Set date as index\n    df.set_index('date', inplace=True)\n\n    # Convert DataFrame to NumPy array\n    arr = df[['x', 'y']].values\n\n    return arr\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 93, "library_problem_id": 93, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 94, "code": ["<code>\ndf.index = pd.to_datetime(df.index)\n    df = df.swaplevel(0, 1)\n    return df\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 94, "library_problem_id": 94, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 95, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Country': ['Argentina', 'Argentina', 'Brazil', 'Brazil'],\n                   'Variable': ['var1', 'var2', 'var1', 'var2'],\n                   '2000': [12, 1, 20, 0],\n                   '2001': [15, 3, 23, 1],\n                   '2002': [18, 2, 25, 2],\n                   '2003': [17, 5, 29, 2],\n                   '2004': [23, 7, 31, 3],\n                   '2005': [29, 5, 32, 3]})\n\n# Melt the DataFrame to reshape it from wide to long format\ndf = df.melt(id_vars=['Country', 'Variable'], var_name='year', value_name='value')\n\n# Sort the DataFrame by 'Country' and 'year'\ndf = df.sort_values(['Country', 'year'])\n\n# Reset the index\ndf = df.reset_index(drop=True)\n\n# Convert 'year' column to integer\ndf['year'] = df['year'].astype(int)\n\ndf\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 95, "library_problem_id": 95, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 96, "code": ["<code>\nHere is the completed function:\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({'Country': ['Argentina', 'Argentina', 'Brazil', 'Brazil'],\n                   'Variable': ['var1', 'var2', 'var1', 'var2'],\n                   '2000': [12, 1, 20, 0],\n                   '2001': [15, 3, 23, 1],\n                   '2002': [18, 2, 25, 2],\n                   '2003': [17, 5, 29, 2],\n                   '2004': [23, 7, 31, 3],\n                   '2005': [29, 5, 32, 3]})\n\n# Melt the DataFrame to convert it from wide to long format\ndf_melt = df.melt(id_vars=['Country', 'Variable'], var_name='Year')\n\n# Convert Year column to integer\ndf_melt['Year'] = df_melt['Year'].astype(int)\n\n# Sort the DataFrame by Country and Year in descending order\ndf_melt = df_melt.sort_values(by=['Country', 'Year'], ascending=[True, False])\n\n# Pivot the DataFrame to convert it from long to wide format\ndf = df_melt.pivot(index=['Country', 'Year'], columns='Variable', values='value')\n\n# Reset the index\ndf.reset_index(inplace=True\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 96, "library_problem_id": 96, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 97, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'A_Name': ['AA', 'BB', 'CC', 'DD', 'EE', 'FF', 'GG'],\n                   'B_Detail': ['X1', 'Y1', 'Z1', 'L1', 'M1', 'N1', 'K1'],\n                   'Value_B': [1.2, 0.76, 0.7, 0.9, 1.3, 0.7, -2.4],\n                   'Value_C': [0.5, -0.7, -1.3, -0.5, 1.8, -0.8, -1.9],\n                   'Value_D': [-1.3, 0.8, 2.5, 0.4, -1.3, 0.9, 2.1]})\n\ndf = df[df.filter(like='Value').abs().max(axis=1) >= 1]\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 97, "library_problem_id": 97, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 98, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'A_Name': ['AA', 'BB', 'CC', 'DD', 'EE', 'FF', 'GG'],\n                   'B_Detail': ['X1', 'Y1', 'Z1', 'L1', 'M1', 'N1', 'K1'],\n                   'Value_B': [1.2, 0.76, 0.7, 0.9, 1.3, 0.7, -2.4],\n                   'Value_C': [0.5, -0.7, -1.3, -0.5, 1.8, -0.8, -1.9],\n                   'Value_D': [-1.3, 0.8, 2.5, 0.4, -1.3, 0.9, 2.1]})\n\n# Filter rows where the absolute value of any column with a \"Value_\" prefix exceeds 1\ndf = df.loc[:, (df.columns.str.startswith('Value_')) & (df.abs().max(axis=1) <= 1)]\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 98, "library_problem_id": 98, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 97}}
{"id": 99, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'A_Name': ['AA', 'BB', 'CC', 'DD', 'EE', 'FF', 'GG'],\n                   'B_Detail': ['X1', 'Y1', 'Z1', 'L1', 'M1', 'N1', 'K1'],\n                   'Value_B': [1.2, 0.76, 0.7, 0.9, 1.3, 0.7, -2.4],\n                   'Value_C': [0.5, -0.7, -1.3, -0.5, 1.8, -0.8, -1.9],\n                   'Value_D': [-1.3, 0.8, 2.5, 0.4, -1.3, 0.9, 2.1]})\n\n# Filter rows where the absolute value of any column starting with \"Value_\" exceeds 1\ndf = df.loc[:, (df.columns.str.startswith(\"Value_\")) & df.abs().max(axis=1) > 1]\n\n# Remove the \"Value_\" prefix from the column names\ndf.columns = df.columns.str.replace(\"Value_\", \"\")\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 99, "library_problem_id": 99, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 100, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'A': ['Good &AMP; bad', 'BB', 'CC', 'DD', 'Good &AMP; bad'], 'B': range(5), 'C': ['Good &AMP; bad'] * 5})\n\ndef replace_amp(df):\n    for col in df.columns:\n        df[col] = df[col].str.replace('&AMP;', '&')\n    return df\n\ndf = replace_amp(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 100, "library_problem_id": 100, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 101, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'A': ['Good &LT bad', 'BB', 'CC', 'DD', 'Good &LT; bad'], 'B': range(5), 'C': ['Good &LT; bad'] * 5})\n\ndef replace_html_entities(df):\n    for col in df.columns:\n        if df[col].dtype == 'object':\n            df[col] = df[col].str.replace('&LT;', '<')\n    return df\n\ndf = replace_html_entities(df)\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 101, "library_problem_id": 101, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 102, "code": ["<code>\ndef f(df=example_df):\n        for col in df.columns:\n            df[col] = df[col].str.replace('&AMP;', '&')\n        return df\n    result = f(example_df)\n    return result\n### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 102, "library_problem_id": 102, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 100}}
{"id": 103, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'A': ['Good &AMP; bad', 'BB', 'CC', 'DD', 'Good &LT; bad'], 'B': range(5), 'C': ['Good &GT; bad'] * 5})\n\n# Replace the HTML entities\ndf = df.replace({'&AMP;': '&', '&LT;': '<', '&GT;': '>'}, regex=True)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 103, "library_problem_id": 103, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 104, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'A': ['1 &AMP; 1', 'BB', 'CC', 'DD', '1 &AMP; 0'], 'B': range(5), 'C': ['0 &AMP; 0'] * 5})\n\ndf = df.replace('&AMP;', '&', regex=True)\ndf['A'] = df['A'].str.replace('&AMP;', '&')\ndf['C'] = df['C'].str.replace('&AMP;', '&')\n\ndf.loc[4, 'A'] = df.loc[4, 'A'].replace('1 &AMP; 0', '1 & 0 = 0')\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 104, "library_problem_id": 104, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 105, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane Smith', 'Zhongli']})\n\ndef split_name(df):\n    df['first_name'] = df['name'].apply(lambda x: x.split()[0] if ' ' in x else x)\n    df['last_name'] = df['name'].apply(lambda x: ' '.join(x.split()[1:]) if ' ' in x else None)\n    return df\n\ndf = split_name(df)\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 105, "library_problem_id": 105, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 106, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane Smith', 'Zhongli']})\n\ndef split_name(df):\n    df['name_count'] = df['name'].apply(lambda x: x.count(' '))\n    df.loc[df['name_count'] == 0, '1_name'] = df['name']\n    df.loc[df['name_count'] == 1, '1_name'] = df['name'].apply(lambda x: x.split(' ')[0])\n    df.loc[df['name_count'] == 1, '2_name'] = df['name'].apply(lambda x: x.split(' ')[1])\n    df.loc[df['name_count'] > 1, '1_name'] = df['name']\n    df = df.drop(columns=['name_count'])\n    return df\n\ndf = split_name(df)\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 106, "library_problem_id": 106, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 105}}
{"id": 107, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'name':['Jack Fine','Kim Q. Danger','Jane 114 514 Smith', 'Zhongli']})\n\ndef split_name(df):\n    names = df['name'].str.split(' ', 2)\n    df['first_name'] = names.str[0]\n    df['middle_name'] = names.str[1]\n    df['last_name'] = names.str[2]\n    return df\n\ndf = split_name(df)\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 107, "library_problem_id": 107, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 105}}
{"id": 108, "code": ["<code>\nimport pandas as pd\n\ndf1 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:01', '2019/04/02 11:00:15', '2019/04/02 11:00:29', '2019/04/02 11:00:30'],\n                    'data': [111, 222, 333, 444]})\ndf2 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:14', '2019/04/02 11:00:15', '2019/04/02 11:00:16', '2019/04/02 11:00:30', '2019/04/02 11:00:31'],\n                    'stuff': [101, 202, 303, 404, 505]})\ndf1['Timestamp'] = pd.to_datetime(df1['Timestamp'])\ndf2['Timestamp'] = pd.to_datetime(df2['Timestamp'])\n\nresult = pd.merge_asof(df2.sort_values('Timestamp'), df1.sort_values('Timestamp'), on='Timestamp', direction='nearest')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 108, "library_problem_id": 108, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 109, "code": ["<code>\nimport pandas as pd\n\ndf1 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:01', '2019/04/02 11:00:15', '2019/04/02 11:00:29', '2019/04/02 11:00:30'],\n                    'data': [111, 222, 333, 444]})\n\ndf2 = pd.DataFrame({'Timestamp': ['2019/04/02 11:00:14', '2019/04/02 11:00:15', '2019/04/02 11:00:16', '2019/04/02 11:00:30', '2019/04/02 11:00:31'],\n                    'stuff': [101, 202, 303, 404, 505]})\n\ndf1['Timestamp'] = pd.to_datetime(df1['Timestamp'])\ndf2['Timestamp'] = pd.to_datetime(df2['Timestamp'])\n\nresult = pd.merge_asof(df1.sort_values('Timestamp'), df2.sort_values('Timestamp'), on='Timestamp', direction='nearest')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 109, "library_problem_id": 109, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 108}}
{"id": 110, "code": ["<code>\ndf['state'] = df.apply(lambda row: row['col1'] if row['col2'] <= 50 and row['col3'] <= 50 else max(row['col1'], row['col2'], row['col3']), axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 110, "library_problem_id": 110, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 111, "code": ["<code>\ndf['state'] = df.apply(lambda row: row['col1'] if row['col2'] > 50 and row['col3'] > 50 else row['col1'] + row['col2'] + row['col3'], axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 111, "library_problem_id": 111, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 110}}
{"id": 112, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\n\ndef find_non_integers(df, column):\n    non_integers = df[df[column].apply(lambda x: not isinstance(x, int))][column].tolist()\n    return non_integers\n\nnon_integers = find_non_integers(df, \"Field1\")\nprint(non_integers)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 112, "library_problem_id": 112, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 113, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({\"ID\": [1,2,3,4,5], \"Field1\": [1.15,2,1,25,\"and\"]})\n\ndef get_integer_values(df, column):\n    return df[df[column].apply(lambda x: isinstance(x, int))][column].tolist()\n\ndf = get_integer_values(df, \"Field1\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 113, "library_problem_id": 113, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 112}}
{"id": 114, "code": ["<code>\ndef f(df=example_df):\n    non_integer_values = []\n    for value in df['Field1']:\n        if not isinstance(value, int):\n            non_integer_values.append(value)\n    return non_integer_values\n\nresult = f(example_df)\nprint(result)\n### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 114, "library_problem_id": 114, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 112}}
{"id": 115, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'cat': ['A', 'B', 'C'],\n                   'val1': [7, 10, 5],\n                   'val2': [10, 2, 15],\n                   'val3': [0, 1, 6],\n                   'val4': [19, 14, 16]})\n\ndef calculate_percentage_contribution(df):\n    # Calculate the sum of each row\n    row_sum = df.iloc[:, 1:].sum(axis=1)\n\n    # Calculate the percentage contribution of each column to the row sum\n    df.iloc[:, 1:] = df.iloc[:, 1:].div(row_sum, axis=0) * 100\n\n    return df\n\ndf = calculate_percentage_contribution(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 115, "library_problem_id": 115, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 116, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'cat': ['A', 'B', 'C'],\n                   'val1': [7, 10, 5],\n                   'val2': [10, 2, 15],\n                   'val3': [0, 1, 6],\n                   'val4': [19, 14, 16]})\n\ndef calculate_percentage(df):\n    for col in df.columns[1:]:\n        df[col] = df.groupby('cat')[col].apply(lambda x: x / x.sum() * 100)\n    return df\n\ndf = calculate_percentage(df)\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 116, "library_problem_id": 116, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 115}}
{"id": 117, "code": ["<code>\nimport pandas as pd\nimport io\n\ndata = io.StringIO(\"\"\"\nrs  alleles  chrom  pos strand  assembly#  center  protLSID  assayLSID\nTP3      A/C      0    3      +        NaN     NaN       NaN        NaN\nTP7      A/T      0    7      +        NaN     NaN       NaN        NaN\nTP12     T/A      0   12      +        NaN     NaN       NaN        NaN\nTP15     C/A      0   15      +        NaN     NaN       NaN        NaN\nTP18     C/T      0   18      +        NaN     NaN       NaN        NaN\n\"\"\")\ndf = pd.read_csv(data, delim_whitespace=True).set_index('rs')\ntest = ['TP3', 'TP7', 'TP18']\n\nresult = df.loc[test]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 117, "library_problem_id": 117, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 118, "code": ["<code>\nresult = df.loc[test]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 118, "library_problem_id": 118, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 117}}
{"id": 119, "code": ["<code>\nresult = df.drop(test)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 119, "library_problem_id": 119, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 117}}
{"id": 120, "code": ["<code>\nimport pandas as pd\n\ndef f(df, test):\n    # return the solution in this function\n    result = df[df.index.isin(test)]\n    result = result[~result.index.duplicated(keep='first')]\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 120, "library_problem_id": 120, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 117}}
{"id": 121, "code": ["<code>\nHere is the completed function:\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.spatial import distance_matrix\n\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\n\n# Create a new column 'coords' that contains the coordinates of each car\ndf['coords'] = list(zip(df.x, df.y))\n\n# Group the dataframe by 'time'\ngrouped = df.groupby('time')\n\n# Initialize an empty list to store the average nearest neighbor distances\navg_nearest_neighbor_distances = []\n\n# For each group (i.e., for each time point)\nfor name, group in grouped:\n    # Calculate the distance matrix between the coordinates of the cars in this group\n    dist_matrix = distance_matrix(group.coords.values, group.coords.values)\n    \n    # For each row in the distance matrix\n    for i in range(dist_matrix.shape[0]):\n        # Set the diagonal element (which is the distance to itself) to infinity\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 121, "library_problem_id": 121, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 122, "code": ["<code>\nHere is the completed function:\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.spatial import distance_matrix\n\ntime = [0, 0, 0, 1, 1, 2, 2]\nx = [216, 218, 217, 280, 290, 130, 132]\ny = [13, 12, 12, 110, 109, 3, 56]\ncar = [1, 2, 3, 1, 3, 4, 5]\ndf = pd.DataFrame({'time': time, 'x': x, 'y': y, 'car': car})\n\n# Create a new dataframe to store the distances\ndf_distances = pd.DataFrame(columns=['time', 'car', 'distance'])\n\n# For each time point, calculate the distance matrix and find the maximum distance\nfor time_point in df['time'].unique():\n    df_time_point = df[df['time'] == time_point]\n    coords = df_time_point[['x', 'y']].values\n    distances = distance_matrix(coords, coords)\n    np.fill_diagonal(distances, np.inf)  # Fill diagonal with infinity to ignore self-comparisons\n    max_distances = distances.max(axis=1)\n    df_distances = df_distances.append(pd.DataFrame({'time': time_point, 'car': df_time_point['car'], 'distance':\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 122, "library_problem_id": 122, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 123, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'keywords_0':[\"a\", np.nan, \"c\"], \n                'keywords_1':[\"d\", \"e\", np.nan],\n                'keywords_2':[np.nan, np.nan, \"b\"],\n                'keywords_3':[\"f\", np.nan, \"g\"]})\n\ndf['new_column'] = df.apply(lambda row: ', '.join(row.dropna().values.astype(str)), axis=1)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 123, "library_problem_id": 123, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 124, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'keywords_0':[\"a\", np.nan, \"c\"], \n                'keywords_1':[\"d\", \"e\", np.nan],\n                'keywords_2':[np.nan, np.nan, \"b\"],\n                'keywords_3':[\"f\", np.nan, \"g\"]})\n\ndf['keywords_all'] = df.apply(lambda row: '-'.join(row.dropna().values.astype(str)), axis=1)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 124, "library_problem_id": 124, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 125, "code": ["<code>\ndf['keywords_all'] = df.apply(lambda row: '-'.join(row[1:]), axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 125, "library_problem_id": 125, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 126, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'users': ['Hu Tao', 'Zhongli', 'Xingqiu'],\n                   'keywords_0': [\"a\", np.nan, \"c\"],\n                   'keywords_1': [\"d\", \"e\", np.nan],\n                   'keywords_2': [np.nan, np.nan, \"b\"],\n                   'keywords_3': [\"f\", np.nan, \"g\"]})\n\n# Reverse the order of the columns\ndf = df.iloc[:, ::-1]\n\n# Use apply function to concatenate non-NaN values with '-' as separator\ndf['keywords_all'] = df.apply(lambda row: '-'.join(row.dropna().values), axis=1)\n\n# Reverse the order of the columns back to original order\ndf = df.iloc[:, ::-1]\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 126, "library_problem_id": 126, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 127, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'UserId': [1, 1, 1, 2, 3, 3],\n                   'ProductId': [1, 4, 7, 4, 2, 1],\n                   'Quantity': [6, 1, 3, 2, 7, 2]})\n\n# Randomly select 20% of rows\nsample_df = df.sample(frac=0.2, random_state=0)\n\n# Set the 'Quantity' column of the selected rows to zero\ndf.loc[sample_df.index, 'Quantity'] = 0\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 127, "library_problem_id": 127, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 128, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'UserId': [1, 1, 1, 2, 3, 3],\n                   'ProductId': [1, 4, 7, 4, 2, 1],\n                   'Quantity': [6, 1, 3, 2, 7, 2]})\n\n# Randomly select 20% of rows\nsample_df = df.sample(frac=0.2, random_state=1)\n\n# Set the 'ProductId' column of the selected rows to zero\ndf.loc[sample_df.index, 'ProductId'] = 0\n\ndf\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 128, "library_problem_id": 128, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 127}}
{"id": 129, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'UserId': [1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3],\n                   'ProductId': [1, 4, 7, 4, 2, 1, 1, 4, 7, 4, 2, 1, 1, 4, 7],\n                   'Quantity': [6, 1, 3, 2, 7, 2, 6, 1, 3, 2, 7, 2, 6, 1, 3]})\n\n# Randomly select 20% of rows for each user\ndf = df.groupby('UserId').apply(lambda x: x.sample(frac=0.2)).reset_index(drop=True)\n\n# Set the 'Quantity' column to zero for the selected rows\ndf['Quantity'] = 0\n\ndf\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 129, "library_problem_id": 129, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 127}}
{"id": 130, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame(data=[[1,2],[3,4],[1,2],[1,4],[1,2]],columns=['col1','col2'])\n\n# Convert the DataFrame to a string representation\ndf['str_rep'] = df.astype(str)\n\n# Identify duplicate rows\nduplicates = df[df.duplicated(subset='str_rep', keep=False)]\n\n# Add a new column indicating the index of the first occurrence of each duplicate\nduplicates['first_occurrence'] = duplicates['str_rep'].map(duplicates.groupby('str_rep').apply(lambda x: x.index[0]).to_dict())\n\n# Replace the 'str_rep' column with the original DataFrame\ndf.drop('str_rep', axis=1, inplace=True)\n\nresult = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 130, "library_problem_id": 130, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 131, "code": ["<code>\nimport pandas as pd\n\ndf=pd.DataFrame(data=[[1,2],[3,4],[1,2],[1,4],[1,2]],columns=['col1','col2'])\n\n# Create a new column 'last_occurrence' with the index of the last occurrence of each duplicate row\ndf['last_occurrence'] = df.duplicated(subset=['col1', 'col2'], keep='last').cumsum() - 1\n\nresult = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 131, "library_problem_id": 131, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 130}}
{"id": 132, "code": ["<code>\ndef f(df):\n        duplicate_rows = df[df.duplicated(keep=False)]\n        first_occurrence_index = {row: idx for idx, row in enumerate(df.values) if tuple(row) in duplicate_rows.values}\n        duplicate_rows['first_occurrence_index'] = duplicate_rows.apply(lambda row: first_occurrence_index[tuple(row.values)], axis=1)\n        return duplicate_rows\n    result = f(example_df)\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 132, "library_problem_id": 132, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 130}}
{"id": 133, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame(data=[[1,1,2,5],[1,3,4,1],[4,1,2,5],[5,1,4,9],[1,1,2,5]],columns=['val', 'col1','col2','3col'])\n\n# Identify duplicate rows based on specified columns\nduplicates = df[df.duplicated(subset=['val', 'col1', 'col2', '3col'], keep=False)]\n\n# Add a new column indicating the index of the first occurrence of each duplicate row\nduplicates['first_occurrence'] = duplicates.duplicated(subset=['val', 'col1', 'col2', '3col'], keep='first')\n\n# Convert the boolean values to indices\nduplicates['first_occurrence'] = duplicates['first_occurrence'].apply(lambda x: 1 if x else 0)\nduplicates['first_occurrence'] = duplicates['first_occurrence'].cumsum() - 1\n\n# Merge the original dataframe with the duplicates dataframe\nresult = pd.concat([df, duplicates]).reset_index(drop=True)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 133, "library_problem_id": 133, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 134, "code": ["<code>\nimport pandas as pd\n\ndf=pd.DataFrame(data=[[1,1,2,5],[1,3,4,1],[4,1,2,5],[5,1,4,9],[1,1,2,5]],columns=['val', 'col1','col2','3col'])\n\n# Identify duplicate rows based on specified columns\nduplicates = df[df.duplicated(subset=['val', 'col1', 'col2', '3col'], keep=False)]\n\n# Add a new column indicating the index of the last occurrence of each duplicate row\nduplicates['last_occurrence'] = duplicates.duplicated(subset=['val', 'col1', 'col2', '3col'], keep='last').cumsum() - 1\n\n# Merge the original DataFrame with the duplicates DataFrame\nresult = pd.concat([df, duplicates]).sort_index()\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 134, "library_problem_id": 134, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 135, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n                   'Mt': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n                   'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n                   'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]})\n\ndef find_max_count_rows(df, group_cols):\n    # Group by the specified columns and find the maximum count value in each group\n    max_count_values = df.groupby(group_cols)['count'].max().reset_index()\n\n    # Merge the original DataFrame with the maximum count values DataFrame on the specified columns and the count column\n    result = pd.merge(df, max_count_values, on=group_cols+['count'])\n\n    return result\n\nresult = find_max_count_rows(df, ['Sp', 'Mt'])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 135, "library_problem_id": 135, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 136, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Sp':['MM2','MM2','MM4','MM4','MM4'],\n                   'Mt':['S4','S4','S2','S2','S2'],\n                   'Value':['bg','dgd','rd','cb','uyi'],\n                   'count':[10,1,2,8,8]})\n\n# Group by 'Sp' and 'Mt', then find the maximum 'count' for each group\nmax_count_df = df.groupby(['Sp', 'Mt'])['count'].max().reset_index()\n\n# Merge the original DataFrame with the maximum 'count' DataFrame on 'Sp', 'Mt' and 'count'\nresult = pd.merge(df, max_count_df, on=['Sp', 'Mt', 'count'])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 136, "library_problem_id": 136, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 135}}
{"id": 137, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n                   'Mt': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n                   'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n                   'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]})\n\n# Create a new dataframe with the minimum 'count' value for each group defined by 'Sp' and 'Mt'\nmin_count_df = df.groupby(['Sp', 'Mt'])['count'].min().reset_index()\n\n# Merge the original dataframe with the new dataframe on 'Sp', 'Mt', and 'count'\nresult = pd.merge(df, min_count_df, on=['Sp', 'Mt', 'count'])\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 137, "library_problem_id": 137, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 135}}
{"id": 138, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Sp':['MM1','MM1','MM1','MM2','MM2','MM2','MM4','MM4','MM4'],\n                   'Value':['S1','S1','S3','S3','S4','S4','S2','S2','S2'],\n                   'Mt':['a','n','cb','mk','bg','dgd','rd','cb','uyi'],\n                   'count':[3,2,5,8,10,1,2,2,7]})\n\n# Group by 'Sp' and 'Value' and find the maximum 'count' value in each group\nmax_count_df = df.groupby(['Sp', 'Value'])['count'].max().reset_index()\n\n# Merge the original DataFrame with the maximum 'count' DataFrame on 'Sp', 'Value' and 'count'\nresult = pd.merge(df, max_count_df, on=['Sp', 'Value', 'count'])\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 138, "library_problem_id": 138, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 135}}
{"id": 139, "code": ["<code>\nimport pandas as pd\n\ndf=pd.DataFrame({\"Category\":['Foo','Bar','Cho','Foo'],'Index':[1,2,3,4]})\nfilter_list=['Foo','Bar']\n\nresult = df[df['Category'].isin(filter_list)]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 139, "library_problem_id": 139, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 140, "code": ["<code>\nimport pandas as pd\n\ndf=pd.DataFrame({\"Category\":['Foo','Bar','Cho','Foo'],'Index':[1,2,3,4]})\nfilter_list=['Foo','Bar']\n\n# Convert filter_list to string format for query function\nfilter_list_str = \"|\".join(filter_list)\n\n# Use query function to filter rows based on the filter_list\nresult = df.query(\"Category not in @filter_list_str\")\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 140, "library_problem_id": 140, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 139}}
{"id": 141, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'col1': {0: 'a', 1: 'b', 2: 'c'},\n                   'col2': {0: 1, 1: 3, 2: 5},\n                   'col3': {0: 2, 1: 4, 2: 6},\n                   'col4': {0: 3, 1: 6, 2: 2},\n                   'col5': {0: 7, 1: 2, 2: 3},\n                   'col6': {0: 2, 1: 9, 2: 5},\n                  })\ndf.columns = [list('AAAAAA'), list('BBCCDD'), list('EFGHIJ')]\n\ndef melt_multiindex(df):\n    return pd.melt(df.reset_index(), id_vars=df.index.names)\n\nresult = melt_multiindex(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 141, "library_problem_id": 141, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 142, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'col1': {0: 'a', 1: 'b', 2: 'c'},\n                   'col2': {0: 1, 1: 3, 2: 5},\n                   'col3': {0: 2, 1: 4, 2: 6},\n                   'col4': {0: 3, 1: 6, 2: 2},\n                   'col5': {0: 7, 1: 2, 2: 3},\n                   'col6': {0: 2, 1: 9, 2: 5},\n                  })\ndf.columns = [list('AAAAAA'), list('BBCCDD'), list('EFGHIJ')]\n\nresult = pd.melt(df.reset_index(), id_vars='index')\nresult.columns = ['index', 'variable', 'value']\nresult = result.set_index(['index', 'variable'])['value'].unstack().reset_index()\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 142, "library_problem_id": 142, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 141}}
{"id": 143, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame.from_dict({'id': ['A', 'B', 'A', 'C', 'D', 'B', 'C'],\n                             'val': [1,2,-3,1,5,6,-2],\n                             'stuff':['12','23232','13','1234','3235','3236','732323']})\n\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 143, "library_problem_id": 143, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 144, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame.from_dict({'id': ['A', 'B', 'A', 'C', 'D', 'B', 'C'],\n                             'val': [1,2,-3,1,5,6,-2],\n                             'stuff':['12','23232','13','1234','3235','3236','732323']})\n\ndf['cumulative_sum'] = df.groupby('id')['val'].cumsum()\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 144, "library_problem_id": 144, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 145, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame.from_dict({'id': ['A', 'B', 'A', 'C', 'D', 'B', 'C'],\n                             'val': [1,2,-3,1,5,6,-2],\n                             'stuff':['12','23232','13','1234','3235','3236','732323']})\n\ndf['cumsum'] = df.groupby('id')['val'].cumsum()\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 145, "library_problem_id": 145, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 146, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame.from_dict({'id': ['A', 'B', 'A', 'C', 'D', 'B', 'C'],\n                             'val': [1,2,-3,1,5,6,-2],\n                             'stuff':['12','23232','13','1234','3235','3236','732323']})\n\ndf['cummax'] = df.groupby('id')['val'].cummax()\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 146, "library_problem_id": 146, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 147, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame.from_dict({'id': ['A', 'B', 'A', 'C', 'D', 'B', 'C'],\n                             'val': [1,2,-3,1,5,6,-2],\n                             'stuff':['12','23232','13','1234','3235','3236','732323']})\n\ndf['cumulative_sum'] = df.groupby('id')['val'].cumsum().clip(0)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 147, "library_problem_id": 147, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 143}}
{"id": 148, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\nd = {'l':  ['left', 'right', 'left', 'right', 'left', 'right'],\n     'r': ['right', 'left', 'right', 'left', 'right', 'left'],\n     'v': [-1, 1, -1, 1, -1, np.nan]}\ndf = pd.DataFrame(d)\n\ndef groupby_sum_with_nan(df, by, var):\n    df['temp'] = df.groupby(by)[var].cumcount() + 1\n    df = df.sort_values(by + 'temp')\n    df['temp'] = df.groupby(by + 'temp')[var].cumsum()\n    df = df.sort_values(by)\n    df = df.drop(by + 'temp', axis=1)\n    return df\n\nresult = groupby_sum_with_nan(df, 'l', 'v')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 148, "library_problem_id": 148, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 149, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\nd = {'l':  ['left', 'right', 'left', 'right', 'left', 'right'],\n     'r': ['right', 'left', 'right', 'left', 'right', 'left'],\n     'v': [-1, 1, -1, 1, -1, np.nan]}\ndf = pd.DataFrame(d)\n\n# Create a new column 'group' that combines 'l' and 'r' columns\ndf['group'] = df['l'] + df['r']\n\n# Group by 'group' column and sum 'v' column, but keep NaN values\nresult = df.groupby('group')['v'].transform('sum')\n\n# Replace NaN values in 'v' column with the calculated sum\ndf['v'] = df['v'].fillna(result)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 149, "library_problem_id": 149, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 148}}
{"id": 150, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\nd = {'l':  ['left', 'right', 'left', 'right', 'left', 'right'],\n     'r': ['right', 'left', 'right', 'left', 'right', 'left'],\n     'v': [-1, 1, -1, 1, -1, np.nan]}\ndf = pd.DataFrame(d)\n\n# Create a new column 'sum' that is the sum of 'v' column, ignoring NaN values\ndf['sum'] = df['v'].apply(lambda x: x if pd.isnull(x) else x)\n\n# Group by 'l' and 'r' columns and sum the 'sum' column\nresult = df.groupby(['l', 'r'])['sum'].sum()\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 150, "library_problem_id": 150, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 151, "code": ["<code>\nHere is the completed function:\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n\ndef determine_relationship(df):\n    result = []\n    for col1 in df.columns:\n        for col2 in df.columns:\n            if col1 != col2:\n                if df[col1].nunique() == df[col2].nunique() == len(df):\n                    relationship = 'one-to-one'\n                elif df[col1].nunique() == len(df):\n                    relationship = 'one-to-many'\n                elif df[col2].nunique() == len(df):\n                    relationship = 'many-to-one'\n                else:\n                    relationship = 'many-to-many'\n                result.append(f'{col1} and {col2} are {relationship}')\n    return result\n\nresult = determine_relationship(df)\nfor r\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 151, "library_problem_id": 151, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 152, "code": ["<code>\nHere is the solution:\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n\ndef relationship(df):\n    result = []\n    for col1 in df.columns:\n        for col2 in df.columns:\n            if col1 != col2:\n                if df[col1].nunique() == df[col2].nunique() == df.shape[0]:\n                    result.append(f\"{col1} and {col2} have a one-to-one relationship\")\n                elif df[col1].nunique() == df.shape[0] and df[col2].nunique() < df.shape[0]:\n                    result.append(f\"{col1} and {col2} have a one-to-many relationship\")\n                elif df[col1].nunique() < df.shape[0] and df[col2].nunique() == df.shape[0]:\n                    result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 152, "library_problem_id": 152, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 153, "code": ["<code>\nHere is the completed function:\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n\ndef relationship(df):\n    relationships = {\n        'Column1': {},\n        'Column2': {},\n        'Column3': {},\n        'Column4': {},\n        'Column5': {}\n    }\n\n    for col1 in df.columns:\n        for col2 in df.columns:\n            if col1 != col2:\n                if df[col1].nunique() == df[col2].nunique() == df.shape[0]:\n                    relationships[col1][col2] = 'one-to-one'\n                elif df[col1].nunique() == df.shape[0] and df[col1].nunique() < df[col2].nunique():\n                    relationships[col1][col2] = 'one-to-many'\n                elif df[col2].nunique()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 153, "library_problem_id": 153, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 154, "code": ["<code>\nHere is the completed function:\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'Column1': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n    'Column2': [4, 3, 6, 8, 3, 4, 1, 4, 3],\n    'Column3': [7, 3, 3, 1, 2, 2, 3, 2, 7],\n    'Column4': [9, 8, 7, 6, 5, 4, 3, 2, 1],\n    'Column5': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n\n# Create a DataFrame to store the relationship\nrelationship_df = pd.DataFrame(columns=df.columns, index=df.columns)\n\n# Determine the type of relationship for each pair of columns\nfor i in df.columns:\n    for j in df.columns:\n        if i != j:\n            if df[i].nunique() == df[j].nunique() == df.shape[0]:\n                relationship_df.loc[i, j] = 'one-to-one'\n            elif df[i].nunique() == df.shape[0] and df[j].nunique() < df.shape[0]:\n                relationship_df.loc[i, j] = 'one-to-many'\n            elif df[i].nunique() < df.shape[0] and df[\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 154, "library_problem_id": 154, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 155, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'firstname': ['foo Bar', 'Bar Bar', 'Foo Bar'],\n                   'lastname': ['Foo Bar', 'Bar', 'Foo Bar'],\n                   'email': ['Foo bar', 'Bar', 'Foo Bar'],\n                   'bank': [np.nan, 'abc', 'xyz']})\n\n# First, we sort the DataFrame by the 'bank' column, making sure that rows with non-null 'bank' values come first.\ndf = df.sort_values('bank', ascending=False)\n\n# Then, we drop duplicates based on the 'firstname', 'lastname', and 'email' columns.\n# Since we sorted the DataFrame by 'bank' beforehand, this will keep the row with the non-null 'bank' value if duplicates exist.\nresult = df.drop_duplicates(subset=['firstname', 'lastname', 'email'])\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 155, "library_problem_id": 155, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 155}}
{"id": 156, "code": ["<code>\nHere is the completed function:\n\n```python\nimport pandas as pd\n\ns = pd.Series(['2,144.78', '2,036.62', '1,916.60', '1,809.40', '1,711.97', '6,667.22', '5,373.59', '4,071.00', '3,050.20', '-0.06', '-1.88', '', '-0.13', '', '-0.14', '0.07', '0', '0'],\n              index=['2016-10-31', '2016-07-31', '2016-04-30', '2016-01-31', '2015-10-31', '2016-01-31', '2015-01-31', '2014-01-31', '2013-01-31', '2016-09-30', '2016-06-30', '2016-03-31', '2015-12-31', '2015-09-30', '2015-12-31', '2014-12-31', '2013-12-31', '2012-12-31'])\n\n# Replace commas with periods and then\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 156, "library_problem_id": 156, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 156}}
{"id": 157, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Survived': [0,1,1,1,0],\n                   'SibSp': [1,1,0,1,0],\n                   'Parch': [0,0,0,0,1]})\n\n# Create a new column 'HasFamily' where 'SibSp' > 0 or 'Parch' > 0\ndf['HasFamily'] = df['SibSp'].apply(lambda x: 1 if x > 0 else 0) + df['Parch'].apply(lambda x: 1 if x > 0 else 0)\n\n# Group the data by 'HasFamily' and calculate the mean of 'Survived'\nresult = df.groupby('HasFamily')['Survived'].mean()\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 157, "library_problem_id": 157, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 158, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Survived': [0,1,1,1,0],\n                   'SibSp': [1,1,0,1,0],\n                   'Parch': [0,0,0,0,1]})\n\ndf['Family'] = df.apply(lambda row: 'Has Family' if row['Survived'] > 0 or row['Parch'] > 0 else 'No Family', axis=1)\nresult = df.groupby('Family')['SibSp'].mean()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 158, "library_problem_id": 158, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 157}}
{"id": 159, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Survived': [0,1,1,1,0],\n                   'SibSp': [1,1,0,1,0],\n                   'Parch': [0,0,0,0,1]})\n\n# Create conditions\nconditions = [(df['SibSp'] == 1) & (df['Parch'] == 1),\n              (df['SibSp'] == 0) & (df['Parch'] == 0),\n              (df['SibSp'] == 0) & (df['Parch'] == 1),\n              (df['SibSp'] == 1) & (df['Parch'] == 0)]\n\n# Create labels\nlabels = ['(SibSp == 1) & (Parch == 1)',\n          '(SibSp == 0) & (Parch == 0)',\n          '(SibSp == 0) & (Parch == 1)',\n          '(SibSp == 1) & (Parch == 0)']\n\n# Add a new column 'Group' to the dataframe\ndf['Group'] = np.select(conditions, labels, default='other')\n\n# Calculate the mean of the 'Survived' column for each group\nresult = df.groupby('Group')['Survived'].mean()\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 159, "library_problem_id": 159, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 157}}
{"id": 160, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'cokey':[11168155,11168155,11168155,11168156,11168156],\n                   'A':[18,0,56,96,0],\n                   'B':[56,18,96,152,96]})\n\nresult = df.sort_values(by=['cokey', 'A'])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 160, "library_problem_id": 160, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 161, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'cokey':[11168155,11168155,11168155,11168156,11168156],\n                   'A':[18,0,56,96,0],\n                   'B':[56,18,96,152,96]})\n\nresult = df.groupby('cokey').apply(lambda x: x.sort_values('A')).reset_index(drop=True)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 161, "library_problem_id": 161, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 160}}
{"id": 162, "code": ["<code>\ndf.columns = pd.MultiIndex.from_tuples(df.columns, names=['Level1', 'Level2'])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 162, "library_problem_id": 162, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 162}}
{"id": 163, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\nl = [('A', '1', 'a'),  ('A', '1', 'b'), ('A', '2', 'a'), ('A', '2', 'b'), ('B', '1','a'),  ('B', '1','b')]\nnp.random.seed(1)\ndf = pd.DataFrame(np.random.randn(5, 6), columns=l)\n\ndf.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Middle', 'Lower'])\ndf\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 163, "library_problem_id": 163, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 162}}
{"id": 164, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\nl = [('A', 'a', '1'), ('A', 'b', '2'), ('B','a', '1'), ('A', 'b', '1'),  ('B','b', '1'),  ('A', 'a', '2')]\nnp.random.seed(1)\ndf = pd.DataFrame(np.random.randn(5, 6), columns=l)\n\ndf.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Middle', 'Lower'])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 164, "library_problem_id": 164, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 162}}
{"id": 165, "code": ["<code>\nresult = pd.DataFrame({'Birds': someTuple[0], 'Counts': someTuple[1]})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 165, "library_problem_id": 165, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 166, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'a':[1,1,1,2,2,2,3,3,3], 'b':[12,13,23,22,23,24,30,35,55]})\n\nresult = df.groupby('a')['b'].agg(['mean', 'std']).reset_index()\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 166, "library_problem_id": 166, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 166}}
{"id": 167, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'a':[12,13,23,22,23,24,30,35,55], 'b':[1,1,1,2,2,2,3,3,3]})\n\nresult = df.groupby('b').agg({'a': ['mean', 'std']})\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 167, "library_problem_id": 167, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 166}}
{"id": 168, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import softmax\nfrom sklearn.preprocessing import MinMaxScaler\n\ndf = pd.DataFrame({'a':[1,1,1,2,2,2,3,3,3], 'b':[12,13,23,22,23,24,30,35,55]})\n\n# Group by 'a' and calculate softmax and min-max normalization of 'b' for each group\ndf['softmax_b'] = df.groupby('a')['b'].transform(lambda x: softmax(x))\ndf['minmax_b'] = df.groupby('a')['b'].transform(lambda x: MinMaxScaler().fit_transform(np.array(x).reshape(-1,1)))\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 168, "library_problem_id": 168, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 166}}
{"id": 169, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame([[1,1,0,1],[0,0,0,0],[1,0,0,1],[0,1,0,0],[1,1,0,1]],columns=['A','B','C','D'])\n\nresult = df.loc[:, (df.sum() != 0)]\nresult = result.loc[(result.sum(axis=1) != 0), :]\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 169, "library_problem_id": 169, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 169}}
{"id": 170, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame([[-1,-1,0,2],[0,0,0,0],[1,0,0,1],[0,1,0,0],[1,1,0,1]],columns=['A','B','C','D'])\n\n# Calculate the sum of each row\nrow_sums = df.sum(axis=1)\n\n# Filter the DataFrame to only include rows with a non-zero sum\ndf = df[row_sums != 0]\n\n# Calculate the sum of each column\ncol_sums = df.sum(axis=0)\n\n# Filter the DataFrame to only include columns with a non-zero sum\ndf = df[col_sums != 0]\n\nresult = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 170, "library_problem_id": 170, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 169}}
{"id": 171, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame([[1,2,3,1],[0,0,0,0],[1,0,0,1],[0,1,2,0],[1,1,0,1]],columns=['A','B','C','D'])\n\nresult = df[(df < 2).all(axis=1)]\nresult = result.loc[:, (result < 2).all(axis=0)]\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 171, "library_problem_id": 171, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 169}}
{"id": 172, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame([[1,2,3,1],[0,0,0,0],[1,0,0,1],[0,1,2,0],[1,1,0,1]],columns=['A','B','C','D'])\n\nresult = df.mask(df.eq(2))\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 172, "library_problem_id": 172, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 169}}
{"id": 173, "code": ["<code>\nimport pandas as pd\n\ns = pd.Series([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0.98,0.93],\n          index=['146tf150p','havent','home','okie','thanx','er','anything','lei','nite','yup','thank','ok','where','beerage','anytime','too','done','645','tick','blank'])\n\nresult = s.sort_values(ascending=(True, False))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 173, "library_problem_id": 173, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 174, "code": ["<code>\nimport pandas as pd\n\ns = pd.Series([1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0.98,0.93],\n              index=['146tf150p','havent','home','okie','thanx','er','anything','lei','nite','yup','thank','ok','where','beerage','anytime','too','done','645','tick','blank'])\n\ndf = s.sort_values(ascending=False).reset_index().rename(columns={'index': 'original_index', 0: '1'})\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 174, "library_problem_id": 174, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 173}}
{"id": 175, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'A': [1, 2, 's', 3, 'b'],\n                   'B': ['green', 'red', 'blue', 'yellow', 'black']})\n\n# Convert column 'A' to string type\ndf['A'] = df['A'].astype(str)\n\n# Filter rows where 'A' is numeric\nresult = df[df['A'].apply(lambda x: x.isnumeric())]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 175, "library_problem_id": 175, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 175}}
{"id": 176, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'A': [1, 2, 's', 3, 'b'],\n                   'B': ['green', 'red', 'blue', 'yellow', 'black']})\n\nresult = df[df['A'].apply(lambda x: isinstance(x, str))]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 176, "library_problem_id": 176, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 175}}
{"id": 177, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n                   'Mt': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n                   'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n                   'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]})\n\n# Group by 'Sp' and 'Mt' and find the maximum 'count' value in each group\nmax_count_df = df.groupby(['Sp', 'Mt'])['count'].max().reset_index()\n\n# Merge the original DataFrame with the maximum 'count' DataFrame on 'Sp', 'Mt' and 'count'\nresult = pd.merge(df, max_count_df, on=['Sp', 'Mt', 'count'])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 177, "library_problem_id": 177, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 177}}
{"id": 178, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Sp':['MM2','MM2','MM4','MM4','MM4'],\n                   'Mt':['S4','S4','S2','S2','S2'],\n                   'Value':['bg','dgd','rd','cb','uyi'],\n                   'count':[10,1,2,8,8]})\n\n# Group by 'Sp' and 'Mt' and find the maximum 'count' value in each group\nmax_count_df = df.groupby(['Sp', 'Mt'])['count'].max().reset_index()\n\n# Merge the original DataFrame with the maximum 'count' DataFrame on 'Sp', 'Mt' and 'count'\nresult = pd.merge(df, max_count_df, on=['Sp', 'Mt', 'count'])\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 178, "library_problem_id": 178, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 177}}
{"id": 179, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Sp': ['MM1', 'MM1', 'MM1', 'MM2', 'MM2', 'MM2', 'MM4', 'MM4', 'MM4'],\n                   'Mt': ['S1', 'S1', 'S3', 'S3', 'S4', 'S4', 'S2', 'S2', 'S2'],\n                   'Value': ['a', 'n', 'cb', 'mk', 'bg', 'dgd', 'rd', 'cb', 'uyi'],\n                   'count': [3, 2, 5, 8, 10, 1, 2, 2, 7]})\n\n# Create a new dataframe with the minimum 'count' value for each group defined by 'Sp' and 'Mt'\nmin_count_df = df.groupby(['Sp', 'Mt'])['count'].min().reset_index()\n\n# Merge the original dataframe with the new dataframe on 'Sp', 'Mt', and 'count'\nresult = pd.merge(df, min_count_df, on=['Sp', 'Mt', 'count'])\n\n# Drop duplicates from the result dataframe\nresult = result.drop_duplicates(subset=['Sp', 'Mt', 'count'])\n\n# Reset the index of the result dataframe\nresult = result.reset_index(drop=True)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 179, "library_problem_id": 179, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 180, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Sp':['MM1','MM1','MM1','MM2','MM2','MM2','MM4','MM4','MM4'],\n                   'Value':['S1','S1','S3','S3','S4','S4','S2','S2','S2'],\n                   'Mt':['a','n','cb','mk','bg','dgd','rd','cb','uyi'],\n                   'count':[3,2,5,8,10,1,2,2,7]})\n\n# Group by 'Sp' and 'Value' and find the maximum 'count' value in each group\nmax_count_df = df.groupby(['Sp', 'Value'])['count'].max().reset_index()\n\n# Merge the original DataFrame with the maximum 'count' DataFrame on 'Sp', 'Value' and 'count'\nresult = pd.merge(df, max_count_df, on=['Sp', 'Value', 'count'])\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 180, "library_problem_id": 180, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 181, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndict = {'abc':'1/2/2003', 'def':'1/5/2017', 'ghi':'4/10/2013'}\ndf = pd.DataFrame({'Member':['xyz', 'uvw', 'abc', 'def', 'ghi'], 'Group':['A', 'B', 'A', 'B', 'B'], 'Date':[np.nan]*5})\n\ndf['Date'] = df['Member'].map(dict)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 181, "library_problem_id": 181, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 181}}
{"id": 182, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndict = {'abc':'1/2/2003', 'def':'1/5/2017', 'ghi':'4/10/2013'}\ndf = pd.DataFrame({'Member':['xyz', 'uvw', 'abc', 'def', 'ghi'], 'Group':['A', 'B', 'A', 'B', 'B'], 'Date':[np.nan, np.nan, np.nan, np.nan, np.nan]})\n\ndf['Date'] = df['Member'].map(dict).fillna('No Match Found')\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 182, "library_problem_id": 182, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 181}}
{"id": 183, "code": ["<code>\nimport pandas as pd\n\nexample_dict = {'abc':'1/2/2003', 'def':'1/5/2017', 'ghi':'4/10/2013'}\nexample_df = pd.DataFrame({'Member':['xyz', 'uvw', 'abc', 'def', 'ghi'], 'Group':['A', 'B', 'A', 'B', 'B'], 'Date':[np.nan, np.nan, np.nan, np.nan, np.nan]})\n\ndef f(dict=example_dict, df=example_df):\n    df['Date'] = df['Member'].map(dict)\n    return df\n\nresult = f(example_dict, example_df)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 183, "library_problem_id": 183, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 181}}
{"id": 184, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndict = {'abc':'1/2/2003', 'def':'1/5/2017', 'ghi':'4/10/2013'}\ndf = pd.DataFrame({'Member':['xyz', 'uvw', 'abc', 'def', 'ghi'], 'Group':['A', 'B', 'A', 'B', 'B'], 'Date':[np.nan, np.nan, np.nan, np.nan, np.nan]})\n\n# Map values from the dictionary to the 'Date' column based on matching keys in the 'Member' column\ndf['Date'] = df['Member'].map(dict).fillna('Not Found')\n\n# Reformat the date column\ndf['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 184, "library_problem_id": 184, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 181}}
{"id": 185, "code": ["<code>\nimport pandas as pd\n\nd = ({'Date': ['1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\n\n# Convert 'Date' column to datetime\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Add 'Count_d' column\ndf['Count_d'] = 1\n\n# Group by 'Date' and 'Val' and count occurrences\ndf_grouped = df.groupby(['Date', 'Val']).count()\n\n# Reset index\ndf_grouped = df_grouped.reset_index()\n\n# Add 'Count_m' and 'Count_y' columns\ndf_grouped['Count_m'] = df_grouped.groupby([df_grouped['Date'].dt.year, df_grouped['Date'].dt.month])['Count_d'].cumsum()\ndf_grouped['Count_y'] = df_grouped.groupby(df_grouped['Date'].dt.year)['Count_d'].cumsum()\n\n# Reset index\ndf_grouped = df_grouped.reset_index(drop=True)\n\ndf = df_grouped\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 185, "library_problem_id": 185, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 186, "code": ["<code>\nSure, here is the completed function:\n\n```python\nimport pandas as pd\n\nd = ({'Date': ['1/1/18','1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\n\n# Convert the Date column to datetime\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Set the Date column as the index\ndf.set_index('Date', inplace=True)\n\n# Group by year, month and day\ndf_grouped = df.groupby([df.index.year, df.index.month, df.index.day]).size().reset_index(name='Daily_Counts')\n\n# Group by year and month\ndf_grouped_monthly = df.groupby([df.index.year, df.index.month]).size().reset_index(name='Monthly_Counts')\n\n# Group by year\ndf_grouped_yearly = df.groupby(df.index.year).size().reset_index(name='Yearly_Counts')\n\n# Group by Val\ndf_grouped_val = df.groupby('Val').size().reset_index(name='Val_Counts')\n\n# Join all the grouped dataframes\ndf = df_grouped.set_index(['Date',\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 186, "library_problem_id": 186, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 185}}
{"id": 187, "code": ["<code>\nSure, here is the completed function:\n\n```python\nimport pandas as pd\n\nd = ({'Date': ['1/1/18','1/1/18','1/1/18','2/1/18','3/1/18','1/2/18','1/3/18','2/1/19','3/1/19'],\n      'Val': ['A','A','B','C','D','A','B','C','D']})\ndf = pd.DataFrame(data=d)\n\n# Convert the Date column to datetime\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Create new columns for year, month, and day\ndf['Year'] = df['Date'].dt.year\ndf['Month'] = df['Date'].dt.month\ndf['Day'] = df['Date'].dt.day\n\n# Group by year, month, and day and count the occurrences of each unique combination\ndf_daily = df.groupby(['Year', 'Month', 'Day', 'Val']).size().reset_index(name='Counts')\n\n# Group by year and month and count the occurrences of each unique combination\ndf_monthly = df.groupby(['Year', 'Month', 'Val']).size().reset_index(name='Counts')\n\n# Group by year and count the occurrences of each unique combination\ndf_yearly = df.groupby(['Year', 'Val']).size().reset_index(name='Counts')\n\n# Group by weekday and count the occurrences of each unique combination\ndf_weekday = df.group\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 187, "library_problem_id": 187, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 185}}
{"id": 188, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Date': ['20.07.2018', '20.07.2018', '21.07.2018', '21.07.2018'],\n                   'B': [10, 1, 0, 1],\n                   'C': [8, 0, 1, 0]})\n\n# Convert 'Date' column to datetime\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Count the number of zero and non-zero values in each of the numerical columns for each unique date\nresult1 = df.groupby('Date').apply(lambda x: (x==0).sum()).reset_index()\nresult2 = df.groupby('Date').apply(lambda x: (x!=0).sum()).reset_index()\n\n# Rename the columns to 'Date', 'Zero', 'Non-Zero'\nresult1.columns = ['Date', 'Zero', 'Non-Zero']\nresult2.columns = ['Date', 'Zero', 'Non-Zero']\n\n# Sort the DataFrames by 'Date'\nresult1 = result1.sort_values('Date')\nresult2 = result2.sort_values('Date')\n\n# Print the DataFrames\nprint(result1)\nprint(result2)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 188, "library_problem_id": 188, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 189, "code": ["<code>\nHere is the completed function:\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({'Date': ['20.07.2018', '20.07.2018', '21.07.2018', '21.07.2018'],\n                   'B': [10, 1, 0, 1],\n                   'C': [8, 0, 1, 0]})\n\n# Create a new dataframe with only the 'Date' column\ndates = df[['Date']].drop_duplicates()\n\n# Initialize empty dataframes for even and odd counts\nresult1 = pd.DataFrame()\nresult2 = pd.DataFrame()\n\n# Loop over each unique date\nfor date in dates['Date']:\n    # Filter the original dataframe for the current date\n    temp_df = df[df['Date'] == date]\n    \n    # Count the number of even and odd values in each numerical column\n    even_counts = temp_df.select_dtypes(include=[int, float]).apply(lambda x: ((x % 2) == 0).sum()).rename(date)\n    odd_counts = temp_df.select_dtypes(include=[int, float]).apply(lambda x: ((x % 2) != 0).sum()).rename(date)\n    \n    # Append the counts to the result dataframes\n    result1 = pd.concat([result1, even_counts.to_frame().T], ignore_index=True)\n    result2 = pd.concat([\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 189, "library_problem_id": 189, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 188}}
{"id": 190, "code": ["<code>\nresult = pd.pivot_table(df, values=['D', 'E'], index=['A', 'B', 'C'], aggfunc={'D': np.sum, 'E': np.mean})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 190, "library_problem_id": 190, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 190}}
{"id": 191, "code": ["<code>\nresult = df.pivot_table(values=['D', 'E'], index='B', aggfunc={'D': np.sum, 'E': np.mean})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 191, "library_problem_id": 191, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 192, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(1)\ndf = pd.DataFrame({\n'A' : ['abc', 'def', 'xyz', 'abc'] * 3,\n'B' : ['A', 'B', 'C'] * 4,\n'D' : np.random.randn(12),\n'E' : np.random.randn(12)\n})\n\nresult = pd.pivot_table(df, values=['D', 'E'], index='A', aggfunc={'D': np.sum, 'E': np.mean})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 192, "library_problem_id": 192, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 193, "code": ["<code>\nresult = pd.pivot_table(df, values=['D', 'E'], index=['A', 'B', 'C'], aggfunc={'D': 'max', 'E': 'min'})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 193, "library_problem_id": 193, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 190}}
{"id": 194, "code": ["<code>\nimport pandas as pd\nimport dask.dataframe as dd\n\ndf = pd.DataFrame([[\"A\", \"Z,Y\"], [\"B\", \"X\"], [\"C\", \"W,U,V\"]], index=[1,2,3], columns=['var1', 'var2'])\n\n# Convert pandas dataframe to dask dataframe\nddf = dd.from_pandas(df, npartitions=2)\n\n# Split 'var2' column by comma and explode it\nresult = ddf.assign(var2=lambda x: x['var2'].str.split(',')).explode('var2')\n\n# Compute the result\nresult = result.compute()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 194, "library_problem_id": 194, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 195, "code": ["<code>\nimport dask.dataframe as dd\n\n# Convert pandas dataframe to dask dataframe\nddf = dd.from_pandas(df, npartitions=2)\n\n# Split 'var2' column by comma and explode it\nresult = ddf.assign(var2=ddf['var2'].str.split(',')).explode('var2')\n\n# Compute the result\nresult = result.compute()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 195, "library_problem_id": 195, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 194}}
{"id": 196, "code": ["<code>\nimport dask.dataframe as dd\n\n# Convert pandas dataframe to dask dataframe\nddf = dd.from_pandas(df, npartitions=2)\n\n# Split the 'var2' column by '-' and stack the resulting dataframes\nresult = ddf.var2.str.split('-', expand=True).stack().to_frame('var2').reset_index(level=1, drop=True).reset_index()\n\n# Verify the result\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 196, "library_problem_id": 196, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 194}}
{"id": 197, "code": ["<code>\nimport pandas as pd\nimport re\n\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\n\ndef count_special_chars(df, column):\n    df['count'] = df[column].apply(lambda x: len(re.findall(r'\\W', x)))\n    return df\n\ndf = count_special_chars(df, 'str')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 197, "library_problem_id": 197, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 197}}
{"id": 198, "code": ["<code>\nimport pandas as pd\nimport re\n\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\n\ndef count_non_alpha(row):\n    return len(re.findall(r'[^a-zA-Z]', row))\n\ndf['non_alpha_count'] = df['str'].apply(count_non_alpha)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 198, "library_problem_id": 198, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 197}}
{"id": 199, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'row': ['00000 UNITED STATES', '01000 ALABAMA',\n                           '01001 Autauga County, AL', '01003 Baldwin County, AL',\n                           '01005 Barbour County, AL']})\n\ndf[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 199, "library_problem_id": 199, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 199}}
{"id": 200, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'row': ['114 AAAAAA', '514 ENENEN',\n                           '1926 HAHAHA', '0817 O-O,O-O',\n                           '998244353 TTTTTT']})\n\ndf[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 200, "library_problem_id": 200, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 199}}
{"id": 201, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'row': ['00000 UNITED STATES', '01000 ALAB AMA',\n                           '01001 Autauga County, AL', '01003 Baldwin County, AL',\n                           '01005 Barbour County, AL']})\n\n# Split the 'row' column into three parts\ndf[['fips', 'medi', 'row']] = df['row'].str.split(' ', 2, expand=True)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 201, "library_problem_id": 201, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 199}}
{"id": 202, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Name': ['Name1', 'Name2', 'Name3'],\n                   '2001': [2, 1, 0],\n                   '2002': [5, 4, 5],\n                   '2003': [0, 2, 0],\n                   '2004': [0, 0, 0],\n                   '2005': [4, 4, 0],\n                   '2006': [6, 0, 2]})\n\ndef cumulative_average(df):\n    for col in df.columns[1:]:\n        df[col] = df[col].where(df[col] != 0).cumsum() / (df[col] != 0).cumsum()\n    return df\n\ndf = cumulative_average(df)\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 202, "library_problem_id": 202, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 203, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Name': ['Name1', 'Name2', 'Name3'],\n                   '2001': [2, 1, 0],\n                   '2002': [5, 4, 5],\n                   '2003': [0, 2, 0],\n                   '2004': [0, 0, 0],\n                   '2005': [4, 4, 0],\n                   '2006': [6, 0, 2]})\n\ndef cumulative_average(df):\n    for col in df.columns[1:]:\n        df[col] = df[col].replace(0, pd.np.nan)\n        df[col] = df[col].iloc[::-1].expanding(1).mean().iloc[::-1]\n        df[col] = df[col].replace(pd.np.nan, 0)\n    return df\n\ndf = cumulative_average(df)\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 203, "library_problem_id": 203, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 202}}
{"id": 204, "code": ["<code>\ndef f(df=example_df):\n    # Calculate the cumulative average for each row of a Pandas DataFrame, ignoring zero values in the calculation.\n    # The cumulative average at each step should only consider non-zero values encountered so far.\n\n    # First, we need to convert the DataFrame to a more suitable format.\n    # We will transpose the DataFrame so that the years are the columns and the names are the rows.\n    df_transposed = df.set_index('Name').T\n\n    # Now, we can calculate the cumulative average.\n    # We will use the apply function to apply a lambda function to each row.\n    # The lambda function will calculate the cumulative average by summing the non-zero values encountered so far.\n    # We will divide by the number of non-zero values encountered so far.\n    # We will ignore the zero values.\n    df_cumulative_average = df_transposed.apply(lambda row: row[row != 0].sum() / row[row != 0].count(), axis=1)\n\n    # Finally, we can return the DataFrame with the cumulative averages.\n    return df_cumulative_average\n\n# Test the function\nprint(f(example_df))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 204, "library_problem_id": 204, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 202}}
{"id": 205, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'Name': ['Name1', 'Name2', 'Name3'],\n                   '2001': [2, 1, 0],\n                   '2002': [5, 4, 5],\n                   '2003': [0, 2, 0],\n                   '2004': [0, 0, 0],\n                   '2005': [4, 4, 0],\n                   '2006': [6, 0, 2]})\n\ndef cumulative_average(df):\n    for col in df.columns[1:]:\n        df[col] = df[col].replace(0, pd.np.nan)\n        df[col] = df[col].iloc[::-1].expanding(1).mean().iloc[::-1]\n        df[col] = df[col].replace(pd.np.nan, 0)\n    return df\n\ndf = cumulative_average(df)\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 205, "library_problem_id": 205, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 206, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'DateTime': ['2000-01-04', '2000-01-05', '2000-01-06', '2000-01-07'],\n                   'Close': [1460, 1470, 1480, 1450]})\n\ndf['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else 0)\ndf.loc[0, 'label'] = 1  # set the label of the first row to 1\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 206, "library_problem_id": 206, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 206}}
{"id": 207, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'DateTime': ['2000-01-04', '2000-01-05', '2000-01-06', '2000-01-07', '2000-01-08'],\n                   'Close': [1460, 1470, 1480, 1480, 1450]})\n\ndf['Close'] = df['Close'].diff()\ndf.loc[0, 'Close'] = 1\ndf['Close'] = df['Close'].apply(lambda x: 1 if x > 0 else -1 if x < 0 else 0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 207, "library_problem_id": 207, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 206}}
{"id": 208, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'DateTime': ['2000-01-04', '2000-01-05', '2000-01-06', '2000-01-07', '2000-01-08'],\n                   'Close': [1460, 1470, 1480, 1480, 1450]})\ndf['DateTime'] = pd.to_datetime(df['DateTime'])\n\n# Calculate the difference between consecutive 'Close' values\ndf['Diff'] = df['Close'].diff()\n\n# Create a new 'label' column\ndf['label'] = df['Diff'].apply(lambda x: 1 if x > 0 else -1 if x < 0 else 0)\n\n# Set the first row's label to 1\ndf.loc[0, 'label'] = 1\n\n# Reformat the 'DateTime' column to 'DD-Mon-YYYY' format\ndf['DateTime'] = df['DateTime'].dt.strftime('%d-%b-%Y')\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 208, "library_problem_id": 208, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 206}}
{"id": 209, "code": ["<code>\nHere is the completed function:\n\n```python\nimport pandas as pd\n\nid=[\"Train A\",\"Train A\",\"Train A\",\"Train B\",\"Train B\",\"Train B\"]\narrival_time = [\"0\",\" 2016-05-19 13:50:00\",\"2016-05-19 21:25:00\",\"0\",\"2016-05-24 18:30:00\",\"2016-05-26 12:15:00\"]\ndeparture_time = [\"2016-05-19 08:25:00\",\"2016-05-19 16:00:00\",\"2016-05-20 07:45:00\",\"2016-05-24 12:50:00\",\"2016-05-25 23:00:00\",\"2016-05-26 19:45:00\"]\ndf = pd.DataFrame({'id': id, 'arrival_time':arrival_time, 'departure_time':departure_time})\n\n# Convert the arrival_time and departure_time to datetime\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\n\n# Sort the dataframe by id and arrival_time\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 209, "library_problem_id": 209, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 210, "code": ["<code>\nHere is the completed function:\n\n```python\nimport pandas as pd\n\nid=[\"Train A\",\"Train A\",\"Train A\",\"Train B\",\"Train B\",\"Train B\"]\narrival_time = [\"0\",\" 2016-05-19 13:50:00\",\"2016-05-19 21:25:00\",\"0\",\"2016-05-24 18:30:00\",\"2016-05-26 12:15:00\"]\ndeparture_time = [\"2016-05-19 08:25:00\",\"2016-05-19 16:00:00\",\"2016-05-20 07:45:00\",\"2016-05-24 12:50:00\",\"2016-05-25 23:00:00\",\"2016-05-26 19:45:00\"]\ndf = pd.DataFrame({'id': id, 'arrival_time':arrival_time, 'departure_time':departure_time})\n\n# Convert the arrival_time and departure_time to datetime64[ns] format\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'], errors='coerce')\ndf['departure_time'] = pd.to_datetime(df['departure_time'], errors\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 210, "library_problem_id": 210, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 209}}
{"id": 211, "code": ["<code>\nHere is the completed function:\n\n```python\nimport pandas as pd\n\nid=[\"Train A\",\"Train A\",\"Train A\",\"Train B\",\"Train B\",\"Train B\"]\narrival_time = [\"0\",\" 2016-05-19 13:50:00\",\"2016-05-19 21:25:00\",\"0\",\"2016-05-24 18:30:00\",\"2016-05-26 12:15:00\"]\ndeparture_time = [\"2016-05-19 08:25:00\",\"2016-05-19 16:00:00\",\"2016-05-20 07:45:00\",\"2016-05-24 12:50:00\",\"2016-05-25 23:00:00\",\"2016-05-26 19:45:00\"]\ndf = pd.DataFrame({'id': id, 'arrival_time':arrival_time, 'departure_time':departure_time})\n\n# Convert the arrival and departure times to datetime format\ndf['arrival_time'] = pd.to_datetime(df['arrival_time'])\ndf['departure_time'] = pd.to_datetime(df['departure_time'])\n\n# Sort the dataframe by train id and arrival time\ndf\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 211, "library_problem_id": 211, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 209}}
{"id": 212, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'key1': ['a', 'a', 'b', 'b', 'a', 'c'],\n                   'key2': ['one', 'two', 'one', 'two', 'one', 'two']})\n\nresult = df[df['key2'] == 'one'].groupby('key1').size().reset_index(name='count')\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 212, "library_problem_id": 212, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 212}}
{"id": 213, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'key1': ['a', 'a', 'b', 'b', 'a', 'c'],\n                   'key2': ['one', 'two', 'one', 'two', 'one', 'two']})\n\nresult = df[df['key2'] == 'two'].groupby('key1').size().reset_index(name='count')\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 213, "library_problem_id": 213, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 212}}
{"id": 214, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'key1': ['a', 'a', 'b', 'b', 'a', 'c'],\n                   'key2': ['one', 'two', 'gee', 'two', 'three', 'two']})\n\nresult = df[df['key2'].str.endswith('e')].groupby('key1').size().reset_index(name='counts')\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 214, "library_problem_id": 214, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 212}}
{"id": 215, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'value':[10000,2000,2000,200,5,70,200,5,25,0.02,12,0.022]},\n                  index=['2014-03-13','2014-03-21','2014-03-27','2014-03-17','2014-03-17','2014-03-17','2014-03-21','2014-03-27','2014-03-27','2014-03-31','2014-03-31','2014-03-31'])\n\nmax_result = df.index.max()\nmin_result = df.index.min()\n\nprint(f\"Max date: {max_result}\")\nprint(f\"Min date: {min_result}\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 215, "library_problem_id": 215, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 216, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'value':[10000,2000,2000,200,5,70,200,5,25,0.02,12,11,0.022]},\n                  index=['2014-03-13','2014-03-21','2014-03-27','2014-03-17','2014-03-17','2014-03-17','2014-03-21','2014-03-27','2014-03-27','2014-03-27','2014-03-31','2014-03-31','2014-03-31'])\n\ndf.index = pd.to_datetime(df.index)\n\nmode_result = df.index.mode()[0]\nmedian_result = df.index.median()\n\nprint(\"Mode: \", mode_result)\nprint(\"Median: \", median_result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 216, "library_problem_id": 216, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 215}}
{"id": 217, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(2)\ndf = pd.DataFrame({'closing_price': np.random.randint(95, 105, 10)})\n\nresult = df[(df['closing_price'] >= 99) & (df['closing_price'] <= 101)]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 217, "library_problem_id": 217, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 217}}
{"id": 218, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(2)\ndf = pd.DataFrame({'closing_price': np.random.randint(95, 105, 10)})\n\nresult = df[~df['closing_price'].between(99, 101)]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 218, "library_problem_id": 218, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 217}}
{"id": 219, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({\"item\": [1, 1, 1, 2, 2, 2, 2, 3, 3],\n                   \"diff\": [2, 1, 3, -1, 1, 4, -6, 0, 2],\n                   \"otherstuff\": [1, 2, 7, 0, 3, 9, 2, 0, 9]})\n\nresult = df.groupby('item', as_index=False).agg({'diff': 'min', 'otherstuff': 'first'})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 219, "library_problem_id": 219, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 219}}
{"id": 220, "code": ["<code>\nimport pandas as pd\n\nstrs = ['Stackoverflow_1234',\n        'Stack_Over_Flow_1234',\n        'Stackoverflow',\n        'Stack_Overflow_1234']\ndf = pd.DataFrame(data={'SOURCE_NAME': strs})\n\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].apply(lambda x: x.split('_')[-1] if '_' in x else x)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 220, "library_problem_id": 220, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 220}}
{"id": 221, "code": ["<code>\nimport pandas as pd\n\nstrs = ['Stackoverflow_1234',\n        'Stack_Over_Flow_1234',\n        'Stackoverflow',\n        'Stack_Overflow_1234']\ndf = pd.DataFrame(data={'SOURCE_NAME': strs})\n\ndf['SOURCE_NAME'] = df['SOURCE_NAME'].apply(lambda x: x.rsplit('_', 1)[0] if '_' in x else x)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 221, "library_problem_id": 221, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 220}}
{"id": 222, "code": ["<code>\ndef f(df=example_df):\n        df['SOURCE_NAME'] = df['SOURCE_NAME'].apply(lambda x: x.split('_')[-1] if '_' in x else x)\n        return df\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 222, "library_problem_id": 222, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 220}}
{"id": 223, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Column_x': [0,0,0,0,0,0,1,1,1,1,1,1,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]})\n\n# Count the number of NaN values\nnan_count = df['Column_x'].isnull().sum()\n\n# Split the count in half\nhalf_count = nan_count // 2\n\n# Fill the first half with 0 and the second half with 1\ndf['Column_x'][:half_count] = df['Column_x'][:half_count].fillna(0)\ndf['Column_x'][half_count:] = df['Column_x'][half_count:].fillna(1)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 223, "library_problem_id": 223, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 223}}
{"id": 224, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Column_x': [0,0,0,0,0,0,1,1,1,1,1,1,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]})\n\n# Calculate the number of NaNs\nnum_nans = df['Column_x'].isnull().sum()\n\n# Calculate the number of NaNs to be replaced with 0, 0.5 and 1\nnum_zeros = int(num_nans * 0.3)\nnum_halfs = int(num_nans * 0.3)\nnum_ones = num_nans - num_zeros - num_halfs\n\n# Generate the replacement values\nreplacements = [0]*num_zeros + [0.5]*num_halfs + [1]*num_ones\n\n# Replace the NaNs with the generated values\ndf['Column_x'] = df['Column_x'].apply(lambda x: np.random.choice(replacements) if pd.isnull(x) else x)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 224, "library_problem_id": 224, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 223}}
{"id": 225, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Column_x': [0,0,0,0,1,1,1,1,1,1,1,1,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]})\n\n# Count the number of NaN values\nnan_count = df['Column_x'].isna().sum()\n\n# If the number of NaN values is odd, add 1 to it to make it even\nif nan_count % 2 != 0:\n    nan_count += 1\n\n# Split the count into two halves\nhalf_count = nan_count // 2\n\n# Fill NaN values with 0s\ndf['Column_x'].loc[df['Column_x'].isna()] = df['Column_x'].loc[df['Column_x'].isna()].apply(lambda x: 0 if x is np.nan else x)\n\n# Fill the remaining NaN values with 1s\ndf['Column_x'].loc[df['Column_x'].isna()] = df['Column_x'].loc[df['Column_x'].isna()].apply(lambda x: 1 if x is np.nan else x)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 225, "library_problem_id": 225, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 223}}
{"id": 226, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\na = pd.DataFrame(np.array([[1, 2],[3, 4]]), columns=['one', 'two'])\nb = pd.DataFrame(np.array([[5, 6],[7, 8]]), columns=['one', 'two'])\n\ndef combine_dfs(*dfs):\n    return pd.concat(dfs).groupby(level=0).apply(tuple).reset_index(drop=True)\n\nresult = combine_dfs(a, b)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 226, "library_problem_id": 226, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 226}}
{"id": 227, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\na = pd.DataFrame(np.array([[1, 2],[3, 4]]), columns=['one', 'two'])\nb = pd.DataFrame(np.array([[5, 6],[7, 8]]), columns=['one', 'two'])\nc = pd.DataFrame(np.array([[9, 10],[11, 12]]), columns=['one', 'two'])\n\n# Use the zip function to create tuples from corresponding elements in the DataFrames\nresult = pd.DataFrame(list(zip(a['one'], b['one'], c['one'], a['two'], b['two'], c['two'])), columns=['a_one', 'b_one', 'c_one', 'a_two', 'b_two', 'c_two'])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 227, "library_problem_id": 227, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 226}}
{"id": 228, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\na = pd.DataFrame(np.array([[1, 2],[3, 4]]), columns=['one', 'two'])\nb = pd.DataFrame(np.array([[5, 6],[7, 8],[9, 10]]), columns=['one', 'two'])\n\n# Concatenate the dataframes along the row axis\ndfs = [a, b]\nresult = pd.concat(dfs, ignore_index=True)\n\n# Fill the missing values with np.nan\nresult = result.reindex(range(max(len(a), len(b))), fill_value=np.nan)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 228, "library_problem_id": 228, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 226}}
{"id": 229, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'username': ['john', 'john', 'john', 'john', 'jane', 'jane', 'jane', 'jane'],\n                   'post_id': [1, 2, 3, 4, 7, 8, 9, 10],\n                   'views': [3, 23, 44, 82, 5, 25,46, 56]})\nbins = [1, 10, 25, 50, 100]\n\n# Create a new column 'bin' in the DataFrame df\ndf['bin'] = pd.cut(df['views'], bins)\n\n# Use pivot_table to create the desired DataFrame\nresult = df.pivot_table(index='username', columns='bin', aggfunc='count')['post_id']\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 229, "library_problem_id": 229, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 229}}
{"id": 230, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'username': ['john', 'john', 'john', 'john', 'jane', 'jane', 'jane', 'jane'],\n                   'post_id': [1, 2, 3, 4, 7, 8, 9, 10],\n                   'views': [3, 23, 44, 82, 5, 25,46, 56]})\nbins = [1, 10, 25, 50, 100]\n\n# Create a new column 'bin' in the dataframe df\ndf['bin'] = pd.cut(df['views'], bins)\n\n# Create a cross-tabulation of users and view count bins\nresult = pd.crosstab(df['username'], df['bin'])\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 230, "library_problem_id": 230, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 231, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'username': ['tom', 'tom', 'tom', 'tom', 'jack', 'jack', 'jack', 'jack'],\n                   'post_id': [10, 8, 7, 6, 5, 4, 3, 2],\n                   'views': [3, 23, 44, 82, 5, 25,46, 56]})\nbins = [1, 10, 25, 50, 100]\n\n# Create a new column 'bin' in the dataframe df\ndf['bin'] = pd.cut(df['views'], bins)\n\n# Group by 'username' and 'bin' and count the number of views in each bin for each user\nresult = df.groupby(['username', 'bin']).size().unstack().fillna(0)\n\n# Rename the bins in the columns\nresult.columns = ['1-10', '11-25', '26-50', '51-100']\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 231, "library_problem_id": 231, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 232, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\n\n# Concatenate the text values from all rows into a single comma-separated string\nresult = pd.DataFrame({'text': [', '.join(df['text'])]})\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 232, "library_problem_id": 232, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 232}}
{"id": 233, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\n\n# Concatenate the text strings from all rows into a single string separated by hyphens\nconcatenated_string = '-'.join(df['text'])\n\n# Create a new DataFrame with the concatenated string\nresult = pd.DataFrame({'text': [concatenated_string]})\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 233, "library_problem_id": 233, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 234, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\n\n# Concatenate the text from all rows into a single comma-separated string\nresult = pd.DataFrame({'text': [', '.join(df['text'])]})\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 234, "library_problem_id": 234, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 235, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\n\n# Concatenate the text from all rows into a single comma-separated string\nresult = pd.DataFrame({'text': [', '.join(df['text'])]})\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 235, "library_problem_id": 235, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 236, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\n\n# Concatenate the text strings from all rows into a single string with a hyphen separator\nconcatenated_string = '-'.join(df['text'][::-1])\n\n# Create a new DataFrame with this single concatenated string\nresult = pd.DataFrame({'text': [concatenated_string]})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 236, "library_problem_id": 236, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 232}}
{"id": 237, "code": ["<code>\nimport pandas as pd\n\ndf1 = pd.DataFrame({'id': [1, 2, 3, 4, 5],\n                   'city': ['bj', 'bj', 'sh', 'sh', 'sh'],\n                   'district': ['ft', 'ft', 'hp', 'hp', 'hp'],\n                   'date': ['2019/1/1', '2019/1/1', '2019/1/1', '2019/1/1', '2019/1/1'],\n                   'value': [1, 5, 9, 13, 17]})\ndf2 = pd.DataFrame({'id': [3, 4, 5, 6, 7],\n                   'date': ['2019/2/1', '2019/2/1', '2019/2/1', '2019/2/1', '2019/2/1'],\n                   'value': [1, 5, 9, 13, 17]})\n\n# Merge df1 and df2 on 'id' column\nmerged_df = pd.merge(df1, df2, on='id', how='outer')\n\n# Fill missing 'city' and 'district' values with corresponding values from df1\nresult = merged_df.fillna(df1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 237, "library_problem_id": 237, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 237}}
{"id": 238, "code": ["<code>\nHere is the completed function:\n\n```python\nimport pandas as pd\n\ndf1 = pd.DataFrame({'id': [1, 2, 3, 4, 5],\n                   'city': ['bj', 'bj', 'sh', 'sh', 'sh'],\n                   'district': ['ft', 'ft', 'hp', 'hp', 'hp'],\n                   'date': ['2019/1/1', '2019/1/1', '2019/1/1', '2019/1/1', '2019/1/1'],\n                   'value': [1, 5, 9, 13, 17]})\n\n\ndf2 = pd.DataFrame({'id': [3, 4, 5, 6, 7],\n                   'date': ['2019/2/1', '2019/2/1', '2019/2/1', '2019/2/1', '2019/2/1'],\n                   'value': [1, 5, 9, 13, 17]})\n\n# Concatenate df1 and df2 based on 'id' column\ndf = pd.concat([df1, df2], ignore_index=True)\n\n# Fill missing 'city' and 'district' values from df1 into df2\ndf = df.fillna({'city': df1['city'], 'district': df1['district']})\n\n# Sort the dataframe by 'id' and 'date'\ndf = df.sort_values\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 238, "library_problem_id": 238, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 239, "code": ["<code>\nHere is the completed function:\n\n```python\nimport pandas as pd\n\ndf1 = pd.DataFrame({'id': [1, 2, 3, 4, 5],\n                   'city': ['bj', 'bj', 'sh', 'sh', 'sh'],\n                   'district': ['ft', 'ft', 'hp', 'hp', 'hp'],\n                   'date': ['2019/1/1', '2019/1/1', '2019/1/1', '2019/1/1', '2019/1/1'],\n                   'value': [1, 5, 9, 13, 17]})\n\n\ndf2 = pd.DataFrame({'id': [3, 4, 5, 6, 7],\n                   'date': ['2019/2/1', '2019/2/1', '2019/2/1', '2019/2/1', '2019/2/1'],\n                   'value': [1, 5, 9, 13, 17]})\n\n# Concatenate df1 and df2 based on 'id' column\ndf = pd.concat([df1, df2]).reset_index(drop=True)\n\n# Fill missing 'city' and 'district' values from df1\ndf.fillna({'city': df1['city'], 'district': df1['district']}, inplace=True)\n\n# Sort the dataframe to group rows with the same 'id' together, ordering by\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 239, "library_problem_id": 239, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 240, "code": ["<code>\nresult = pd.merge(C, D, on='A', how='left').fillna(C)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 240, "library_problem_id": 240, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 240}}
{"id": 241, "code": ["<code>\nresult = pd.merge(C, D, on='A', how='left')\nresult['B_x'] = result['B_x'].fillna(result['B_y'])\nresult = result.drop(['B_y'], axis=1)\nresult = result.rename(columns={'B_x': 'B'})\nresult\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 241, "library_problem_id": 241, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 240}}
{"id": 242, "code": ["<code>\n# Merge C and D on column 'A' with indicator=True to create a new column '_merge'\nmerged = pd.merge(C, D, on='A', how='left', indicator=True)\n\n# Replace '_merge' column values with boolean values\nmerged['duplicated'] = merged['_merge'].apply(lambda x: True if x == 'both' else False)\n\n# Drop '_merge' column\nresult = merged.drop(columns=['_merge'])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 242, "library_problem_id": 242, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 240}}
{"id": 243, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'user':[1,1,2,2,3], 'time':[20,10,11,18, 15], 'amount':[10.99, 4.99, 2.99, 1.99, 10.99]})\n\n# Sort the dataframe by 'user' and 'time'\ndf = df.sort_values(by=['user', 'time'])\n\n# Group by 'user' and convert the 'time' and 'amount' into lists\nresult = df.groupby('user')[['time', 'amount']].apply(lambda x: x.values.tolist())\n\n# Convert the grouped data into a series\nresult = result.reset_index()\n\n# Rename the 'time' and 'amount' columns to 'transactions'\nresult = result.rename(columns={'time': 'transactions', 'amount': 'transactions'})\n\n# Set the index to 'user'\nresult = result.set_index('user')\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 243, "library_problem_id": 243, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 243}}
{"id": 244, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'user':[1,1,2,2,3], 'time':[20,10,11,18, 15], 'amount':[10.99, 4.99, 2.99, 1.99, 10.99]})\n\nresult = df.groupby('user').apply(lambda x: list(zip(x['time'], x['amount']))).tolist()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 244, "library_problem_id": 244, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 243}}
{"id": 245, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'user':[1,1,2,2,3], 'time':[20,10,11,18, 15], 'amount':[10.99, 4.99, 2.99, 1.99, 10.99]})\n\n# Sort the dataframe by 'user' and 'time' in descending order\ndf = df.sort_values(by=['user', 'time'], ascending=[True, False])\n\n# Group by 'user' and transform the 'time' and 'amount' into lists\nresult = df.groupby('user').apply(lambda x: list(zip(x['time'], x['amount'])))\n\n# Reset the index to make 'user' a column again\nresult = result.reset_index()\n\n# Rename the column 'user' to 'User' and '0' to 'Transactions'\nresult = result.rename(columns={'user': 'User', 0: 'Transactions'})\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 245, "library_problem_id": 245, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 243}}
{"id": 246, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\nseries = pd.Series([np.array([1,2,3,4]), np.array([5,6,7,8]), np.array([9,10,11,12])], index=['file1', 'file2', 'file3'])\n\ndf = pd.DataFrame(series.tolist(), index=series.index)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 246, "library_problem_id": 246, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 246}}
{"id": 247, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\nseries = pd.Series([np.array([1,2,3,4]), np.array([5,6,7,8]), np.array([9,10,11,12])], index=['file1', 'file2', 'file3'])\n\ndf = pd.DataFrame(series.tolist(), index=series.index).reset_index().rename(columns={'index':'name'})\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 247, "library_problem_id": 247, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 246}}
{"id": 248, "code": ["<code>\nimport pandas as pd\n\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n\nresult = [col for col in df.columns if s in col]\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 248, "library_problem_id": 248, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 248}}
{"id": 249, "code": ["<code>\ndef find_column_with_substring(df, substring):\n    for col in df.columns:\n        if substring in col:\n            return col\n    return None\n\nresult = find_column_with_substring(df, s)\nprint(result)  # Output: spike-2\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 249, "library_problem_id": 249, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 248}}
{"id": 250, "code": ["<code>\nimport pandas as pd\n\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\ns = 'spike'\n\n# Identify column names containing a specific substring but not exactly matching it\nmatching_columns = [col for col in df.columns if s in col and col != s]\n\n# Rename these columns to a sequential numbering system\nrenamed_columns = {col: f'{s}{i+1}' for i, col in enumerate(matching_columns)}\ndf = df.rename(columns=renamed_columns)\n\nresult = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 250, "library_problem_id": 250, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 248}}
{"id": 251, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'codes':[[71020], [77085], [36415], [99213, 99287], [99233, 99233, 99233]]})\n\n# First, find the maximum length of the lists in the 'codes' column\nmax_len = df['codes'].apply(len).max()\n\n# Then, use the 'apply' function to expand each list to the maximum length, filling in NaN values where necessary\ndf = df['codes'].apply(lambda x: x + [None]*(max_len-len(x))).to_frame()\n\n# Finally, use the 'set_index' function to set the index of the DataFrame to the 'codes' column, and use the 'transpose' function to swap the rows and columns\ndf = df.set_index('codes').transpose()\n\n# Rename the columns to 'code_0', 'code_1', etc.\ndf.columns = ['code_' + str(i) for i in range(max_len)]\n\nresult = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 251, "library_problem_id": 251, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 251}}
{"id": 252, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'codes':[[71020], [77085], [36415], [99213, 99287], [99233, 99233, 99233]]})\n\n# First, we need to find the maximum length of the lists in the 'codes' column\nmax_len = max([len(lst) for lst in df['codes']])\n\n# Then, we create new columns for each element in the lists\nfor i in range(max_len):\n    df[f'code_{i+1}'] = df['codes'].apply(lambda x: x[i] if len(x) > i else None)\n\n# Replace None values with NaN\nresult = df.replace({None: pd.np.nan})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 252, "library_problem_id": 252, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 251}}
{"id": 253, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'codes':[[71020], [77085], [36415], [99213, 99287], [99234, 99233, 99233]]})\n\n# First, find the maximum length of the lists\nmax_len = max([len(lst) for lst in df['codes']])\n\n# Then, for each position in the lists, create a new column\nfor i in range(max_len):\n    df[f'code_{i+1}'] = df['codes'].apply(lambda x: x[i] if len(x) > i else np.nan)\n\nresult = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 253, "library_problem_id": 253, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 251}}
{"id": 254, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame(dict(col1=[['[1, 2, 3]']] * 2))\n\ndef convert_string_lists_to_actual_lists(df, column):\n    df[column] = df[column].apply(lambda x: [int(i) for i in x.strip('[]').split(',')])\n    return df\n\ndf = convert_string_lists_to_actual_lists(df, 'col1')\n\nresult = df['col1'].sum()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 254, "library_problem_id": 254, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 254}}
{"id": 255, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame(dict(col1=[['1,2,3', '4,5,6'],['7,8', '9']]))\n\ndef reverse_and_concatenate(df, column):\n    df[column] = df[column].apply(lambda x: [','.join(i.split(',')[::-1]) for i in x])\n    df[column] = df[column].apply(lambda x: ','.join(x))\n    df[column] = df[column].apply(lambda x: x.split(','))\n    df[column] = df[column].apply(lambda x: [int(i) for i in x])\n    return df\n\nresult = reverse_and_concatenate(df, 'col1')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 255, "library_problem_id": 255, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 256, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame(dict(col1=[[1, 2, 3]] * 2))\n\nresult = df['col1'].apply(lambda x: ','.join(map(str, x)))\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 256, "library_problem_id": 256, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 257, "code": ["<code>\nimport pandas as pd\n\ndef bin_and_average(df, bin_size='2min', interpolate=True):\n    df['Time'] = pd.to_datetime(df['Time'])\n    df.set_index('Time', inplace=True)\n    df = df.resample(bin_size).mean()\n    if interpolate:\n        df = df.interpolate()\n    return df\n\ndf = bin_and_average(df)\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 257, "library_problem_id": 257, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 257}}
{"id": 258, "code": ["<code>\nSure, here is the completed function:\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({'Time': ['2015-04-24 06:38:49', '2015-04-24 06:39:19', '2015-04-24 06:43:49', '2015-04-24 06:44:18',\n                            '2015-04-24 06:44:48', '2015-04-24 06:45:18', '2015-04-24 06:47:48', '2015-04-24 06:48:18',\n                            '2015-04-24 06:50:48', '2015-04-24 06:51:18', '2015-04-24 06:51:48', '2015-04-24 06:52:18',\n                            '2015-04-24 06:52:48', '2015-04-24 06:53:48', '2015-04-24 06:55:18', '2015-04-24 07:00:47',\n                            '2015-04\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 258, "library_problem_id": 258, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 257}}
{"id": 259, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'ID': ['01', '01', '01', '02', '02'],\n                   'TIME': ['2018-07-11 11:12:20', '2018-07-12 12:00:23', '2018-07-13 12:00:00', '2019-09-11 11:00:00', '2019-09-12 12:00:00']})\n\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf = df.sort_values(by=['ID', 'TIME'])\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method='min')\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 259, "library_problem_id": 259, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 259}}
{"id": 260, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'ID': ['01', '01', '01', '02', '02'],\n                   'TIME': ['2018-07-11 11:12:20', '2018-07-12 12:00:23', '2018-07-13 12:00:00', '2019-09-11 11:00:00', '2019-09-12 12:00:00']})\n\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf.sort_values(['ID', 'TIME'], inplace=True)\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method='min', ascending=False)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 260, "library_problem_id": 260, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 259}}
{"id": 261, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'ID': ['01', '01', '01', '02', '02'],\n                   'TIME': ['2018-07-11 11:12:20', '2018-07-12 12:00:23', '2018-07-13 12:00:00', '2019-09-11 11:00:00', '2019-09-12 12:00:00']})\n\n# Convert TIME column to datetime\ndf['TIME'] = pd.to_datetime(df['TIME'])\n\n# Format TIME column\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %a %H:%M:%S')\n\n# Rank TIME column within ID groups\ndf['RANK'] = df.groupby('ID')['TIME'].rank(method='min', ascending=False)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 261, "library_problem_id": 261, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 259}}
{"id": 262, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'a': [1,1,1,2,2,2,3,3,3],\n                    'b': [1,2,3,1,2,3,1,2,3],\n                    'c': range(9)}).set_index(['a', 'b'])\nfilt = pd.Series({1:True, 2:False, 3:True})\n\nresult = df[df.index.get_level_values(0).isin(filt[filt==True].index)]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 262, "library_problem_id": 262, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 262}}
{"id": 263, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'a': [1,1,1,2,2,2,3,3,3],\n                    'b': [1,2,3,1,2,3,1,2,3],\n                    'c': range(9)}).set_index(['a', 'b'])\nfilt = pd.Series({1:True, 2:False, 3:True})\n\nresult = df[df.index.get_level_values(0).isin(filt[filt==True].index)]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 263, "library_problem_id": 263, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 262}}
{"id": 264, "code": ["<code>\ndef compare_rows(df, row1, row2):\n    diff_indices = []\n    for col in df.columns:\n        if pd.isna(df.at[row1, col]) and pd.isna(df.at[row2, col]):\n            continue\n        try:\n            if df.at[row1, col] != df.at[row2, col]:\n                diff_indices.append(col)\n        except TypeError:\n            continue\n    return diff_indices\n\n# Test the function\nresult = compare_rows(df, 0, 1)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 264, "library_problem_id": 264, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 264}}
{"id": 265, "code": ["<code>\ndef compare_rows(df, row1, row2):\n    return df.columns[(df.iloc[row1] == df.iloc[row2]) | (df.iloc[row1].isna() & df.iloc[row2].isna())].tolist()\n\n# Test the function\nresult = compare_rows(df, 0, 1)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 265, "library_problem_id": 265, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 266, "code": ["<code>\ndef compare_rows(df, row1, row2):\n    diff_columns = []\n    for col in df.columns:\n        if pd.isna(df.at[row1, col]) and pd.isna(df.at[row2, col]):\n            continue\n        if df.at[row1, col] != df.at[row2, col]:\n            diff_columns.append(col)\n    return diff_columns\n\n# Test the function\nresult = compare_rows(df, 0, 1)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 266, "library_problem_id": 266, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 267, "code": ["<code>\ndef compare_rows(row1, row2):\n    diffs = []\n    for i in range(len(row1)):\n        if not pd.isna(row1[i]) and not pd.isna(row2[i]):\n            if row1[i] != row2[i]:\n                diffs.append((row1[i], row2[i]))\n        elif pd.isna(row1[i]) and pd.isna(row2[i]):\n            continue\n        else:\n            diffs.append((row1[i], row2[i]))\n    return diffs\n\n# Test the function\nresult = compare_rows(df.iloc[0], df.iloc[1])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 267, "library_problem_id": 267, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 264}}
{"id": 268, "code": ["<code>\nts = pd.Series(df['Value'], index=df['Date'])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 268, "library_problem_id": 268, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 268}}
{"id": 269, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15]],columns=['A','B','C','D','E'])\n\ndf = df.T\ndf.columns = [f'Row_{i+1}' for i in range(df.shape[0])]\ndf = df.T\ndf.columns = [f'Col_{i+1}' for i in range(df.shape[1])]\ndf = df.reset_index().T\ndf.columns = ['Value']\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 269, "library_problem_id": 269, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 269}}
{"id": 270, "code": ["<code>\nHere is the completed function:\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15]],columns=['A','B','C','D','E'])\n\ndf = df.T.reset_index().T\ndf.columns = [f'Row_{i}' for i in range(1, len(df.columns)+1)]\ndf = df.T\ndf.columns = [f'Col_{i}' for i in range(1, len(df.columns)+1)]\ndf = df.T\ndf = df.reset_index().T\ndf.columns = [f'{df.loc[\"A\", col]}_{col}' for col in df.columns]\ndf = df.T\ndf.columns = [f'{df.loc[\"A\", col]}_{col}' for col in df.columns]\ndf = df.T\ndf = df.reset_index().T\ndf = df.rename(columns={0: 'Value'})\ndf = df.set_index('Value')\ndf = df.T\ndf = df.reset_index().T\ndf.columns = [f'{df.loc[\"A\", col]}_{col}' for col in df.columns]\ndf = df.T\ndf.columns = [f'{df.loc[\"A\", col]}_{col}' for col in df.columns]\ndf = df.T\ndf = df.reset_index().T\ndf = df.rename(columns={\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 270, "library_problem_id": 270, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 269}}
{"id": 271, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame([(.21, .3212), (.01, .61237), (.66123, .03), (.21, .18),(np.nan, .18)],\n                  columns=['dogs', 'cats'])\n\ndef round_float64(df, decimal_places):\n    return df.apply(lambda x: x.round(decimal_places) if pd.api.types.is_float_dtype(x) else x)\n\ndf = round_float64(df, 2)\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 271, "library_problem_id": 271, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 271}}
{"id": 272, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame([(.21, .3212), (.01, .61237), (.66123, pd.NA), (.21, .18),(pd.NA, .188)],\n                  columns=['dogs', 'cats'])\n\ndf = df.applymap(lambda x: round(x, 2) if pd.notna(x) else np.nan)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 272, "library_problem_id": 272, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 271}}
{"id": 273, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(10)\ndata = {}\nfor i in [chr(x) for x in range(65,91)]:\n    data['Col '+i] = np.random.randint(1,100,10)\ndf = pd.DataFrame(data)\nlist_of_my_columns = ['Col A', 'Col E', 'Col Z']\n\ndf = df[list_of_my_columns].sum()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 273, "library_problem_id": 273, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 273}}
{"id": 274, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(10)\ndata = {}\nfor i in [chr(x) for x in range(65,91)]:\n    data['Col '+i] = np.random.randint(1,100,10)\ndf = pd.DataFrame(data)\nlist_of_my_columns = ['Col A', 'Col E', 'Col Z']\n\ndf['Row Average'] = df[list_of_my_columns].mean(axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 274, "library_problem_id": 274, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 273}}
{"id": 275, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(10)\ndata = {}\nfor i in [chr(x) for x in range(65,91)]:\n    data['Col '+i] = np.random.randint(1,100,10)\ndf = pd.DataFrame(data)\nlist_of_my_columns = ['Col A', 'Col E', 'Col Z']\n\ndef calculate_stats(df, list_of_my_columns):\n    stats_df = df[list_of_my_columns].describe().transpose()\n    stats_df['average'] = stats_df['mean']\n    stats_df['minimum'] = stats_df['min']\n    stats_df['maximum'] = stats_df['max']\n    stats_df['median'] = stats_df['50%']\n    stats_df = stats_df[['average', 'minimum', 'maximum', 'median']]\n    return stats_df\n\nstats_df = calculate_stats(df, list_of_my_columns)\nprint(stats_df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 275, "library_problem_id": 275, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 273}}
{"id": 276, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'VIM':[-0.158406,0.039158,-0.052608,0.157153,0.206030,0.132580,-0.144209,-0.093910,-0.166819,0.097548,0.026664,-0.008032]},\n                  index=pd.MultiIndex.from_tuples([('TGFb',0.1,2),('TGFb',1,2),('TGFb',10,2),('TGFb',0.1,24),('TGFb',1,24),('TGFb',10,24),('TGFb',0.1,48),('TGFb',1,48),('TGFb',10,48),('TGFb',0.1,6),('TGFb',1,6),('TGFb',10,6)],\n                                                 names=['treatment','dose','time']))\n\nresult = df.sort_index(level='time')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 276, "library_problem_id": 276, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 276}}
{"id": 277, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'VIM':[-0.158406,0.039158,-0.052608,0.157153,0.206030,0.132580,-0.144209,-0.093910,-0.166819,0.097548,0.026664,-0.008032]},\n                  index=pd.MultiIndex.from_tuples([('TGFb',0.1,2),('TGFb',1,2),('TGFb',10,2),('TGFb',0.1,24),('TGFb',1,24),('TGFb',10,24),('TGFb',0.1,48),('TGFb',1,48),('TGFb',10,48),('TGFb',0.1,6),('TGFb',1,6),('TGFb',10,6)],\n                                                 names=['treatment','dose','time']))\n\nresult = df.sort_values(by='VIM')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 277, "library_problem_id": 277, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 276}}
{"id": 278, "code": ["<code>\ndef remove_dates(df, dates):\n    \"\"\"\n    Remove rows from a DataFrame corresponding to specific dates.\n\n    Parameters:\n    df (DataFrame): The DataFrame to remove rows from.\n    dates (list): A list of dates to remove.\n\n    Returns:\n    DataFrame: The DataFrame with the specified dates removed.\n    \"\"\"\n    df = df[~df.index.isin(dates)]\n    return df\n\ndates_to_remove = ['2020-02-16 15:31:00', '2020-02-18 15:33:00']\nresult = remove_dates(df, dates_to_remove)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 278, "library_problem_id": 278, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 278}}
{"id": 279, "code": ["<code>\nSure, here is the completed function:\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({'Date': ['2020-02-15 15:30:00', '2020-02-16 15:31:00', '2020-02-17 15:32:00', '2020-02-18 15:33:00', '2020-02-19 15:34:00'],\n                   'Open': [2898.75, 2899.25, 2898.5, 2898.25, 2898.5],\n                   'High': [2899.25, 2899.75, 2899, 2899.25, 2899.5],\n                   'Low': [2896.5, 2897.75, 2896.5, 2897.75, 2898.25],\n                   'Last': [2899.25, 2898.5, 2898, 2898, 2898.75],\n                   'Volume': [1636, 630, 1806, 818, 818],\n                   '# of Trades': [862, 328, 562, 273, 27\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 279, "library_problem_id": 279, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 278}}
{"id": 280, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.rand(10,5))\ncorr = df.corr()\n\n# Extract all values greater than 0.3\nresult = corr[corr > 0.3]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 280, "library_problem_id": 280, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 280}}
{"id": 281, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.rand(10,5))\ncorr = df.corr()\n\n# Use the 'stack' function to convert the correlation matrix into a series\n# Then use boolean indexing to select values greater than 0.3\nresult = corr.stack()[corr.stack() > 0.3]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 281, "library_problem_id": 281, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 280}}
{"id": 282, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=list('ABA'))\n\ndef rename_last_column(df):\n    last_column_name = df.columns[-1]\n    new_column_name = 'New_' + last_column_name\n    df.rename(columns={last_column_name: new_column_name}, inplace=True)\n    return df\n\nresult = rename_last_column(df)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 282, "library_problem_id": 282, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 282}}
{"id": 283, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=list('ABA'))\n\ndef rename_first_column(df):\n    for col in df.columns:\n        if col == df.columns[0]:\n            df.rename(columns={col: 'NewName'}, inplace=True)\n            break\n    return df\n\nresult = rename_first_column(df)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 283, "library_problem_id": 283, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 282}}
{"id": 284, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'bit1': [0, 1, 1],\n                   'bit2': [0, 1, 0],\n                   'bit3': [1, 0, 1],\n                   'bit4': [1, 0, 1],\n                   'bit5': [0, 1, 1]})\n\n# Create a new column 'most_frequent' and 'frequency'\ndf['most_frequent'] = df.mode(axis=1)[0]\ndf['frequency'] = df.apply(lambda row: list(row).count(row['most_frequent']), axis=1)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 284, "library_problem_id": 284, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 284}}
{"id": 285, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'bit1': [0, 2, 4],\n                   'bit2': [0, 2, 0],\n                   'bit3': [3, 0, 4],\n                   'bit4': [3, 0, 4],\n                   'bit5': [0, 2, 4]})\n\n# Use apply function along axis=1 (for each row) to find the most frequent value and its count\ndf['most_frequent'], df['count'] = df.apply(lambda row: pd.Series(row.value_counts().index[0]), axis=1), df.apply(lambda row: pd.Series(row.value_counts().values[0]), axis=1)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 285, "library_problem_id": 285, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 284}}
{"id": 286, "code": ["<code>\nimport pandas as pd\n\n\ndf = pd.DataFrame({'bit1': [0, 2, 4],\n                   'bit2': [0, 2, 0],\n                   'bit3': [3, 0, 4],\n                   'bit4': [3, 0, 4],\n                   'bit5': [0, 2, 4],\n                   'bit6': [3, 0, 5]})\n\ndef most_frequent(df):\n    mode_df = df.mode(axis=1)\n    count_df = df.apply(pd.Series.value_counts).fillna(0)\n    return mode_df, count_df\n\nmode_df, count_df = most_frequent(df)\n\nprint(\"Most Frequent Values:\")\nprint(mode_df)\nprint(\"\\nFrequency Counts:\")\nprint(count_df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 286, "library_problem_id": 286, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 284}}
{"id": 287, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({\"foo\":[8,5,3,4,7,9,5,7], \n                   \"id1\":[1,1,1,1,1,1,1,1], \n                   \"bar\":['NULL','NULL','NULL',1,3,4,2,3], \n                   \"id2\":[1,1,1,2,2,3,3,1]})\n\n# Convert 'bar' column to numeric, replace 'NULL' with NaN\ndf['bar'] = pd.to_numeric(df['bar'].replace('NULL', pd.np.nan))\n\n# Calculate mean of 'foo' and 'bar' grouped by 'id1' and 'id2'\nresult = df.groupby(['id1', 'id2']).mean()\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 287, "library_problem_id": 287, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 287}}
{"id": 288, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\"foo\":[8,5,3,4,7,9,5,7], \n                   \"id1\":[1,1,1,1,1,1,1,1], \n                   \"bar\":['NULL','NULL','NULL',1,3,4,2,3], \n                   \"id2\":[1,1,1,2,2,3,3,1]})\n\n# Convert 'bar' column to numeric, treating 'NULL' as NaN\ndf['bar'] = pd.to_numeric(df['bar'], errors='coerce')\n\n# Fill NaN values in 'bar' with 0\ndf['bar'].fillna(0, inplace=True)\n\n# Calculate mean of 'foo' and 'bar' grouped by 'id1' and 'id2'\nresult = df.groupby(['id1', 'id2']).mean().reset_index()\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 288, "library_problem_id": 288, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 287}}
{"id": 289, "code": ["<code>\nimport pandas as pd\n\ndf_a = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'foo':[100,50,200]})\ndf_b = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'a_col':['alice','bob','777'],'b_col':[7,8,9]})\n\nresult = pd.merge(df_a, df_b[['EntityNum', 'a_col']], on='EntityNum', how='left')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 289, "library_problem_id": 289, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 289}}
{"id": 290, "code": ["<code>\nimport pandas as pd\n\ndf_a = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'foo':[100,50,200]})\ndf_b = pd.DataFrame({'EntityNum':[1001.01,1002.02,1003.03],'a_col':['alice','bob','777'],'b_col':[7,8,9]})\n\nresult = pd.merge(df_a, df_b, on='EntityNum', how='left')[['EntityNum', 'foo']]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 290, "library_problem_id": 290, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 289}}
{"id": 291, "code": ["<code>\nimport numpy as np\na = np.array([[1,2],[3,4]])\nresult = a.shape\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 291, "library_problem_id": 0, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 292, "code": ["<code>\nimport numpy as np\n\nx = np.array([1400, 1500, 1600, np.nan, np.nan, np.nan ,1700])\n\nx = x[~np.isnan(x)]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 292, "library_problem_id": 1, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 293, "code": ["<code>\nimport numpy as np\n\nx = np.array([1400, 1500, 1600, np.nan, np.nan, np.nan ,1700])\nx[np.isnan(x)] = np.inf\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 293, "library_problem_id": 2, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 294, "code": ["<code>\nimport numpy as np\n\nx = np.array([[1400, 1500, 1600, np.nan], [1800, np.nan, np.nan ,1700]])\n\nresult = x[~np.isnan(x).any(axis=(1,2))].tolist()\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 294, "library_problem_id": 3, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 1}}
{"id": 295, "code": ["<code>\nimport numpy as np\n\na = np.array([1, 0, 3])\n\n# Find the maximum value in the array\nmax_val = np.max(a)\n\n# Create a 2D array of zeros with shape (len(a), max_val + 1)\nb = np.zeros((len(a), max_val + 1))\n\n# For each row, set the corresponding index to 1\nfor i in range(len(a)):\n    b[i, a[i]] = 1\n\nprint(b)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 295, "library_problem_id": 4, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 296, "code": ["<code>\nimport numpy as np\n\na = np.array([1, 0, 3])\n\nb = np.eye(a.max()+1)[a]\n\nprint(b)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 296, "library_problem_id": 5, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 297, "code": ["<code>\nimport numpy as np\n\na = np.array([-1, 0, 3])\n\ndef one_hot_encode(a):\n    # Get the min and max values in the array\n    min_val = np.min(a)\n    max_val = np.max(a)\n    \n    # Create a range of numbers from min to max\n    range_val = np.arange(min_val, max_val + 1)\n    \n    # Use numpy's broadcasting to create the one-hot encoding\n    b = (a[:, None] == range_val).astype(int)\n    \n    return b\n\nb = one_hot_encode(a)\nprint(b)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 297, "library_problem_id": 6, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 298, "code": ["<code>\nimport numpy as np\n\na = np.array([1.5, -0.4, 1.3])\n\n# Get unique elements and sort them\nunique_elements = np.unique(a)\nsorted_elements = np.sort(unique_elements)\n\n# Create a one-hot encoded array\nb = np.zeros((len(a), len(sorted_elements)))\nfor i, element in enumerate(a):\n    index = np.where(element == sorted_elements)[0]\n    b[i, index] = 1\n\nprint(b)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 298, "library_problem_id": 7, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 299, "code": ["<code>\nimport numpy as np\n\na = np.array([[1,0,3], [2,4,1]])\n\n# Get the min and max values in the array\nmin_val = np.min(a)\nmax_val = np.max(a)\n\n# Create a range of values from min to max\nrange_val = np.arange(min_val, max_val + 1)\n\n# Use np.digitize to convert the array to one-hot encoding\nb = np.eye(max_val + 1)[np.digitize(a.flatten(), range_val)]\n\n# Reshape the array back to its original shape\nb = b.reshape(a.shape[0], a.shape[1], -1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 299, "library_problem_id": 8, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 300, "code": ["<code>\nimport numpy as np\n\na = np.array([1,2,3,4,5])\np = 25\n\nresult = np.percentile(a, p)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 300, "library_problem_id": 9, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 301, "code": ["<code>\nimport numpy as np\n\ndef reshape_array(A, ncol):\n    B = A.reshape(-1, ncol)\n    return B\n\nA = np.array([1,2,3,4,5,6])\nncol = 2\nB = reshape_array(A, ncol)\nprint(B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 301, "library_problem_id": 10, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 302, "code": ["<code>\nimport numpy as np\n\ndef reshape_to_2d(arr, nrow):\n    ncol = -(-len(arr) // nrow)  # calculate the number of columns\n    return arr.reshape(nrow, ncol)\n\nA = np.array([1,2,3,4,5,6])\nnrow = 3\nB = reshape_to_2d(A, nrow)\nprint(B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 302, "library_problem_id": 11, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 303, "code": ["<code>\nimport numpy as np\n\ndef reshape_array(A, ncol):\n    return A[:A.size//ncol*ncol].reshape(-1, ncol)\n\nA = np.array([1,2,3,4,5,6,7])\nncol = 2\nB = reshape_array(A, ncol)\nprint(B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 303, "library_problem_id": 12, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 304, "code": ["<code>\nimport numpy as np\n\nA = np.array([1,2,3,4,5,6,7])\nncol = 2\n\nB = np.zeros(((len(A) + ncol - 1) // ncol, ncol))\nB = B.flatten()\nB[:len(A)] = A[::-1]\nB = B.reshape(-1, ncol)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 304, "library_problem_id": 13, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 305, "code": ["<code>\nimport numpy as np\n\na = np.array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])\nshift = 3\n\ndef shift_with_nan(arr, shift):\n    result = np.empty(len(arr))\n    result[:] = np.nan\n    result[shift:] = arr[:-shift]\n    return result\n\nresult = shift_with_nan(a, shift)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 305, "library_problem_id": 14, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 306, "code": ["<code>\nimport numpy as np\n\ndef shift_2d(a, shift, fill_value=np.nan):\n    # Create a 2D array of fill_value with the same shape as a\n    result = np.full_like(a, fill_value)\n    \n    # Calculate the shift in indices\n    shift_row, shift_col = shift\n    \n    # Calculate the start and end indices for slicing a\n    start_row = max(0, -shift_row)\n    end_row = min(a.shape[0], a.shape[0] - shift_row)\n    start_col = max(0, -shift_col)\n    end_col = min(a.shape[1], a.shape[1] - shift_col)\n    \n    # Slice a and assign it to the corresponding part of result\n    result[max(0, shift_row):end_row, max(0, shift_col):end_col] = a[start_row:end_row, start_col:end_col]\n    \n    return result\n\na = np.array([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n              [1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]])\nshift = (3, 2)\n\nresult = shift_2d(a, shift)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 306, "library_problem_id": 15, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 307, "code": ["<code>\nimport numpy as np\n\ndef shift_2d_array(a, shift):\n    result = np.full_like(a, np.nan)\n    for i in range(a.shape[0]):\n        result[i] = np.roll(a[i], shift[i])\n    return result\n\na = np.array([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n              [1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]])\nshift = [-2, 3]\n\nresult = shift_2d_array(a, shift)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 307, "library_problem_id": 16, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 14}}
{"id": 308, "code": ["<code>\nimport numpy as np\n\ndef generate_array():\n    np.random.seed(0)\n    r_old = np.random.randint(-1, 2, size=(100, 2000))\n    np.random.seed(0)\n    r_new = np.random.randint(-1, 2, size=(100, 2000))\n    return r_old, r_new\n\nr_old, r_new = generate_array()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 308, "library_problem_id": 17, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 309, "code": ["<code>\nimport numpy as np\n\na = np.array([[10,50,30],[60,20,40]])\n\nresult = np.unravel_index(np.argmax(a, axis=None), a.shape)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 309, "library_problem_id": 18, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 310, "code": ["<code>\nimport numpy as np\n\na = np.array([[10,50,30],[60,20,40]])\n\nresult = np.unravel_index(np.argmin(a, axis=None), a.shape, order='C')\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 310, "library_problem_id": 19, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 311, "code": ["<code>\nimport numpy as np\n\na = np.array([[10,50,30],[60,20,40]])\nresult = np.unravel_index(np.argmax(a, axis=None), a.shape, order='F')\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 311, "library_problem_id": 20, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 312, "code": ["<code>\nimport numpy as np\n\na = np.array([[10,50,30],[60,20,40]])\n\nresult = np.unravel_index(np.argmax(a, axis=None), a.shape)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 312, "library_problem_id": 21, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 313, "code": ["<code>\nimport numpy as np\n\nexample_a = np.array([[10,50,30],[60,20,40]])\n\ndef f(a = example_a):\n    # return the solution in this function\n    return np.unravel_index(np.argmax(a, axis=None), a.shape)\n\n# Test the function\nprint(f(example_a))  # Output: (1, 0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 313, "library_problem_id": 22, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 18}}
{"id": 314, "code": ["<code>\nimport numpy as np\n\na = np.array([[10,50,30],[60,20,40]])\n\n# Flatten the array and sort it in descending order\nsorted_array = np.sort(a, axis=None)[::-1]\n\n# Find the second largest element\nsecond_largest = sorted_array[1]\n\n# Find the index of the second largest element\nresult = np.where(a == second_largest)\n\n# Print the result\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 314, "library_problem_id": 23, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 18}}
{"id": 315, "code": ["<code>\nimport numpy as np\n\na = np.array([[np.nan, 2., 3., np.nan],\n              [1., 2., 3., 9]])\n\n# Use np.isnan to create a mask of True for NaN values and False otherwise\nmask = ~np.isnan(a).any(axis=0)\n\n# Use the mask to index the original array\na = a[:, mask]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 315, "library_problem_id": 24, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 316, "code": ["<code>\nimport numpy as np\n\na = np.array([[np.nan, 2., 3., np.nan],\n              [1., 2., 3., 9]])\n\n# Use the np.isnan() function to create a mask of True for NaN values and False otherwise.\n# Then use the np.all() function to ensure that all values in a row are not NaN.\n# Finally, use the np.any() function to select only the rows that have at least one non-NaN value.\n\nmask = np.all(~np.isnan(a), axis=1)\na = a[mask]\n\nprint(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 316, "library_problem_id": 25, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 24}}
{"id": 317, "code": ["<code>\nimport numpy as np\n\na = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] \n\nresult = np.array(a)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 317, "library_problem_id": 26, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 318, "code": ["<code>\nimport numpy as np\n\na = np.array([[10, 20, 30, 40, 50],\n       [ 6,  7,  8,  9, 10]])\npermutation = [0, 4, 1, 3, 2]\n\na = a[:, permutation]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 318, "library_problem_id": 27, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 319, "code": ["<code>\nimport numpy as np\n\na = np.array([[[10, 20],\n        [30, 40]],\n       [[6,  7],\n        [8,  9]],\n       [[10, 11],\n        [12, 13]]])\n\npermutation = [1, 0, 2]\n\nresult = np.take(a, permutation, axis=0)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 319, "library_problem_id": 28, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 27}}
{"id": 320, "code": ["<code>\nimport numpy as np\n\ndef find_min_index(a):\n    return np.unravel_index(np.argmin(a, axis=None), a.shape)\n\na = np.array([[1, 2], [3, 0]])\nresult = find_min_index(a)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 320, "library_problem_id": 29, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 321, "code": ["<code>\nimport numpy as np\n\ndef find_max_index(a):\n    return np.unravel_index(np.argmax(a, axis=None), a.shape)\n\na = np.array([[1, 2], [3, 0]])\nresult = find_max_index(a)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 321, "library_problem_id": 30, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 29}}
{"id": 322, "code": ["<code>\nimport numpy as np\n\na = np.array([[1, 0], [0, 2]])\n\n# Find the minimum value in the array\nmin_value = np.min(a)\n\n# Find the indices of the minimum value\nindices = np.where(a == min_value)\n\n# Stack the row and column indices into a single array\nresult = np.column_stack((indices[0], indices[1]))\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 322, "library_problem_id": 31, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 29}}
{"id": 323, "code": ["<code>\nimport numpy as np\n\ndegree = 90\n# Convert the degree to radians\nradian = np.deg2rad(degree)\n# Calculate the sine of the radian\nresult = np.sin(radian)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 323, "library_problem_id": 32, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 324, "code": ["<code>\nimport numpy as np\n\ndef calculate_cosine(degree):\n    # Convert the degree to radians\n    radian = np.deg2rad(degree)\n    # Calculate the cosine of the radian\n    result = np.cos(radian)\n    # Return the result\n    return result\n\ndegree = 90\nresult = calculate_cosine(degree)\nprint(f\"The cosine of {degree} degrees is {result}\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 324, "library_problem_id": 33, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 325, "code": ["<code>\nimport numpy as np\nimport math\n\n# Generate a random number between 0 and 360\nnumber = np.random.randint(0, 360)\n\n# Convert the number to radians\nnumber_radians = math.radians(number)\n\n# Calculate the sine of the number in degrees and radians\nsine_degrees = np.sin(number)\nsine_radians = np.sin(number_radians)\n\n# Determine if the number is in degrees or radians\nif sine_degrees > sine_radians:\n    result = 0\nelse:\n    result = 1\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 325, "library_problem_id": 34, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 326, "code": ["<code>\nimport numpy as np\n\nvalue = 1.0\nresult = np.arcsin(value) * 180 / np.pi\n\n# convert the result from radians to degrees\nresult = np.degrees(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 326, "library_problem_id": 35, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 327, "code": ["<code>\nimport numpy as np\n\nA = np.array([1,2,3,4,5])\nlength = 8\n\n# Calculate the number of elements needed to reach the desired length\nn_elements = ((len(A) - 1) // length + 1) * length\n\n# Pad A with zeros\nresult = np.pad(A, (0, n_elements - len(A)))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 327, "library_problem_id": 36, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 328, "code": ["<code>\nimport numpy as np\n\nA = np.array([1,2,3,4,5])\nlength = 8\n\n# Calculate the number of elements needed to reach the desired length\nn_elements = ((len(A) - 1) // length + 1) * length\n\n# Pad A with zeros\nresult = np.pad(A, (0, n_elements - len(A)))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 328, "library_problem_id": 37, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 329, "code": ["<code>\nimport numpy as np\na = np.arange(4).reshape(2, 2)\npower = 5\n\na = np.power(a, power)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 329, "library_problem_id": 38, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 330, "code": ["<code>\nresult = np.power(a, power)\n    return result\n### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 330, "library_problem_id": 39, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 331, "code": ["<code>\nimport numpy as np\nfrom fractions import Fraction\n\nnumerator = 98\ndenominator = 42\n\n# Convert the numerator and denominator to Fraction\nfraction = Fraction(numerator, denominator)\n\n# Reduce the fraction to its simplest form\nreduced_fraction = fraction.limit_denominator()\n\n# Return the reduced numerator and denominator as a tuple\nresult = (reduced_fraction.numerator, reduced_fraction.denominator)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 331, "library_problem_id": 40, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 332, "code": ["<code>\nimport numpy as np\nfrom fractions import Fraction\n\ndef f(numerator = 98, denominator = 42):\n    # return the solution in this function\n    fraction = Fraction(numerator, denominator)\n    return (fraction.numerator, fraction.denominator)\n\n# Test the function\nprint(f(98, 42))  # Output: (7, 3)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 332, "library_problem_id": 41, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 40}}
{"id": 333, "code": ["<code>\nimport numpy as np\n\ndef simplify_fraction(numerator, denominator):\n    if denominator == 0:\n        return (np.nan, np.nan)\n    else:\n        gcd = np.gcd(numerator, denominator)\n        return (numerator // gcd, denominator // gcd)\n\nnumerator = 98\ndenominator = 42\n\nresult = simplify_fraction(numerator, denominator)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 333, "library_problem_id": 42, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 40}}
{"id": 334, "code": ["<code>\nimport numpy as np\n\ndef average_arrays(*args):\n    return np.mean(np.vstack(args), axis=0)\n\na = np.array([10, 20, 30])\nb = np.array([30, 20, 20])\nc = np.array([50, 20, 40])\n\nresult = average_arrays(a, b, c)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 334, "library_problem_id": 43, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 43}}
{"id": 335, "code": ["<code>\nimport numpy as np\n\ndef max_arrays(*args):\n    return np.maximum.reduce(args)\n\na = np.array([10, 20, 30])\nb = np.array([30, 20, 20])\nc = np.array([50, 20, 40])\n\nresult = max_arrays(a, b, c)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 335, "library_problem_id": 44, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 43}}
{"id": 336, "code": ["<code>\nimport numpy as np\n\na = np.array([[ 0,  1,  2,  3,  4],\n   [ 5,  6,  7,  8,  9],\n   [10, 11, 12, 13, 14],\n   [15, 16, 17, 18, 19],\n   [20, 21, 22, 23, 24]])\n\ndef get_antidiagonal(a):\n    return a[::-1].diagonal()[::-1]\n\nresult = get_antidiagonal(a)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 336, "library_problem_id": 45, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 337, "code": ["<code>\nimport numpy as np\n\ndef extract_diagonal_tr(a):\n    rows, cols = a.shape\n    indices = np.diag_indices_from(a[:, ::-1])\n    return a[np.min(indices) : np.max(indices) + 1, np.min(indices) : np.max(indices) + 1]\n\na = np.array([[ 0,  1,  2,  3,  4, 5],\n   [ 5,  6,  7,  8,  9, 10],\n   [10, 11, 12, 13, 14, 15],\n   [15, 16, 17, 18, 19, 20],\n   [20, 21, 22, 23, 24, 25]])\n\nresult = extract_diagonal_tr(a)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 337, "library_problem_id": 46, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 338, "code": ["<code>\nimport numpy as np\n\na = np.array([[ 0,  1,  2,  3,  4],\n   [ 5,  6,  7,  8,  9],\n   [10, 11, 12, 13, 14],\n   [15, 16, 17, 18, 19],\n   [20, 21, 22, 23, 24]])\n\n# Get the main diagonal\nmain_diagonal = np.diagonal(a)\n\n# Get the anti-diagonal\nanti_diagonal = np.diagonal(np.fliplr(a))\n\nresult = (main_diagonal, anti_diagonal)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 338, "library_problem_id": 47, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 339, "code": ["<code>\nimport numpy as np\n\na = np.array([[ 0,  1,  2,  3,  4, 5],\n   [ 5,  6,  7,  8,  9, 10],\n   [10, 11, 12, 13, 14, 15],\n   [15, 16, 17, 18, 19, 20],\n   [20, 21, 22, 23, 24, 25]])\n\n# Get the indices for the top-left diagonal\nindices_tl = np.diag_indices(min(a.shape))\n\n# Get the indices for the bottom-left diagonal\nindices_bl = (np.arange(max(a.shape)), np.arange(max(a.shape))[::-1])\n\n# Extract the diagonals\ndiag_tl = a[indices_tl]\ndiag_bl = a[indices_bl]\n\n# Combine the diagonals\nresult = np.concatenate((diag_tl, diag_bl))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 339, "library_problem_id": 48, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 45}}
{"id": 340, "code": ["<code>\nimport numpy as np\n\nX = np.random.randint(2, 10, (5, 6))\nresult = X.flatten().tolist()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 340, "library_problem_id": 49, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 341, "code": ["<code>\nimport numpy as np\n\ndef flatten_array(X):\n    result = []\n    for row in X:\n        for item in row:\n            result.append(item)\n    return result\n\nX = np.random.randint(2, 10, (5, 6))\nresult = flatten_array(X)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 341, "library_problem_id": 50, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 49}}
{"id": 342, "code": ["<code>\nimport numpy as np\nexample_X = np.random.randint(2, 10, (5, 6))\ndef f(X = example_X):\n    # return the solution in this function\n    result = []\n    for row in X:\n        for item in row:\n            result.append(item)\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 342, "library_problem_id": 51, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 343, "code": ["<code>\nimport numpy as np\n\ndef flatten_array(X):\n    return X.flatten().tolist()\n\nX = np.random.randint(2, 10, (5, 6))\nresult = flatten_array(X)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 343, "library_problem_id": 52, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 49}}
{"id": 344, "code": ["<code>\nimport numpy as np\n\ndef binary_to_numpy(mystr):\n    result = np.array([int(i) for i in mystr], dtype=np.int8)\n    return result\n\nmystr = \"100110\"\nresult = binary_to_numpy(mystr)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 344, "library_problem_id": 53, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 53}}
{"id": 345, "code": ["<code>\nimport numpy as np\n\n# Given data\na = np.random.rand(8, 5)\ncol = 2\nmultiply_number = 5.2\n\n# Multiply the specific column by a scalar value\na[:, col] = a[:, col] * multiply_number\n\n# Compute the cumulative sum of the resulting column\nresult = np.cumsum(a[:, col])\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 345, "library_problem_id": 54, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 346, "code": ["<code>\nimport numpy as np\n\na = np.random.rand(8, 5)\nrow = 2\nmultiply_number = 5.2\n\n# Select the specified row\nrow_data = a[row, :]\n\n# Multiply each element in the row by the scalar value\nmultiplied_data = row_data * multiply_number\n\n# Compute the cumulative sum of the resulting row\nresult = np.cumsum(multiplied_data)\n\nresult\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 346, "library_problem_id": 55, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 347, "code": ["<code>\nimport numpy as np\n\na = np.random.rand(8, 5)\nrow = 2\ndivide_number = 5.2\n\n# Divide each element in the specified row by the scalar value\na[row, :] = a[row, :] / divide_number\n\n# Compute the product of the resulting elements in the row\nresult = np.prod(a[row, :])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 347, "library_problem_id": 56, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 54}}
{"id": 348, "code": ["<code>\nimport numpy as np\n\ndef find_independent_vectors(matrix):\n    # Compute the rank of the matrix\n    rank = np.linalg.matrix_rank(matrix)\n    \n    # The rank of a matrix is the maximum number of linearly independent rows (or columns)\n    # So, we can take the first 'rank' number of rows (or columns) as the maximal set of linearly independent vectors\n    independent_vectors = matrix[:rank]\n    \n    return independent_vectors\n\na = np.array([[0,1,0,0], [0,0,1,0], [0,1,1,0], [1,0,0,1]])\nresult = find_independent_vectors(a)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 348, "library_problem_id": 57, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 349, "code": ["<code>\nimport numpy as np\n\n# Generate a random 2D numpy array\na = np.random.rand(np.random.randint(5, 10), np.random.randint(6, 10))\n\n# Determine the length of a row in the 2D numpy array\nresult = a.shape[0]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 349, "library_problem_id": 58, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 350, "code": ["<code>\nimport numpy as np\nimport scipy.stats\n\na = np.random.randn(40)\nb = 4*np.random.randn(50)\n\n# Calculate the mean of each sample\nmean_a = np.mean(a)\nmean_b = np.mean(b)\n\n# Calculate the standard deviation of each sample\nstd_a = np.std(a)\nstd_b = np.std(b)\n\n# Calculate the number of observations in each sample\nn_a = len(a)\nn_b = len(b)\n\n# Calculate the pooled standard deviation\nsp = np.sqrt(((n_a - 1) * (std_a ** 2) + (n_b - 1) * (std_b ** 2)) / (n_a + n_b - 2))\n\n# Calculate the t-statistic\nt_stat = (mean_a - mean_b) / (sp * np.sqrt(1/n_a + 1/n_b))\n\n# Calculate the degrees of freedom\ndf = n_a + n_b - 2\n\n# Calculate the p-value\np_value = 2 * (1 - scipy.stats.t.cdf(abs(t_stat), df))\n\nprint(\"The p-value is:\", p_value)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 350, "library_problem_id": 59, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 351, "code": ["<code>\nimport numpy as np\nimport scipy.stats\n\n# Generate random samples\na = np.random.randn(40)\nb = 4*np.random.randn(50)\n\n# Remove NaN values\na = a[~np.isnan(a)]\nb = b[~np.isnan(b)]\n\n# Calculate the t-statistic and the degrees of freedom\nt_stat, p_val = scipy.stats.ttest_ind(a, b, equal_var=False, nan_policy='omit')\n\nprint(\"The p-value is:\", p_val)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 351, "library_problem_id": 60, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 352, "code": ["<code>\nimport numpy as np\nimport scipy.stats\n\n# Given values\namean = -0.0896\navar = 0.954\nanobs = 40\nbmean = 0.719\nbvar = 11.87\nbnobs = 50\n\n# Calculate the pooled variance and degrees of freedom\nsp2 = ((anobs - 1) * avar + (bnobs - 1) * bvar) / (anobs + bnobs - 2)\ndf = anobs + bnobs - 2\n\n# Calculate the t-statistic\nt_stat = (bmean - amean) / np.sqrt(avar/anobs + bvar/bnobs)\n\n# Calculate the p-value\np_value = 2 * (1 - scipy.stats.t.cdf(t_stat, df))\n\nprint(\"The p-value is:\", p_value)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 352, "library_problem_id": 61, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 353, "code": ["<code>\nimport numpy as np\n\nA=np.asarray([[1,1,1], [1,1,2], [1,1,3], [1,1,4]])\nB=np.asarray([[0,0,0], [1,0,2], [1,0,3], [1,0,4], [1,1,0], [1,1,1], [1,1,4]])\n\n# Convert the 2D arrays to 1D and find the unique elements in A that are not in B\noutput = np.setdiff1d(A.flatten(), B.flatten())\n\nprint(output)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 353, "library_problem_id": 62, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 354, "code": ["<code>\nimport numpy as np\n\ndef find_difference(A, B):\n    # Convert the 2D arrays to 1D\n    A = A.flatten()\n    B = B.flatten()\n\n    # Find the elements in A but not in B\n    in_A_not_in_B = np.setdiff1d(A, B)\n\n    # Find the elements in B but not in A\n    in_B_not_in_A = np.setdiff1d(B, A)\n\n    # Combine the two arrays\n    result = np.concatenate((in_A_not_in_B, in_B_not_in_A))\n\n    return result\n\nA = np.asarray([[1,1,1], [1,1,2], [1,1,3], [1,1,4]])\nB = np.asarray([[0,0,0], [1,0,2], [1,0,3], [1,0,4], [1,1,0], [1,1,1], [1,1,4]])\n\noutput = find_difference(A, B)\nprint(output)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 354, "library_problem_id": 63, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 62}}
{"id": 355, "code": ["<code>\nimport numpy as np\n\na = np.random.rand(3, 3, 3)\nb = np.arange(3*3*3).reshape((3, 3, 3))\n\n# Sort a along the specified axis\na_sorted = np.sort(a, axis=0)\n\n# Use numpy.argsort to get the indices that would sort a along the specified axis\nindices = np.argsort(a, axis=0)\n\n# Use these indices to sort b along the same axis\nc = b[indices]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 355, "library_problem_id": 64, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 356, "code": ["<code>\nimport numpy as np\n\na = np.random.rand(3, 3, 3)\nb = np.arange(3*3*3).reshape((3, 3, 3))\n\n# Get the sorted indices of a along axis 0\nindices = np.argsort(a, axis=0)\n\n# Use these indices to sort b along axis 0\nc = np.take_along_axis(b, indices, axis=0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 356, "library_problem_id": 65, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 357, "code": ["<code>\nimport numpy as np\n\na = np.random.rand(3, 3, 3)\nb = np.arange(3*3*3).reshape((3, 3, 3))\n\n# Sort a along axis 0\na_sorted_indices = np.argsort(a, axis=0)\n\n# Use these indices to sort b along axis 0\nc = b[a_sorted_indices, :, :]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 357, "library_problem_id": 66, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 358, "code": ["<code>\nimport numpy as np\n\ndef sort_based_on_sum(a, b):\n    # Calculate the sum of each 3D array in a\n    sums_a = np.sum(a, axis=(1, 2))\n\n    # Sort the indices of a based on the sums\n    sorted_indices = np.argsort(-sums_a)\n\n    # Use the sorted indices to sort b\n    result = b[sorted_indices]\n\n    return result\n\na = np.random.rand(3, 3, 3)\nb = np.arange(3*3*3).reshape((3, 3, 3))\n\nresult = sort_based_on_sum(a, b)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 358, "library_problem_id": 67, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 64}}
{"id": 359, "code": ["<code>\nimport numpy as np\n\na = np.arange(12).reshape(3, 4)\na = np.delete(a, 2, axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 359, "library_problem_id": 68, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 360, "code": ["<code>\nimport numpy as np\n\na = np.arange(12).reshape(3, 4)\na = np.delete(a, 2, axis=0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 360, "library_problem_id": 69, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 361, "code": ["<code>\nimport numpy as np\n\na = np.arange(12).reshape(3, 4)\n\n# Delete the 1st and 3rd columns\na = np.delete(a, [0, 2], axis=1)\n\nprint(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 361, "library_problem_id": 70, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 362, "code": ["<code>\nimport numpy as np\n\ndef delete_columns(a, del_col):\n    valid_indices = np.where(del_col < a.shape[1])[0]\n    result = np.delete(a, valid_indices, axis=1)\n    return result\n\na = np.arange(12).reshape(3, 4)\ndel_col = np.array([1, 2, 4, 5])\n\nresult = delete_columns(a, del_col)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 362, "library_problem_id": 71, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 363, "code": ["<code>\nimport numpy as np\n\na = np.asarray([1,2,3,4])\npos = 2\nelement = 66\n\na = np.insert(a, pos, element)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 363, "library_problem_id": 72, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 364, "code": ["<code>\nimport numpy as np\n\na = np.array([[1,2],[3,4]])\npos = 1\nelement = [3,5]\n\na = np.insert(a, pos, element, axis=0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 364, "library_problem_id": 73, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 365, "code": ["<code>\nimport numpy as np\n\nexample_a = np.asarray([1,2,3,4])\n\ndef f(a = example_a, pos=2, element = 66):\n    # Create a new array with one more element\n    new_a = np.zeros(len(a) + 1, dtype=a.dtype)\n    \n    # Copy the elements before the insertion position\n    new_a[:pos] = a[:pos]\n    \n    # Insert the new element\n    new_a[pos] = element\n    \n    # Copy the elements after the insertion position\n    new_a[pos+1:] = a[pos:]\n    \n    return new_a\n\n# Test the function\na = f()\nprint(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 365, "library_problem_id": 74, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 72}}
{"id": 366, "code": ["<code>\nimport numpy as np\n\ndef insert_rows(a, pos, element):\n    for i in range(len(pos)):\n        a = np.insert(a, pos[i], element[i], axis=0)\n    return a\n\na = np.array([[1,2],[3,4]])\npos = [1, 2]\nelement = np.array([[3, 5], [6, 6]])\n\na = insert_rows(a, pos, element)\nprint(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 366, "library_problem_id": 75, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 72}}
{"id": 367, "code": ["<code>\nimport numpy as np\n\npairs = [(2, 3), (3, 4), (4, 5)]\narray_of_arrays = np.array([np.arange(a*b).reshape(a,b) for (a, b) in pairs])\n\ndef deep_copy_nested_array(nested_array):\n    result = []\n    for sub_array in nested_array:\n        if isinstance(sub_array, np.ndarray):\n            result.append(sub_array.copy())\n        else:\n            result.append(deep_copy_nested_array(sub_array))\n    return np.array(result)\n\nresult = deep_copy_nested_array(array_of_arrays)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 367, "library_problem_id": 76, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 368, "code": ["<code>\nimport numpy as np\n\na = np.repeat(np.arange(1, 6).reshape(1, -1), 3, axis = 0)\n\nresult = np.all(a[0] == a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 368, "library_problem_id": 77, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 369, "code": ["<code>\nimport numpy as np\n\na = np.repeat(np.arange(1, 6).reshape(-1, 1), 3, axis = 1)\n\nresult = np.all(a[:, 0] == a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 369, "library_problem_id": 78, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Semantic", "perturbation_origin_id": 77}}
{"id": 370, "code": ["<code>\ndef f(a = example_a):\n    # We can use the np.ptp function to find the range of values in each row. If all rows are identical, the range will be 0.\n    return np.all(np.ptp(a, axis=1) == 0)\n\n# Test the function\nprint(f(example_a))  # Should return True\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 370, "library_problem_id": 79, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Surface", "perturbation_origin_id": 77}}
{"id": 371, "code": ["<code>\nimport numpy as np\nfrom scipy.interpolate import griddata\n\n# Define the x and y coordinates\nx = np.linspace(0, 1, 20)\ny = np.linspace(0, 1, 30)\n\n# Generate some random data\ndata = np.random.rand(len(y), len(x))\n\n# Create a grid of x and y coordinates\nxg, yg = np.meshgrid(x, y, indexing='ij')\n\n# Interpolate the data to the grid\nzi = griddata((x, y), data, (xg, yg), method='cubic')\n\n# Perform the numerical integration\nresult = np.trapz(np.trapz(zi, x, axis=1), y)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 371, "library_problem_id": 80, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 372, "code": ["<code>\nfrom scipy.integrate import dblquad\n\ndef f(x = example_x, y = example_y):\n    def integrand(y, x):\n        # replace this with your function\n        return x * y\n\n    # calculate the integral\n    result, error = dblquad(integrand, 0, 1, lambda x: 0, lambda x: 1)\n    return result\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 372, "library_problem_id": 81, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 373, "code": ["<code>\nimport numpy as np\n\ngrades = np.array([93.5,93,60.8,94.5,82,87.5,91.5,99.5,86,93.5,92.5,78,76,69,94.5,\n          89.5,92.8,78,65.5,98,98.5,92.3,95.5,76,91,95,61])\n\ndef ecdf(data):\n    x = np.sort(data)\n    n = x.size\n    y = np.arange(1, n+1) / n\n    return x, y\n\nx, y = ecdf(grades)\nresult = (x, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 373, "library_problem_id": 82, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 374, "code": ["<code>\nimport numpy as np\n\ndef ecdf(data):\n    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n    # Number of data points: n\n    n = len(data)\n\n    # x-data for the ECDF: x\n    x = np.sort(data)\n\n    # y-data for the ECDF: y\n    y = np.arange(1, n+1) / n\n\n    return x, y\n\n# Your data\ngrades = np.array([93.5,93,60.8,94.5,82,87.5,91.5,99.5,86,93.5,92.5,78,76,69,94.5,\n          89.5,92.8,78,65.5,98,98.5,92.3,95.5,76,91,95,61])\n\n# Calculate ECDF\nx, y = ecdf(grades)\n\n# Apply ECDF to the same dataset\nresult = np.interp(grades, x, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 374, "library_problem_id": 83, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 375, "code": ["<code>\nimport numpy as np\nfrom scipy import stats\n\ngrades = np.array([93.5,93,60.8,94.5,82,87.5,91.5,99.5,86,93.5,92.5,78,76,69,94.5,\n          89.5,92.8,78,65.5,98,98.5,92.3,95.5,76,91,95,61])\nthreshold = 0.5\n\n# Sort the grades\ngrades = np.sort(grades)\n\n# Calculate the cumulative distribution function (CDF)\ncdf = stats.cumfreq(grades, numbins=len(grades))\n\n# Find the longest interval [low, high) where the CDF is below a specified threshold\nlow = cdf.lowerlimit\nhigh = cdf.lowerlimit + cdf.binsize * np.argmax(cdf.cumcount < threshold * len(grades))\n\nprint(f\"The longest interval [low, high) where the CDF is below {threshold} is [{low}, {high})\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 375, "library_problem_id": 84, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 376, "code": ["<code>\nimport numpy as np\n\none_ratio = 0.9\nsize = 1000\n\nnums = np.random.choice([0, 1], size=size, p=[1-one_ratio, one_ratio])\n\nprint(nums)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 376, "library_problem_id": 85, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 377, "code": ["<code>\nimport torch\nimport numpy as np\n\na = torch.ones(5)\na_np = a.numpy()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 377, "library_problem_id": 86, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 378, "code": ["<code>\nimport torch\nimport numpy as np\n\na = np.ones(5)\na_pt = torch.from_numpy(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 378, "library_problem_id": 87, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 86}}
{"id": 379, "code": ["<code>\nimport tensorflow as tf\nimport numpy as np\n\na = tf.ones([2,3,4])\na_np = a.numpy()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 379, "library_problem_id": 88, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 380, "code": ["<code>\nimport tensorflow as tf\nimport numpy as np\n\na = np.ones([2,3,4])\na_tf = tf.convert_to_tensor(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 380, "library_problem_id": 89, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 88}}
{"id": 381, "code": ["<code>\nimport numpy as np\n\na = np.array([4, 1, 0, 8, 5, 2])\n\n# Get the indices of the sorted array in decreasing order\nindices = np.argsort(a)[::-1]\n\n# Create a new array with the indices\nresult = indices\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 381, "library_problem_id": 90, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 382, "code": ["<code>\nimport numpy as np\n\na = np.array([4, 1, 0, 8, 5, 2])\n\n# Get the indices of the sorted array\nindices = np.argsort(a)\n\n# Create a new array that maps each element's value to its original index\nresult = np.array([(value, index) for value, index in zip(a[indices], indices)])\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 382, "library_problem_id": 91, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 383, "code": ["<code>\nimport numpy as np\n\ndef find_indices_of_n_largest(arr, N):\n    return np.argsort(arr)[-N:][::-1]\n\na = np.array([4, 1, 0, 8, 5, 2])\nN = 3\n\nresult = find_indices_of_n_largest(a, N)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 383, "library_problem_id": 92, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 90}}
{"id": 384, "code": ["<code>\nimport numpy as np\n\ndef raise_2d_array_to_power(A, n):\n    return np.linalg.matrix_power(A, n)\n\nA = np.arange(16).reshape(4, 4)\nn = 5\n\nresult = raise_2d_array_to_power(A, n)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 384, "library_problem_id": 93, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 93}}
{"id": 385, "code": ["<code>\nimport numpy as np\n\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\n\ndef extract_patches(arr, patch_shape):\n    patch_strides = arr.strides\n    sub_shape = tuple(np.subtract(arr.shape, patch_shape) + 1)\n    sub_strides = tuple(np.multiply(patch_strides, patch_shape))\n    view_shape = tuple(np.concatenate((sub_shape, patch_shape)))\n    view_strides = tuple(np.concatenate((sub_strides, patch_strides)))\n    patches = np.lib.stride_tricks.as_strided(arr, view_shape, view_strides)\n    return patches\n\nresult = extract_patches(a, (2,2))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 385, "library_problem_id": 94, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 386, "code": ["<code>\nimport numpy as np\n\ndef extract_patches(arr, patch_size=2):\n    # Get the shape of the input array\n    n_rows, n_cols = arr.shape\n\n    # Calculate the number of patches in each dimension\n    n_rows_patches = n_rows - patch_size + 1\n    n_cols_patches = n_cols - patch_size + 1\n\n    # Initialize an empty list to hold the patches\n    patches = []\n\n    # Loop over the array and extract patches\n    for i in range(n_rows_patches):\n        for j in range(n_cols_patches):\n            patch = arr[i:i+patch_size, j:j+patch_size]\n            patches.append(patch)\n\n    # Convert the list of patches to a numpy array and return\n    return np.array(patches)\n\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\n\nresult = extract_patches(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 386, "library_problem_id": 95, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 387, "code": ["<code>\nimport numpy as np\n\ndef extract_patches(a):\n    # Get the shape of the array\n    nrows, ncols = a.shape\n\n    # Initialize an empty list to store the patches\n    patches = []\n\n    # Loop over the array in steps of 2\n    for i in range(0, nrows-1, 2):\n        for j in range(0, ncols-1, 2):\n            # Extract a 2x2 patch and append it to the list\n            patch = a[i:i+2, j:j+2]\n            patches.append(patch)\n\n    # Convert the list of patches to a 3D NumPy array\n    result = np.array(patches)\n\n    return result\n\na = np.array([[1,5,9,13],\n              [2,6,10,14],\n              [3,7,11,15],\n              [4,8,12,16]])\n\nresult = extract_patches(a)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 387, "library_problem_id": 96, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 388, "code": ["<code>\nimport numpy as np\n\ndef extract_patches(arr, patch_size):\n    # Get the shape of the array\n    arr_shape = np.array(arr.shape)\n    \n    # Calculate the number of patches along each dimension\n    patch_count = arr_shape // patch_size\n    \n    # Create an array to store the patches\n    patches = np.empty((*patch_count, patch_size, patch_size), dtype=arr.dtype)\n    \n    # Iterate over each patch\n    for i in range(patch_count[0]):\n        for j in range(patch_count[1]):\n            # Extract the patch\n            patches[i, j] = arr[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n    \n    return patches\n\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\npatch_size = 2\n\nresult = extract_patches(a, patch_size)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 388, "library_problem_id": 97, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 389, "code": ["<code>\nimport numpy as np\n\na = np.array([[[ 0,  1,  2],\n        [ 6,  7,  8]],    \n       [[ 3,  4,  5],\n        [ 9, 10, 11]], \n       [[12, 13, 14],\n        [18, 19, 20]],    \n       [[15, 16, 17],\n        [21, 22, 23]]])\n\nresult = np.reshape(a, (a.shape[0], a.shape[1]*a.shape[2]))\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 389, "library_problem_id": 98, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 390, "code": ["<code>\nimport numpy as np\n\ndef extract_patches(a, patch_size):\n    # Get the shape of the array\n    array_shape = np.array(a.shape)\n    \n    # Calculate the number of patches along each dimension\n    n_patches = array_shape // patch_size\n    \n    # Create an array to store the patches\n    patches = np.empty((*n_patches, patch_size, patch_size), dtype=a.dtype)\n    \n    # Loop over each patch\n    for i in range(n_patches[0]):\n        for j in range(n_patches[1]):\n            # Extract the patch\n            patches[i, j] = a[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n    \n    return patches\n\na = np.array([[1,5,9,13,17],\n              [2,6,10,14,18],\n              [3,7,11,15,19],\n              [4,8,12,16,20]])\npatch_size = 2\n\nresult = extract_patches(a, patch_size)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 390, "library_problem_id": 99, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 391, "code": ["<code>\nimport numpy as np\n\ndef extract_subarray(a, low, high):\n    return a[:, low:high+1]\n\na = np.array([[ 0,  1,  2,  3, 5, 6, 7, 8],\n              [ 4,  5,  6,  7, 5, 3, 2, 5],\n              [ 8,  9, 10, 11, 4, 5, 3, 5]])\nlow = 1\nhigh = 5\n\nresult = extract_subarray(a, low, high)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 391, "library_problem_id": 100, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 392, "code": ["<code>\nimport numpy as np\n\ndef extract_subarray(a, low, high):\n    return a[low:high+1, :]\n\na = np.array([[ 0,  1,  2,  3, 5, 6, 7, 8],\n              [ 4,  5,  6,  7, 5, 3, 2, 5],\n              [ 8,  9, 10, 11, 4, 5, 3, 5]])\nlow = 0\nhigh = 2\n\nresult = extract_subarray(a, low, high)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 392, "library_problem_id": 101, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 393, "code": ["<code>\nimport numpy as np\n\na = np.array([[ 0,  1,  2,  3, 5, 6, 7, 8],\n              [ 4,  5,  6,  7, 5, 3, 2, 5],\n              [ 8,  9, 10, 11, 4, 5, 3, 5]])\nlow = 1\nhigh = 10\n\nresult = a[:, low:min(high, a.shape[1])]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 393, "library_problem_id": 102, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 394, "code": ["<code>\nimport numpy as np\n\nstring = \"[[ 0.5544  0.4456], [ 0.8811  0.1189]]\"\na = np.array(eval(string))\n\nprint(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 394, "library_problem_id": 103, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 395, "code": ["<code>\nimport numpy as np\n\ndef loguni(min, max, n):\n    return np.power(10, np.random.uniform(np.log10(min), np.log10(max), n))\n\nmin = 1\nmax = np.e\nn = 10000\n\nresult = loguni(min, max, n)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 395, "library_problem_id": 104, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 396, "code": ["<code>\nimport numpy as np\n\ndef loguni(min, max, n):\n    return np.exp(np.random.uniform(min, max, n))\n\nmin = 0\nmax = 1\nn = 10000\n\nresult = loguni(min, max, n)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 396, "library_problem_id": 105, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 104}}
{"id": 397, "code": ["<code>\nimport numpy as np\n\ndef f(min=1, max=np.e, n=10000):\n    # Generate a random sample from a uniform distribution\n    u = np.random.uniform(0, 1, n)\n    # Apply the log-uniform transformation\n    result = min * (max / min) ** u\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 397, "library_problem_id": 106, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 398, "code": ["<code>\nimport numpy as np\nimport pandas as pd\n\n# Given data\nA = pd.Series(np.random.randn(10,))\na = 2\nb = 3\n\n# Recursive calculation\nB = pd.Series(A[0]*a, index=[0])\nfor t in range(1, len(A)):\n    B[t] = a * A[t] + b * B[t-1]\n\nprint(B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 398, "library_problem_id": 107, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 399, "code": ["<code>\nimport numpy as np\nimport pandas as pd\n\nA = pd.Series(np.random.randn(10,))\na = 2\nb = 3\nc = 4\n\ndef compute_B(A, a, b, c):\n    B = pd.Series(index=A.index)\n    B[0] = a * A[0]\n    B[1] = a * A[1] + b * B[0]\n    for t in range(2, len(A)):\n        B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n    return B\n\nB = compute_B(A, a, b, c)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 399, "library_problem_id": 108, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 107}}
{"id": 400, "code": ["<code>\nimport numpy as np\n\nresult = np.array([])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 400, "library_problem_id": 109, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 401, "code": ["<code>\nimport numpy as np\n\nresult = np.empty((3, 0))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 401, "library_problem_id": 110, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 109}}
{"id": 402, "code": ["<code>\nimport numpy as np\n\ndef sub2ind(dims, index):\n    index = list(index)\n    for i in range(len(index)):\n        index[i] -= 1\n    return np.ravel_multi_index(tuple(index), dims, mode='F')\n\ndims = (3, 4, 2)\na = np.random.rand(*dims)\nindex = (1, 0, 1)\n\nresult = sub2ind(dims, index)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 402, "library_problem_id": 111, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 403, "code": ["<code>\nimport numpy as np\n\ndef sub2ind(dims, index):\n    # Compute the linear index in C order (row-major)\n    result = np.ravel_multi_index(index, dims, mode='raise')\n    return result\n\ndims = (3, 4, 2)\na = np.random.rand(*dims)\nindex = (1, 0, 1)\n\nresult = sub2ind(dims, index)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 403, "library_problem_id": 112, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 404, "code": ["<code>\nimport numpy as np\nimport pandas as pd\n\nindex = ['x', 'y']\ncolumns = ['a','b','c']\n\n# Create a numpy array with mixed data types\ndata = np.zeros((2, 3), dtype=object)\ndata[:, 0] = data[:, 0].astype(int)\ndata[:, 1:] = data[:, 1:].astype(float)\n\n# Create a pandas DataFrame from the numpy array\ndf = pd.DataFrame(data, index=index, columns=columns)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 404, "library_problem_id": 113, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 405, "code": ["<code>\nimport numpy as np\n\na = np.arange(1,11)\naccmap = np.array([0,1,0,0,0,1,1,2,2,1])\n\nresult = np.bincount(accmap, weights=a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 405, "library_problem_id": 114, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 406, "code": ["<code>\nimport numpy as np\n\na = np.arange(1,11)\nindex = np.array([0,1,0,0,0,1,1,2,2,1])\n\nresult = np.bincount(index, weights=a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 406, "library_problem_id": 115, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 407, "code": ["<code>\nimport numpy as np\n\na = np.arange(1,11)\naccmap = np.array([0,1,0,0,0,-1,-1,2,2,1])\n\nresult = np.bincount(accmap, weights=a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 407, "library_problem_id": 116, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 408, "code": ["<code>\nimport numpy as np\n\na = np.arange(1,11)\nindex = np.array([0,1,0,0,0,-1,-1,2,2,1])\n\nresult = np.empty(len(index))\n\nfor i in range(len(index)):\n    if index[i] < 0:\n        result[i] = a[index[i]+len(a)]\n    else:\n        result[i] = a[index[i]]\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 408, "library_problem_id": 117, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 114}}
{"id": 409, "code": ["<code>\nimport numpy as np\n\nx = np.array([[2, 2, 2],\n              [2, 2, 2],\n              [2, 2, 2]])\ny = np.array([[3, 3, 3],\n              [3, 3, 3],\n              [3, 3, 1]])\n\nz = np.add(x, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 409, "library_problem_id": 118, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 410, "code": ["<code>\nimport numpy as np\n\nprobabilit = [0.333, 0.334, 0.333]\nlista_elegir = [(3, 3), (3, 4), (3, 5)]\nsamples = 1000\n\nresult = np.random.choice(a=[x[0] for x in lista_elegir], size=samples, replace=True, p=probabilit)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 410, "library_problem_id": 119, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 411, "code": ["<code>\nimport numpy as np\n\na = np.ones((3, 3))\nlow_index = -1\nhigh_index = 2\n\nresult = a[:, low_index:high_index]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 411, "library_problem_id": 120, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 412, "code": ["<code>\nimport numpy as np\n\nx = np.array([-2, -1.4, -1.1, 0, 1.2, 2.2, 3.1, 4.4, 8.3, 9.9, 10, 14, 16.2])\nresult = x[x >= 0]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 412, "library_problem_id": 121, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 413, "code": ["<code>\nimport numpy as np\n\nx = np.array([-2+1j, -1.4, -1.1, 0, 1.2, 2.2+2j, 3.1, 4.4, 8.3, 9.9, 10+0j, 14, 16.2])\n\nresult = x[np.iscomplex(x)]\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 413, "library_problem_id": 122, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 414, "code": ["<code>\nimport numpy as np\n\ndef bin_data(data, bin_size):\n    # Calculate the number of bins\n    num_bins = len(data) // bin_size\n    # Reshape the data into the bins\n    binned_data = data[:num_bins*bin_size].reshape(num_bins, bin_size)\n    # Calculate the mean of each bin\n    bin_data_mean = np.mean(binned_data, axis=1)\n    return bin_data_mean\n\ndata = np.array([4, 2, 5, 6, 7, 5, 4, 3, 5, 7])\nbin_size = 3\nbin_data_mean = bin_data(data, bin_size)\nprint(bin_data_mean)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 414, "library_problem_id": 123, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 415, "code": ["<code>\nimport numpy as np\n\ndef bin_data_max(data, bin_size):\n    # Calculate the number of bins\n    num_bins = len(data) // bin_size\n    # Reshape the data into the bins\n    reshaped_data = data[:num_bins*bin_size].reshape(num_bins, bin_size)\n    # Calculate the maximum value within each bin\n    max_values = np.max(reshaped_data, axis=1)\n    return max_values\n\ndata = np.array([4, 2, 5, 6, 7, 5, 4, 3, 5, 7])\nbin_size = 3\nbin_data_max = bin_data_max(data, bin_size)\nprint(bin_data_max)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 415, "library_problem_id": 124, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 416, "code": ["<code>\nimport numpy as np\n\ndef bin_data(data, bin_size):\n    # Calculate the number of bins\n    num_bins = data.shape[1] // bin_size\n    # Reshape the data into a 2D array with num_bins columns\n    binned_data = data[:, :num_bins*bin_size].reshape(data.shape[0], num_bins, bin_size)\n    # Calculate the mean of each bin across the rows\n    bin_data_mean = np.mean(binned_data, axis=2)\n    return bin_data_mean\n\ndata = np.array([[4, 2, 5, 6, 7], [5, 4, 3, 5, 7]])\nbin_size = 3\nbin_data_mean = bin_data(data, bin_size)\nprint(bin_data_mean)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 416, "library_problem_id": 125, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 417, "code": ["<code>\nimport numpy as np\n\ndef calculate_bin_data_mean(data, bin_size):\n    # Calculate the number of bins\n    num_bins = len(data) // bin_size\n    # Create an array of indices for the bins\n    bin_indices = np.arange(num_bins) * bin_size\n    # Use numpy's digitize function to assign each data point to a bin\n    bins = np.digitize(np.arange(len(data)), bin_indices)\n    # Calculate the mean of each bin\n    bin_data_mean = np.array([np.mean(data[bins == i]) for i in range(1, num_bins + 1)])\n    return bin_data_mean\n\ndata = np.array([4, 2, 5, 6, 7, 5, 4, 3, 5, 7])\nbin_size = 3\nbin_data_mean = calculate_bin_data_mean(data, bin_size)\nprint(bin_data_mean)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 417, "library_problem_id": 126, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 418, "code": ["<code>\nimport numpy as np\n\ndef bin_data(data, bin_size):\n    # Calculate the number of bins\n    num_bins = data.shape[1] // bin_size\n    if data.shape[1] % bin_size != 0:\n        num_bins -= 1\n\n    # Reshape the data to 2D array with shape (num_bins, bin_size)\n    data = data[:, :num_bins*bin_size].reshape(data.shape[0], num_bins, bin_size)\n\n    # Calculate the mean of each bin\n    bin_data_mean = np.mean(data, axis=2)\n\n    return bin_data_mean\n\ndata = np.array([[4, 2, 5, 6, 7], [5, 4, 3, 5, 7]])\nbin_size = 3\nbin_data_mean = bin_data(data, bin_size)\nprint(bin_data_mean)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 418, "library_problem_id": 127, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 419, "code": ["<code>\nimport numpy as np\n\ndef bin_data(data, bin_size):\n    # Calculate the number of bins\n    num_bins = data.shape[1] // bin_size\n    if data.shape[1] % bin_size != 0:\n        num_bins += 1\n\n    # Initialize an empty list to store the mean of each bin\n    bin_data_mean = []\n\n    # Loop over each bin\n    for i in range(num_bins):\n        # Calculate the start and end indices of the bin\n        start = data.shape[1] - (i + 1) * bin_size\n        end = data.shape[1] - i * bin_size\n\n        # If the bin is not full, discard the leading elements\n        if end < 0:\n            end = 0\n\n        # Calculate the mean of the bin and append it to the list\n        bin_data_mean.append(np.mean(data[:, start:end]))\n\n    # Convert the list to a NumPy array and return it\n    return np.array(bin_data_mean)\n\ndata = np.array([[4, 2, 5, 6, 7], [5, 4, 3, 5, 7]])\nbin_size = 3\nbin_data_mean = bin_data(data, bin_size)\nprint(bin_data_mean)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 419, "library_problem_id": 128, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 420, "code": ["<code>\nimport numpy as np\n\ndef smoothclamp(x, x_min, x_max):\n    if x < x_min:\n        return x_min\n    elif x > x_max:\n        return x_max\n    else:\n        return x\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 420, "library_problem_id": 129, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 421, "code": ["<code>\nimport numpy as np\n\ndef smoothclamp(x, x_min, x_max, N):\n    t = np.clip((x - x_min)/(x_max - x_min), 0.0, 1.0)\n    return x_min + (x_max - x_min) * (6*t**5 - 15*t**4 + 10*t**3)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 421, "library_problem_id": 130, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 129}}
{"id": 422, "code": ["<code>\nimport numpy as np\n\ndef circular_crosscorr(a, b):\n    # Perform circular shift of array a\n    a_shifted = np.roll(a, len(a)//2)\n    \n    # Perform cross-correlation\n    crosscorr = np.correlate(a_shifted, b, mode='full')\n    \n    return crosscorr\n\na = np.array([1,2,3,4])\nb = np.array([5, 4, 3, 2])\n\nresult = circular_crosscorr(a, b)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 422, "library_problem_id": 131, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 423, "code": ["<code>\nSure, here is the completed function:\n\n```python\nimport numpy as np\nimport pandas as pd\n\nnames = ['One', 'Two', 'Three', 'Four', 'Five', 'Six', 'Seven', 'Eight', 'Nine', 'Ten', 'Eleven', 'Twelve', 'Thirteen', 'Fourteen', 'Fifteen']\ntimes = [pd.Timestamp('2019-01-22 18:12:00'), pd.Timestamp('2019-01-22 18:13:00'), pd.Timestamp('2019-01-22 18:14:00'), pd.Timestamp('2019-01-22 18:15:00'), pd.Timestamp('2019-01-22 18:16:00')]\n\ndf = pd.DataFrame(np.random.randint(10, size=(15*5, 4)), index=pd.MultiIndex.from_product([names, times], names=['major','timestamp']), columns=list('colu'))\n\ndef convert_to_3d(df):\n    # Get unique categories and time index\n    categories = df.index.get_level_values('major').unique()\n    time_index = df.index.get_level_values('timestamp').unique()\n\n    # Initialize 3D numpy array\n    result = np.zeros((len(df.columns), len(categories), len(time_index)))\n\n    # Fill the 3D\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 423, "library_problem_id": 132, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 424, "code": ["<code>\nSure, here is the completed function:\n\n```python\nimport numpy as np\nimport pandas as pd\n\nnames = ['One', 'Two', 'Three', 'Four', 'Five', 'Six', 'Seven', 'Eight', 'Nine', 'Ten', 'Eleven', 'Twelve', 'Thirteen', 'Fourteen', 'Fifteen']\ntimes = [pd.Timestamp('2019-01-22 18:12:00'), pd.Timestamp('2019-01-22 18:13:00'), pd.Timestamp('2019-01-22 18:14:00'), pd.Timestamp('2019-01-22 18:15:00'), pd.Timestamp('2019-01-22 18:16:00')]\ndf = pd.DataFrame(np.random.randint(10, size=(15*5, 4)), index=pd.MultiIndex.from_product([names, times], names=['major','timestamp']), columns=list('abcd'))\n\ndef convert_to_3d(df):\n    # Get unique categories in 'major' index\n    majors = df.index.levels[0].values\n    # Get number of unique categories\n    num_majors = len(majors)\n    # Get number of columns in DataFrame\n    num_cols = df.shape[1]\n    # Get length of time index\n    num_times = len(df.index.levels[1])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 424, "library_problem_id": 133, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 132}}
{"id": 425, "code": ["<code>\nimport numpy as np\n\ndef convert_to_binary(a, m):\n    max_val = 2**m - 1\n    result = np.zeros((len(a), m), dtype=int)\n    for i in range(len(a)):\n        if a[i] > max_val:\n            raise ValueError(f\"Value {a[i]} is out of range for m={m}\")\n        binary = format(a[i], f\"0{m}b\")\n        result[i] = [int(b) for b in binary]\n    return result\n\na = np.array([1, 2, 3, 4, 5])\nm = 3\nresult = convert_to_binary(a, m)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 425, "library_problem_id": 134, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 426, "code": ["<code>\nimport numpy as np\n\ndef convert_to_binary(arr, m):\n    # Initialize an empty list to store the binary representations\n    binary_list = []\n    \n    # Iterate over each number in the array\n    for num in arr:\n        # Convert the number to binary and remove the '0b' prefix\n        binary = bin(num)[2:]\n        \n        # If the number is negative, pad with zeros to the left\n        if num < 0:\n            binary = '0' * (m - len(binary) - 1) + '1' + binary\n        else:\n            # If the number is positive, pad with zeros to the left\n            binary = '0' * (m - len(binary)) + binary\n        \n        # Append the binary representation to the list\n        binary_list.append(binary)\n    \n    # Convert the list to a NumPy array and reshape it to (n, m)\n    result = np.array(binary_list, dtype=int).reshape(-1, m)\n    \n    return result\n\na = np.array([1, 2, 3, 4, 5])\nm = 6\nresult = convert_to_binary(a, m)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 426, "library_problem_id": 135, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 427, "code": ["<code>\nimport numpy as np\n\ndef convert_and_xor(a, m):\n    # Convert each element to its binary representation\n    binary_arrays = [list(np.binary_repr(i, width=m)) for i in a]\n    \n    # Convert binary arrays to numpy arrays and compute XOR\n    result = np.zeros(m, dtype=int)\n    for binary_array in binary_arrays:\n        result ^= np.array(binary_array, dtype=int)\n    \n    return result\n\na = np.array([1, 2, 3, 4, 5])\nm = 6\nresult = convert_and_xor(a, m)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 427, "library_problem_id": 136, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 428, "code": ["<code>\nimport numpy as np\n\ndef calculate_interval(a):\n    mean = np.mean(a)\n    std_dev = np.std(a)\n    start = mean - 3*std_dev\n    end = mean + 3*std_dev\n    return (start, end)\n\na = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\nresult = calculate_interval(a)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 428, "library_problem_id": 137, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 429, "code": ["<code>\nimport numpy as np\n\ndef calculate_stats(a):\n    mean = np.mean(a)\n    std_dev = np.std(a)\n    interval = (mean - 2*std_dev, mean + 2*std_dev)\n    return mean, std_dev, interval\n\na = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\n\nmean, std_dev, interval = calculate_stats(a)\n\nprint(f\"Mean: {mean}\")\nprint(f\"Standard Deviation: {std_dev}\")\nprint(f\"Interval: {interval}\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 429, "library_problem_id": 138, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 137}}
{"id": 430, "code": ["<code>\nimport numpy as np\n\nexample_a = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\n\ndef f(a = example_a):\n    # calculate mean and standard deviation\n    mu = np.mean(a)\n    sigma = np.std(a)\n\n    # calculate the interval\n    interval = (mu - 3*sigma, mu + 3*sigma)\n\n    return interval\n\n# test the function\nprint(f(example_a))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 430, "library_problem_id": 139, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 137}}
{"id": 431, "code": ["<code>\nimport numpy as np\n\ndef find_outliers(a):\n    mean = np.mean(a)\n    std_dev = np.std(a)\n    lower_bound = mean - 2 * std_dev\n    upper_bound = mean + 2 * std_dev\n    return (a < lower_bound) | (a > upper_bound)\n\na = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\nresult = find_outliers(a)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 431, "library_problem_id": 140, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 137}}
{"id": 432, "code": ["<code>\nimport numpy as np\nfrom numpy.ma import masked_less, masked_equal\n\nDataArray = np.arange(-5.5, 10.5)\nDataArray = masked_less(DataArray, 0)  # masking negative values\nDataArray = masked_equal(DataArray, -3.40282347e+38)  # masking NoData values\n\npercentile = 50\nprob = np.percentile(DataArray.compressed(), percentile)\nprint(prob)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 432, "library_problem_id": 141, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 433, "code": ["<code>\nimport numpy as np\n\ndef zero_row_col(a, zero_rows, zero_cols):\n    a[zero_rows, :] = 0\n    a[:, zero_cols] = 0\n    return a\n\na = np.array([[0, 3, 1, 3], [3, 0, 0, 0], [1, 0, 0, 0], [3, 0, 0, 0]])\nzero_rows = 0\nzero_cols = 0\na = zero_row_col(a, zero_rows, zero_cols)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 433, "library_problem_id": 142, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 142}}
{"id": 434, "code": ["<code>\nimport numpy as np\n\ndef zero_out_rows_cols(a, zero_rows, zero_cols):\n    a[zero_rows, :] = 0\n    a[:, zero_cols] = 0\n    return a\n\na = np.array([[0, 3, 1, 3], [3, 0, 0, 0], [1, 0, 0, 0], [3, 0, 0, 0]])\nzero_rows = [1, 3]\nzero_cols = [1, 2]\n\na = zero_out_rows_cols(a, zero_rows, zero_cols)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 434, "library_problem_id": 143, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 435, "code": ["<code>\nimport numpy as np\n\na = np.array([[0, 3, 1, 3], [3, 0, 0, 0], [1, 0, 0, 0], [3, 0, 0, 0]])\n\na[1:, 0] = 0\na[0, :] = 0\n\nprint(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 435, "library_problem_id": 144, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 436, "code": ["<code>\nimport numpy as np\n\na = np.array([[0, 1], [2, 1], [4, 8]])\n\nmask = np.equal.outer(a, a.max(axis=1)[:, None])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 436, "library_problem_id": 145, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 145}}
{"id": 437, "code": ["<code>\nimport numpy as np\n\na = np.array([[0, 1], [2, 1], [4, 8]])\naxis = 1  # specify the axis\n\nmask = np.zeros_like(a, dtype=bool)  # create a mask with the same shape as a and all values as False\nmask[np.arange(a.shape[0])[:, None], a.argmin(axis=axis)] = True  # set the minimum value along the specified axis as True\n\nprint(mask)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 437, "library_problem_id": 146, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 145}}
{"id": 438, "code": ["<code>\nimport numpy as np\nfrom scipy.stats import pearsonr\n\npost = [2, 5, 6, 10]\ndistance = [50, 100, 500, 1000]\n\n# Convert the distance to upper bounds\ndistance_bounds = [50, 100, 500, 1000]\n\n# Calculate the Pearson correlation coefficient\nresult, _ = pearsonr(post, distance_bounds)\n\nprint('Pearson correlation coefficient: ', result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 438, "library_problem_id": 147, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 439, "code": ["<code>\nimport numpy as np\n\nX = np.random.randint(2, 10, (5, 6))\n\nresult = np.array([np.dot(x.reshape(-1, 1), x.reshape(1, -1)) for x in X.T]).reshape(X.shape[1], X.shape[0], X.shape[0])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 439, "library_problem_id": 148, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 440, "code": ["<code>\nimport numpy as np\n\nY = np.array([[[81, 63, 63],\n        [63, 49, 49],\n        [63, 49, 49]],\n\n       [[ 4, 12,  8],\n        [12, 36, 24],\n        [ 8, 24, 16]],\n\n       [[25, 35, 25],\n        [35, 49, 35],\n        [25, 35, 25]],\n\n       [[25, 30, 10],\n        [30, 36, 12],\n        [10, 12,  4]]])\n\nX = np.zeros((Y.shape[1], Y.shape[0]))\n\nfor i in range(Y.shape[0]):\n    X[:, i] = np.sqrt(Y[i, 0, 0] - Y[i, 1, 1]) * np.array([1, -Y[i, 0, 1] / np.sqrt(Y[i, 0, 0] - Y[i, 1, 1]), -Y[i, 0, 2] / np.sqrt(Y[i, 0, 0] - Y[i, 1, 1)])\n\nX = X.T\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 440, "library_problem_id": 149, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 441, "code": ["<code>\nimport numpy as np\n\na = np.array([9, 2, 7, 0])\nnumber = 0\n\nis_contained = number in a\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 441, "library_problem_id": 150, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 442, "code": ["<code>\nimport numpy as np\n\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,2,8])\n\nC = np.setdiff1d(A, B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 442, "library_problem_id": 151, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 443, "code": ["<code>\nimport numpy as np\n\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,2,8])\n\nC = np.intersect1d(A, B)\nprint(C)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 443, "library_problem_id": 152, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 444, "code": ["<code>\nimport numpy as np\n\ndef find_elements_in_intervals(A, B):\n    C = []\n    for i in range(len(B) - 1):\n        C.extend(list(A[(A >= B[i]) & (A <= B[i+1])]))\n    return np.array(C)\n\nA = np.array([1,1,2,3,3,3,4,5,6,7,8,8])\nB = np.array([1,4,8])\n\nC = find_elements_in_intervals(A, B)\nprint(C)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 444, "library_problem_id": 153, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 445, "code": ["<code>\nimport numpy as np\nfrom scipy.stats import rankdata\n\ndef reverse_rankdata(a):\n    return len(a) - rankdata(a) + 1\n\na = [1,2,3,4,3,2,3,4]\nresult = reverse_rankdata(a)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 445, "library_problem_id": 154, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 446, "code": ["<code>\nimport numpy as np\nfrom scipy.stats import rankdata\n\ndef generate_ranking(a):\n    # Use the rankdata function from scipy.stats to generate the ranks\n    ranks = rankdata(a)\n    # Since we want the highest value to get rank 0, we subtract the ranks from the length of the array\n    ranks = len(a) - ranks\n    return ranks\n\na = [1,2,3,4,3,2,3,4]\nresult = generate_ranking(a)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 446, "library_problem_id": 155, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 154}}
{"id": 447, "code": ["<code>\nimport numpy as np\nfrom scipy.stats import rankdata\n\nexample_a = [1,2,3,4,3,2,3,4]\n\ndef f(a = example_a):\n    # calculate the rank of the elements in the list\n    ranks = rankdata(a)\n    # subtract the rank from the length of the list to get the reverse rank\n    reverse_ranks = len(a) - ranks + 1\n    return reverse_ranks\n\nprint(f(example_a))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 447, "library_problem_id": 156, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 154}}
{"id": 448, "code": ["<code>\nimport numpy as np\n\nx_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\n\ny_dists = np.array([[ 0, 1, -2],\n                 [ -1,  0, 1],\n                 [ -2,  1,  0]])\n\ndists = np.dstack((x_dists, y_dists))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 448, "library_problem_id": 157, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 449, "code": ["<code>\nimport numpy as np\n\nx_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\n\ny_dists = np.array([[ 0, -1, -2],\n                 [ 1,  0, -1],\n                 [ 2,  1,  0]])\n\ndists = np.dstack((x_dists, y_dists))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 449, "library_problem_id": 158, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 157}}
{"id": 450, "code": ["<code>\nimport numpy as np\n\na = np.random.rand(5, 5, 5)\nsecond = [1, 2]\nthird = [3, 4]\n\nresult = a[np.ix_(range(a.shape[0]), second, third)]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 450, "library_problem_id": 159, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 159}}
{"id": 451, "code": ["<code>\nimport numpy as np\n\narr = np.zeros((20, 10, 10, 2))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 451, "library_problem_id": 160, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 452, "code": ["<code>\nfrom numpy import linalg as LA\nimport numpy as np\n\nX = np.array([[1, -2, 3, 6],\n              [4, 5, -6, 5],\n              [-1, 2, 5, 5],\n              [4, 5,10,-25],\n              [5, -2,10,25]])\n\n# Calculate the L1 norm of each row\nrow_norms = LA.norm(X, 1, axis=1)\n\n# Repeat the L1 norms for each column in the row\nrow_norms = np.repeat(row_norms[:, np.newaxis], X.shape[1], axis=1)\n\n# Divide each element of X by the corresponding row norm\nresult = X / row_norms\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 452, "library_problem_id": 161, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 161}}
{"id": 453, "code": ["<code>\nfrom numpy import linalg as LA\nimport numpy as np\n\nX = np.array([[1, -2, 3, 6],\n              [4, 5, -6, 5],\n              [-1, 2, 5, 5],\n              [4, 5,10,-25],\n              [5, -2,10,25]])\n\nresult = X / LA.norm(X, axis=1, keepdims=True)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 453, "library_problem_id": 162, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 454, "code": ["<code>\nfrom numpy import linalg as LA\nimport numpy as np\n\nX = np.array([[1, -2, 3, 6],\n              [4, 5, -6, 5],\n              [-1, 2, 5, 5],\n              [4, 5,10,-25],\n              [5, -2,10,25]])\n\nresult = X / np.abs(X).max(axis=1)[:, np.newaxis]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 454, "library_problem_id": 163, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 455, "code": ["<code>\nimport numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame({'properties_path': ['/a/b/c', 'd/e/f', 'g/h/i']})\ntarget = 'f'\nchoices = ['XX']\n\nconditions = [df['properties_path'].str.contains(target)]\nresult = np.select(conditions, choices, default='YY')\n\ndf['new_column'] = result\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 455, "library_problem_id": 164, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 164}}
{"id": 456, "code": ["<code>\nimport numpy as np\nfrom scipy.spatial import distance_matrix\n\na = np.array([[1,2,8],\n     [7,4,2],\n     [9,1,7],\n     [0,1,5],\n     [6,4,3]])\n\nresult = distance_matrix(a, a)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 456, "library_problem_id": 165, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 457, "code": ["<code>\nimport numpy as np\nfrom scipy.spatial import distance_matrix\n\ndim = np.random.randint(4, 8)\na = np.random.rand(np.random.randint(5, 10),dim)\n\nresult = distance_matrix(a, a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 457, "library_problem_id": 166, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 165}}
{"id": 458, "code": ["<code>\nimport numpy as np\nfrom scipy.spatial import distance_matrix\n\ndim = np.random.randint(4, 8)\na = np.random.rand(np.random.randint(5, 10),dim)\n\nresult = distance_matrix(a, a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 458, "library_problem_id": 167, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 165}}
{"id": 459, "code": ["<code>\nimport numpy as np\n\nA = ['33.33', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\n\nAVG = np.mean(np.array(NA, dtype=float))\nprint(AVG)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 459, "library_problem_id": 168, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 168}}
{"id": 460, "code": ["<code>\nimport numpy as np\n\nA = ['inf', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\n\n# Convert the array to float, replacing 'inf' with np.inf\nNA = np.where(NA == 'inf', np.inf, NA.astype(float))\n\n# Calculate the mean\nAVG = np.mean(NA)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 460, "library_problem_id": 169, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 168}}
{"id": 461, "code": ["<code>\nimport numpy as np\n\nA = ['np.inf', '33.33', '33.33', '33.37']\nNA = np.asarray(A)\n\ndef calculate_mean(arr):\n    numerical = []\n    for i in arr:\n        if i == 'np.inf':\n            numerical.append(float(i))\n        else:\n            numerical.append(float(i))\n    return np.mean(numerical)\n\nAVG = calculate_mean(NA)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 461, "library_problem_id": 170, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 168}}
{"id": 462, "code": ["<code>\nimport numpy as np\n\ndef remove_adjacent_duplicates(a):\n    # Create a mask where the condition is True if the current element is not equal to the next element\n    mask = a[:-1] != a[1:]\n    # Apply the mask to the array and remove the zero values\n    result = a[mask & (a!=0)]\n    return result\n\na = np.array([0, 0, 1, 1, 1, 2, 2, 0, 1, 3, 3, 3])\nresult = remove_adjacent_duplicates(a)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 462, "library_problem_id": 171, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 171}}
{"id": 463, "code": ["<code>\nimport numpy as np\n\ndef remove_consecutive_duplicates(a):\n    # Remove consecutive duplicates\n    a = np.where(a[:-1] != a[1:], a, 0)\n    # Remove all zero values\n    a = a[a!=0]\n    return a\n\na = np.array([0, 0, 1, 1, 1, 2, 2, 0, 1, 3, 3, 3]).reshape(-1, 1)\nresult = remove_consecutive_duplicates(a)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 463, "library_problem_id": 172, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 171}}
{"id": 464, "code": ["<code>\nimport numpy as np\nimport pandas as pd\n\nlat=np.array([[10, 20, 30],\n              [20, 11, 33],\n              [21, 20, 10]])\n\nlon=np.array([[100, 102, 103],\n              [105, 101, 102],\n              [100, 102, 103]])\n\nval=np.array([[17, 2, 11],\n              [86, 84, 1],\n              [9, 5, 10]])\n\ndf = pd.DataFrame({'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 464, "library_problem_id": 173, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 465, "code": ["<code>\ndef f(lat = example_lat, lon = example_lon, val = example_val):\n    # Create a dictionary with the arrays\n    data = {'lat': lat.flatten(), 'lon': lon.flatten(), 'val': val.flatten()}\n    # Create a DataFrame from the dictionary\n    df = pd.DataFrame(data)\n    return df\n\n# Test the function\ndf = f(example_lat, example_lon, example_val)\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 465, "library_problem_id": 174, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 173}}
{"id": 466, "code": ["<code>\nimport numpy as np\nimport pandas as pd\n\nlat=np.array([[10, 20, 30],\n              [20, 11, 33],\n              [21, 20, 10]])\n\nlon=np.array([[100, 102, 103],\n              [105, 101, 102],\n              [100, 102, 103]])\n\nval=np.array([[17, 2, 11],\n              [86, 84, 1],\n              [9, 5, 10]])\n\n# Create a Pandas DataFrame from the numpy arrays\ndf = pd.DataFrame({'latitude': lat.flatten(), 'longitude': lon.flatten(), 'value': val.flatten()})\n\n# Add a new column containing the maximum value of each row\ndf['max_value'] = df.max(axis=1)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 466, "library_problem_id": 175, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 173}}
{"id": 467, "code": ["<code>\nimport numpy as np\n\ndef moving_window(a, size):\n    # Get the shape of the input array\n    shape = a.shape\n\n    # Calculate the output shape\n    output_shape = (shape[0] - size[0] + 1, shape[1] - size[1] + 1)\n\n    # Create an empty array to store the output\n    result = np.empty(output_shape)\n\n    # Perform the moving window operation\n    for i in range(output_shape[0]):\n        for j in range(output_shape[1]):\n            result[i, j] = a[i:i+size[0], j:j+size[1]]\n\n    return result\n\na = np.array([[1,2,3,4],\n       [2,3,4,5],\n       [3,4,5,6],\n       [4,5,6,7]])\nsize = (3, 3)\n\nresult = moving_window(a, size)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 467, "library_problem_id": 176, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 176}}
{"id": 468, "code": ["<code>\nimport numpy as np\n\ndef moving_window(a, size):\n    pad_width = [(int((size[0]-1)/2), int(size[0]/2)) if a.shape[0] > size[0] else (0,0) for a in [a]*2]\n    a = np.pad(a, pad_width, mode='constant')\n    output = np.zeros((a.shape[0]-size[0]+1, a.shape[1]-size[1]+1, size[0], size[1]))\n\n    for i in range(output.shape[0]):\n        for j in range(output.shape[1]):\n            output[i, j] = a[i:i+size[0], j:j+size[1]]\n\n    return output\n\na = np.array([[1,2,3,4],\n       [2,3,4,5],\n       [3,4,5,6],\n       [4,5,6,7]])\nsize = (3, 3)\n\nresult = moving_window(a, size)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 468, "library_problem_id": 177, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 176}}
{"id": 469, "code": ["<code>\nimport numpy as np\n\ndef compute_mean_complex(arr):\n    # Create a mask to filter out complex infinities\n    mask = np.isinf(arr.real) & np.isinf(arr.imag)\n    # Replace complex infinities with NaNs\n    arr[mask] = np.nan\n    # Compute the mean ignoring NaNs\n    mean = np.nanmean(arr, dtype=np.complex)\n    return mean\n\na = np.array([1 + 0j, 2 + 0j, np.inf + 0j])\nresult = compute_mean_complex(a)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 469, "library_problem_id": 178, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 178}}
{"id": 470, "code": ["<code>\nimport numpy as np\n\ndef f(a = np.array([1 + 0j, 2 + 3j, np.inf + 0j])):\n    # Remove the complex infinity values from the array\n    a = a[np.isfinite(a)]\n\n    # Compute the mean of the array\n    mean = np.mean(a)\n\n    return mean\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 470, "library_problem_id": 179, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 178}}
{"id": 471, "code": ["<code>\nimport numpy as np\nZ = np.random.rand(*np.random.randint(2, 10, (np.random.randint(2, 10))))\n\nresult = Z.flat[-1]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 471, "library_problem_id": 180, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 180}}
{"id": 472, "code": ["<code>\nimport numpy as np\n\n# Generate a random array with arbitrary dimensions\na = np.random.rand(*np.random.randint(2, 10, (np.random.randint(2, 10))))\n\n# Use -1 to select the last element along the first axis\nresult = a[-1]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 472, "library_problem_id": 181, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 180}}
{"id": 473, "code": ["<code>\nSure, here is the solution:\n\n```python\nimport numpy as np\n\nc = np.array([[[ 75, 763]],\n              [[ 57, 763]],\n              [[ 57, 749]],\n              [[ 75, 749]]])\n\nCNTS = [np.array([[[  78, 1202]],\n                  [[  63, 1202]],\n                  [[  63, 1187]],\n                  [[  78, 1187]]]),\n        np.array([[[ 75, 763]],\n                  [[ 57, 763]],\n                  [[ 57, 749]],\n                  [[ 75, 749]]]),\n        np.array([[[ 72, 742]],\n                  [[ 58, 742]],\n                  [[ 57, 741]],\n                  [[ 57, 727]],\n                  [[ 58, 726]],\n                  [[ 72, 726]]]),\n        np.array([[[ 66, 194]],\n                  [[ 51, 194]],\n                  [[ 51, 179]],\n                  [[ 66, 179]]])]\n\n# Convert the arrays in CNTS to strings\nCNTS_str = [str(arr) for arr in CNTS]\n\n# Convert the array c to a string\nc_str = str(\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 473, "library_problem_id": 182, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 182}}
{"id": 474, "code": ["<code>\nHere is the completed function:\n\n```python\nimport numpy as np\n\ndef is_array_in_list(array, list_of_arrays):\n    # Replace NaN values with a unique value that won't be in the arrays\n    array = np.nan_to_num(array, nan=999999)\n    for arr in list_of_arrays:\n        # Replace NaN values in the array with a unique value\n        arr = np.nan_to_num(arr, nan=999999)\n        # Compare the arrays\n        if np.array_equal(array, arr):\n            return True\n    return False\n\nc = np.array([[[ 75, 763]],\n              [[ 57, 763]],\n              [[ np.nan, 749]],\n              [[ 75, 749]]])\n\nCNTS = [np.array([[[  np.nan, 1202]],\n                  [[  63, 1202]],\n                  [[  63, 1187]],\n                  [[  78, 1187]]]),\n        np.array([[[ 75, 763]],\n                  [[ 57, 763]],\n                  [[ np.nan, 749]],\n                  [[ 75, 749]]]),\n        np.array([[[ 72, 742]],\n                  [[ 58, 742]],\n                  [[ 57, 741]],\n                  [[\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 474, "library_problem_id": 183, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 182}}
{"id": 475, "code": ["<code>\nimport numpy as np\nfrom scipy import interpolate as intp\n\na = np.arange(0, 4, 1).reshape(2, 2)\na = a.repeat(2, axis=0).repeat(2, axis=1)\nx_new = np.linspace(0, 2, 4)\ny_new = np.linspace(0, 2, 4)\n\n# Create a 2D interpolation function\nf = intp.interp2d(np.arange(a.shape[1]), np.arange(a.shape[0]), a, kind='linear')\n\n# Use the function to upsample the array\nresult = f(x_new, y_new)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 475, "library_problem_id": 184, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 184}}
{"id": 476, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndata = {'D':[2015,2015,2015,2015,2016,2016,2016,2017,2017,2017], 'Q':np.arange(10)}\ndf = pd.DataFrame(data)\n\ndf['Q_cum'] = df.groupby('D')['Q'].cumsum()\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 476, "library_problem_id": 185, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 477, "code": ["<code>\nimport numpy as np\n\n# Given matrix\na = np.matrix([[3, 4, 3, 1],[1,3,2,6],[2,4,1,5],[3,3,5,2]])\n\n# Perform SVD\nU, s, V = np.linalg.svd(a,full_matrices=True)\n\n# Convert singular values to diagonal matrix\ni = np.zeros((len(s), len(s)))\nnp.fill_diagonal(i, s)\n\nprint(i)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 477, "library_problem_id": 186, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 186}}
{"id": 478, "code": ["<code>\nimport numpy as np\n\na = np.array([[1,0,2,3],[0,5,3,4],[2,3,2,10],[3,4, 10, 7]])\n\na = np.triu(a) + np.tril(a, -1).T\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 478, "library_problem_id": 187, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 187}}
{"id": 479, "code": ["<code>\nimport numpy as np\nimport pandas as pd\n\nstart = \"23-FEB-2015 23:09:19.445506\"\nend = \"24-FEB-2015 01:09:22.404973\"\nn = 50\n\nstart = pd.to_datetime(start)\nend = pd.to_datetime(end)\n\nresult = np.linspace(start.value//10**9, end.value//10**9, num=n)\nresult = pd.to_datetime(result, unit='s')\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 479, "library_problem_id": 188, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 480, "code": ["<code>\nimport numpy as np\n\ndef find_index(x, y, a, b):\n    for i in range(len(x)):\n        if x[i] == a and y[i] == b:\n            return i\n    return -1\n\nx = np.array([0, 1, 1, 1, 3, 1, 5, 5, 5])\ny = np.array([0, 2, 3, 4, 2, 4, 3, 4, 5])\na = 1\nb = 4\n\nresult = find_index(x, y, a, b)\nprint(result)  # Output: -1\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 480, "library_problem_id": 189, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 189}}
{"id": 481, "code": ["<code>\nimport numpy as np\n\ndef find_indices(x, y, a, b):\n    return np.where((x == a) & (y == b))[0]\n\nx = np.array([0, 1, 1, 1, 3, 1, 5, 5, 5])\ny = np.array([0, 2, 3, 4, 2, 4, 3, 4, 5])\na = 1\nb = 4\n\nresult = find_indices(x, y, a, b)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 481, "library_problem_id": 190, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 189}}
{"id": 482, "code": ["<code>\nimport numpy as np\n\ndef find_quadratic_coefficients(x, y):\n    # Convert lists to numpy arrays\n    x = np.array(x)\n    y = np.array(y)\n\n    # Create the matrix A using the given x values\n    A = np.vstack([x**2, x, np.ones(len(x))]).T\n\n    # Use the numpy.linalg.lstsq function to find the coefficients\n    coeff, _, _, _ = np.linalg.lstsq(A, y, rcond=None)\n\n    return coeff\n\nx = [-1, 2, 5, 100]\ny = [123, 456, 789, 1255]\n\nresult = find_quadratic_coefficients(x, y)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 482, "library_problem_id": 191, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 191}}
{"id": 483, "code": ["<code>\nimport numpy as np\n\ndef fit_polynomial(x, y, degree):\n    matrix = np.zeros((len(x), degree+1))\n    for i in range(degree+1):\n        matrix[:,i] = np.power(x, i)\n    return np.linalg.lstsq(matrix, y, rcond=None)[0]\n\nx = [-1, 2, 5, 100]\ny = [123, 456, 789, 1255]\ndegree = 3\n\nresult = fit_polynomial(x, y, degree)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 483, "library_problem_id": 192, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 191}}
{"id": 484, "code": ["<code>\nimport numpy as np\nimport pandas as pd\n\na = np.arange(4)\ndf = pd.DataFrame(np.repeat([1, 2, 3, 4], 4).reshape(4, -1))\n\ndf = df.sub(a, axis=0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 484, "library_problem_id": 193, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 193}}
{"id": 485, "code": ["<code>\nimport numpy as np\n\ndef elementwise_multiply(A, B):\n    # Reshape A to (x*y, 3)\n    A = A.reshape(-1, 3)\n    # Perform element-wise multiplication\n    result = np.einsum('ij,aj->ai', B, A)\n    # Reshape result back to (x, y, 3)\n    result = result.reshape(A.shape[0], 3)\n    return result\n\nA = np.random.rand(5, 6, 3)\nB = np.random.rand(3, 3)\n\nresult = elementwise_multiply(A, B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 485, "library_problem_id": 194, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 486, "code": ["<code>\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\na = np.array([[-1, 2], [-0.5, 6]])\n\n# reshape the array to 1D\na_reshaped = a.reshape(-1, 1)\n\n# initialize MinMaxScaler\nscaler = MinMaxScaler()\n\n# fit and transform the reshaped array\nresult = scaler.fit_transform(a_reshaped)\n\n# reshape the result back to 2D\nresult = result.reshape(a.shape)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 486, "library_problem_id": 195, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 195}}
{"id": 487, "code": ["<code>\nimport numpy as np\n\ndef rescale_array(arr):\n    # Compute the minimum and maximum values for each row\n    row_min = np.min(arr, axis=1)[:, np.newaxis]\n    row_max = np.max(arr, axis=1)[:, np.newaxis]\n\n    # Subtract the minimum and divide by the range to rescale the values\n    rescaled_arr = (arr - row_min) / (row_max - row_min)\n\n    return rescaled_arr\n\narr = np.array([[1.0,2.0,3.0],[0.1, 5.1, 100.1],[0.01, 20.1, 1000.1]])\nresult = rescale_array(arr)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 487, "library_problem_id": 196, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 488, "code": ["<code>\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\na = np.array([[[1, 0.5, -2], [-0.5,1, 6], [1,1,1]], [[-2, -3, 1], [-0.5, 10, 6], [1,1,1]]])\n\nscaler = MinMaxScaler()\n\nresult = np.array([scaler.fit_transform(matrix) for matrix in a])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 488, "library_problem_id": 197, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 489, "code": ["<code>\nimport numpy as np\n\narr = (np.random.rand(100, 50)-0.5) * 50\n\n# Create a copy of the array to avoid setting values in a view\narr_copy = arr.copy()\n\n# Boolean indexing\narr_copy[(arr_copy < -10)] = 0\narr_copy[(arr_copy >= 15)] = 30\narr_copy[(arr_copy >= -10) & (arr_copy < 15)] += 5\n\narr = arr_copy\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 489, "library_problem_id": 198, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 198}}
{"id": 490, "code": ["<code>\nimport numpy as np\n\narr = (np.random.rand(5, 50)-0.5) * 50\nn1 = [1,2,3,4,5]\nn2 = [6,7,8,9,10]\n\nfor i in range(arr.shape[0]):\n    arr[i, (arr[i] < n1[i])] = 0\n    arr[i, (arr[i] >= n2[i])] += 30\n    arr[i, ((arr[i] >= n1[i]) & (arr[i] < n2[i]))] += 5\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 490, "library_problem_id": 199, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 198}}
{"id": 491, "code": ["<code>\nimport numpy as np\n\nn = 20\nm = 10\ntag = np.random.rand(n, m)\ns1 = np.sum(tag, axis=1)\ns2 = np.sum(tag[:, ::-1], axis=1)\n\n# We can use np.isclose() to compare the two sums. This function returns a boolean array where two numbers are considered equal if they are close within a tolerance.\n# Here, we set the relative tolerance to a small value (1e-9) to account for the limitations of floating-point arithmetic.\nresult = np.sum(~np.isclose(s1, s2, rtol=1e-9))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 491, "library_problem_id": 200, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 200}}
{"id": 492, "code": ["<code>\nimport numpy as np\n\ndef compare_arrays(n, m):\n    tag = np.random.rand(n, m)\n    s1 = np.sum(tag, axis=1)\n    s2 = np.sum(tag[:, ::-1], axis=1)\n    s1 = np.append(s1, np.nan)\n    s2 = np.append(s2, np.nan)\n\n    # Use np.isclose to compare the arrays, considering NaN values as equal\n    result = np.sum(~np.isclose(s1, s2))\n\n    return result\n\n# Test the function\nn = 20\nm = 10\nprint(compare_arrays(n, m))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 492, "library_problem_id": 201, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 200}}
{"id": 493, "code": ["<code>\nimport numpy as np\n\ndef are_all_equal(arrays):\n    first = arrays[0]\n    return all(np.array_equal(first, array) for array in arrays)\n\na = [np.array([1,2,3]),np.array([1,2,3]),np.array([1,2,3])]\nresult = are_all_equal(a)\nprint(result)  # This should print: True\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 493, "library_problem_id": 202, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 494, "code": ["<code>\nimport numpy as np\n\na = [np.array([np.nan,2,3]),np.array([1,np.nan,3]),np.array([1,2,np.nan])]\n\nresult = all(np.isnan(arr).all() for arr in a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 494, "library_problem_id": 203, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 495, "code": ["<code>\nimport numpy as np\n\ndef zero_pad(a, shape):\n    result = np.zeros(shape)\n    result[:a.shape[0], :a.shape[1]] = a\n    return result\n\na = np.ones((41, 13))\nshape = (93, 13)\nresult = zero_pad(a, shape)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 495, "library_problem_id": 204, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 204}}
{"id": 496, "code": ["<code>\nimport numpy as np\n\na = np.ones((41, 12))\nshape = (93, 13)\n\n# Calculate the padding values\npad_width = [(0, shape[0] - a.shape[0]), (0, shape[1] - a.shape[1])]\n\n# Pad the array\nresult = np.pad(a, pad_width)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 496, "library_problem_id": 205, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 497, "code": ["<code>\nimport numpy as np\n\ndef pad_array(a, shape, element):\n    # Create a new array with the target shape filled with the specified element\n    result = np.full(shape, element)\n    \n    # Copy the original array into the center of the new array\n    result[0:a.shape[0], 0:a.shape[1]] = a\n    \n    return result\n\na = np.ones((41, 12))\nshape = (93, 13)\nelement = 5\n\nresult = pad_array(a, shape, element)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 497, "library_problem_id": 206, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 204}}
{"id": 498, "code": ["<code>\nimport numpy as np\n\nexample_arr = np.ones((41, 13))\n\ndef f(arr = example_arr, shape=(93,13)):\n    # Calculate the difference between the target shape and the current shape\n    diff_rows = shape[0] - arr.shape[0]\n    diff_cols = shape[1] - arr.shape[1]\n\n    # Pad the array with zeros\n    result = np.pad(arr, ((0, diff_rows), (0, diff_cols)))\n\n    return result\n\n# Test the function\nresult = f(example_arr, shape=(93,13))\nprint(result.shape)  # Should print (93, 13)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 498, "library_problem_id": 207, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 499, "code": ["<code>\nimport numpy as np\n\ndef pad_array(a, shape):\n    # Calculate the difference in shape\n    diff_shape = tuple(np.subtract(shape, a.shape))\n\n    # Calculate the padding for each dimension\n    pad_width = tuple((diff_shape[i]//2, diff_shape[i] - diff_shape[i]//2) for i in range(len(diff_shape)))\n\n    # Pad the array\n    result = np.pad(a, pad_width)\n\n    return result\n\na = np.ones((41, 12))\nshape = (93, 13)\nresult = pad_array(a, shape)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 499, "library_problem_id": 208, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 204}}
{"id": 500, "code": ["<code>\nimport numpy as np\n\na = np.arange(12)\na = a.reshape(-1, 3)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 500, "library_problem_id": 209, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 501, "code": ["<code>\nimport numpy as np\n\na = np.array( \n    [[[ 0,  1],\n     [ 2,  3],\n     [ 4,  5]],\n    [[ 6,  7],\n     [ 8,  9],\n     [10, 11]],\n    [[12, 13],\n     [14, 15],\n     [16, 17]]]\n)\n\nb = np.array( \n    [[0, 1, 1],\n    [1, 0, 1],\n    [1, 1, 0]]\n)\n\nresult = np.take_along_axis(a, np.expand_dims(b, axis=-1), axis=-1).squeeze(-1)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 501, "library_problem_id": 210, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 210}}
{"id": 502, "code": ["<code>\nimport numpy as np\n\na = np.array(\n    [[[ 0,  1],\n     [ 2,  3],\n     [ 4,  5]],\n    [[ 6,  7],\n     [ 8,  9],\n     [10, 11]],\n    [[12, 13],\n     [14, 15],\n     [16, 17]]]\n)\n\nb = np.array(\n    [[1, 1, 1],\n    [1, 1, 1],\n    [1, 1, 1]]\n)\n\nresult = np.where(b == 0, a[:,:,0], a[:,:,1])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 502, "library_problem_id": 211, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 210}}
{"id": 503, "code": ["<code>\nimport numpy as np\n\na = np.array( \n    [[[ 0,  1, 2, 3],\n     [ 2,  3, 4, 5],\n     [ 4,  5, 6, 7]],\n    [[ 6,  7, 8, 9],\n     [ 8,  9, 10, 11],\n     [10, 11, 12, 13]],\n    [[12, 13, 14, 15],\n     [14, 15, 16, 17],\n     [16, 17, 18, 19]]]\n)\n\nb = np.array( \n    [[0, 1, 2],\n    [2, 1, 3],\n[1, 0, 3]]\n)\n\nresult = np.take_along_axis(a, b[:, :, np.newaxis], axis=2).squeeze(-1)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 503, "library_problem_id": 212, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 210}}
{"id": 504, "code": ["<code>\nimport numpy as np\n\na = np.array( \n    [[[ 0,  1, 2, 3],\n     [ 2,  3, 4, 5],\n     [ 4,  5, 6, 7]],\n    [[ 6,  7, 8, 9],\n     [ 8,  9, 10, 11],\n     [10, 11, 12, 13]],\n    [[12, 13, 14, 15],\n     [14, 15, 16, 17],\n     [16, 17, 18, 19]]]\n)\n\nb = np.array( \n    [[0, 1, 2],\n    [2, 1, 3],\n[1, 0, 3]]\n)\n\nresult = np.sum(a[np.arange(a.shape[0])[:, None], np.arange(a.shape[1]), b])\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 504, "library_problem_id": 213, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 505, "code": ["<code>\nimport numpy as np\n\na = np.array( \n    [[[ 0,  1, 2, 3],\n     [ 2,  3, 4, 5],\n     [ 4,  5, 6, 7]],\n    [[ 6,  7, 8, 9],\n     [ 8,  9, 10, 11],\n     [10, 11, 12, 13]],\n    [[12, 13, 14, 15],\n     [14, 15, 16, 17],\n     [16, 17, 18, 19]]]\n)\n\nb = np.array( \n    [[0, 1, 2],\n    [2, 1, 3],\n[1, 0, 3]]\n)\n\nresult = np.sum(a[:,:,~np.isin(np.arange(a.shape[2]), b)])\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 505, "library_problem_id": 214, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 506, "code": ["<code>\nimport numpy as np\nimport pandas as pd\n\ndata = {'a': [1, 2, 3, 4, 5], 'b': [2, -6, 0, -4, 100]}\ndf = pd.DataFrame(data)\n\n# Create a mask for values in column 'a' that are between 1 and 4\nmask = (df['a'] > 1) & (df['a'] <= 4)\n\n# Use the mask to select values from column 'b' and replace other values with NaN\ndf.loc[mask, 'b'] = df.loc[mask, 'b']\ndf.loc[~mask, 'b'] = np.nan\n\nresult = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 506, "library_problem_id": 215, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 507, "code": ["<code>\nimport numpy as np\n\ndef remove_zeros(im):\n    while np.sum(im[0]) == 0:\n        im = np.delete(im, 0, 0)\n        if len(im) == 0:\n            return im\n\n    while np.sum(im[-1]) == 0:\n        im = np.delete(im, -1, 0)\n        if len(im) == 0:\n            return im\n\n    while np.sum(im[:,0]) == 0:\n        im = np.delete(im, 0, 1)\n        if len(im) == 0:\n            return im\n\n    while np.sum(im[:,-1]) == 0:\n        im = np.delete(im, -1, 1)\n        if len(im) == 0:\n            return im\n\n    return im\n\nim = np.array([[0,0,0,0,0,0],\n               [0,0,1,1,1,0],\n               [0,1,1,0,1,0],\n               [0,0,0,1,1,0],\n               [0,0,0,0,0,0]])\n\nresult = remove_zeros(im)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 507, "library_problem_id": 216, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 216}}
{"id": 508, "code": ["<code>\nimport numpy as np\n\ndef remove_zeros(A):\n    rows = np.any(A, axis=1)\n    cols = np.any(A, axis=0)\n    return A[np.ix_(rows, cols)]\n\nA = np.array([[0, 0, 0, 0, 0, 0, 0],\n              [0, 0, 0, 0, 0, 0, 0],\n              [0, 0, 1, 0, 0, 0, 0],\n              [0, 0, 1, 1, 0, 0, 0],\n              [0, 0, 0, 0, 1, 0, 0],\n              [0, 0, 0, 0, 0, 0, 0],\n              [0, 0, 0, 0, 0, 0, 0]])\n\nresult = remove_zeros(A)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 508, "library_problem_id": 217, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 216}}
{"id": 509, "code": ["<code>\nimport numpy as np\n\ndef process_image(im):\n    # Find rows and columns with only peripheral non-zero elements\n    rows = np.where((im == 0) & (np.sum(im, axis=1) != 0))[0]\n    cols = np.where((im == 0) & (np.sum(im, axis=0) != 0))[0]\n\n    # If there are no such rows or columns, return an empty array\n    if len(rows) == im.shape[0] or len(cols) == im.shape[1]:\n        return np.array([])\n\n    # Remove peripheral non-zero rows and columns\n    im = np.delete(im, rows, axis=0)\n    im = np.delete(im, cols, axis=1)\n\n    return im\n\nim = np.array([[1,1,1,1,1,5],\n               [1,0,0,1,2,0],\n               [2,1,0,0,1,0],\n               [1,0,0,7,1,0],\n               [1,0,0,0,0,0]])\n\nresult = process_image(im)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 509, "library_problem_id": 218, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 510, "code": ["<code>\nimport numpy as np\n\ndef crop_image(im):\n    # Find the rows and columns that contain at least one non-zero pixel\n    rows = np.any(im, axis=1)\n    cols = np.any(im, axis=0)\n\n    # Use these to index into the original image\n    im_cropped = im[np.ix_(rows, cols)]\n\n    return im_cropped\n\nim = np.array([[0,0,0,0,0,0],\n               [0,0,5,1,2,0],\n               [0,1,8,0,1,0],\n               [0,0,0,7,1,0],\n               [0,0,0,0,0,0]])\n\nresult = crop_image(im)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 510, "library_problem_id": 219, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 511, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = 10 * np.random.randn(10)\ny = x\n\nplt.figure(figsize=(8, 6))\nplt.scatter(x, y, label='x-y')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.title('Plot of x vs y')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 511, "library_problem_id": 0, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 512, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = np.random.rand(10)\ny = np.random.rand(10)\nplt.scatter(x, y)\n\n# SOLUTION START\nplt.minorticks_on()  # Turns on minor ticks on both axes.\nplt.yticks(np.arange(0, 1.1, 0.1))  # Sets the y-axis ticks to be at the specified range.\n# SOLUTION END\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 512, "library_problem_id": 1, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 513, "code": ["<code>\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 513, "library_problem_id": 2, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 514, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = np.random.rand(10)\ny = np.random.rand(10)\n\nplt.scatter(x, y)\n\n# Turn on minor ticks on x axis only\nplt.xticks(np.arange(min(x), max(x), 0.1), minor=True)\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 514, "library_problem_id": 3, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 515, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = np.arange(10)\n\n# SOLUTION START\n# Define the line styles\nline_styles = ['-', '--', '-.', ':']\n\n# Generate random y values\ny = np.random.rand(4, 10)\n\n# Plot the lines\nfor i in range(4):\n    plt.plot(x, y[i], linestyle=line_styles[i], label=f'Line {i+1}')\n\n# Add a legend\nplt.legend()\n\n# Show the plot\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 515, "library_problem_id": 4, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 516, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = np.arange(10)\n\n# SOLUTION START\n# Define the line styles\nline_styles = ['-', '--', '-.', ':']\n\n# Generate random y values\ny = np.random.rand(4, 10)\n\n# Plot the lines\nfor i in range(4):\n    plt.plot(x, y[i], linestyle=line_styles[i], label=f'Line {i+1}')\n\n# Add a legend\nplt.legend()\n\n# Show the plot\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 516, "library_problem_id": 5, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 517, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = np.arange(10)\ny = np.random.randn(10)\n\nplt.plot(x, y, marker='d', linestyle='-', linewidth=1)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 517, "library_problem_id": 6, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 518, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = np.arange(10)\ny = np.random.randn(10)\n\nplt.plot(x, y, marker='D', markersize=10, linestyle='-')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 518, "library_problem_id": 7, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 519, "code": ["<code>\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 519, "library_problem_id": 8, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 8}}
{"id": 520, "code": ["<code>\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 520, "library_problem_id": 9, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 521, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# SOLUTION START\n# Create a line from (0,0) to (1,2)\nx = np.linspace(0, 1, 100)\ny = 2*x\n\n# Plot the line\nplt.plot(x, y)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 521, "library_problem_id": 10, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 522, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# SOLUTION START\n# Create a line segment from (0,0) to (1,2)\nx = np.linspace(0, 1, 100)\ny = 2*x\n\nplt.plot(x, y)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 522, "library_problem_id": 11, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 523, "code": ["<code>\nimport numpy\nimport pandas\nimport matplotlib.pyplot as plt\nimport seaborn\nseaborn.set(style=\"ticks\")\nnumpy.random.seed(0)\nN = 37\n_genders = [\"Female\", \"Male\", \"Non-binary\", \"No Response\"]\ndf = pandas.DataFrame(\n    {\n        \"Height (cm)\": numpy.random.uniform(low=130, high=200, size=N),\n        \"Weight (kg)\": numpy.random.uniform(low=30, high=100, size=N),\n        \"Gender\": numpy.random.choice(_genders, size=N),\n    }\n)\n\n# SOLUTION START\n# Create a scatter plot with seaborn, color by the 'Gender' column\nseaborn.relplot(x=\"Height (cm)\", y=\"Weight (kg)\", hue=\"Gender\", data=df, kind=\"scatter\")\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 523, "library_problem_id": 12, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 12}}
{"id": 524, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = np.arange(10)\ny = 2 * np.random.rand(10)\n\n# Create a dataframe\ndf = pd.DataFrame({'x': x, 'y': y})\n\n# Plot using seaborn\nsns.set_style(\"whitegrid\")  # Set the style of the plot\nsns.lineplot(x='x', y='y', data=df)\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 524, "library_problem_id": 13, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 525, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.sin(x)\n\n# Create a pandas DataFrame\ndf = pd.DataFrame({'x': x, 'y': y})\n\n# Use seaborn to draw the line plot\nsns.lineplot(x='x', y='y', data=df)\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 525, "library_problem_id": 14, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 526, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.random.randn(10)\ny = np.random.randn(10)\n\nplt.plot(x, y, '+', markersize=7)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 526, "library_problem_id": 15, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 527, "code": ["<code>\n# SOLUTION END\nplt.legend(fontsize=20)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 527, "library_problem_id": 16, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 528, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = np.linspace(0, 2 * np.pi, 10)\ny = np.cos(x)\n\nplt.plot(x, y)\nplt.legend(['cosine'])\nplt.title('Plot of Cosine Function')\nplt.xlabel('x')\nplt.ylabel('cos(x)')\nplt.legend(title='Legend', title_fontsize='20')\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 528, "library_problem_id": 17, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 529, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = np.random.randn(10)\ny = np.random.randn(10)\n(l,) = plt.plot(range(10), \"o-\", lw=5, markersize=30)\n\n# Set the face color of the markers to have an alpha (transparency) of 0.2\nl.set_markerfacecolor((0, 0, 0, 0.2))  # RGBA color, where the last value is alpha\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 529, "library_problem_id": 18, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 530, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = np.random.randn(10)\ny = np.random.randn(10)\n(l,) = plt.plot(range(10), \"o-\", lw=5, markersize=30)\n\n# SOLUTION START\nplt.setp(l, color='black', linewidth=5)\n# SOLUTION END\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 530, "library_problem_id": 19, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 531, "code": ["<code>\n# SOLUTION END\nplt.setp(l, color='red')\nplt.setp(plt.gca().lines, color='red')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 531, "library_problem_id": 20, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 532, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = np.linspace(0, 2 * np.pi, 10)\ny = np.cos(x)\nplt.plot(x, y, label=\"sin\")\n\n# Rotate the x axis labels clockwise by 45 degrees\nplt.xticks(rotation=45)\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 532, "library_problem_id": 21, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 21}}
{"id": 533, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = np.linspace(0, 2 * np.pi, 10)\ny = np.cos(x)\nplt.plot(x, y, label=\"sin\")\n\n# Rotate the x axis labels counter clockwise by 45 degrees\nplt.xticks(rotation=-45)\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 533, "library_problem_id": 22, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 534, "code": ["<code>\n# SOLUTION END\nplt.xlabel('x - axis')\nplt.ylabel('y - axis')\nplt.title('Cosine Function')\nplt.legend()\nplt.xticks([0, np.pi, 2*np.pi])\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 534, "library_problem_id": 23, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 535, "code": ["<code>\n# SOLUTION END\nplt.legend(labels=['a', 'b'])\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 535, "library_problem_id": 24, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 536, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nH = np.random.randn(10, 10)\n\n# SOLUTION START\nplt.imshow(H, cmap='hot', interpolation='nearest')\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 536, "library_problem_id": 25, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 537, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nH = np.random.randn(10, 10)\n\n# SOLUTION START\nplt.imshow(H, cmap='gray')\nplt.colorbar()\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 537, "library_problem_id": 26, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 538, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = np.linspace(0, 2 * np.pi, 10)\ny = np.cos(x)\n\nplt.plot(x, y)\nplt.xlabel(\"X\")\nplt.xticks(rotation=90)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 538, "library_problem_id": 27, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 539, "code": ["<code>\n# SOLUTION END\nplt.xticks(rotation=90)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 539, "library_problem_id": 28, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 540, "code": ["<code>\nplt.title(myTitle)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 540, "library_problem_id": 29, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 541, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ny = 2 * np.random.rand(10)\nx = np.arange(10)\n\nplt.plot(x, y)\nplt.gca().invert_yaxis()\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 541, "library_problem_id": 30, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 542, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = np.random.randn(10)\ny = x\nplt.scatter(x, y)\n\n# SOLUTION START\nplt.xticks([0, 1.5])\nplt.yticks([0, 1.5])\n# SOLUTION END\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 542, "library_problem_id": 31, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 31}}
{"id": 543, "code": ["<code>\n# SOLUTION END\nplt.yticks([-1, 1])\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 543, "library_problem_id": 32, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 31}}
{"id": 544, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.random.rand(10)\ny = np.random.rand(10)\nz = np.random.rand(10)\n\nplt.figure(figsize=(10,5))\n\n# Plotting x\nplt.subplot(1,3,1)\nplt.plot(x, label='x')\nplt.title('x')\nplt.legend()\n\n# Plotting y\nplt.subplot(1,3,2)\nplt.plot(y, label='y')\nplt.title('y')\nplt.legend()\n\n# Plotting z\nplt.subplot(1,3,3)\nplt.plot(z, label='z')\nplt.title('z')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 544, "library_problem_id": 33, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 545, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.random.randn(10)\ny = np.random.randn(10)\n\nplt.scatter(x, y, facecolor='blue', edgecolor='black')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 545, "library_problem_id": 34, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 546, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ny = 2 * np.random.rand(10)\nx = np.arange(10)\n\nfig, ax = plt.subplots()\n\nax.plot(x, y, 'o-')\n\nax.xaxis.set_ticks(np.arange(0, 11, 1))\nax.yaxis.set_ticks(np.arange(0, max(y)+1, 1))\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 546, "library_problem_id": 35, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 547, "code": ["<code>\n# SOLUTION END\nplt.yticks(df['coverage'])\nplt.ticklabel_format(style='plain', axis='y')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 547, "library_problem_id": 36, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 548, "code": ["<code>\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 548, "library_problem_id": 37, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 549, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = np.linspace(0, 2 * np.pi, 400)\ny1 = np.sin(x)\ny2 = np.cos(x)\n\nfig, axs = plt.subplots(2, sharex=True)\naxs[0].plot(x, y1)\naxs[0].set_title('y1 = sin(x)')\naxs[1].plot(x, y2)\naxs[1].set_title('y2 = cos(x)')\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 549, "library_problem_id": 38, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 550, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = np.linspace(0, 2 * np.pi, 400)\ny1 = np.sin(x)\ny2 = np.cos(x)\n\nfig, axs = plt.subplots(2)\nfig.subplots_adjust(hspace=0)\naxs[0].plot(x, y1)\naxs[0].set_title('y1 vs x')\naxs[0].spines['top'].set_color('none')\naxs[0].spines['bottom'].set_color('none')\naxs[0].spines['left'].set_color('none')\naxs[0].spines['right'].set_color('none')\n\naxs[1].plot(x, y2)\naxs[1].set_title('y2 vs x')\naxs[1].spines['top'].set_color('none')\naxs[1].spines['bottom'].set_color('none')\naxs[1].spines['left'].set_color('none')\naxs[1].spines['right'].set_color('none')\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 550, "library_problem_id": 39, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 551, "code": ["<code>\n# SOLUTION END\nplt.xlabel('')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 551, "library_problem_id": 40, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 552, "code": ["<code>\n# SOLUTION END\nplt.xticks([])\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 552, "library_problem_id": 41, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 40}}
{"id": 553, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = np.arange(10)\ny = np.random.randn(10)\nplt.scatter(x, y)\n\n# SOLUTION START\nplt.xticks([3, 4])  # Set xticks at x positions 3 and 4\nplt.grid(axis='x')  # Show vertical grid at x positions 3 and 4\n# SOLUTION END\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 553, "library_problem_id": 42, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 554, "code": ["<code>\n# SOLUTION END\nplt.yticks([3, 4])\nplt.grid(axis='y')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 554, "library_problem_id": 43, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 555, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = np.arange(10)\ny = np.random.randn(10)\nplt.scatter(x, y)\n\n# SOLUTION START\nplt.yticks([3, 4])\nplt.xticks([1, 2])\nplt.grid(axis='y')\nplt.grid(axis='x')\n# SOLUTION END\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 555, "library_problem_id": 44, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 556, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = np.arange(10)\ny = np.random.randn(10)\n\nplt.scatter(x, y)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 556, "library_problem_id": 45, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 557, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nx = 10 * np.random.randn(10)\ny = x\nplt.plot(x, y, label=\"x-y\")\n\n# Adding legend to the lower right\nplt.legend(loc='lower right')\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 557, "library_problem_id": 46, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 46}}
{"id": 558, "code": ["<code>\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6))\naxes = axes.flatten()\nfor ax in axes:\n    ax.set_ylabel(r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\")\n    ax.set_xlabel(r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\")\n\n# Adjust subplot padding to have enough space to display axis labels\nplt.subplots_adjust(hspace=0.5, wspace=0.5)\n\nplt.show()\nplt.clf()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 558, "library_problem_id": 47, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 559, "code": ["<code>\n# SOLUTION END\nplt.xlabel('X')\nplt.ylabel('Value')\nplt.legend(['Y', 'Z'])\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 559, "library_problem_id": 48, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 560, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ncolumn_labels = list(\"ABCD\")\nrow_labels = list(\"WXYZ\")\ndata = np.random.rand(4, 4)\nfig, ax = plt.subplots()\nheatmap = ax.pcolor(data, cmap=plt.cm.Blues)\n\n# Create a new axis for the colorbar\ncax = fig.add_axes([ax.get_position().x1+0.03,ax.get_position().y0,0.02,ax.get_position().y1-ax.get_position().y0])\nfig.colorbar(heatmap, cax=cax)\n\n# Set the ticks to the column_labels and row_labels\nax.set_xticks(np.arange(data.shape[0])+0.5, minor=False)\nax.set_yticks(np.arange(data.shape[1])+0.5, minor=False)\nax.set_xticklabels(row_labels, minor=False)\nax.set_yticklabels(column_labels, minor=False)\n\n# Move the x-axis to the top\nax.xaxis.tick_top()\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 560, "library_problem_id": 49, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 561, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y)\nplt.xlabel(\"X\")\nplt.xticks(np.arange(min(x), max(x)+1, 1))\nplt.subplots_adjust(hspace=0.5)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 561, "library_problem_id": 50, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 562, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y)\nplt.xticks([])\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 562, "library_problem_id": 51, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 563, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y)\nplt.yticks(rotation=0)\nplt.yticks(y)\nplt.ylabel('y')\nplt.xlabel('x')\nplt.title('Plot of y over x')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 563, "library_problem_id": 52, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 564, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y)\nplt.ylabel('Y')\nplt.yticks(np.arange(0, max(y)+1, 1))\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 564, "library_problem_id": 53, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 52}}
{"id": 565, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np, pandas as pd\nimport seaborn as sns\n\ntips = sns.load_dataset(\"tips\")\n\n# SOLUTION START\n# Create a joint plot\njoint_plot = sns.jointplot(x='total_bill', y='tip', data=tips, kind='reg', color='green')\n\n# Change the line and scatter plot color to green\njoint_plot.plot_joint(sns.scatterplot, s=100, color=\"green\")\njoint_plot.plot_joint(sns.lineplot, color=\"green\")\n\n# Change the distribution plot color to blue\njoint_plot.ax_marg_x.hist(tips['total_bill'], color='blue', alpha=.6, bins=20)\njoint_plot.ax_marg_y.hist(tips['tip'], color='blue', alpha=.6, bins=20, orientation=\"horizontal\")\n\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 565, "library_problem_id": 54, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 566, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np, pandas as pd\nimport seaborn as sns\n\ntips = sns.load_dataset(\"tips\")\n\n# SOLUTION START\nsns.jointplot(x='total_bill', y='tip', data=tips, kind='reg', line_kws={\"color\": \"green\"})\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 566, "library_problem_id": 55, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 567, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np, pandas as pd\nimport seaborn as sns\n\ntips = sns.load_dataset(\"tips\")\n\n# SOLUTION START\nsns.jointplot(x='total_bill', y='tip', data=tips, kind='reg')\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 567, "library_problem_id": 56, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 568, "code": ["<code>\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.DataFrame(\n    {\n        \"celltype\": [\"foo\", \"bar\", \"qux\", \"woz\"],\n        \"s1\": [5, 9, 1, 7],\n        \"s2\": [12, 90, 13, 87],\n    }\n)\n\n# SOLUTION START\n# Create a bar plot for s1 and s2\nplt.bar(df['celltype'], df['s1'], label='s1')\nplt.bar(df['celltype'], df['s2'], label='s2')\n\n# Set the x-axis tick labels horizontal\nplt.xticks(rotation=90)\n\n# Set the xlabel\nplt.xlabel('Cell Type')\n\n# Set the ylabel\nplt.ylabel('Values')\n\n# Set the title\nplt.title('Bar plot of s1 and s2 with celltype as xlabel')\n\n# Show the legend\nplt.legend()\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 568, "library_problem_id": 57, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 569, "code": ["<code>\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 569, "library_problem_id": 58, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 57}}
{"id": 570, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y)\nplt.xlabel(\"X\", color=\"red\")\nplt.xticks(color=\"red\")\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 570, "library_problem_id": 59, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 571, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y)\nplt.xlabel('X')\nplt.xticks(color='red')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 571, "library_problem_id": 60, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 59}}
{"id": 572, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y)\nplt.xticks(fontsize=10, rotation='vertical')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 572, "library_problem_id": 61, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 573, "code": ["<code>\nimport matplotlib.pyplot as plt\n\ndef draw_vertical_lines(x_positions):\n    plt.vlines(x_positions, 0, 1)\n    plt.show()\n\n# Test the function\nx_positions = [0.22058956, 0.33088437, 2.20589566]\ndraw_vertical_lines(x_positions)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 573, "library_problem_id": 62, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 574, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nxlabels = list(\"ABCD\")\nylabels = list(\"CDEF\")\nrand_mat = np.random.rand(4, 4)\n\nfig, ax = plt.subplots()\nheatmap = ax.pcolor(rand_mat, cmap=plt.cm.Blues)\n\n# put the major ticks at the middle of each cell\nax.set_xticks(np.arange(rand_mat.shape[1])+0.5, minor=False)\nax.set_yticks(np.arange(rand_mat.shape[0])+0.5, minor=False)\n\n# want a more natural, table-like display\nax.invert_yaxis()\nax.xaxis.tick_top()\n\nax.set_xticklabels(xlabels, minor=False)\nax.set_yticklabels(ylabels[::-1], minor=False)\n\nplt.colorbar(heatmap)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 574, "library_problem_id": 63, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 575, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nrc(\"mathtext\", default=\"regular\")\ntime = np.arange(10)\ntemp = np.random.random(10) * 30\nSwdown = np.random.random(10) * 100 - 10\nRn = np.random.random(10) * 100 - 10\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(time, Swdown, \"-\", label=\"Swdown\")\nax.plot(time, Rn, \"-\", label=\"Rn\")\nax2 = ax.twinx()\nax2.plot(time, temp, \"-r\", label=\"temp\")\nax.legend(loc=0)\nax.grid()\nax.set_xlabel(\"Time (h)\")\nax.set_ylabel(r\"Radiation ($MJ\\,m^{-2}\\,d^{-1}$)\")\nax2.set_ylabel(r\"Temperature ($^\\circ$C)\")\nax2.set_ylim(0, 35)\nax.set_ylim(-20, 100)\nplt.show()\nplt.clf()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 575, "library_problem_id": 64, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 576, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\naxs[0].plot(x, y)\naxs[0].set_title('Y')\n\naxs[1].plot(x, y)\naxs[1].set_title('Y')\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 576, "library_problem_id": 65, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 577, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = sns.load_dataset(\"penguins\")[\n    [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n]\n\n# SOLUTION START\nsns.scatterplot(data=df, x='bill_length_mm', y='bill_depth_mm', s=30)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 577, "library_problem_id": 66, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 578, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\na = [2.56422, 3.77284, 3.52623]\nb = [0.15, 0.3, 0.45]\nc = [58, 651, 393]\n\n# Create a dataframe\ndf = pd.DataFrame({'a': a, 'b': b, 'c': c})\n\n# Create scatter plot\nplt.scatter(df['a'], df['b'])\n\n# Annotate each data point with correspond numbers in c\nfor i, txt in enumerate(df['c']):\n    plt.annotate(txt, (df['a'][i], df['b'][i]))\n\nplt.xlabel('a')\nplt.ylabel('b')\nplt.title('Scatter plot of a over b')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 578, "library_problem_id": 67, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
{"id": 579, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y, label=\"y over x\")\nplt.legend(title=\"Legend\")\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Plot of y over x')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 579, "library_problem_id": 68, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 580, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y, label=\"y over x\")\nplt.legend(title=\"Legend\", title_fontsize='large', title_fontweight='bold')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 580, "library_problem_id": 69, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 581, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.random.rand(10)\ny = np.random.rand(10)\n\n# Create a histogram of x\nplt.hist(x, edgecolor=\"black\", linewidth=1.2)\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 581, "library_problem_id": 70, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 582, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\n# First subplot\naxs[0].plot(x, y)\naxs[0].set_title('Subplot 1')\n\n# Second subplot\naxs[1].plot(x, y)\naxs[1].set_title('Subplot 2')\n\n# Adjust the width ratio\nfig.set_size_inches(15, 5)\n\nplt.tight_layout()\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 582, "library_problem_id": 71, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 583, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.random.rand(10)\ny = np.random.rand(10)\nbins = np.linspace(-1, 1, 100)\n\nplt.hist(x, bins=bins, alpha=0.5, label='x', color='b')\nplt.hist(y, bins=bins, alpha=0.5, label='y', color='r')\n\nplt.legend(loc='upper right')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 583, "library_problem_id": 72, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 584, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.random.rand(10)\ny = np.random.rand(10)\n\nplt.figure(figsize=(10, 6))\n\n# Create a histogram for x\nplt.hist(x, bins=np.linspace(0, 1, 11), alpha=0.5, label='x', color='r')\n\n# Create a histogram for y\nplt.hist(y, bins=np.linspace(0, 1, 11), alpha=0.5, label='y', color='b')\n\nplt.legend(loc='upper right')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 584, "library_problem_id": 73, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 585, "code": ["<code>\nimport matplotlib.pyplot as plt\n\n# define the coordinates\na, b = 1, 1\nc, d = 3, 4\n\n# create a new figure\nplt.figure()\n\n# plot the line\nplt.plot([a, c], [b, d])\n\n# set the xlim and ylim\nplt.xlim(0, 5)\nplt.ylim(0, 5)\n\n# show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 585, "library_problem_id": 74, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 586, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.random.random((10, 10))\ny = np.random.random((10, 10))\n\nfig, axs = plt.subplots(2, 1, figsize=(5, 10))\n\np1 = axs[0].pcolor(x, cmap='viridis')\np2 = axs[1].pcolor(y, cmap='viridis')\n\nfig.colorbar(p1, ax=axs[0])\nfig.colorbar(p2, ax=axs[1])\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 586, "library_problem_id": 75, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 587, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.random.random((10, 2))\n\n# Convert the numpy array to a pandas DataFrame\ndf = pd.DataFrame(x, columns=[\"a\", \"b\"])\n\n# Plot each column in the DataFrame\ndf.plot(kind='line')\n\n# Add labels\nplt.xlabel('Index')\nplt.ylabel('Value')\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 587, "library_problem_id": 76, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 588, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\nz = np.arange(10)\na = np.arange(10)\n\nfig, axs = plt.subplots(2)\nfig.suptitle('Y and Z')\naxs[0].plot(x, y)\naxs[1].plot(a, z)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 588, "library_problem_id": 77, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 589, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\npoints = [(3, 5), (5, 10), (10, 150)]\n\n# Convert the points to pandas DataFrame\ndf = pd.DataFrame(points, columns=['x', 'y'])\n\n# Plot the line plot\nplt.plot(df['x'], df['y'])\n\n# Set the y-axis to log scale\nplt.yscale('log')\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 589, "library_problem_id": 78, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 78}}
{"id": 590, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y)\n\nplt.title('Plot of y over x', fontsize=20)\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', fontsize=16)\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 590, "library_problem_id": 79, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 79}}
{"id": 591, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.arange(10)\ny = np.arange(10)\n\nf = plt.figure()\nax = f.add_subplot(111)\n\nax.plot(x, y)\nax.set_xticks(np.arange(1, 11))\nax.set_yticks(np.arange(1, 11))\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 591, "library_problem_id": 80, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 592, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nlines = [[(0, 1), (1, 1)], [(2, 3), (3, 3)], [(1, 2), (1, 3)]]\nc = np.array([(1, 0, 0, 1), (0, 1, 0, 1), (0, 0, 1, 1)])\n\nfig, ax = plt.subplots()\n\nfor line, color in zip(lines, c):\n    xs, ys = zip(*line)\n    ax.plot(xs, ys, color=color)\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 592, "library_problem_id": 81, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 593, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(0, 1000, 50)\ny = np.arange(0, 1000, 50)\n\n# Create a log-log plot\nplt.loglog(x, y)\n\n# Mark the axes with numbers like 1, 10, 100\nplt.xticks([1, 10, 100, 1000], ['1', '10', '100', '1000'])\nplt.yticks([1, 10, 100, 1000], ['1', '10', '100', '1000'])\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 593, "library_problem_id": 82, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 594, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    np.random.randn(50, 4),\n    index=pd.date_range(\"1/1/2000\", periods=50),\n    columns=list(\"ABCD\"),\n)\ndf = df.cumsum()\n\n# SOLUTION START\nfig, ax = plt.subplots(4, 1, figsize=(10,10))\n\nfor i, col in enumerate(df.columns):\n    ax[i].plot(df[col], label=col)\n    ax[i].legend(loc='best')\n    ax[i].set_title(f'Line plot of {col}')\n\nplt.tight_layout()\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 594, "library_problem_id": 83, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 595, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = [1000, 1000, 5000, 3000, 4000, 16000, 2000]\n\n# Renormalize the data to sum up to 1\ndata_normalized = [i/sum(data) for i in data]\n\n# Create a histogram\nplt.hist(data_normalized, bins=len(data_normalized), edgecolor='black')\n\n# Format the y tick labels into percentage and set y tick labels as 10%, 20%, etc.\nyticks = [i/100 for i in range(0, 110, 10)]\nplt.yticks(yticks, ['{:0%}'.format(x) for x in yticks])\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 595, "library_problem_id": 84, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 84}}
{"id": 596, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y, marker='o', markersize=5, markerfacecolor='none', markeredgecolor='blue', markeredgewidth=1.5, alpha=0.5)\nplt.grid(True)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 596, "library_problem_id": 85, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 597, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\na = np.arange(10)\nz = np.arange(10)\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 4))\n\naxs[0].plot(x, y, label='y')\naxs[1].plot(z, a, label='a')\n\naxs[0].set_xlabel('x')\naxs[0].set_ylabel('y')\naxs[0].set_title('y over x')\n\naxs[1].set_xlabel('z')\naxs[1].set_ylabel('a')\naxs[1].set_title('a over z')\n\nfig.legend(loc='lower right')\n\nplt.tight_layout()\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 597, "library_problem_id": 86, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 598, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = sns.load_dataset(\"penguins\")[\n    [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n]\n\n# SOLUTION START\n# Create a figure and a set of subplots\nfig, axs = plt.subplots(2)\n\n# First subplot\nsns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=df, ax=axs[0])\naxs[0].set_ylabel('bill_depth_mm')\n\n# Second subplot\nsns.regplot(x=\"bill_length_mm\", y=\"flipper_length_mm\", data=df, ax=axs[1])\naxs[1].set_ylabel('flipper_length_mm')\n\n# Remove shared y axis\nfig.subplots_adjust(hspace=0)\nplt.setp(axs[0].get_xticklabels(), visible=False)\nplt.setp(axs[1].get_xticklabels(), visible=False)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 598, "library_problem_id": 87, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 87}}
{"id": 599, "code": ["<code>\n# SOLUTION END\nfig.canvas.draw()\nlabels = [item.get_text() for item in ax.get_xticklabels()]\nlabels[1] = 'second'\nax.set_xticklabels(labels)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 599, "library_problem_id": 88, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 600, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y, label='$\\lambda$')\nplt.legend()\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 600, "library_problem_id": 89, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 601, "code": ["<code>\n# SOLUTION START\n# Get the current xticks\ncurrent_ticks = plt.gca().get_xticks()\n# Add the new ticks\nnew_ticks = np.append(current_ticks, [2.1, 3, 7.6])\n# Set the new ticks\nplt.gca().set_xticks(new_ticks)\n# SOLUTION END\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 601, "library_problem_id": 90, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 602, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(2010, 2020)\ny = np.arange(10)\nplt.plot(x, y)\n\n# SOLUTION START\n# Rotate the xticklabels to -60 degree\nplt.xticks(rotation=-60, ha='left')\n# SOLUTION END\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 602, "library_problem_id": 91, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 603, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(2010, 2020)\ny = np.arange(10)\nplt.plot(x, y)\n\n# Rotate the yticklabels to -60 degree\nplt.yticks(rotation=-60)\n\n# Set the xticks vertical alignment to top\nplt.gca().xaxis.tick_top()\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 603, "library_problem_id": 92, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 604, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(2010, 2020)\ny = np.arange(10)\nplt.plot(x, y)\n\n# Set the transparency of xtick labels to be 0.5\nfor label in plt.gca().get_xticklabels():\n    label.set_alpha(0.5)\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 604, "library_problem_id": 93, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 605, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y)\n\n# Set the margin before the first xtick to zero\nplt.xlim(left=0)\n\n# Set the margin for the yaxis to be greater than zero\nplt.ylim(bottom=0)\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 605, "library_problem_id": 94, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 606, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y)\n\n# Set the margin before the first ytick to 0\nplt.yticks(np.arange(1,10))\n\n# Use greater than zero margin for the xaxis\nplt.xmargin(0.2)\n\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 606, "library_problem_id": 95, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 607, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 4))\n\naxs[0].plot(x, y)\naxs[0].set_title('Subplot 1')\naxs[1].plot(x, y)\naxs[1].set_title('Subplot 2')\n\nfig.suptitle('Figure')\n\nplt.tight_layout()\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 607, "library_problem_id": 96, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 96}}
{"id": 608, "code": ["<code>\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nvalues = [[1, 2], [3, 4]]\ndf = pd.DataFrame(values, columns=[\"Type A\", \"Type B\"], index=[\"Index 1\", \"Index 2\"])\n\n# SOLUTION START\n# Plotting the dataframe\ndf.plot(kind='line')\n\n# Labeling the x and y axis\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\n\n# Showing the plot\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 608, "library_problem_id": 97, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 609, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.scatter(x, y, marker='|', linewidth=5, hatch='|')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 609, "library_problem_id": 98, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 610, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.scatter(x, y, marker='|', edgecolors='none')\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 610, "library_problem_id": 99, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 611, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.scatter(x, y, marker='*', hatch='*')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 611, "library_problem_id": 100, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 612, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.scatter(x, y, s=100, marker='$\\u2605$')\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 612, "library_problem_id": 101, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 613, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.random((10, 10))\n\n# Set xlim and ylim to be between 0 and 10\nplt.xlim(0, 10)\nplt.ylim(0, 10)\n\n# Plot a heatmap of data in the rectangle where right is 5, left is 1, bottom is 1, and top is 4.\nplt.imshow(data, extent=(1, 5, 1, 4), aspect='auto')\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 613, "library_problem_id": 102, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 614, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0.1, 2 * np.pi, 41)\ny = np.exp(np.sin(x))\n\nplt.stem(y, x, orientation='horizontal')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 614, "library_problem_id": 103, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 615, "code": ["<code>\nimport matplotlib.pyplot as plt\n\nd = {\"a\": 4, \"b\": 5, \"c\": 7}\nc = {\"a\": \"red\", \"c\": \"green\", \"b\": \"blue\"}\n\n# SOLUTION START\nkeys = list(d.keys())\nvalues = list(d.values())\ncolors = [c[key] for key in keys]\n\nplt.bar(keys, values, color=colors)\nplt.xlabel('Keys')\nplt.ylabel('Values')\nplt.title('Bar plot using data in d')\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 615, "library_problem_id": 104, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 616, "code": ["<code>\nimport matplotlib.pyplot as plt\n\ndef plot_with_cutoff():\n    # Assuming we have a data\n    x = [1, 2, 3, 4, 5]\n    y = [1, 4, 3, 6, 5]\n\n    # Plotting the data\n    plt.plot(x, y)\n\n    # Creating a vertical line at x=3\n    plt.axvline(x=3, color='r', label='cutoff')\n\n    # Adding legend\n    plt.legend()\n\n    # Showing the plot\n    plt.show()\n\n# Call the function\nplot_with_cutoff()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 616, "library_problem_id": 105, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 617, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nlabels = [\"a\", \"b\"]\nheight = [3, 4]\n\n# Convert labels to radians\ntheta = np.linspace(0, np.pi, len(labels))\n\n# Create a polar plot\nplt.figure(figsize=(8, 8))\nax = plt.subplot(111, projection='polar')\n\n# Plot the data\nbars = ax.bar(theta, height, width=0.5)\n\n# Add labels\nfor label, theta in zip(labels, theta):\n    ax.text(theta, 1.1, label, horizontalalignment='center')\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 617, "library_problem_id": 106, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 106}}
{"id": 618, "code": ["<code>\nimport matplotlib.pyplot as plt\n\nl = [\"a\", \"b\", \"c\"]\ndata = [225, 90, 50]\n\ndef donut_plot(l, data):\n    fig, ax = plt.subplots()\n    ax.pie(data, labels=l, wedgeprops={'width': 0.4})\n    plt.show()\n\ndonut_plot(l, data)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 618, "library_problem_id": 107, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 619, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y)\nplt.grid(True, linestyle='--', color='blue')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 619, "library_problem_id": 108, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 620, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nfig, ax = plt.subplots()\n\nax.plot(x, y)\n\nax.minorticks_on()\nax.grid(which='minor', linestyle=':', linewidth='0.5', color='gray')\nax.grid(which='major', linestyle='', linewidth='0')\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 620, "library_problem_id": 109, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 621, "code": ["<code>\nimport matplotlib.pyplot as plt\n\ndef create_pie_chart(labels, sizes, colors):\n    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n    plt.axis('equal')\n    plt.show()\n\nlabels = [\"Walking\", \"Talking\", \"Sleeping\", \"Working\"]\nsizes = [23, 45, 12, 20]\ncolors = [\"red\", \"blue\", \"green\", \"yellow\"]\n\ncreate_pie_chart(labels, sizes, colors)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 621, "library_problem_id": 110, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 622, "code": ["<code>\nimport matplotlib.pyplot as plt\n\ndef create_pie_chart(labels, sizes, colors):\n    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n    plt.axis('equal')\n    plt.show()\n\nlabels = [\"Walking\", \"Talking\", \"Sleeping\", \"Working\"]\nsizes = [23, 45, 12, 20]\ncolors = [\"red\", \"blue\", \"green\", \"yellow\"]\n\ncreate_pie_chart(labels, sizes, colors)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 622, "library_problem_id": 111, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 623, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y, marker='o', markersize=10, markerfacecolor='none', markeredgecolor='blue')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 623, "library_problem_id": 112, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 624, "code": ["<code>\n# SOLUTION END\nplt.axvline(x=55, color='green', linestyle='-')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 624, "library_problem_id": 113, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 625, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nblue_bar = (23, 25, 17)\norange_bar = (19, 18, 14)\n\n# Create a figure and a set of subplots\nfig, ax = plt.subplots()\n\n# Set the width of the bars\nbar_width = 0.3\n\n# Create a bar plot for blue bars\nax.bar(np.arange(len(blue_bar)), blue_bar, bar_width, color='b', label='Blue')\n\n# Create a bar plot for orange bars, with the bottom set to the height of the blue bars\nax.bar(np.arange(len(orange_bar)) + bar_width, orange_bar, bar_width, color='orange', label='Orange')\n\n# Set the x-axis labels\nax.set_xticks(np.arange(len(blue_bar)) + bar_width / 2)\nax.set_xticklabels(('A', 'B', 'C'))\n\n# Add a legend\nax.legend()\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 625, "library_problem_id": 114, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 626, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.random.rand(10)\nz = np.random.rand(10)\na = np.arange(10)\n\nfig, ax1 = plt.subplots()\n\nax2 = ax1.twinx()\nax1.plot(x, y, 'g-', label='y over x')\nax2.plot(a, z, 'b-', label='z over a')\n\nax1.set_xlabel('x')\nax1.set_ylabel('y', color='g')\nax2.set_ylabel('z', color='b')\n\nfig.tight_layout()\n\nlines, labels = ax1.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax2.legend(lines + lines2, labels + labels2, loc='best')\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 626, "library_problem_id": 115, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 627, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\n\nx = np.arange(10)\ny = np.linspace(0, 1, 10)\n\n# Create a colormap\ncmap = plt.get_cmap('Spectral')\n\n# Normalize the y-values to the range [0, 1]\nnorm = matplotlib.colors.Normalize(vmin=min(y), vmax=max(y))\n\n# Create a scatter plot\nplt.scatter(x, y, c=y, cmap=cmap, norm=norm)\n\n# Add colorbar\nplt.colorbar(matplotlib.cm.ScalarMappable(norm=norm, cmap=cmap), label='y-value')\n\n# Add labels\nplt.xlabel('x')\nplt.ylabel('y')\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 627, "library_problem_id": 116, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 116}}
{"id": 628, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y)\nplt.xticks(np.arange(0, max(x)+1, 1))\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 628, "library_problem_id": 117, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 629, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = sns.load_dataset(\"penguins\")[[\"bill_length_mm\", \"species\", \"sex\"]]\n\n# SOLUTION START\ng = sns.catplot(data=df, x=\"species\", y=\"bill_length_mm\", hue=\"sex\", col=\"sex\", kind=\"bar\", sharey=False)\ng.despine(left=True)\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 629, "library_problem_id": 118, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 630, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\ndef draw_circle():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n\n    circle = patches.Circle((0.5, 0.5), 0.2, edgecolor='b', facecolor='none')\n    ax.add_patch(circle)\n\n    plt.xlim(0, 1)\n    plt.ylim(0, 1)\n    plt.show()\n\ndraw_circle()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 630, "library_problem_id": 119, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 631, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y)\nplt.title(r'$\\boldsymbol{\\phi}$', fontsize=20)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 631, "library_problem_id": 120, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 632, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y, label=\"Line\")\nplt.legend(labelspacing=0.1)\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 632, "library_problem_id": 121, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 633, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y, label=\"Line\")\nplt.legend(scatterpoints=1, frameon=False, labelspacing=0.3)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 633, "library_problem_id": 122, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 634, "code": ["<code>\n# SOLUTION END\nplt.legend(loc='upper left')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 634, "library_problem_id": 123, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 635, "code": ["<code>\n# SOLUTION END\nplt.legend()\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 635, "library_problem_id": 124, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 636, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = np.random.random((10, 10))\n\nplt.imshow(data, cmap='viridis')\nplt.colorbar()\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 636, "library_problem_id": 125, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 125}}
{"id": 637, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y)\nplt.title(r'Figure $\\bf{1}$')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 637, "library_problem_id": 126, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 126}}
{"id": 638, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\ndf = pd.DataFrame(\n    {\n        \"id\": [\"1\", \"2\", \"1\", \"2\", \"2\"],\n        \"x\": [123, 22, 356, 412, 54],\n        \"y\": [120, 12, 35, 41, 45],\n    }\n)\n\ndef pairplot(df, x, y, id):\n    g = sns.pairplot(df, x_vars=x, y_vars=y, hue=id)\n    g.legend.remove()\n    plt.show()\n\npairplot(df, 'x', 'y', 'id')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 638, "library_problem_id": 127, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 639, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y)\nplt.gca().invert_xaxis()\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 639, "library_problem_id": 128, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 128}}
{"id": 640, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(11)\ny = np.arange(11)\n\nplt.xlim(0, 10)\nplt.ylim(0, 10)\n\nplt.scatter(x, y)\n\nplt.gca().set_clip_on(False)\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 640, "library_problem_id": 129, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 641, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.scatter(x, y, facecolor='red', edgecolor='black')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 641, "library_problem_id": 130, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 642, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nfig, axs = plt.subplots(2, 2, figsize=(15, 15))\nfor ax in axs.flatten():\n    ax.plot(x, y)\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 642, "library_problem_id": 131, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 643, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.random.rand(100) * 10\n\nplt.hist(x, bins=np.arange(0, 11, 2), edgecolor='black')\nplt.xlabel('Bins')\nplt.ylabel('Frequency')\nplt.title('Histogram of x')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 643, "library_problem_id": 132, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 644, "code": ["<code>\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nx = np.arange(10)\ny = np.arange(1, 11)\nerror = np.random.random(y.shape)\n\nplt.figure(figsize=(10, 5))\n\n# Plot y over x\nplt.plot(x, y, label='y over x')\n\n# Plot error as a shaded region\nplt.fill_between(x, y - error, y + error, color='grey', alpha=0.2, label='error')\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 644, "library_problem_id": 133, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 133}}
{"id": 645, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nxvec = np.linspace(-5.0, 5.0, 100)\nx, y = np.meshgrid(xvec, xvec)\nz = -np.hypot(x, y)\n\n# Draw x=0 and y=0 axis in white color\nplt.contour(x, y, z, levels=[0], colors='white')\n\n# Plot the contour\nplt.contourf(x, y, z)\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 645, "library_problem_id": 134, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 646, "code": ["<code>\n# SOLUTION END\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 646, "library_problem_id": 135, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 647, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\nz = np.arange(10)\na = np.arange(10)\n\nfig, axs = plt.subplots(2)\nfig.suptitle('Y vs Z')\n\naxs[0].plot(x, y)\naxs[0].set_title('Y')\naxs[1].plot(a, z)\naxs[1].set_title('Z')\n\nplt.tight_layout()\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 647, "library_problem_id": 136, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 136}}
{"id": 648, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nfig, axs = plt.subplots(4, 4, figsize=(5, 5))\nfor i in range(4):\n    for j in range(4):\n        axs[i, j].plot(x, y)\n        axs[i, j].tick_params(axis='both', which='major', labelsize=5)\n\nplt.tight_layout()\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 648, "library_problem_id": 137, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 649, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nd = np.random.random((10, 10))\n\nplt.figure(figsize=(8, 8))\nplt.matshow(d, fignum=1)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 649, "library_problem_id": 138, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 138}}
{"id": 650, "code": ["<code>\nimport matplotlib.table as mplt_table\n\nfig, ax = plt.subplots(1, 1)\nax.axis('tight')\nax.axis('off')\n\ntable_data = df.values\ntable_cols = df.columns\n\nthe_table = mplt_table.Table(ax, bbox=[0, 0, 1, 1])\n\n# Adding the column labels\nthe_table.set_label('Table Label', 'A')\n\n# Adding the data\nfor col_num in range(table_data.shape[1]):\n    the_table.add_cell(0, col_num, table_cols[col_num], loc='right')\n\nfor row_num in range(table_data.shape[0]):\n    for col_num in range(table_data.shape[1]):\n        the_table.add_cell(row_num + 1, col_num, table_data[row_num, col_num], loc='center')\n\n# Adjusting layout\nax.set_title('Penguin Data')\nax.set_xticks([])\nax.set_yticks([])\n\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 650, "library_problem_id": 139, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 651, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.figure(figsize=(10,5))\nplt.plot(x, y)\n\nplt.xticks(np.arange(0, max(x)+1, 1))\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Line chart of y over x')\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 651, "library_problem_id": 140, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 140}}
{"id": 652, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.figure(figsize=(10,5))\nplt.plot(x, y)\nplt.xticks(ticks=range(0, max(x)+1), labels=range(0, max(x)+1), rotation=45)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Line chart of y over x')\nplt.grid(True)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 652, "library_problem_id": 141, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 653, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y)\nplt.xticks(x)\nplt.tick_params(axis='x', which='both', length=0)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 653, "library_problem_id": 142, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 654, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = sns.load_dataset(\"exercise\")\n\n# Create a new column 'group' based on 'diet'\ndf['group'] = df['diet'].apply(lambda x: 'Fat' if x == 'chicken' else 'No Fat')\n\n# Create a catplot\ng = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"group\", data=df, kind=\"swarm\")\n\n# Change the subplot titles\ng.set_titles(\"Group: {col_name}\")\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 654, "library_problem_id": 143, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 655, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = sns.load_dataset(\"exercise\")\n\n# Create a catplot\ng = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"swarm\")\n\n# Change the xlabels\ng.set_xlabels(\"Exercise Time\")\ng.set_ylabels(\"Pulse\")\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 655, "library_problem_id": 144, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 656, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = sns.load_dataset(\"exercise\")\n\n# Create a catplot\ng = sns.catplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=df, kind=\"swarm\")\n\n# Remove ylabel\ng.set_axis_labels(x_var=\"time\", y_var=\"\")\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 656, "library_problem_id": 145, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 657, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y, label=\"y\")\nplt.legend(fontsize=8)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 657, "library_problem_id": 146, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 146}}
{"id": 658, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.figure(figsize=(5, 5), dpi=300)\nplt.plot(x, y)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 658, "library_problem_id": 147, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 659, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y, label=\"y\")\nplt.legend(frameon=False)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 659, "library_problem_id": 148, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 660, "code": ["<code>\nimport numpy as np\nimport math\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nt = np.linspace(0, 2 * math.pi, 400)\na = np.sin(t)\nb = np.cos(t)\nc = a + b\n\nplt.figure()\nplt.plot(t, a, label='sin(t)')\nplt.plot(t, b, label='cos(t)')\nplt.plot(t, c, label='sin(t) + cos(t)')\nplt.legend()\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 660, "library_problem_id": 149, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 149}}
{"id": 661, "code": ["<code>\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 661, "library_problem_id": 150, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 662, "code": ["<code>\nimport seaborn as sns\nimport matplotlib.pylab as plt\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    {\n        \"a\": np.arange(1, 31),\n        \"b\": [\"A\",] * 10 + [\"B\",] * 10 + [\"C\",] * 10,\n        \"c\": np.random.rand(30),\n    }\n)\n\n# Create a FacetGrid\ng = sns.FacetGrid(df, row=\"b\", hue=\"b\", aspect=4)\n\n# Map a pointplot of 'c' over 'a', and use a few customizations\ng.map(sns.pointplot, \"a\", \"c\", order=df.sort_values('a')['a'], \n      palette=\"viridis\", ci=None, legend=False)\n\n# Adding some customizations\ng.set(xlim=(0, 31), xlabel=\"\", ylabel=\"\", xticks=range(1, 31, 2))\n\n# Rotate the x-axis labels\ng.set_xticklabels(rotation=90)\n\n# Remove the legend\ng.add_legend()\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 662, "library_problem_id": 151, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 663, "code": ["<code>\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\n\nx = np.random.random(10)\ny = np.random.random(10)\nz = np.random.random(10)\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(x, y, z)\nax.view_init(azim=100, elev=50)\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 663, "library_problem_id": 152, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 152}}
{"id": 664, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\nplt.xticks([])\nplt.yticks([])\n\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 664, "library_problem_id": 153, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 153}}
{"id": 665, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nx = np.random.random((10, 10))\nfrom matplotlib import gridspec\nnrow = 2\nncol = 2\nfig = plt.figure(figsize=(ncol + 1, nrow + 1))\ngs = gridspec.GridSpec(nrow, ncol)\nfor i in range(nrow):\n    for j in range(ncol):\n        ax = plt.subplot(gs[i, j])\n        ax.imshow(x)\n        ax.set_xticks([])\n        ax.set_yticks([])\nplt.tight_layout()\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 665, "library_problem_id": 154, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 666, "code": ["<code>\nimport tensorflow as tf\n\n# Initialize the variable x with a value of 0\nx = tf.Variable(0)\n\n# Update the value of x to 1\nx.assign(1)\n\n# Retrieve the value of x as an integer\nx_value = x.numpy().item()\n\nprint(x_value)  # Output: 1\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 666, "library_problem_id": 0, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 667, "code": ["<code>\nimport tensorflow as tf\n\n# Initialize the variable x\nx = tf.Variable(0)\n\n# Update the variable x to the value 114514\nx.assign(114514)\n\n# Initialize a TensorFlow session\nsess = tf.Session()\n\n# Run the session\nsess.run(tf.global_variables_initializer())\n\n# Retrieve the value of x\nx_value = sess.run(x)\n\n# Print the value of x\nprint(x_value)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 667, "library_problem_id": 1, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 668, "code": ["<code>\nimport tensorflow as tf\nimport numpy as np\n\nlabels = [0, 6, 5, 4, 2]\nnum_classes = 10\n\nresult = tf.one_hot(labels, depth=num_classes, dtype=tf.int32)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 668, "library_problem_id": 2, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 669, "code": ["<code>\nimport numpy as np\n\nlabels = [0, 6, 5, 4, 2]\nnum_classes = 10\n\nresult = np.zeros((len(labels), num_classes), dtype=np.int32)\nfor i, label in enumerate(labels):\n    result[i, label] = 1\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 669, "library_problem_id": 3, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 670, "code": ["<code>\nimport tensorflow as tf\n\nlabels = [0, 6, 5, 4, 2]\nnum_classes = 10\n\nresult = tf.one_hot(labels, depth=num_classes, dtype=tf.int32)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 670, "library_problem_id": 4, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 671, "code": ["<code>\nimport tensorflow as tf\nimport numpy as np\n\nexample_labels = [0, 6, 5, 4, 2]\ndef f(labels=example_labels):\n    # Create a NumPy array from the labels\n    labels_np = np.array(labels, dtype=np.int32)\n    \n    # Create a one-hot encoded tensor\n    labels_tensor = tf.one_hot(labels_np, depth=10, dtype=tf.int32)\n    \n    return labels_tensor\n\n# Test the function\nprint(f(example_labels))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 671, "library_problem_id": 5, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 672, "code": ["<code>\nimport numpy as np\n\nlabels = [0, 6, 5, 4, 2]\nnum_classes = 10\n\nresult = np.zeros((len(labels), num_classes), dtype=int)\nfor i, label in enumerate(labels):\n    result[i, label] = 1\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 672, "library_problem_id": 6, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 673, "code": ["<code>\nimport tensorflow as tf\n\n# Disable eager execution\ntf.compat.v1.disable_eager_execution()\n\n# Input data\ninput = [10, 20, 30]\n\n# Create a TensorFlow dataset from the input data\ndataset = tf.data.Dataset.from_tensor_slices(input)\n\n# Define a custom map function that transforms a single input element into multiple output elements\ndef custom_map_func(x):\n    return [x, x*2]\n\n# Apply the custom map function to the dataset\ndataset = dataset.map(custom_map_func)\n\n# Flatten the resulting dataset into a single sequence\nflat_dataset = dataset.flat_map(lambda x: x)\n\n# Convert the flattened dataset back to a Python list\nresult = list(flat_dataset.as_numpy_iterator())\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 673, "library_problem_id": 7, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 674, "code": ["<code>\nimport tensorflow as tf\n\nexample_input = [10, 20, 30]\n\ndef f(input=example_input):\n    # Create a TensorFlow dataset from the input\n    dataset = tf.data.Dataset.from_tensor_slices(input)\n\n    # Define a map function that returns multiple elements\n    def map_function(x):\n        return [x, x*2, x*3]\n\n    # Apply the map function to the dataset\n    dataset = dataset.map(map_function)\n\n    # Flatten the dataset into a one-dimensional list\n    flat_list = [element for sublist in list(dataset.as_numpy_iterator()) for element in sublist]\n\n    return flat_list\n\nresult = f(example_input)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 674, "library_problem_id": 8, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 7}}
{"id": 675, "code": ["<code>\nimport tensorflow as tf\n\nlengths = tf.constant([4, 3, 5, 2])\ntotal_length = 8\n\n# Create a range tensor from 0 to total_length\nrange_tensor = tf.range(total_length)\n\n# Expand dimensions to match the shape of lengths tensor\nexpanded_lengths = tf.expand_dims(lengths, 1)\n\n# Create a binary mask\nmask = tf.less(range_tensor, expanded_lengths)\n\n# Convert the mask to tf.int32 and then to tf.float32 for further computations\nmask = tf.cast(mask, tf.int32)\nmask = tf.cast(mask, tf.float32)\n\nresult = mask\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 675, "library_problem_id": 9, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 676, "code": ["<code>\nimport tensorflow as tf\n\nlengths = tf.constant([4, 3, 5, 2])\nmax_length = tf.reduce_max(lengths)\nmask = tf.sequence_mask(lengths, max_length, dtype=tf.int32)\n\nprint(mask)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 676, "library_problem_id": 10, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 677, "code": ["<code>\nimport tensorflow as tf\n\nlengths = tf.constant([4, 3, 5, 2])\ntotal_length = 6\n\n# Create a range tensor from 0 to total_length\nrange_tensor = tf.range(total_length)\n\n# Expand dimensions to match the shape of lengths tensor\nrange_tensor = tf.expand_dims(range_tensor, 0)\n\n# Compare range tensor with lengths tensor\nmask = tf.less(range_tensor, tf.expand_dims(lengths, 1))\n\n# Convert boolean values to integers\nmask = tf.cast(mask, tf.int32)\n\nresult = mask\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 677, "library_problem_id": 11, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 678, "code": ["<code>\nimport tensorflow as tf\n\nexample_lengths = [4, 3, 5, 2]\ndef f(lengths=example_lengths):\n    max_length = tf.reduce_max(lengths)\n    range_tensor = tf.range(max_length)\n    range_tensor = tf.expand_dims(range_tensor, 0)  # add a dimension for broadcasting\n    lengths = tf.expand_dims(lengths, 1)  # add a dimension for broadcasting\n    mask = tf.less(range_tensor, lengths)\n    mask = tf.cast(mask, tf.int32)\n    return mask\n\nprint(f(example_lengths))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 678, "library_problem_id": 12, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 679, "code": ["<code>\nimport tensorflow as tf\n\nlengths = tf.constant([4, 3, 5, 2])\ntotal_length = 8\n\n# Create a range tensor from 0 to total_length\nrange_tensor = tf.range(total_length)\n\n# Expand dimensions to match the shape of lengths tensor\nrange_tensor = tf.expand_dims(range_tensor, 0)\n\n# Compare range tensor with lengths tensor\nmask = tf.less(range_tensor, tf.expand_dims(lengths, 1))\n\n# Convert boolean mask to int32\nmask = tf.cast(mask, tf.int32)\n\nresult = mask\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 679, "library_problem_id": 13, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 680, "code": ["<code>\nimport tensorflow as tf\n\na = tf.constant([1,2,3])\nb = tf.constant([4,5,6,7])\n\na = tf.expand_dims(a, -1)  # Add a new axis to a\nb = tf.transpose(b)  # Transpose b\n\nresult = tf.transpose(tf.stack([tf.tile(a, [tf.shape(b)[-1]]), tf.repeat(b, tf.shape(a)[0])]))\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 680, "library_problem_id": 14, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 681, "code": ["<code>\nimport tensorflow as tf\n\nexample_a = tf.constant([1,2,3])\nexample_b = tf.constant([4,5,6,7])\n\ndef f(a=example_a, b=example_b):\n    # Create a tensor of indices for the first dimension of a and b\n    a_indices = tf.range(tf.shape(a)[0])\n    b_indices = tf.range(tf.shape(b)[0])\n\n    # Create a meshgrid of indices\n    a_indices, b_indices = tf.meshgrid(a_indices, b_indices, indexing='ij')\n\n    # Flatten the indices\n    a_indices = tf.reshape(a_indices, [-1])\n    b_indices = tf.reshape(b_indices, [-1])\n\n    # Gather the corresponding elements from a and b\n    a_elements = tf.gather(a, a_indices)\n    b_elements = tf.gather(b, b_indices)\n\n    # Stack the elements into a tensor\n    result = tf.stack([a_elements, b_elements], axis=1)\n\n    return result\n\nresult = f(example_a, example_b)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 681, "library_problem_id": 15, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 682, "code": ["<code>\nimport tensorflow as tf\nimport numpy as np\n\nnp.random.seed(10)\na = tf.constant(np.random.rand(50, 100, 1, 512))\n\nresult = tf.squeeze(a, [2])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 682, "library_problem_id": 16, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 683, "code": ["<code>\nimport tensorflow as tf\nimport numpy as np\n\nnp.random.seed(10)\na = tf.constant(np.random.rand(50, 100, 512))\n\n# Add a new dimension at the end of the tensor\nresult = tf.expand_dims(a, axis=2)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 683, "library_problem_id": 17, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 684, "code": ["<code>\nimport tensorflow as tf\nimport numpy as np\n\nnp.random.seed(10)\na = tf.constant(np.random.rand(50, 100, 512))\n\n# Add a new dimension at the start and end\nresult = tf.expand_dims(a, axis=[0, 3])\n\nprint(result.shape)  # should print (1, 50, 100, 1, 512)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 684, "library_problem_id": 18, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 685, "code": ["<code>\nimport tensorflow as tf\nimport numpy as np\n\nnp.random.seed(10)\nA = tf.constant(np.random.randint(100,size=(5, 3)))\n\nresult = tf.reduce_sum(A, axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 685, "library_problem_id": 19, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 686, "code": ["<code>\nimport tensorflow as tf\nimport numpy as np\n\nnp.random.seed(10)\nA = tf.constant(np.random.randint(100,size=(5, 3)))\n\nresult = tf.reduce_prod(A, axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 686, "library_problem_id": 20, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 687, "code": ["<code>\nimport tensorflow as tf\n\nA = tf.constant([-0.5, -0.1, 0, 0.1, 0.5, 2], dtype=tf.float32)\n\nresult = tf.math.reciprocal(A)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 687, "library_problem_id": 21, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 19}}
{"id": 688, "code": ["<code>\nimport tensorflow as tf\n\na = tf.constant([\n  [1,1,1],\n  [1,1,1]\n])\nb = tf.constant([\n  [0,0,0],\n  [1,1,1]\n])\n\nresult = tf.reduce_sum(tf.square(tf.subtract(a, b)), axis=1)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 688, "library_problem_id": 22, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 689, "code": ["<code>\nimport tensorflow as tf\n\na = tf.constant([\n  [1,1,1],\n  [0,1,1]\n])\nb = tf.constant([\n  [0,0,1],\n  [1,1,1]\n])\n\n# Calculate the L2 distance\nresult = tf.reduce_sum(tf.square(a - b), axis=0)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 689, "library_problem_id": 23, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 22}}
{"id": 690, "code": ["<code>\nimport tensorflow as tf\n\nexample_a = tf.constant([\n  [1,1,1],\n  [1,1,1]\n])\nexample_b = tf.constant([\n  [0,0,0],\n  [1,1,1]\n])\ndef f(A=example_a,B=example_b):\n    # Subtract corresponding elements\n    subtract = A - B\n    # Square the differences\n    square = tf.square(subtract)\n    # Sum the squared differences for each row\n    result = tf.reduce_sum(square, axis=1)\n    return result\n\nresult = f(example_a, example_b)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 690, "library_problem_id": 24, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 691, "code": ["<code>\n# Use tf.gather_nd to retrieve elements from x using y and z as indices\nindices = tf.stack([y, z], axis=1)\nresult = tf.gather_nd(x, indices)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 691, "library_problem_id": 25, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 692, "code": ["<code>\nresult = tf.gather_nd(x, tf.stack([row, col], axis=1))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 692, "library_problem_id": 26, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 693, "code": ["<code>\nresult = tf.gather_nd(tf.gather_nd(x, tf.stack([y, z], axis=1)), tf.stack([z, y], axis=1))\n    return result\n    ### END SOLUTION\n\n# Test the function\nprint(f(example_x, example_y, example_z))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 693, "library_problem_id": 27, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 694, "code": ["<code>\nimport tensorflow as tf\nimport numpy as np\n\nnp.random.seed(10)\nA = tf.constant(np.random.randint(low=0, high=5, size=(10, 20, 30)))\nB = tf.constant(np.random.randint(low=0, high=5, size=(10, 20, 30)))\n\n# Use tf.einsum to perform the dot product\nresult = tf.einsum('ijk,ilk->ijl', A, B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 694, "library_problem_id": 28, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 695, "code": ["<code>\nimport tensorflow as tf\nimport numpy as np\n\nnp.random.seed(10)\nA = tf.constant(np.random.randint(low=0, high=5, size=(10, 20, 30)))\nB = tf.constant(np.random.randint(low=0, high=5, size=(10, 20, 30)))\n\nresult = tf.einsum('ijk,ilk->ijl', A, B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 695, "library_problem_id": 29, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 696, "code": ["<code>\nimport tensorflow as tf\n\nx=[b'\\xd8\\xa8\\xd9\\x85\\xd8\\xb3\\xd8\\xa3\\xd9\\x84\\xd8\\xa9',\n   b'\\xd8\\xa5\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1',\n   b'\\xd9\\x82\\xd8\\xb6\\xd8\\xa7\\xd8\\xa1',\n   b'\\xd8\\xac\\xd9\\x86\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a',\n   b'\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a']\n\n# Convert the list of byte strings to a TensorFlow tensor\nbyte_tensor = tf.constant(x)\n\n# Decode the byte strings to Unicode strings\nresult = tf.strings.unicode_transcode(byte_tensor, \"UTF-8\", \"UTF-32\")\n\n# Convert the TensorFlow tensor to a Python list\nresult = result.numpy().tolist()\n\n# Print the result\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 696, "library_problem_id": 30, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 697, "code": ["<code>\nimport tensorflow as tf\n\nexample_x=[b'\\xd8\\xa8\\xd9\\x85\\xd8\\xb3\\xd8\\xa3\\xd9\\x84\\xd8\\xa9',\n    b'\\xd8\\xa5\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd9\\x82\\xd8\\xb6\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd8\\xac\\xd9\\x86\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a',\n    b'\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a']\n\ndef f(x=example_x):\n    # Convert the list of byte strings to a TensorFlow tensor of Unicode strings\n    result = tf.strings.unicode_transcode(x, \"UTF-8\", \"UTF-32\")\n    return result\n\nresult = f(example_x)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 697, "library_problem_id": 31, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 698, "code": ["<code>\nHere is the completed function:\n\n```python\nimport tensorflow as tf\n\nx = tf.convert_to_tensor([[[[1, 2, 3], [2, 3, 4], [0, 0, 0]],\n      [[1, 2, 3], [2, 0, 4], [3, 4, 5]],\n      [[1, 2, 3], [0, 0, 0], [0, 0, 0]],\n      [[1, 2, 3], [1, 2, 3], [0, 0, 0]]],\n     [[[1, 2, 3], [0, 1, 0], [0, 0, 0]],\n      [[1, 2, 3], [2, 3, 4], [0, 0, 0]],\n      [[1, 2, 3], [0, 0, 0], [0, 0, 0]],\n      [[1, 2, 3], [1, 2, 3], [1, 2, 3]]]], dtype=tf.float32)\n\n# Create a mask to exclude zero-padded values\nmask = tf.not_equal(x, 0)\n\n# Compute the sum of non-zero entries along the second to last dimension\nsum_non_zero = tf.reduce_sum(tf.boolean_mask(x, mask))\n\n# Compute the count of non-zero entries along the second to last dimension\ncount_non_zero = tf.reduce_sum(\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 698, "library_problem_id": 32, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 699, "code": ["<code>\nimport tensorflow as tf\n\nx = [[[[1, 2, 3], [2, 3, 4], [0, 0, 0]],\n      [[1, 2, 3], [2, 0, 4], [3, 4, 5]],\n      [[1, 2, 3], [0, 0, 0], [0, 0, 0]],\n      [[1, 2, 3], [1, 2, 3], [0, 0, 0]]],\n     [[[1, 2, 3], [0, 1, 0], [0, 0, 0]],\n      [[1, 2, 3], [2, 3, 4], [0, 0, 0]],\n      [[1, 2, 3], [0, 0, 0], [0, 0, 0]],\n      [[1, 2, 3], [1, 2, 3], [1, 2, 3]]]]\nx = tf.convert_to_tensor(x, dtype=tf.float32)\n\n# Create a mask to ignore zero-padded values\nmask = tf.not_equal(x, 0)\n\n# Calculate the variance along the second to last dimension\nresult = tf.math.reduce_variance(tf.boolean_mask(x, mask), axis=-2)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 699, "library_problem_id": 33, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 700, "code": ["<code>\nHere is the completed function:\n\n```python\nimport tensorflow as tf\n\nexample_x = [[[[1, 2, 3], [2, 3, 4], [0, 0, 0]],\n      [[1, 2, 3], [2, 0, 4], [3, 4, 5]],\n      [[1, 2, 3], [0, 0, 0], [0, 0, 0]],\n      [[1, 2, 3], [1, 2, 3], [0, 0, 0]]],\n     [[[1, 2, 3], [0, 1, 0], [0, 0, 0]],\n      [[1, 2, 3], [2, 3, 4], [0, 0, 0]],\n      [[1, 2, 3], [0, 0, 0], [0, 0, 0]],\n      [[1, 2, 3], [1, 2, 3], [1, 2, 3]]]]\nexample_x = tf.convert_to_tensor(example_x, dtype=tf.float32)\n\ndef f(x=example_x):\n    # Compute the sum of non-zero entries along the second to last dimension\n    sum_non_zero = tf.reduce_sum(x, axis=-2, keepdims=True)\n\n    # Compute the count of non-zero entries along the second to last dimension\n    count_non_zero = tf.cast(tf.math.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 700, "library_problem_id": 34, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 701, "code": ["<code>\nimport tensorflow as tf\n\n# Assuming you have a TensorFlow 1.5 code using `tf.Session`\n\n# Create a TensorFlow 1.5 session\nsess = tf.Session()\n\n# Define a TensorFlow operation\nx = tf.constant(1)\ny = tf.constant(2)\nz = tf.add(x, y)\n\n# Run the operation\nresult = sess.run(z)\n\n# Close the session\nsess.close()\n\n# Now, let's convert this to TensorFlow 2.3.0\n\n# Define a TensorFlow operation\nx = tf.constant(1)\ny = tf.constant(2)\nz = tf.add(x, y)\n\n# Since TensorFlow 2.0, you don't need to create a session to run a TensorFlow operation\n# You can simply evaluate the operation\nresult = z\n\n# The result will be automatically disposed of when it goes out of scope\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 701, "library_problem_id": 35, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 702, "code": ["<code>\nimport tensorflow as tf\n\na = tf.constant(\n    [[0.3232, -0.2321, 0.2332, -0.1231, 0.2435, 0.6728],\n     [0.2323, -0.1231, -0.5321, -0.1452, 0.5435, 0.1722],\n     [0.9823, -0.1321, -0.6433, 0.1231, 0.023, 0.0711]]\n)\n\nresult = tf.argmax(a, axis=1)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 702, "library_problem_id": 36, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 703, "code": ["<code>\nimport tensorflow as tf\n\na = tf.constant(\n    [[0.3232, -0.2321, 0.2332, -0.1231, 0.2435, 0.6728],\n     [0.2323, -0.1231, -0.5321, -0.1452, 0.5435, 0.1722],\n     [0.9823, -0.1321, -0.6433, 0.1231, 0.023, 0.0711]]\n)\n\nresult = tf.argmax(a, axis=0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 703, "library_problem_id": 37, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 704, "code": ["<code>\ndef f(a=example_a):\n    result = tf.argmax(a, axis=1)\n    return result\n\n# Test the function\nprint(f(example_a))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 704, "library_problem_id": 38, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 705, "code": ["<code>\nimport tensorflow as tf\n\na = tf.constant(\n    [[0.3232, -0.2321, 0.2332, -0.1231, 0.2435, 0.6728],\n     [0.2323, -0.1231, -0.5321, -0.1452, 0.5435, 0.1722],\n     [0.9823, -0.1321, -0.6433, 0.1231, 0.023, 0.0711]]\n)\n\nresult = tf.argmin(a, axis=0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 705, "library_problem_id": 39, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 706, "code": ["<code>\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nnetwork_layout = []\nfor i in range(3):\n    network_layout.append(8)\n\nmodel = Sequential()\n\ninputdim = 4\nactivation = 'relu'\noutputdim = 2\nopt='rmsprop'\nepochs = 50\n#Adding input layer and first hidden layer\nmodel.add(Dense(network_layout[0],\n                name=\"Input\",\n                input_dim=inputdim,\n                kernel_initializer='he_normal',\n                activation=activation))\n\n#Adding the rest of hidden layer\nfor numneurons in network_layout[1:]:\n    model.add(Dense(numneurons,\n                    kernel_initializer = 'he_normal',\n                    activation=activation))\n\n#Adding the output layer\nmodel.add(Dense(outputdim,\n                name=\"Output\",\n                kernel_initializer=\"he_normal\",\n                activation=\"relu\"))\n\n#Compiling the model\nmodel.compile(optimizer=opt,loss='mse',metrics=['mse','mae','mape'])\nmodel.summary()\n\n#Save the model in \"export/1\"\nmodel.save(\"export/1\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 706, "library_problem_id": 40, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 707, "code": ["<code>\nimport tensorflow as tf\n\nseed_x = 10\n\ntf.random.set_seed(seed_x)\n\nresult = tf.random.uniform(shape=(10,), minval=1, maxval=5, dtype=tf.int32)\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 707, "library_problem_id": 41, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 708, "code": ["<code>\nimport tensorflow as tf\n\nseed_x = 10\n\ntf.random.set_seed(seed_x)\nresult = tf.random.uniform(shape=(114,), minval=2, maxval=6, dtype=tf.int32)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 708, "library_problem_id": 42, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 709, "code": ["<code>\ntf.random.set_seed(seed_x)\n    result = tf.random.uniform(shape=(10,), minval=1, maxval=5, dtype=tf.int32)\n    return result\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 709, "library_problem_id": 43, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 710, "code": ["<code>\nimport tensorflow as tf\n\nresult = tf.__version__\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 710, "library_problem_id": 44, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 711, "code": ["<code>\n# Use polyfit to find the coefficients A and B for a logarithmic function\ncoefficients = np.polyfit(np.log(x), y, 1)\nA = coefficients[0]\nB = coefficients[1]\n\nresult = (A, B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 711, "library_problem_id": 0, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 712, "code": ["<code>\nimport numpy as np\nimport scipy\n\nx = np.array([1, 7, 20, 50, 79])\ny = np.array([10, 19, 30, 35, 51])\n\n# Use np.log1p to avoid log(0)\nlog_x = np.log1p(x)\n\n# Use np.polyfit to find the coefficients\ncoefficients = np.polyfit(log_x, y, 1)\n\nresult = coefficients\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 712, "library_problem_id": 1, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 713, "code": ["<code>\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\ndef func(x, a, b, c):\n    return a * np.exp(b * x) + c\n\ny = np.array([1, 7, 20, 50, 79])\nx = np.array([10, 19, 30, 35, 51])\np0 = (4, 0.1, 1)\n\npopt, pcov = curve_fit(func, x, y, p0)\n\nresult = popt\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 713, "library_problem_id": 2, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 714, "code": ["<code>\nfrom scipy import stats\nimport numpy as np\nnp.random.seed(42)\nx = np.random.normal(0, 1, 1000)\ny = np.random.normal(0, 1, 1000)\n\n# Perform two-sample KS test\nstatistic, p_value = stats.kstest(x, 'norm', args=(np.mean(x), np.std(x)))\nprint(f'KS statistic: {statistic}, p-value: {p_value}')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 714, "library_problem_id": 3, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 3}}
{"id": 715, "code": ["<code>\nresult = stats.kstest(x, y)\np_value = result.pvalue\n\nif p_value < alpha:\n    conclusion = \"The distributions are significantly different.\"\nelse:\n    conclusion = \"The distributions are not significantly different.\"\n\nresult = (p_value, conclusion)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 715, "library_problem_id": 4, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 3}}
{"id": 716, "code": ["<code>\nimport scipy.optimize as optimize\n\n# Define the function\ndef func(x, y, z):\n    return (x - 1)**2 + (y + 2)**2 + (z - 2)**2\n\n# Initial guess\ninitial_guess = [-1, 0, -3]\n\n# Use scipy's minimize function\nresult = optimize.minimize(func, initial_guess)\n\n# Print the result\nprint(f\"The minimum value is {result.fun} at x={result.x[0]}, y={result.x[1]}, z={result.x[2]}\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 716, "library_problem_id": 5, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 5}}
{"id": 717, "code": ["<code>\nimport numpy as np\nimport scipy.stats\n\nz_scores = np.array([-3, -2, 0, 2, 2.5])\np_values = scipy.stats.norm.sf(z_scores)\n\nprint(p_values)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 717, "library_problem_id": 6, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 718, "code": ["<code>\nimport scipy.stats\nimport numpy as np\n\nz_scores = [-3, -2, 0, 2, 2.5]\nmu = 3\nsigma = 4\n\np_values = [scipy.stats.norm.cdf(z, loc=mu, scale=sigma) for z in z_scores]\nprint(p_values)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 718, "library_problem_id": 7, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 719, "code": ["<code>\nimport numpy as np\nimport scipy.stats\n\np_values = [0.1, 0.225, 0.5, 0.75, 0.925, 0.95]\n\nz_scores = scipy.stats.norm.ppf(1-np.array(p_values))\n\nprint(z_scores)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 719, "library_problem_id": 8, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 720, "code": ["<code>\nimport numpy as np\nfrom scipy import stats\n\nstddev = 2.0785\nmu = 1.744\nx = 25\n\n# Calculate the z-score\nz = (np.log(x) - mu) / stddev\n\n# Calculate the cumulative distribution function\nresult = stats.norm.cdf(z)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 720, "library_problem_id": 9, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 721, "code": ["<code>\nimport numpy as np\nfrom scipy import stats\n\nstddev = 2.0785\nmu = 1.744\n\n# Convert mu and sigma from normal distribution to lognormal distribution\nsigma = stddev / np.sqrt(2 * np.log(2))\n\n# Calculate expected value and median\nexpected_value = np.exp(mu + sigma**2 / 2)\nmedian = np.exp(mu - sigma**2)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 721, "library_problem_id": 10, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 722, "code": ["<code>\nresult = sa.multiply(sb)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 722, "library_problem_id": 11, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 723, "code": ["<code>\nresult = sA.multiply(sB)\n    return result\n### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 723, "library_problem_id": 12, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 724, "code": ["<code>\nSure, I can help you with that. The function you're looking for is `scipy.interpolate.griddata`. It's a function that can be used for interpolation on a regular grid. However, since your data is irregular, you'll need to use a different method. The `scipy.interpolate.NearestNDInterpolator` or `scipy.interpolate.LinearNDInterpolator` can be used for this purpose.\n\nHere is the completed code:\n\n```python\nimport numpy as np\nfrom scipy.interpolate import NearestNDInterpolator\n\npoints = np.array([\n        [ 27.827,  18.53 , -30.417], [ 24.002,  17.759, -24.782],\n        [ 22.145,  13.687, -33.282], [ 17.627,  18.224, -25.197],\n        [ 29.018,  18.841, -38.761], [ 24.834,  20.538, -33.012],\n        [ 26.232,  22.327, -27.735], [ 23.017,  23.037, -29.23 ],\n        [ 28.761,  21.565, -31.586], [ 26\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 724, "library_problem_id": 13, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 725, "code": ["<code>\nHere is the completed function:\n\n```python\nimport numpy as np\nfrom scipy.interpolate import griddata\n\npoints = np.array([\n        [ 27.827,  18.53 , -30.417], [ 24.002,  17.759, -24.782],\n        [ 22.145,  13.687, -33.282], [ 17.627,  18.224, -25.197],\n        [ 29.018,  18.841, -38.761], [ 24.834,  20.538, -33.012],\n        [ 26.232,  22.327, -27.735], [ 23.017,  23.037, -29.23 ],\n        [ 28.761,  21.565, -31.586], [ 26.263,  23.686, -32.766]])\nV = np.array([0.205,  0.197,  0.204,  0.197,  0.212,\n                   0.208,  0.204,  0.205, 0.211,  0.215])\nrequest = np.array([[25,\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 725, "library_problem_id": 14, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 726, "code": ["<code>\nfrom scipy import misc\nfrom scipy.ndimage import rotate\nimport numpy as np\n\ndata_orig = misc.face()\nx0,y0 = 580,300 # left eye; (xrot,yrot) should point there\nangle = np.random.randint(1, 360)\n\n# rotate the image\ndata_rot = rotate(data_orig, angle)\n\n# calculate the rotation matrix\ntheta_rad = np.radians(angle)\nc, s = np.cos(theta_rad), np.sin(theta_rad)\nrotation_matrix = np.array(((c, -s), (s, c)))\n\n# calculate the center of the image\ncenter = np.array(data_orig.shape) / 2\n\n# calculate the coordinates of the point in the rotated image\nxrot = x0 - center[1]\nyrot = y0 - center[0]\nxrot, yrot = np.dot(rotation_matrix, (xrot, yrot)) + center\nxrot, yrot = round(xrot), round(yrot)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 726, "library_problem_id": 15, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 727, "code": ["<code>\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\narr = np.random.rand(4, 4)\nM = csr_matrix(arr)\n\ndef extract_diagonal(M):\n    if not isinstance(M, csr_matrix):\n        raise TypeError(\"Input must be a sparse CSR matrix\")\n    if M.shape[0] != M.shape[1]:\n        raise ValueError(\"Input must be a square matrix\")\n\n    return M.data[M.indices == np.arange(M.shape[0])]\n\nresult = extract_diagonal(M)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 727, "library_problem_id": 16, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 728, "code": ["<code>\nfrom scipy import stats\nimport random\nimport numpy as np\n\ndef poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\n\nrate = 1.0\nT = 100.0\ntimes = poisson_simul(rate, T)\n\n# Create a uniform distribution to compare with\nuniform_times = np.random.uniform(0, T, len(times))\n\n# Perform the Kolmogorov-Smirnov test\nresult = stats.kstest(times, 'uniform', args=(0, T))\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 728, "library_problem_id": 17, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 729, "code": ["<code>\nfrom scipy import stats\nimport random\nimport numpy as np\n\ndef poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\n\nexample_rate = 1.0\nexample_T = 100.0\nexample_times = poisson_simul(example_rate, example_T)\n\ndef f(times = example_times, rate = example_rate, T = example_T):\n    # Generate uniform distribution data\n    uniform_data = np.random.uniform(0, T, len(times))\n    # Perform Kolmogorov-Smirnov test\n    ks_test_result = stats.kstest(times, 'uniform', args=(0, T))\n    return ks_test_result\n\nprint(f())\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 729, "library_problem_id": 18, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 730, "code": ["<code>\nfrom scipy import stats\nimport random\nimport numpy as np\n\ndef poisson_simul(rate, T):\n    time = random.expovariate(rate)\n    times = [0]\n    while (times[-1] < T):\n        times.append(time+times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\n\ndef ks_test_uniform(times, T):\n    # Generate uniform distribution data\n    uniform_data = np.random.uniform(0, T, len(times))\n\n    # Perform K-S test\n    ks_test_result = stats.kstest(times, 'uniform', args=(0, T))\n\n    # Check if the p-value is less than 0.05 (95% confidence level)\n    if ks_test_result.pvalue < 0.05:\n        return True\n    else:\n        return False\n\nrate = 1.0\nT = 100.0\ntimes = poisson_simul(rate, T)\nresult = ks_test_uniform(times, T)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 730, "library_problem_id": 19, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 17}}
{"id": 731, "code": ["<code>\nfrom scipy import sparse\n\nc1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\nc2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\n\n# Use hstack function from scipy.sparse to horizontally concatenate c1 and c2\nFeature = sparse.hstack((c1, c2))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 731, "library_problem_id": 20, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 732, "code": ["<code>\nfrom scipy import sparse\n\nc1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\nc2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\n\n# Concatenate c1 and c2 horizontally\nFeature = sparse.hstack((c1, c2))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 732, "library_problem_id": 21, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 733, "code": ["<code>\nfrom scipy import sparse\n\nc1 = sparse.csr_matrix([[0, 0, 1, 0], [2, 0, 0, 0], [0, 0, 0, 0]])\nc2 = sparse.csr_matrix([[0, 3, 4, 0], [0, 0, 0, 5], [6, 7, 0, 8]])\n\n# Get the number of rows in c1 and c2\nn1, m1 = c1.shape\nn2, m2 = c2.shape\n\n# Check if the matrices have the same number of columns\nassert m1 == m2, \"Matrices do not have the same number of columns\"\n\n# Create a new data array by concatenating c1.data and c2.data\ndata = np.concatenate([c1.data, c2.data])\n\n# Create a new indices array by concatenating c1.indices and c2.indices\nindices = np.concatenate([c1.indices, c2.indices + n1])\n\n# Create a new indptr array\nindptr = np.concatenate([c1.indptr, c2.indptr + n1], dtype=np.int32)\n\n# Create the new CSR matrix\nFeature = sparse.csr_matrix((data, indices, indptr), shape=(n1 + n2, m1))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 733, "library_problem_id": 22, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 734, "code": ["<code>\nimport numpy as np\nimport scipy.spatial\nimport scipy.optimize\n\npoints1 = np.array([(x, y) for x in np.linspace(-1,1,7) for y in np.linspace(-1,1,7)])\nN = points1.shape[0]\npoints2 = 2*np.random.rand(N,2)-1\n\ndef distance_matrix(points1, points2):\n    tree = scipy.spatial.KDTree(points2)\n    return tree.query(points1)[0]\n\ndef optimize_mapping(points1, points2):\n    D = distance_matrix(points1, points2)\n    D[D==0] = np.inf\n    c = np.ones(len(points1))\n    A = -np.ones((len(points1), len(points2)))\n    A[np.arange(len(points1)), np.argmin(D, axis=1)] = 0\n    b = np.zeros(len(points1))\n    return scipy.optimize.linprog(c, A, b, (None, None), method='highs').x\n\nresult = optimize_mapping(points1, points2)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 734, "library_problem_id": 23, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 735, "code": ["<code>\nimport numpy as np\nimport scipy.spatial\nimport scipy.optimize\n\npoints1 = np.array([(x, y) for x in np.linspace(-1,1,7) for y in np.linspace(-1,1,7)])\nN = points1.shape[0]\npoints2 = 2*np.random.rand(N,2)-1\n\ndef manhattan_distance(x, y):\n    return np.sum(np.abs(x - y))\n\ndef total_manhattan_distance(mapping):\n    return sum(manhattan_distance(points1[i], points2[j]) for i, j in mapping)\n\nmapping = scipy.optimize.linear_sum_assignment(scipy.spatial.distance.cdist(points1, points2, metric='cityblock'))\nresult = total_manhattan_distance(mapping)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 735, "library_problem_id": 24, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 736, "code": ["<code>\nThe `setdiag` function in SciPy's sparse module works differently for `lil_matrix` and `csr_matrix`. The `lil_matrix` is a list of lists representation of a sparse matrix, while the `csr_matrix` is a compressed sparse row representation.\n\nThe `setdiag` function in `lil_matrix` replaces the diagonal elements with the input values, but it also removes the diagonal elements from the matrix. This is because `lil_matrix` stores all non-zero elements, including the diagonal ones, in a list of lists format. When you set the diagonal elements, it also removes the diagonal elements from the list of lists.\n\nOn the other hand, the `csr_matrix` uses a more efficient storage format for sparse matrices. The `csr_matrix` stores only the non-zero elements of the matrix in a compressed format, and the indices of these non-zero elements are stored in three one-dimensional arrays: `data`, `indices`, and `indptr`. The `indptr` array stores the position of each row in the `data` and `indices` arrays.\n\nThe `setdiag` function in `csr_matrix` does not remove the diagonal elements. Instead, it replaces the existing diagonal elements with the new values. This is because the `csr_matrix` stores the diagonal elements in the `data` and `indices` arrays, and the `indptr` array stores the position of the diagonal elements.\n\nTo remove the diagonal elements from a `csr_matrix` without converting it to a dense matrix, you can create a new `csr_matrix` with the non-diagonal elements. Here is an example:\n\n```python\nfrom sc\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 736, "library_problem_id": 25, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 737, "code": ["<code>\nimport numpy as np\nfrom scipy import ndimage\n\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nimg /= img.max()\nthreshold = 0.75\n\n# Create a mask where the values are above the threshold\nmask = img > threshold\n\n# Use the label function to find all connected regions\nlabels, num_labels = ndimage.label(mask)\n\nresult = num_labels\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 737, "library_problem_id": 26, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 738, "code": ["<code>\nimport numpy as np\nfrom scipy import ndimage\n\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nimg /= img.max()\nthreshold = 0.75\n\n# Create a mask where the values are below the threshold\nmask = img < threshold\n\n# Use the label function from scipy to label the connected regions\nlabels, num_labels = ndimage.label(mask)\n\nresult = num_labels\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 738, "library_problem_id": 27, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 739, "code": ["<code>\ndef f(img = example_img):\n        threshold = 0.75\n        # Convert the image to boolean where values exceed the threshold\n        mask = img > threshold\n        # Use scipy's label function to find connected regions\n        labeled_img, num_labels = ndimage.label(mask)\n        return num_labels\n    result = f(img)\n    ### END SOLUTION\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 739, "library_problem_id": 28, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 740, "code": ["<code>\nimport numpy as np\nfrom scipy import ndimage\nfrom scipy.spatial import distance\n\nnp.random.seed(10)\ngen = np.random.RandomState(0)\nimg = gen.poisson(2, size=(512, 512))\nimg = ndimage.gaussian_filter(img.astype(np.double), (30, 30))\nimg -= img.min()\nimg /= img.max()\nthreshold = 0.75\n\n# Create a binary image where regions exceed the threshold are True\nbinary_img = img > threshold\n\n# Label the connected regions in the binary image\nlabels, num_labels = ndimage.label(binary_img)\n\n# Initialize an empty list to store the distances\ndistances = []\n\n# Loop over each label\nfor i in range(1, num_labels+1):\n    # Get the coordinates of the pixels in the current label\n    y, x = np.where(labels == i)\n    \n    # Calculate the center of mass of the current label\n    com = ndimage.measurements.center_of_mass(img, labels, i)\n    \n    # Calculate the distance between the center of mass and the top-left corner\n    dist = distance.euclidean((0, 0), com)\n    \n    # Append the distance to the list\n    distances.append(dist)\n\nresult = distances\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 740, "library_problem_id": 29, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 741, "code": ["<code>\nimport numpy as np\nfrom scipy.sparse import lil_matrix\nfrom scipy import sparse\n\ndef make_symmetric(M):\n    M = M.maximum(M.transpose())\n    M = M.todok()\n    return M\n\nM = sparse.random(10, 10, density=0.1, format='lil')\nM = make_symmetric(M)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 741, "library_problem_id": 30, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 742, "code": ["<code>\nimport numpy as np\nfrom scipy.sparse import lil_matrix\n\ndef make_symmetric(sA):\n    if isinstance(sA, lil_matrix):\n        sA = sA.tolil()\n    else:\n        sA = sA.tocsr()\n\n    for i in range(sA.shape[0]):\n        for j in range(i+1, sA.shape[1]):\n            sA[j,i] = sA[i,j]\n\n    return sA\n\nexample_sA = lil_matrix(np.random.rand(10, 10))\nexample_sA = make_symmetric(example_sA)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 742, "library_problem_id": 31, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 743, "code": ["<code>\nimport numpy as np\nimport scipy.ndimage\n\n# Create a 32x32 binary array\nsquare = np.zeros((32, 32))\n\n# Set a square of 1's in the middle of the array\nsquare[10:-10, 10:-10] = 1\n\n# Set random 1's in the array\nnp.random.seed(12)\nx, y = (32*np.random.random((2, 20))).astype(int)\nsquare[x, y] = 1\n\n# Use scipy.ndimage.morphology.distance_transform_edt to calculate the distance to the nearest 0 for each cell\n# Then, use a mask to set all cells that are more than 1 distance from a 0 to 0\n# This will remove all isolated 1's\nsquare = np.where(scipy.ndimage.morphology.distance_transform_edt(square) > 1, 0, square)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 743, "library_problem_id": 32, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 744, "code": ["<code>\nimport numpy as np\nimport scipy.ndimage\n\n# Create a square array\nsquare = np.zeros((32, 32))\n\n# Fill the square array with random values\nsquare[10:-10, 10:-10] = np.random.randint(1, 255, size = (12, 12))\n\n# Set a random seed\nnp.random.seed(12)\n\n# Generate random x and y coordinates\nx, y = (32*np.random.random((2, 20))).astype(int)\n\n# Set the random coordinates to random values\nsquare[x, y] = np.random.randint(1, 255, size = (20,))\n\n# Create a binary mask where the isolated single-valued cells are True\nmask = scipy.ndimage.binary_dilation(square != 0) & ~scipy.ndimage.binary_dilation(square == 0)\n\n# Apply the mask to the square array\nsquare[mask] = 0\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 744, "library_problem_id": 33, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 745, "code": ["<code>\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\n\n# Calculate mean\nmean = col.sum() / col.nnz\n\n# Calculate standard deviation\nsquared_diff = np.square(col.data - mean)\nstd_dev = np.sqrt(squared_diff.sum() / col.nnz)\n\nprint(f\"Mean: {mean}\")\nprint(f\"Standard Deviation: {std_dev}\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 745, "library_problem_id": 34, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 746, "code": ["<code>\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\n\n# Get the indices and values of non-zero elements\nindices, values = col.nonzero()\n\n# The minimum value is the minimum of the values\nMin = np.min(values)\n\n# The maximum value is the maximum of the values\nMax = np.max(values)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 746, "library_problem_id": 35, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 34}}
{"id": 747, "code": ["<code>\nimport numpy as np\nfrom scipy.sparse import csr_matrix\nfrom scipy import stats\n\nnp.random.seed(10)\narr = np.random.randint(4,size=(988,988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\n\n# Median\nmedian = np.median(col.data)\n\n# Mode\nmode = stats.mode(col.data)[0][0]\n\nMedian, Mode = median, mode\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 747, "library_problem_id": 36, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 34}}
{"id": 748, "code": ["<code>\nfrom scipy.optimize import curve_fit\nimport numpy as np\n\ndef fourier_series(x, a0, a, b, c):\n    return a0 + sum(ai*np.cos(n*x) + bi*np.sin(n*x) for n in range(1, degree+1) for ai, bi in zip(a, b)) + c*x\n\npopt, pcov = curve_fit(fourier_series, z, Ua, p0=[Ua.mean(), [0]*degree, [0]*degree, 0])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 748, "library_problem_id": 37, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 749, "code": ["<code>\nHere is the completed function:\n\n```python\nimport numpy as np\nimport scipy.spatial.distance\n\ndef calculate_distances(array):\n    # Get unique values from the array\n    unique_values = np.unique(array)\n\n    # Initialize the output array\n    output = np.zeros((len(unique_values), len(unique_values)))\n\n    # For each unique value\n    for i in range(len(unique_values)):\n        for j in range(i+1, len(unique_values)):\n            # Get the coordinates of the unique values\n            coords_i = np.argwhere(array == unique_values[i])\n            coords_j = np.argwhere(array == unique_values[j])\n\n            # Calculate the pairwise distances\n            distances = scipy.spatial.distance.cdist(coords_i, coords_j, metric='euclidean')\n\n            # Update the output array\n            output[i, j] = distances.min()\n            output[j, i] = distances.min()\n\n    return output\n\nexample_array = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n                          [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n                          [0\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 749, "library_problem_id": 38, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 750, "code": ["<code>\nHere is the completed function:\n\n```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nfrom scipy import ndimage\n\ndef calculate_distances(raster):\n    # Get unique region IDs\n    labels, num_labels = ndimage.label(raster)\n    ids = np.unique(labels)[1:]  # Exclude background label\n\n    # Create a mask for each region\n    masks = [labels == id for id in ids]\n\n    # Calculate pairwise distances\n    distances = np.zeros((len(ids), len(ids)))\n    for i, mask1 in enumerate(masks):\n        for j, mask2 in enumerate(masks):\n            if i != j:\n                coords1 = np.argwhere(mask1)\n                coords2 = np.argwhere(mask2)\n                distances[i, j] = cdist(coords1, coords2).min()\n\n    return distances\n\nexample_array = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n                          [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 0, 0, 0, 0,\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 750, "library_problem_id": 39, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 751, "code": ["<code>\nHere is the completed function:\n\n```python\nimport numpy as np\nimport scipy.spatial.distance\n\nexample_arr = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n                          [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3],\n                          [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0],\n                          [1, 0,\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 751, "library_problem_id": 40, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 752, "code": ["<code>\nHere is the completed code:\n\n```python\nfrom scipy import interpolate\nimport numpy as np\n\nx = np.array([[0.12, 0.11, 0.1, 0.09, 0.08],\n              [0.13, 0.12, 0.11, 0.1, 0.09],\n              [0.15, 0.14, 0.12, 0.11, 0.1],\n              [0.17, 0.15, 0.14, 0.12, 0.11],\n              [0.19, 0.17, 0.16, 0.14, 0.12],\n              [0.22, 0.19, 0.17, 0.15, 0.13],\n              [0.24, 0.22, 0.19, 0.16, 0.14],\n              [0.27, 0.24, 0.21, 0.18, 0.15],\n              [0.29, 0.26, 0.22, 0.19, 0.16]])\ny = np.array([[71.64, 78.52, 84.91, 89.35, 97.58],\n              [66.28, 73.67, 79.87, 85\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 752, "library_problem_id": 41, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 753, "code": ["<code>\nHere is the completed function:\n\n```python\nimport numpy as np\nimport scipy.stats as ss\n\nx1=[38.7,  41.5,  43.8,  44.5,  45.5,  46.0,  47.7,  58.0]\nx2=[39.2,  39.3,  39.7,  41.4,  41.8,  42.9,  43.3,  45.8]\nx3=[34.0,  35.0,  39.0,  40.0,  43.0,  43.0,  44.0,  45.0]\nx4=[34.0,  34.8,  34.8,  35.4,  37.2,  37.8,  41.2,  42.8]\n\n# Convert the datasets to numpy arrays\nx1 = np.array(x1)\nx2 = np.array(x2)\nx3 = np.array(x3)\nx4 = np.array(x4)\n\n# Create a list of datasets\ndatasets = [x1, x2, x3, x4]\n\n# Use the anderson_ksamp function\nstatistic, critical_values, significance_level = ss.anderson_ksamp(datasets)\n\nprint('Statistic: ', statistic)\nprint('Critical values: ', critical_values)\nprint('Significance level: ', significance_level\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 753, "library_problem_id": 42, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 754, "code": ["<code>\nimport numpy as np\nimport scipy.stats as ss\n\nx1=[38.7,  41.5,  43.8,  44.5,  45.5,  46.0,  47.7,  58.0]\nx2=[39.2,  39.3,  39.7,  41.4,  41.8,  42.9,  43.3,  45.8]\n\n# Convert the lists to numpy arrays\nx1 = np.array(x1)\nx2 = np.array(x2)\n\n# Create a 2D array where each row is a sample\nsamples = np.array([x1, x2])\n\n# Perform the Anderson-Darling test\nresult = ss.anderson_ksamp(samples)\n\n# Check if the p-value is less than 0.05 (5% significance level)\nresult.significance_level[result.statistic < result.critical_values[result.df]]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 754, "library_problem_id": 43, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 755, "code": ["<code>\ndef kendall_tau(x, y):\n    return stats.kendalltau(x, y).correlation\n\ndef rolling_kendall_tau(df, window):\n    rolling_kendall = lambda x: stats.kendalltau(x['A'], x['B']).correlation\n    return df.rolling(window=window).apply(rolling_kendall)\n\ndf = rolling_kendall_tau(df, 2)\ndf\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 755, "library_problem_id": 44, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 756, "code": ["<code>\nfrom scipy import sparse\n\ndef is_csr_matrix_empty(csr_matrix):\n    return csr_matrix.nnz == 0\n\nsa = sparse.random(10, 10, density = 0.01, format = 'csr')\nresult = is_csr_matrix_empty(sa)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 756, "library_problem_id": 45, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 757, "code": ["<code>\nfrom scipy import sparse\n\ndef is_lil_matrix_empty(matrix):\n    return not any(matrix.data)\n\nsa = sparse.random(10, 10, density = 0.01, format = 'lil')\nresult = is_lil_matrix_empty(sa)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 757, "library_problem_id": 46, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 758, "code": ["<code>\nimport numpy as np\nfrom scipy.linalg import block_diag\nnp.random.seed(10)\na = np.random.rand(100,2,2)\n\nresult = np.array([])\nfor i in range(a.shape[0]):\n    if result.size == 0:\n        result = a[i]\n    else:\n        result = block_diag(result, a[i])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 758, "library_problem_id": 47, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 759, "code": ["<code>\nimport numpy as np\nfrom scipy import stats\nnp.random.seed(10)\npre_course_scores = np.random.randn(10)\nduring_course_scores = np.random.randn(10)\n\np_value = stats.ranksums(pre_course_scores, during_course_scores)[1]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 759, "library_problem_id": 48, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 760, "code": ["<code>\nimport numpy as np\nfrom scipy import stats\n\nexample_pre_course_scores = np.random.randn(10)\nexample_during_course_scores = np.random.randn(10)\n\ndef f(pre_course_scores = example_pre_course_scores, during_course_scores = example_during_course_scores):\n    # return the solution in this function\n    _, p_value = stats.ranksums(pre_course_scores, during_course_scores)\n    return p_value\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 760, "library_problem_id": 49, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 761, "code": ["<code>\nimport numpy as np\n\ndef calculate_kurtosis(data):\n    mean = np.mean(data)\n    var = np.var(data)\n    n = len(data)\n    \n    kurtosis = np.sum((data - mean)**4) / (n * var**2)\n    return kurtosis\n\na = np.array([1. , 2. , 2.5, 400. , 6. , 0.])\nkurtosis_result = calculate_kurtosis(a)\nprint(kurtosis_result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 761, "library_problem_id": 50, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 762, "code": ["<code>\nimport numpy as np\nimport scipy.stats\n\na = np.array([1. , 2. , 2.5, 400. , 6. , 0.])\n\nkurtosis_result = scipy.stats.kurtosis(a)\nprint(kurtosis_result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 762, "library_problem_id": 51, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 763, "code": ["<code>\nimport numpy as np\nimport scipy.interpolate\n\n# Given data\ns = np.linspace(-1, 1, 50)\nt = np.linspace(-2, 0, 50)\nx, y = np.ogrid[-1:1:10j,-2:0:10j]\nz = (x + y)*np.exp(-6.0 * (x * x + y * y))\n\n# Interpolation\ninterp_func = scipy.interpolate.interp2d(s, t, z, kind='cubic')\n\n# New data points\nnew_s = np.linspace(-1, 1, 10)\nnew_t = np.linspace(-2, 0, 10)\n\n# Interpolate the function's value at new data points\nresult = interp_func(new_s, new_t)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 763, "library_problem_id": 52, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 764, "code": ["<code>\n# Create a grid of x, y values\n    x, y = np.ogrid[-1:1:10j,-2:0:10j]\n    # Create a function z based on x, y\n    z = (x + y)*np.exp(-6.0 * (x * x + y * y))\n    # Create a cubic interpolation function\n    interpolation_function = scipy.interpolate.interp2d(x, y, z, kind='cubic')\n    # Return the interpolated values for the input vectors s, t\n    return interpolation_function(s, t)\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 764, "library_problem_id": 53, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 52}}
{"id": 765, "code": ["<code>\nimport scipy.spatial\n\npoints = [[0,0], [1,4], [2,3], [4,1], [1,1], [2,2], [5,3]]\nvor = scipy.spatial.Voronoi(points)\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\n\ndef count_extra_points(vor, extraPoints):\n    point_region = vor.point_region\n    region_counts = {region: 0 for region in point_region}\n    for point in extraPoints:\n        min_index = min(range(len(vor.points)), key=lambda i: (vor.points[i][0]-point[0])**2 + (vor.points[i][1]-point[1])**2)\n        region = point_region[min_index]\n        region_counts[region] += 1\n    return region_counts\n\nresult = count_extra_points(vor, extraPoints)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 765, "library_problem_id": 54, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 766, "code": ["<code>\nHere is the completed function:\n\n```python\nimport scipy.spatial\n\npoints = [[0,0], [1,4], [2,3], [4,1], [1,1], [2,2], [5,3]]\nvor = scipy.spatial.Voronoi(points)\nextraPoints = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\n\ndef count_extra_points(vor, extraPoints):\n    point_region = vor.point_region\n    region_coords = vor.regions[point_region[0]]\n    region_coords = [coord for coord in region_coords if not -1 in coord]\n    region_coords = [coord for coord in region_coords if len(coord) > 1]\n    region_coords = [coord for sublist in region_coords for coord in sublist]\n    region_coords = [coord for i in range(len(region_coords)//2) for coord in region_coords[i*2:i*2+2]]\n    region_coords = [coord for i in range(len(region_coords)//2) for coord in region_coords[i*2:i*2+2]]\n    region_coords = [coord for i in range(len(region_coords)//2) for coord in region_coords[i*2:i*2+2]]\n    region_coords = [coord for i in range(len(region_coords)//2) for\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 766, "library_problem_id": 55, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 767, "code": ["<code>\nimport numpy as np\nimport scipy.sparse as sparse\n\nnp.random.seed(10)\nmax_vector_size = 1000\nvectors = [np.random.randint(100,size=900),np.random.randint(100,size=max_vector_size),np.random.randint(100,size=950)]\n\n# Pad shorter vectors with zeros\npadded_vectors = []\nfor vector in vectors:\n    padded_vector = np.pad(vector, (0, max_vector_size - len(vector)), 'constant', constant_values=0)\n    padded_vectors.append(padded_vector)\n\n# Convert the list of padded vectors to a 2D numpy array\narray = np.array(padded_vectors)\n\n# Create a sparse matrix from the 2D numpy array\nresult = sparse.csr_matrix(array)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 767, "library_problem_id": 56, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 768, "code": ["<code>\nimport numpy as np\nimport scipy.ndimage\n\na= np.zeros((5, 5))\na[1:4, 1:4] = np.arange(3*3).reshape((3, 3))\n\nb = scipy.ndimage.median_filter(a, size=3, mode='constant', origin=[0, 1])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 768, "library_problem_id": 57, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 769, "code": ["<code>\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\narr = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]])\nM = csr_matrix(arr)\nrow = 2\ncolumn = 3\n\ndef get_element(M, row, column):\n    if M.indices[M.indptr[row]:M.indptr[row+1]].__contains__(column):\n        return M.data[M.indptr[row]:M.indptr[row+1]][M.indices[M.indptr[row]:M.indptr[row+1]].tolist().index(column)]\n    else:\n        return 0\n\nresult = get_element(M, row, column)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 769, "library_problem_id": 58, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 770, "code": ["<code>\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\narr = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]])\nM = csr_matrix(arr)\nrow = [2, 1]\ncolumn = [3, 0]\n\ndef get_values(M, row, column):\n    result = []\n    for r, c in zip(row, column):\n        result.append(M[r, c])\n    return result\n\nresult = get_values(M, row, column)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 770, "library_problem_id": 59, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 771, "code": ["<code>\nimport numpy as np\nimport scipy.interpolate\n\narray = np.random.randint(0, 9, size=(10, 10, 10))\nx = np.linspace(0, 10, 10)\nx_new = np.linspace(0, 10, 100)\n\n# Create a new array with the same shape as the input array\nnew_array = np.empty_like(array)\n\n# Loop over the first axis of the input array\nfor i in range(array.shape[0]):\n    # Loop over the second and third axes of the input array\n    for j in range(array.shape[1]):\n        for k in range(array.shape[2]):\n            # Interpolate the data for each element in the second and third axes\n            f = scipy.interpolate.interp1d(x, array[i, j, k])\n            new_array[i, j, k] = f(x_new)\n\n# Print the new array\nprint(new_array)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 771, "library_problem_id": 60, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 60}}
{"id": 772, "code": ["<code>\nimport scipy.integrate\nimport math\nimport numpy as np\n\ndef NDfx(x, u, o2):\n    return((1/math.sqrt((2*math.pi)*o2))*(math.e**((-.5)*(x-u)**2/o2)))\n\nx = 2.5\nu = 1\no2 = 3\n\nprob, error = scipy.integrate.quad(NDfx, -np.inf, x, args=(u, o2))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 772, "library_problem_id": 61, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 773, "code": ["<code>\nimport scipy.integrate\nimport math\nimport numpy as np\n\ndef NDfx(x, u, o2):\n    return((1/math.sqrt((2*math.pi)*o2))*(math.e**((-.5)*(x-u)**2/o2)))\n\ndef f(x = 2.5, u = 1, o2 = 3):\n    prob, error = scipy.integrate.quad(NDfx, -np.inf, x, args=(u, o2))\n    return prob\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 773, "library_problem_id": 62, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 774, "code": ["<code>\nimport numpy as np\nimport scipy.fft as sf\n\ndef dct_matrix(N):\n    \"\"\"\n    Generate a NxN Discrete Cosine Transform (DCT) matrix.\n    \"\"\"\n    # Create a NxN matrix of indices\n    indices = np.indices((N, N))\n\n    # Calculate the DCT matrix\n    dct_mtx = np.cos((np.pi / N) * (indices[0] + 0.5) * indices[1])\n\n    # Normalize the DCT matrix\n    dct_mtx = dct_mtx * np.sqrt(2 / N)\n\n    # If N is even, normalize the first column and row\n    if N % 2 == 0:\n        dct_mtx[0, :] = dct_mtx[0, :] / np.sqrt(2)\n        dct_mtx[:, 0] = dct_mtx[:, 0] / np.sqrt(2)\n\n    return dct_mtx\n\nN = 8\nresult = dct_matrix(N)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 774, "library_problem_id": 63, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 775, "code": ["<code>\nfrom scipy import sparse\nimport numpy as np\n\n# Define the arrays representing the subdiagonal, diagonal, and superdiagonal elements\nsubdiag = np.array([-5., -23., -53., -95., -149.])\ndiag = np.array([3.5, 13., 28.5, 50., 77.5])\nsuperdiag = np.array([2.5, 11., 25.5, 46., 72.5])\n\n# Construct the tridiagonal matrix\nmatrix = sparse.diags([subdiag, diag, superdiag], [-1, 0, 1]).toarray()\n\n# Print the result\nprint(matrix)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 775, "library_problem_id": 64, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 776, "code": ["<code>\nimport numpy as np\nimport scipy.special\n\ndef binomial_matrix(N, p):\n    # Create a (N+1) x (N+1) matrix of indices\n    i, j = np.indices((N+1, N+1))\n\n    # Compute the binomial probability for each pair of (i, j)\n    # We use scipy.special.comb to compute choose(i, j)\n    # We use np.where to handle the case where j > i (in which case choose(i, j) is undefined)\n    M = scipy.special.comb(i, j, exact=True) * p**j * (1-p)**(i-j)\n    M = np.where(j > i, 0, M)\n\n    return M\n\nN = 3\np = 0.5\nresult = binomial_matrix(N, p)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 776, "library_problem_id": 65, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 777, "code": ["<code>\nimport pandas as pd\nimport io\nfrom scipy import stats\n\ntemp=u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp),index_col='probegenes')\n\ndef z_score_normalization(df):\n    return pd.DataFrame(stats.zscore(df, axis=1), index=df.index, columns=df.columns)\n\nresult = z_score_normalization(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 777, "library_problem_id": 66, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 778, "code": ["<code>\nimport pandas as pd\nimport io\nfrom scipy import stats\n\ntemp=u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp),index_col='probegenes')\n\ndef calculate_z_scores(df):\n    return pd.DataFrame(stats.zscore(df, axis=0), index=df.index, columns=df.columns)\n\nresult = calculate_z_scores(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 778, "library_problem_id": 67, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 66}}
{"id": 779, "code": ["<code>\nimport pandas as pd\nimport io\nfrom scipy import stats\n\ntemp=u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp),index_col='probegenes')\n\n# Calculate z-scores\nz_scores = df.apply(lambda x: stats.zscore(x), axis=0)\n\n# Combine original dataframe and z-scores into a new dataframe\nresult = pd.concat([df, z_scores], axis=1)\nresult.columns = ['sample1', 'sample2', 'sample3', 'z_sample1', 'z_sample2', 'z_sample3']\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 779, "library_problem_id": 68, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 780, "code": ["<code>\nimport pandas as pd\nimport io\nimport numpy as np\nfrom scipy import stats\n\ntemp=u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp),index_col='probegenes')\n\n# Calculate z-scores\nz_scores = pd.DataFrame(stats.zscore(df, axis=0, nan_policy='omit'), columns=df.columns)\n\n# Combine original data and z-scores\nresult = pd.concat([df, z_scores], axis=1)\n\n# Round all values to three decimal places\nresult = result.round(3)\n\n# Handle potential NaN values\nresult = result.fillna('NaN')\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 780, "library_problem_id": 69, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 781, "code": ["<code>\nimport scipy\nimport scipy.optimize\nimport numpy as np\n\ndef test_func(x):\n    return (x[0])**2+(x[1])**2\n\ndef test_grad(x):\n    return [2*x[0],2*x[1]]\n\nstarting_point = [1.8, 1.7]\ndirection = [-1, -1]\n\nresult = scipy.optimize.line_search(test_func, test_grad, starting_point, direction)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 781, "library_problem_id": 70, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 782, "code": ["<code>\nimport numpy as np\nfrom scipy.spatial import distance\n\nshape = (6, 6)\n\n# Create a grid of indices\nx, y = np.indices(shape)\n\n# Find the center of the grid\ncenter = np.array([(x.max() - x.min()) / 2, (y.max() - y.min()) / 2])\n\n# Calculate the Euclidean distance from the center\nresult = distance.cdist(np.dstack([x, y]).reshape(-1, 2), center.reshape(1, -1))\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 782, "library_problem_id": 71, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 783, "code": ["<code>\nimport numpy as np\n\nshape = (6, 6)\n\n# Create a grid of coordinates\nx, y = np.ogrid[:shape[0], :shape[1]]\n\n# Calculate the Manhattan distance from the center\ndist = np.abs(x - shape[0] // 2) + np.abs(y - shape[1] // 2)\n\nresult = dist\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 783, "library_problem_id": 72, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 71}}
{"id": 784, "code": ["<code>\nimport numpy as np\nfrom scipy.spatial import distance\n\ndef f(shape = (6, 6)):\n    # Create a 2D array with the shape of the first two dimensions of the given 3D array\n    x = np.zeros(shape)\n\n    # Calculate the center of the array\n    center = np.array(shape) // 2\n\n    # Create a grid of indices\n    indices = np.indices(shape)\n\n    # Calculate the Euclidean distance from the center for each element in the array\n    for i in range(shape[0]):\n        for j in range(shape[1]):\n            x[i, j] = distance.euclidean((i, j), center)\n\n    return x\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 784, "library_problem_id": 73, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 785, "code": ["<code>\nimport numpy as np\nimport scipy.ndimage\n\nx = np.arange(9).reshape(3, 3)\nshape = (6, 8)\n\n# Calculate the scale factors\nscale_x = shape[0] / x.shape[0]\nscale_y = shape[1] / x.shape[1]\n\n# Resample the array\nresult = scipy.ndimage.zoom(x, (scale_x, scale_y))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 785, "library_problem_id": 74, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 786, "code": ["<code>\nimport scipy.optimize\nimport numpy as np\n\nnp.random.seed(42)\na = np.random.rand(3,5)\nx_true = np.array([10, 13, 5, 8, 40])\ny = a.dot(x_true ** 2)\nx0 = np.array([2, 3, 1, 4, 20])\n\ndef objective_function(x):\n    return a.dot(x ** 2)\n\nres = scipy.optimize.minimize(objective_function, x0, method='BFGS')\nout = res.x\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 786, "library_problem_id": 75, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 787, "code": ["<code>\nimport scipy.optimize\nimport numpy as np\n\nnp.random.seed(42)\na = np.random.rand(3,5)\nx_true = np.array([10, 13, 5, 8, 40])\ny = a.dot(x_true ** 2)\nx0 = np.array([2, 3, 1, 4, 20])\nx_lower_bounds = x_true / 2\n\ndef objective_function(x):\n    return np.sum((y - a.dot(x ** 2)) ** 2)\n\nresult = scipy.optimize.minimize(objective_function, x0, method='L-BFGS-B', bounds=scipy.optimize.Bounds(x_lower_bounds, np.inf))\n\nout = result.x\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 787, "library_problem_id": 76, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 75}}
{"id": 788, "code": ["<code>\nimport scipy.integrate\nimport numpy as np\n\ndef solve_ivp_with_sinusoidal_input(N0, time_span):\n    def dy_dt(t, y):\n        return -100*y + np.sin(t)\n\n    sol = scipy.integrate.solve_ivp(dy_dt, time_span, [N0])\n    return sol.y\n\nN0 = 10\ntime_span = [-0.1, 0.1]\nsol = solve_ivp_with_sinusoidal_input(N0, time_span)\nresult = sol.y\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 788, "library_problem_id": 77, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 789, "code": ["<code>\nimport scipy.integrate\nimport numpy as np\n\ndef solve_ode_with_variable_input(N0, time_span):\n    def ode_func(t, N):\n        # Define the ODE function here.\n        # For example, we can consider a simple model where N is proportional to the sinusoidal input.\n        return np.sin(t) * N\n\n    sol = scipy.integrate.solve_ivp(ode_func, time_span, [N0])\n    return sol.y\n\n# Example usage:\nN0 = 1\ntime_span = [0, 10]\nresult = solve_ode_with_variable_input(N0, time_span)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 789, "library_problem_id": 78, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 790, "code": ["<code>\nimport scipy.integrate\nimport numpy as np\n\ndef solve_ode_with_variable_input(N0, time_span):\n    def ode_func(t, N):\n        # Define the ODE function here.\n        # For example, let's assume the ODE is N' = -0.1 * N + 0.5 * sin(2 * np.pi * t)\n        return -0.1 * N + 0.5 * np.sin(2 * np.pi * t)\n\n    sol = scipy.integrate.solve_ivp(ode_func, time_span, [N0])\n    return sol.y\n\n# Example usage:\nN0 = 10\ntime_span = [-0.1, 0.1]\nresult = solve_ode_with_variable_input(N0, time_span)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 790, "library_problem_id": 79, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 791, "code": ["<code>\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef function(x):\n    return -1*(18*x[0]+16*x[1]+12*x[2]+11*x[3])\n\nI=np.array((20,50,50,80))\nx0=I\n\ncons=[]\nsteadystate={'type':'eq', 'fun': lambda x: x.sum()-I.sum() }\ncons.append(steadystate)\n\n# Add non-negativity constraints\nfor i in range(len(I)):\n    cons.append({'type':'ineq', 'fun': lambda x, i=i: x[i]})\n\nres = minimize(function, x0, constraints=cons)\nprint(res.x)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 791, "library_problem_id": 80, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 792, "code": ["<code>\nfrom scipy import sparse\n\ndef merge_csr_matrices(a, b):\n    # Add the number of rows of 'a' to the row indices of 'b'\n    b.indices += a.shape[0]\n    b.indptr += a.shape[0]\n\n    # Concatenate 'a' and 'b'\n    result = sparse.vstack((a, b), format='csr')\n\n    return result\n\nsa = sparse.random(10, 10, density = 0.01, format = 'csr')\nsb = sparse.random(10, 10, density = 0.01, format = 'csr')\n\nresult = merge_csr_matrices(sa, sb)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 792, "library_problem_id": 81, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 793, "code": ["<code>\nfrom scipy import sparse\n\ndef merge_csr_matrices(a, b):\n    assert a.shape[0] == b.shape[0], \"Matrices must have the same number of rows\"\n    return sparse.hstack((a, b), format='csr')\n\nsa = sparse.random(10, 10, density = 0.01, format = 'csr')\nsb = sparse.random(10, 10, density = 0.01, format = 'csr')\n\nresult = merge_csr_matrices(sa, sb)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 793, "library_problem_id": 82, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 81}}
{"id": 794, "code": ["<code>\nimport scipy.integrate\n\ndef calculate_integral(c_values, low, high):\n    result = []\n    for c in c_values:\n        integral, error = scipy.integrate.quad(lambda x: 2*c*x, low, high)\n        result.append(integral)\n    return result\n\n# Assuming c_values is a list of constants loaded from a file\nc_values = [5, 10, 15]\nlow = 0\nhigh = 1\n\nresult = calculate_integral(c_values, low, high)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 794, "library_problem_id": 83, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 795, "code": ["<code>\nimport scipy.integrate\n\ndef f(c=5, low=0, high=1):\n    def integrand(x):\n        return 2*c*x\n    result, error = scipy.integrate.quad(integrand, low, high)\n    return result\n\n# Test the function\nprint(f(c=5, low=0, high=1))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 795, "library_problem_id": 84, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 83}}
{"id": 796, "code": ["<code>\nimport numpy as np\nfrom scipy import sparse\n\n# Create a sparse matrix V\nV = sparse.random(10, 10, density = 0.05, format = 'dok', random_state = 42)\n\n# Scalar value x\nx = 99\n\n# Iterate over the non-zero elements in V\nfor key, value in V.items():\n    # Check if the element is non-zero\n    if value != 0:\n        # Add x to the non-zero element\n        V[key] += x\n\nV = V.todok()  # Convert back to dok_matrix format\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 796, "library_problem_id": 85, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 797, "code": ["<code>\nfrom scipy import sparse\n\n# Create a sparse matrix V in COO format\nV = sparse.random(10, 10, density = 0.05, format = 'coo', random_state = 42)\n\n# Define the scalar value x\nx = 100\n\n# Add the scalar value to the non-zero elements of the sparse matrix\nV.data += x\n\n# Convert the sparse matrix back to COO format\nV = V.tocoo()\n\n# Print the sparse matrix\nprint(V)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 797, "library_problem_id": 86, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 798, "code": ["<code>\nfrom scipy import sparse\n\ndef add_scalar_to_sparse(V, x, y):\n    # Create a new COO matrix with the same shape as V\n    V_new = sparse.coo_matrix((V.data.copy(), V.rows.copy(), V.cols.copy()), shape=V.shape)\n\n    # Add scalar values to the non-zero elements\n    V_new.data += x\n    V_new.data[V_new.data > y] = y\n\n    return V_new\n\nV = sparse.random(10, 10, density = 0.05, format = 'coo', random_state = 42)\nx = 100\ny = 99\n\nV = add_scalar_to_sparse(V, x, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 798, "library_problem_id": 87, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 799, "code": ["<code>\nfrom scipy import sparse\nimport numpy as np\nimport math\n\n# Create a sparse matrix in CSC format\nsa = sparse.random(10, 10, density = 0.3, format = 'csc', random_state = 42)\n\n# Calculate the Euclidean norm of each column\nnorms = np.sqrt(np.square(sa).sum(axis=0))\n\n# Convert the norms to a 1D array\nnorms = norms.A1\n\n# Normalize each column by its Euclidean norm\nsa = sa.multiply(1/norms)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 799, "library_problem_id": 88, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 800, "code": ["<code>\nfrom scipy import sparse\nimport numpy as np\nimport math\n\n# Create a sparse matrix\nsa = sparse.random(10, 10, density = 0.3, format = 'csr', random_state = 42)\n\n# Calculate the Euclidean norm of each column\nnorms = np.sqrt(np.square(sa).sum(axis=0))\n\n# Convert the norms to a dense array\ndense_norms = np.asarray(norms).ravel()\n\n# Divide each column of sa by its corresponding norm\nsa = sa.multiply(1/dense_norms)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 800, "library_problem_id": 89, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 88}}
{"id": 801, "code": ["<code>\nimport scipy\nimport numpy as np\n\na = np.array([[26, 3, 0], [3, 195, 1], [0, 1, 17]])\n\n# Convert the matrix to a binary matrix where a connection exists if the frequency is greater than 0\na = (a > 0).astype(int)\n\n# Make the matrix symmetric by adding the transpose of the matrix to itself\na = a + a.T\n\n# Make sure the diagonal elements are 0 as self-loops are not allowed\nnp.fill_diagonal(a, 0)\n\nprint(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 801, "library_problem_id": 90, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 802, "code": ["<code>\nimport scipy\nimport numpy as np\n\na = np.array([[26, 3, 0], [3, 195, 1], [0, 1, 17]])\n\n# Convert the matrix to a binary matrix where communication exists if the number of emails exchanged is greater than 0\na = np.where(a > 0, 1, 0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 802, "library_problem_id": 91, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 803, "code": ["<code>\nimport numpy as np\nimport scipy.spatial\n\ndef find_closest_to_centroids(centroids, data, labels):\n    result = {}\n    for i in range(len(centroids)):\n        cluster_points = data[labels==i]\n        distances = scipy.spatial.distance.cdist(cluster_points, [centroids[i]])\n        closest_point_index = np.argmin(distances)\n        result[i] = closest_point_index\n    return result\n\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\nlabels = ... # assume you have the labels from scipy.cluster.hierarchy.cut_tree\n\nresult = find_closest_to_centroids(centroids, data, labels)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 803, "library_problem_id": 92, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 92}}
{"id": 804, "code": ["<code>\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef find_closest_to_centroids(data, labels, centroids):\n    # Calculate the centroids of each cluster\n    cluster_centroids = np.array([np.mean(data[labels==i], axis=0) for i in range(centroids.shape[0])])\n    \n    # Calculate the pairwise distances between data points and centroids\n    distances = cdist(data, cluster_centroids)\n    \n    # Find the index of the closest data point to each centroid\n    closest_indices = np.argmin(distances, axis=0)\n    \n    # Return the data points closest to the centroids\n    return data[closest_indices]\n\n# Test the function\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\nlabels = np.random.choice(range(centroids.shape[0]), size=data.shape[0])\n\nresult = find_closest_to_centroids(data, labels, centroids)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 804, "library_problem_id": 93, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 92}}
{"id": 805, "code": ["<code>\nimport numpy as np\nimport scipy.spatial\n\ndef find_kth_closest(distance_matrix, cluster_assignments, k):\n    # Calculate centroids\n    centroids = np.array([np.mean(data[cluster_assignments == i], axis=0) for i in np.unique(cluster_assignments)])\n    \n    # Find the k-th closest data point to each centroid\n    result = {i: np.argsort(scipy.spatial.distance.cdist(centroids[i:i+1], data).flatten())[:k] for i in range(len(centroids))}\n    \n    return result\n\n# Test the function\ncentroids = np.random.rand(5, 3)\ndata = np.random.rand(100, 3)\nk = 3\nresult = find_kth_closest(scipy.spatial.distance.cdist(data), np.array([0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4]), k)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 805, "library_problem_id": 94, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 92}}
{"id": 806, "code": ["<code>\nimport numpy as np\nfrom scipy.optimize import fsolve\n\ndef eqn(x, a, b):\n    return x + 2*a - b**2\n\nxdata = np.arange(4)+3\nbdata = np.random.randint(0, 10, (4,))\n\nresult = []\nfor a, b in zip(xdata, bdata):\n    sol = fsolve(lambda x : eqn(x, a, b), 0)\n    result.append(sol[0])\n\nresult = np.array(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 806, "library_problem_id": 95, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 807, "code": ["<code>\nimport numpy as np\nfrom scipy.optimize import fsolve\n\ndef eqn(x, a, b):\n    return x + 2*a - b**2\n\nxdata = np.arange(4)+3\nadata = np.random.randint(0, 10, (4,))\n\n# Initialize an empty list to store the results\nresult = []\n\n# Loop over each pair of (x, a)\nfor x, a in zip(xdata, adata):\n    # Use fsolve to find the root of the equation\n    b_root = fsolve(lambda b: eqn(x, a, b), 0)[0]\n    # Append the root and the pair (x, a) to the result list\n    result.append((b_root, (x, a)))\n\n# Sort the result list by the root\nresult.sort()\n\n# Extract the pairs (x, a) from the sorted result list\nresult = [pair[1] for pair in result]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 807, "library_problem_id": 96, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 808, "code": ["<code>\nimport numpy as np\nimport scipy as sp\nfrom scipy import integrate,stats\nfrom scipy.optimize import curve_fit\nfrom scipy.stats import kstest\n\ndef bekkers(x, a, m, d):\n    p = a*np.exp((-1*(x**(1/3) - m)**2)/(2*d**2))*x**(-2/3)\n    return(p)\n\nrange_start = 1\nrange_end = 10\nestimated_a, estimated_m, estimated_d = 1,1,1\nsample_data = [1.5,1.6,1.8,2.1,2.2,3.3,4,6,8,9]\n\n# Curve fitting\nx = np.linspace(range_start, range_end, 1000)\np0 = [estimated_a, estimated_m, estimated_d]\npopt, pcov = curve_fit(bekkers, x, sample_data, p0)\n\n# Generate CDF\ncdf = lambda x: integrate.quad(bekkers, range_start, x, args=popt)[0] / integrate.quad(bekkers, range_start, range_end, args=popt)[0]\n\n# Apply KS test\nks_statistic, p_value = kstest(sample_data, cdf)\n\nresult = ks_statistic, p_value\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 808, "library_problem_id": 97, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 809, "code": ["<code>\nSure, here is the completed function:\n\n```python\nimport numpy as np\nimport scipy as sp\nfrom scipy import integrate,stats\n\ndef bekkers(x, a, m, d):\n    p = a*np.exp((-1*(x**(1/3) - m)**2)/(2*d**2))*x**(-2/3)\n    return(p)\n\nrange_start = 1\nrange_end = 10\nestimated_a, estimated_m, estimated_d = 1,1,1\nsample_data = [1.5,1.6,1.8,2.1,2.2,3.3,4,6,8,9]\n\n# Define the cumulative distribution function (CDF) from the PDF\ndef cdf(x, a, m, d):\n    return integrate.quad(lambda t: bekkers(t, a, m, d), 0, x)[0]\n\n# Generate the theoretical CDF values for the range of data\nx_theor = np.linspace(range_start, range_end, 1000)\ny_theor = [cdf(x, estimated_a, estimated_m, estimated_d) for x in x_theor]\n\n# Generate the empirical CDF values from the sample data\nx_emp = np.sort(sample_data)\ny_emp = np.arange(len(x_emp)) / (len(x_emp) - 1)\n\n# Perform the KS test\nresult = stats.kstest(x_emp, lambda x: np.interp(x\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 809, "library_problem_id": 98, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 810, "code": ["<code>\nHere is the completed function:\n\n```python\nimport pandas as pd\nimport io\nfrom scipy import integrate\n\nstring = '''\nTime                      A\n2017-12-18-19:54:40   -50187.0\n2017-12-18-19:54:45   -60890.5\n2017-12-18-19:54:50   -28258.5\n2017-12-18-19:54:55    -8151.0\n2017-12-18-19:55:00    -9108.5\n2017-12-18-19:55:05   -12047.0\n2017-12-18-19:55:10   -19418.0\n2017-12-18-19:55:15   -50686.0\n2017-12-18-19:55:20   -57159.0\n2017-12-18-19:55:25   -42847.0\n'''\ndf = pd.read_csv(io.StringIO(string), sep = '\\s+', parse_dates=['Time'])\n\ndef rolling_integral(df, window, column):\n    df['integral'] = df[column].rolling\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 810, "library_problem_id": 99, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 811, "code": ["<code>\nimport scipy.interpolate\nimport numpy as np\n\ndef interpolate_value(x, y, eval):\n    # Convert the x and y coordinates to numpy arrays\n    x = np.array([i[0] for i in x])\n    y = np.array([i[1] for i in y])\n\n    # Create a grid from the x and y coordinates\n    xi, yi = np.meshgrid(x, y)\n\n    # Interpolate the values\n    interp_func = scipy.interpolate.interp2d(xi, yi, z, kind='cubic')\n\n    # Evaluate the interpolated function at the given point\n    result = interp_func(eval[0][0], eval[0][1])\n\n    return result\n\nx = [(2,2), (1,2), (2,3), (3,2), (2,1)]\ny = [5,7,8,10,3]\nz = [1,2,3,4,5]\neval = [(2.7, 2.3)]\n\nresult = interpolate_value(x, y, eval)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 811, "library_problem_id": 100, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 812, "code": ["<code>\nimport scipy.optimize as sciopt\nimport numpy as np\nimport pandas as pd\n\ndef mle_multinomial(data):\n    # Define the multinomial likelihood function\n    def multinomial_likelihood(params, data):\n        return np.prod(np.power(params, data))\n\n    # Define the negative log-likelihood function\n    def neg_log_likelihood(params, data):\n        return -np.sum(np.log(multinomial_likelihood(params, data)))\n\n    # Initialize the parameters\n    params0 = np.ones(len(data)) / len(data)\n\n    # Use scipy's optimization function to find the parameters that minimize the negative log-likelihood\n    result = sciopt.minimize(neg_log_likelihood, params0, args=(data), method='Nelder-Mead')\n\n    return result.x\n\na=pd.DataFrame({'A1':[0,1,2,3,2,1,6,0,1,1,7,10]})\nweights = mle_multinomial(a['A1'].values)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 812, "library_problem_id": 101, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 101}}
{"id": 813, "code": ["<code>\nThe task is to find a suitable optimization function from SciPy that can handle multiple parameters and bounds to minimize the given objective function. The `fminbound` function, which works for single-parameter optimization with bounds, throws a `ValueError` when used with multiple parameters. Here, we can use the `fmin` function from SciPy's optimization module, which is capable of handling multiple parameters and bounds.\n\nHere is the completed code:\n\n```python\nimport numpy as np\nimport scipy.optimize as sciopt\n\nx = np.array([[ 1247.04,  1274.9 ,  1277.81,  1259.51,  1246.06,  1230.2 ,\n     1207.37,  1192.  ,  1180.84,  1182.76,  1194.76,  1222.65],\n   [  589.  ,   581.29,   576.1 ,   570.28,   566.45,   575.99,\n      601.1 ,   620.6 ,   637.04,   631.68,   611.79,   599.19]])\ny = np.array([ 1872.81,  1875.41,  1871.43,  1865.94,  1854.8 ,  183\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 813, "library_problem_id": 102, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 814, "code": ["<code>\nimport numpy as np\nfrom scipy import signal\n\narr = np.array([-624.59309896, -624.59309896, -624.59309896, -625., -625., -625.])\nn = 2\n\n# Create a window of size 2n+1\nwindow = np.ones(2*n+1)\n\n# Create a mask for the window\nmask = np.ones_like(window)\nmask[n] = 0\n\n# Apply the mask to the window\nwindow *= mask\n\n# Find the indices of the relative extrema\nextrema_indices = signal.argrelextrema(arr, comparator=np.less_equal, order=n)\n\n# Get the values of the relative extrema\nextrema_values = arr[extrema_indices]\n\n# Print the indices of the relative extrema\nresult = extrema_indices\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 814, "library_problem_id": 103, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 815, "code": ["<code>\nimport numpy as np\nfrom scipy import signal\n\narr = np.array([[-624.59309896, -624.59309896, -624.59309896, -625., -625., -625.,], [3, 0, 0, 1, 2, 4]])\nn = 2\n\ndef find_relative_extrema(arr, n):\n    # Create a kernel of ones of size (1, n*2+1)\n    kernel = np.ones((1, n*2+1))\n\n    # Apply the kernel to the array\n    result = signal.convolve(arr, kernel, mode='same')\n\n    # Find the indices where the array and the result are not equal\n    indices = np.where(arr == result)[0]\n\n    # Return the indices as a list of [row, column]\n    return [[i, j] for i, j in enumerate(indices)]\n\nresult = find_relative_extrema(arr, n)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 815, "library_problem_id": 104, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 103}}
{"id": 816, "code": ["<code>\ndef remove_outliers(df):\n    for column in df:\n        if df[column].dtype != 'O':  # 'O' is pandas way of saying 'object' which is categorical\n            df = df[np.abs(stats.zscore(df[column])) < 3]  # remove rows where the z-score is greater than 3\n    return df\n\ndf = remove_outliers(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 816, "library_problem_id": 105, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 817, "code": ["<code>\niris = load_iris()\ndata1 = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n                     columns= iris['feature_names'] + ['target'])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 817, "library_problem_id": 0, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 818, "code": ["<code>\niris = load_iris()\ndata1 = pd.DataFrame(data=np.c_[iris['data'], iris['target']],\n                     columns=iris['feature_names'] + ['target'])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 818, "library_problem_id": 1, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 819, "code": ["<code>\nimport numpy as np\nfrom sklearn.datasets import fetch_california_housing\nimport pandas as pd\n\ndef convert_bunch_to_dataframe(bunch):\n    data = pd.DataFrame(data=np.c_[bunch['data'], bunch['target']],\n                     columns=list(bunch['feature_names']) + ['target'])\n    return data\n\ndata = fetch_california_housing()\ndata1 = convert_bunch_to_dataframe(data)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 819, "library_problem_id": 2, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 820, "code": ["<code>\ndef solve(data):\n    # Convert the data and target attributes into a DataFrame\n    df = pd.DataFrame(data=np.c_[data['data'], data['target']],\n                     columns=np.append(data['feature_names'], ['target']))\n    return df\n\n# Load the iris dataset\ndata = load_iris()\n\n# Call the function\nresult = solve(data)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 820, "library_problem_id": 3, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 821, "code": ["<code>\ndef one_hot_encode(df, column):\n    # Get the unique elements in the column\n    unique_elements = set(sum(df[column].tolist(), []))\n\n    # Create new columns for each unique element\n    for element in unique_elements:\n        df[f'{column}_{element}'] = df[column].apply(lambda x: 1 if element in x else 0)\n\n    # Drop the original column\n    df = df.drop(columns=[column])\n\n    return df\n\n# Load your data\ndf = pd.read_csv('your_data.csv')  # replace 'your_data.csv' with your actual data file\n\n# Call the function\ndf_out = one_hot_encode(df, 'your_column')  # replace 'your_column' with your actual column name\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 821, "library_problem_id": 4, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 822, "code": ["<code>\nimport pandas as pd\nimport numpy as np\nimport sklearn\n\n# Assuming df is your DataFrame and 'column_name' is the column you want to transform\ndef transform_column(df, column_name):\n    # Get unique strings\n    unique_strings = np.unique(np.concatenate(df[column_name].values).flatten())\n\n    # Create new columns for each unique string\n    for string in unique_strings:\n        df[string] = df[column_name].apply(lambda x: 1 if string in x else 0)\n\n    # Drop the original column\n    df = df.drop(columns=[column_name])\n\n    return df\n\n# Example usage:\ndf = pd.DataFrame({\n    'column_name': [['string1', 'string2'], ['string2', 'string3'], ['string1', 'string3']]\n})\n\ndf_out = transform_column(df, 'column_name')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 822, "library_problem_id": 5, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 823, "code": ["<code>\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\ndef one_hot_encode_last_column(df):\n    mlb = MultiLabelBinarizer()\n    df_out = df.copy()\n    df_out[df_out.columns[-1]] = mlb.fit_transform(df[df.columns[-1]])\n    for i, column in enumerate(mlb.classes_):\n        df_out.insert(len(df.columns), f'{column}', df_out.pop(df_out.columns[-1]))\n    return df_out\n\ndf = load_data()  # replace this with your data loading code\ndf_out = one_hot_encode_last_column(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 823, "library_problem_id": 6, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 824, "code": ["<code>\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\ndef one_hot_encode_last_column(df):\n    mlb = MultiLabelBinarizer()\n    last_column = df.iloc[:, -1]\n    encoded = mlb.fit_transform(last_column)\n    df_out = pd.DataFrame(encoded, columns=mlb.classes_)\n    return df_out\n\ndf = load_data()  # replace this with your data loading code\ndf_out = one_hot_encode_last_column(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 824, "library_problem_id": 7, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 825, "code": ["<code>\nimport pandas as pd\nimport numpy as np\nimport sklearn\n\ndef one_hot_encode_last_column(df):\n    # Get the last column\n    last_column = df.columns[-1]\n    \n    # Get the unique elements in the last column\n    unique_elements = df[last_column].unique()\n    \n    # Create new columns for each unique element\n    for element in unique_elements:\n        df[f'{last_column}_{element}'] = df[last_column].apply(lambda x: 0 if element in x else 1)\n    \n    # Drop the original last column\n    df = df.drop(columns=[last_column])\n    \n    return df\n\ndf = load_data()  # replace this with your data loading code\ndf_out = one_hot_encode_last_column(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 825, "library_problem_id": 8, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 826, "code": ["<code>\n# Fit model:\nsvmmodel=suppmach.LinearSVC()\nsvmmodel.fit(X, y)\n\n# Predict decision scores:\ndecision_scores = svmmodel.decision_function(x_test)\n\n# Apply logistic function to get probabilities:\nproba = np.exp(decision_scores) / (1 + np.exp(decision_scores))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 826, "library_problem_id": 9, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 827, "code": ["<code>\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# Fit the LinearSVC model\nmodel.fit(X, y)\n\n# Get the decision scores\ndecision_scores = model.decision_function(x_predict)\n\n# Apply the logistic function to the decision scores\nproba_logistic = (1 / (1 + np.exp(-decision_scores)))\n\n# Use CalibratedClassifierCV to calibrate the probabilities\ncalibrated_model = CalibratedClassifierCV(model, cv=5)\ncalibrated_model.fit(X, y)\nproba_calibrated = calibrated_model.predict_proba(x_predict)\n\n# Return the probabilities\nproba = {\"logistic\": proba_logistic, \"calibrated\": proba_calibrated}\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 827, "library_problem_id": 10, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 828, "code": ["<code>\n# Convert the sparse matrix back to a dense matrix\ndense_output = transform_output.toarray()\n\n# Convert the dense matrix to a DataFrame\ndf_output = pd.DataFrame(dense_output)\n\n# Concatenate the original DataFrame with the transformed DataFrame\ndf = pd.concat([df_origin, df_output], axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 828, "library_problem_id": 11, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 829, "code": ["<code>\nimport pandas as pd\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\n# Assuming df_origin is a DataFrame and transform_output is a sparse matrix\n\n# Convert the sparse matrix to a dense matrix\ndense_matrix = transform_output.todense()\n\n# Convert the dense matrix to a DataFrame\ndf_sparse = pd.DataFrame(dense_matrix)\n\n# Concatenate the original DataFrame with the DataFrame containing the sparse matrix\ndf = pd.concat([df_origin, df_sparse], axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 829, "library_problem_id": 12, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 830, "code": ["<code>\n# Convert the sparse matrix back to a dense matrix\n    dense_output = transform_output.toarray()\n\n    # Convert the dense matrix to a DataFrame\n    df_output = pd.DataFrame(dense_output)\n\n    # Concatenate the original DataFrame with the transformed DataFrame\n    result = pd.concat([df, df_output], axis=1)\n\n    return result\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 830, "library_problem_id": 13, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 831, "code": ["<code>\n# Adding a new step\nclf.steps.append(('new_step', SVC()))\nprint(clf.named_steps)\n\n# Removing a step\nclf.steps.remove(('reduce_dim', PCA()))\nprint(clf.named_steps)\n\n# Modifying a step\nclf.steps[0] = ('modified_reduce_dim', PCA())\nprint(clf.named_steps)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 831, "library_problem_id": 14, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 832, "code": ["<code>\n# Adding a new step\nclf.steps.append(('new_step', 'new_transformer'))\nprint(clf.steps)\n\n# Removing a step\nclf.steps.remove(('reduce_poly', PolynomialFeatures()))\nprint(clf.steps)\n\n# Replacing a step\nclf.steps[clf.steps.index(('dim_svm', PCA()))] = ('replaced_step', 'new_transformer')\nprint(clf.steps)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 832, "library_problem_id": 15, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 833, "code": ["<code>\nfrom sklearn.preprocessing import StandardScaler\n\n# Adding a new step\nclf.steps.append(('scale', StandardScaler()))\nprint(clf.steps)\n\n# Removing a step\nclf.steps.remove(('reduce_dIm', PCA()))\nprint(clf.steps)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 833, "library_problem_id": 16, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 834, "code": ["<code>\n# Adding a new step\nclf.steps.append(('new_step', NewTransformer()))\n\n# Removing a step\nclf.steps = [step for step in clf.steps if step[0] != 'reduce_dim']\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 834, "library_problem_id": 17, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 835, "code": ["<code>\n# Adding a new step\nclf.steps.append(('new_step', SomeTransformer()))\n\n# Removing a step\nclf.steps = [step for step in clf.steps if step[0] != 'reduce_poly']\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 835, "library_problem_id": 18, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 836, "code": ["<code>\n# Adding a step\nclf.steps.append(('new_step', SomeTransformer()))\n\n# Removing a step\nclf.steps = [step for step in clf.steps if step[0] != 'reduce_dIm']\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 836, "library_problem_id": 19, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 17}}
{"id": 837, "code": ["<code>\nHere is the solution for the problem:\n\n```python\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error\n\n# Load your data\n# gridsearch, testX, testY, trainX, trainY = load_data()\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [100, 500, 1000],\n    'max_depth': [2, 4, 6],\n    'learning_rate': [0.01, 0.1, 0.2]\n}\n\n# Define the XGBoost classifier\nxgb_clf = xgb.XGBRegressor(n_jobs=-1)\n\n# Define the time series cross-validation\ntscv = TimeSeriesSplit(n_splits=5)\n\n# Define the GridSearchCV\ngridsearch = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, cv=tscv, scoring='neg_mean_squared_error', n_jobs=-1)\n\n# Fit the GridSearchCV\ngridsearch.fit(trainX, trainY)\n\n# Get the best parameters\nb = gridsearch.best_params_\n\n# Predict on the test set\nc = gridsearch.predict(testX)\n\n# Calculate the score\nscore = mean_squared_error(testY, c)\nprint(f'Best parameters:\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 837, "library_problem_id": 20, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 838, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport xgboost.sklearn as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\n\n# Load your data\n# gridsearch, testX, testY, trainX, trainY = load_data()\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [4, 6, 8],\n    'learning_rate': [0.01, 0.1, 0.2]\n}\n\n# Define the XGBoost classifier\nxgb_clf = xgb.XGBClassifier(objective='binary:logistic', n_jobs=1)\n\n# Define the GridSearchCV\ngridsearch = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, cv=TimeSeriesSplit(n_splits=5), verbose=2, n_jobs=-1)\n\n# Fit the GridSearchCV with early stopping\ngridsearch.fit(trainX, trainY, early_stopping_rounds=10, eval_metric=\"auc\", eval_set=[(testX, testY)])\n\n# Get the best parameters\nb = gridsearch.best_params_\n\n# Make predictions\nc = gridsearch.predict(testX)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 838, "library_problem_id": 21, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 839, "code": ["<code>\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\nproba = np.array(proba)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 839, "library_problem_id": 22, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 840, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\n\n# Assuming load_data is a function that returns X and y\nX, y = load_data()\nassert type(X) == np.ndarray\nassert type(y) == np.ndarray\ncv = StratifiedKFold(5).split(X, y)\nlogreg = LogisticRegression()\n\nproba = []\nfor train_index, test_index in cv:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    logreg.fit(X_train, y_train)\n    proba.append(logreg.predict_proba(X_test))\n\nproba = np.concatenate(proba)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 840, "library_problem_id": 23, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 841, "code": ["<code>\ninversed = scaler.inverse_transform(scaled)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 841, "library_problem_id": 24, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 842, "code": ["<code>\ninversed = scaler.inverse_transform(scaled)\n    return inversed\n### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 842, "library_problem_id": 25, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 24}}
{"id": 843, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel_name = model.__class__.__name__\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 843, "library_problem_id": 26, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 844, "code": ["<code>\nmodel_name = model.__class__.__name__\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 844, "library_problem_id": 27, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 845, "code": ["<code>\nmodel_name = model.__class__.__name__\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 845, "library_problem_id": 28, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 846, "code": ["<code>\npipe.fit_transform(data)\ntf_idf_out = pipe.named_steps['tf_idf'].fit_transform(data)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 846, "library_problem_id": 29, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 847, "code": ["<code>\ntf_idf_out = pipe[\"tf_idf\"].fit_transform(data)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 847, "library_problem_id": 30, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 848, "code": ["<code>\npipe.fit(data, target)\nselect_out = pipe.named_steps['select'].transform(data)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 848, "library_problem_id": 31, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 849, "code": ["<code>\n# Create a GridSearchCV object\ngrid_search = GridSearchCV(bc, param_grid, cv=5, scoring='accuracy')\n\n# Fit the GridSearchCV object to the data\ngrid_search.fit(X_train, y_train)\n\n# Get the best estimator\nbest_bc = grid_search.best_estimator_\n\n# Predict the probabilities on the test data\nproba = best_bc.predict_proba(X_test)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 849, "library_problem_id": 32, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 850, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Assuming that load_data() returns the 1D arrays as (X, y, X_test)\nX, y, X_test = load_data()\n\n# Reshape the 1D arrays to 2D arrays\nX = X.reshape(-1, 1)\nX_test = X_test.reshape(-1, 1)\n\n# Create a RandomForestRegressor model\nregressor = RandomForestRegressor(n_estimators=100, random_state=0)\n\n# Fit the model\nregressor.fit(X, y)\n\n# Make predictions\npredict = regressor.predict(X_test)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 850, "library_problem_id": 33, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 851, "code": ["<code>\n# Assuming you have more than one sample in your data\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])  # Replace this with your actual data\ny = np.array([1, 2, 3])  # Replace this with your actual labels\n\n# Create a RandomForestRegressor model\nregressor = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Fit the model\nregressor.fit(X, y)\n\n# Make predictions\nX_test = np.array([[10, 11, 12]])  # Replace this with your actual test data\npredict = regressor.predict(X_test)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 851, "library_problem_id": 34, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 33}}
{"id": 852, "code": ["<code>\ndef uppercase_preprocessor(text):\n    return text.upper()\n\n# Create a TfidfVectorizer object with the custom preprocessor\nvectorizer = TfidfVectorizer(preprocessor=uppercase_preprocessor)\n\n# Let's say we have the following data\ndata = [\"This is the first document.\", \"This is another sample document.\", \"And the last one.\"]\n\n# Fit and transform the data\ntfidf = vectorizer.fit_transform(data)\n\n# Now, the TfidfVectorizer has been applied to the data, and all the text has been converted to uppercase.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 852, "library_problem_id": 35, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 853, "code": ["<code>\ndef preprocess(text):\n    return text.lower()\n\ntfidf = TfidfVectorizer(preprocessor=preprocess)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 853, "library_problem_id": 36, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Semantic", "perturbation_origin_id": 35}}
{"id": 854, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\n\n# Assuming load_data() is a function that loads your data\ndata = load_data()\n\n# Convert the DataFrame to a NumPy array\ndata_array = data.values\n\n# Apply preprocessing.scale()\nscaled_data_array = preprocessing.scale(data_array)\n\n# Convert the scaled NumPy array back to a DataFrame\ndf_out = pd.DataFrame(scaled_data_array, index=data.index, columns=data.columns)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 854, "library_problem_id": 37, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 855, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\n\n# Assuming data is loaded from a csv file\ndata = pd.read_csv('data.csv')\n\n# Apply preprocessing.scale to the DataFrame\nscaled_data = pd.DataFrame(preprocessing.scale(data), index=data.index, columns=data.columns)\n\ndf_out = scaled_data\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 855, "library_problem_id": 38, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 37}}
{"id": 856, "code": ["<code>\ngrid.fit(X, y)\ncoef = grid.best_estimator_.named_steps['model'].coef_\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 856, "library_problem_id": 39, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 857, "code": ["<code>\n# Fit the GridSearchCV\ngrid.fit(X, y)\n\n# Get the best estimator\nbest_model = grid.best_estimator_\n\n# Get the coefficients from the RidgeClassifier\ncoef = best_model.named_steps['model'].coef_\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 857, "library_problem_id": 40, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 858, "code": ["<code>\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport numpy as np\n\nX, y = load_data()\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\n\nsfm = SelectFromModel(clf, prefit=True)\nX_new = sfm.transform(X)\n\nsupport = sfm.get_support()\ncolumn_names = X.columns[support]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 858, "library_problem_id": 41, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 859, "code": ["<code>\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport numpy as np\n\n# Assuming you have loaded your data into X and y\n# X, y = load_data()\n\n# Create a ExtraTreesClassifier\nclf = ExtraTreesClassifier(random_state=42)\n\n# Fit the classifier\nclf = clf.fit(X, y)\n\n# Create a SelectFromModel object\nsfm = SelectFromModel(clf, prefit=True)\n\n# Get the mask of selected features\nmask = sfm.get_support()\n\n# Get the names of the selected features\ncolumn_names = X.columns[mask]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 859, "library_problem_id": 42, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 860, "code": ["<code>\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport numpy as np\n\n# Assuming X and y are loaded from some data source\nX, y = load_data()\n\n# Fit the ExtraTreesClassifier\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\n\n# Get the importance of each feature\nimportances = clf.feature_importances_\n\n# Create a mask for features with importance greater than a certain threshold\nmask = importances > 0.05\n\n# Use the mask to select the corresponding feature names\nselected_features = X.columns[mask]\n\n# Put the solution in the variable column_names\ncolumn_names = selected_features\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 860, "library_problem_id": 43, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 861, "code": ["<code>\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport numpy as np\n\n# Assuming you have loaded your data into X and y\n# X, y = load_data()\n\n# Create a DataFrame from X for getting column names\ndf = pd.DataFrame(X)\n\nclf = ExtraTreesClassifier(random_state=42)\nclf = clf.fit(X, y)\n\nsfm = SelectFromModel(clf, prefit=True)\nX_new = sfm.transform(X)\n\nsupport = sfm.get_support()\n\ncolumn_names = df.columns[support]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 861, "library_problem_id": 44, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 862, "code": ["<code>\n# Fit the model\nkm = km.fit(X)\n\n# Get the cluster centers\ncenters = km.cluster_centers_\n\n# Get the 2nd center\nsecond_center = centers[1]\n\n# Calculate the Euclidean distance from each point to the 2nd center\ndistances = np.linalg.norm(X - second_center, axis=1)\n\n# Get the indices of the 50 points closest to the 2nd center\nindices = np.argpartition(distances, 50)[:50]\n\n# Get the 50 closest points\nclosest_50_samples = X[indices]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 862, "library_problem_id": 45, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 863, "code": ["<code>\nfrom sklearn.metrics import pairwise_distances\n\n# Assuming that the KMeans model is already fitted and the data is loaded\n# p is the cluster center index\n\n# Calculate the distance of each sample from the pth cluster center\ndistances = pairwise_distances(X, [km.cluster_centers_[p]])\n\n# Get the indices of the 50 samples closest to the pth cluster center\nindices = np.argsort(distances)[:50]\n\n# Get the full data samples\nclosest_50_samples = X[indices]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 863, "library_problem_id": 46, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 864, "code": ["<code>\nkm = KMeans(n_clusters=k)\nkm.fit(X)\n\n# Get the cluster center\ncenter = km.cluster_centers_[p]\n\n# Calculate the distance from each sample to the center\ndistances = np.linalg.norm(X - center, axis=1)\n\n# Get the indices of the 100 samples closest to the center\nindices = np.argsort(distances)[:100]\n\n# Get the samples\nclosest_100_samples = X[indices]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 864, "library_problem_id": 47, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 865, "code": ["<code>\n# Fit the model\n    km.fit(X)\n\n    # Get the cluster centers\n    cluster_centers = km.cluster_centers_\n\n    # Check if the specified cluster center index 'p' is valid\n    if p < 0 or p >= cluster_centers.shape[0]:\n        raise ValueError(\"Invalid cluster center index 'p'\")\n\n    # Get the 50 closest data points\n    distances = np.linalg.norm(X - cluster_centers[p], axis=1)\n    closest_indices = np.argsort(distances)[:50]\n    closest_samples = X[closest_indices]\n\n    return closest_samples\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 865, "library_problem_id": 48, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 866, "code": ["<code>\nX_train = pd.get_dummies(X_train)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 866, "library_problem_id": 49, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 867, "code": ["<code>\n# Convert the categorical data into one-hot encoding\nX_train = pd.get_dummies(X_train)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 867, "library_problem_id": 50, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 868, "code": ["<code>\nfrom sklearn.svm import SVR\n\n# Assuming that you have loaded the data into X and y\n# X, y = load_data()\n\n# Create a SVM with Gaussian kernel\nsvm = SVR(kernel='rbf')\n\n# Fit the model\nsvm.fit(X, y)\n\n# Predict\npredict = svm.predict(X)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 868, "library_problem_id": 51, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 869, "code": ["<code>\nfrom sklearn.svm import SVR\n\n# Create a SVR with a Gaussian kernel\nsvr = SVR(kernel='rbf')\n\n# Fit the model to the data\nsvr.fit(X, y)\n\n# Predict the output\npredict = svr.predict(X)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 869, "library_problem_id": 52, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 51}}
{"id": 870, "code": ["<code>\nfrom sklearn.svm import SVR\n\n# Create a SVR with a polynomial kernel\nsvr = SVR(kernel='poly', degree=2)\n\n# Fit the model to the data\nsvr.fit(X, y)\n\n# Predict the output\npredict = svr.predict(X)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 870, "library_problem_id": 53, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 51}}
{"id": 871, "code": ["<code>\nfrom sklearn.svm import SVR\n\n# Create a polynomial kernel SVR model\nsvr = SVR(kernel='poly', degree=2)\n\n# Fit the model to the data\nsvr.fit(X, y)\n\n# Predict the output\npredict = svr.predict(X)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 871, "library_problem_id": 54, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 53}}
{"id": 872, "code": ["<code>\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Assuming queries and documents are lists of strings\nqueries, documents = load_data()\n\n# Compute TF-IDF representation of the documents\ntfidf = TfidfVectorizer()\ntfidf_vectors = tfidf.fit_transform(documents)\n\n# Compute cosine similarity between each query and each document\ncosine_similarities_of_queries = cosine_similarity(tfidf.transform(queries), tfidf_vectors)\n\n# The output should be a 3x5 matrix representing the similarity scores\nprint(cosine_similarities_of_queries)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 872, "library_problem_id": 55, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 55}}
{"id": 873, "code": ["<code>\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Transform the documents and queries into TF-IDF vectors\ntfidf_vectors = tfidf.transform(documents)\nquery_vectors = tfidf.transform(queries)\n\n# Compute the cosine similarity between the query and document vectors\ncosine_similarities_of_queries = cosine_similarity(query_vectors, tfidf_vectors)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 873, "library_problem_id": 56, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 874, "code": ["<code>\n# Calculate the TF-IDF matrix\n    tfidf_matrix = tfidf.transform(documents + queries)\n\n    # Calculate the cosine similarity\n    from sklearn.metrics.pairwise import cosine_similarity\n    cosine_similarities = cosine_similarity(tfidf_matrix, tfidf_matrix).tolist()\n\n    # Split the cosine similarities into 3x5 matrix\n    cosine_similarities_of_queries = cosine_similarities[:len(documents)]\n    cosine_similarities_of_documents = cosine_similarities[len(documents):]\n\n    return cosine_similarities_of_queries\n\nqueries, documents = load_data()\ncosine_similarities_of_queries = solve(queries, documents)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 874, "library_problem_id": 57, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 875, "code": ["<code>\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Assuming features is a list of lists of strings\n# Convert the list of lists to a DataFrame\ndf = pd.DataFrame(features)\n\n# Initialize the OneHotEncoder\nencoder = OneHotEncoder(sparse=False)\n\n# Fit and transform the DataFrame\nnew_features = encoder.fit_transform(df)\n\n# Convert the resulting numpy array back to a list of lists\nnew_features = new_features.tolist()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 875, "library_problem_id": 58, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 876, "code": ["<code>\nimport pandas as pd\nimport numpy as np\nimport sklearn\n\ndef convert_to_2d_array(lst):\n    # Convert the list of lists to a DataFrame\n    df = pd.DataFrame(lst)\n    \n    # Convert the DataFrame to a 2D array\n    arr = df.values\n    \n    return arr\n\n# Test the function\nlst = [['0', '1', '0', '0'], ['0', '0', '1', '0'], ['1', '0', '0', '1']]\nnew_f = convert_to_2d_array(lst)\nprint(new_f)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 876, "library_problem_id": 59, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 877, "code": ["<code>\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\n# Assuming features is a list of lists of strings\nfeatures = [['feature1', 'feature2'], ['feature1', 'feature3'], ['feature2', 'feature3']]\n\n# Use MultiLabelBinarizer to convert the list of lists into a binary matrix\nmlb = MultiLabelBinarizer()\nnew_features = mlb.fit_transform(features)\n\n# Print the new features\nprint(new_features)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 877, "library_problem_id": 60, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 878, "code": ["<code>\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\ndef solve(features):\n    mlb = MultiLabelBinarizer()\n    new_features = mlb.fit_transform(features)\n    return new_features\n\n# Example usage:\nfeatures = [['red', 'large'], ['green', 'small'], ['blue', 'large'], ['red', 'small']]\nnew_features = solve(features)\nprint(new_features)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 878, "library_problem_id": 61, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 879, "code": ["<code>\nimport pandas as pd\nimport numpy as np\nimport sklearn\n\ndef convert_to_numpy(features):\n    # Convert the list of lists to a pandas DataFrame\n    df = pd.DataFrame(features)\n    \n    # Convert the DataFrame to a NumPy array\n    new_features = df.values\n    \n    return new_features\n\nfeatures = load_data()\nnew_features = convert_to_numpy(features)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 879, "library_problem_id": 62, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 58}}
{"id": 880, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Assuming that the data_matrix is a numpy array\n# and it represents the distance matrix\n\n# Create an instance of AgglomerativeClustering\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit the model to the data\ncluster_labels = cluster.fit_predict(data_matrix)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 880, "library_problem_id": 63, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 881, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Assuming that the data_matrix is a numpy array\ndata_matrix = np.array([[0, 0.5, 0.3], [0.5, 0, 0.9], [0.3, 0.9, 0]])\n\n# Create an instance of AgglomerativeClustering\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit and predict the clusters\ncluster_labels = cluster.fit_predict(data_matrix)\n\nprint(cluster_labels)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 881, "library_problem_id": 64, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 882, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Assuming that load_data() is a function that loads the distance matrix\nsimM = load_data()\n\n# Create an instance of AgglomerativeClustering with 2 clusters\ncluster = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='average')\n\n# Fit the model to the data\ncluster_labels = cluster.fit_predict(simM)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 882, "library_problem_id": 65, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 883, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n# Assuming that the data_matrix is a numpy array\ndef hierarchical_clustering(data_matrix):\n    # Perform hierarchical clustering\n    Z = linkage(data_matrix, 'ward')\n    # Obtain cluster labels\n    cluster_labels = fcluster(Z, 2, criterion='maxclust')\n    return cluster_labels\n\n# Load your data\ndata_matrix = load_data()\n\n# Get cluster labels\ncluster_labels = hierarchical_clustering(data_matrix)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 883, "library_problem_id": 66, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 884, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n# Assuming that the data_matrix is a numpy array\ndef hierarchical_clustering(data_matrix):\n    # Perform hierarchical clustering\n    Z = linkage(data_matrix, 'ward')\n    # Obtain cluster labels\n    cluster_labels = fcluster(Z, 2, criterion='maxclust')\n    return cluster_labels\n\n# Load your data\ndata_matrix = load_data()\n\n# Get cluster labels\ncluster_labels = hierarchical_clustering(data_matrix)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 884, "library_problem_id": 67, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 885, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n# Assuming that the similarity matrix is loaded and stored in simM\n# simM = load_data()\n\n# Perform hierarchical clustering\nZ = linkage(simM, 'ward')\n\n# Obtain cluster labels\nfrom scipy.cluster.hierarchy import fcluster\ncluster_labels = fcluster(Z, 2, criterion='maxclust')\n\n# Print the cluster labels\nprint(cluster_labels)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 885, "library_problem_id": 68, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 886, "code": ["<code>\nfrom sklearn.preprocessing import StandardScaler\n\n# Initialize a scaler\nscaler = StandardScaler()\n\n# Fit and transform the data\ncentered_scaled_data = scaler.fit_transform(data)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 886, "library_problem_id": 69, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 69}}
{"id": 887, "code": ["<code>\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n# Initialize the scalers\nscaler_standard = StandardScaler()\nscaler_minmax = MinMaxScaler()\n\n# Fit and transform the data\nstandard_scaled_data = scaler_standard.fit_transform(data)\nminmax_scaled_data = scaler_minmax.fit_transform(data)\n\n# Combine the scaled and centered data\ncentered_scaled_data = np.hstack((standard_scaled_data, minmax_scaled_data))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 887, "library_problem_id": 70, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 69}}
{"id": 888, "code": ["<code>\nfrom scipy import stats\n\n# Assuming data is a numpy array\nbox_cox_data, fitted_lambda = stats.boxcox(data)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 888, "library_problem_id": 71, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 889, "code": ["<code>\nfrom sklearn.preprocessing import PowerTransformer\n\n# Initialize PowerTransformer\npt = PowerTransformer(method='box-cox')\n\n# Fit and transform the data\nbox_cox_data = pt.fit_transform(data)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 889, "library_problem_id": 72, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 890, "code": ["<code>\nfrom sklearn.preprocessing import PowerTransformer\n\n# Initialize PowerTransformer with Yeo-Johnson method\npt = PowerTransformer(method='yeo-johnson')\n\n# Fit and transform the data\nyeo_johnson_data = pt.fit_transform(data)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 890, "library_problem_id": 73, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 891, "code": ["<code>\nfrom sklearn.preprocessing import PowerTransformer\n\n# Initialize PowerTransformer\npt = PowerTransformer(method='box-cox')\n\n# Fit and transform the data\nyeo_johnson_data = pt.fit_transform(data)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 891, "library_problem_id": 74, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 73}}
{"id": 892, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Assuming you have loaded your data into the variable 'text'\ntext = load_data()\n\n# Use CountVectorizer with a custom token_pattern\nvectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w\\w+\\b|!|\\?|,|'\")\n\n# Fit and transform the text data\ntransformed_text = vectorizer.fit_transform(text)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 892, "library_problem_id": 75, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 893, "code": ["<code>\nfrom sklearn.model_selection import train_test_split\n\n# Assuming that the dataset is loaded and stored in a variable named 'df'\n# df = pd.read_csv('your_file.csv')\n\n# Split the data into features (x) and target (y)\nx = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n\n# Split the data into 80% training and 20% testing sets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 893, "library_problem_id": 76, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 894, "code": ["<code>\nfrom sklearn.model_selection import train_test_split\n\n# Assuming data is a pandas DataFrame\nX = data.iloc[:, :-1]  # features\ny = data.iloc[:, -1]  # target\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 894, "library_problem_id": 77, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 895, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef load_data():\n    # Load your data here\n    pass\n\ndataset = load_data()\n\n# Split the data into features (x) and labels (y)\nx = dataset.iloc[:, :-1]\ny = dataset.iloc[:, -1]\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 895, "library_problem_id": 78, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 896, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef solve(data):\n    # Split the data into features (x) and labels (y)\n    x = data.iloc[:, :-1]\n    y = data.iloc[:, -1]\n\n    # Split the data into training and testing sets\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n    return x_train, y_train, x_test, y_test\n\n# Assuming that the data is loaded from a csv file\ndata = pd.read_csv('data.csv', header=None)\nx_train, y_train, x_test, y_test = solve(data)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 896, "library_problem_id": 79, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 897, "code": ["<code>\nfrom sklearn.cluster import KMeans\nimport pandas as pd\n\n# Load data\ndf = pd.read_csv('your_file.csv')  # replace 'your_file.csv' with your actual csv file\n\n# Reshape the data to 2D array\nmse_values = df['mse'].values.reshape(-1,1)\n\n# Define the model\nkmeans = KMeans(n_clusters=3)  # you can change the number of clusters\n\n# Fit the model\nkmeans.fit(mse_values)\n\n# Get the cluster labels\nlabels = kmeans.predict(mse_values)\n\n# Get the mean of each cluster\ncluster_means = kmeans.cluster_centers_\n\nprint(\"Cluster labels: \", labels)\nprint(\"Cluster means: \", cluster_means)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 897, "library_problem_id": 80, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 898, "code": ["<code>\nimport pandas as pd\nfrom sklearn.cluster import KMeans\n\n# Load the data\ndf = pd.read_csv('your_data.csv')  # replace 'your_data.csv' with your actual csv file\n\n# Create a second artificial feature using a range of values\ndf['feature2'] = range(1, len(df) + 1)\n\n# Perform KMeans clustering on the MSE values and the second feature\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(df[['MSE', 'feature2']])\n\n# Get the cluster labels\nlabels = kmeans.labels_\n\n# Get the centroid values for each cluster\ncentroids = kmeans.cluster_centers_\n\n# Add the labels to the original dataframe\ndf['cluster'] = labels\n\n# Print the centroids\nprint(centroids)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 898, "library_problem_id": 81, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 899, "code": ["<code>\n# Fit the LinearSVC\nclf = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n\n# Get the indices of the selected features\nselected_indices = np.where(clf.coef_ != 0)[1]\n\n# Get the names of the selected features\nselected_feature_names = np.array(vectorizer.get_feature_names_out())[selected_indices]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 899, "library_problem_id": 82, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 900, "code": ["<code>\n# Fit the LinearSVC model\nclf = LinearSVC()\nclf.fit(X, y)\n\n# Get the feature names from the vectorizer\nfeature_names = vectorizer.get_feature_names_out()\n\n# Get the coefficients of the model\ncoefs = clf.coef_\n\n# Get the indices of the non-zero coefficients\nnon_zero_indices = np.flatnonzero(coefs)\n\n# Get the feature names corresponding to the non-zero coefficients\nselected_feature_names = [feature_names[i] for i in non_zero_indices]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 900, "library_problem_id": 83, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 901, "code": ["<code>\n### BEGIN SOLUTION\n    svc = LinearSVC(C=1.0, penalty='l1', dual=False)\n    svc.fit(X, y)\n    selected_features = np.where(svc.coef_ != 0)[1]\n    selected_feature_names = vectorizer.get_feature_names_out()[selected_features]\n    return selected_feature_names\n\nselected_feature_names = solve(corpus, y, vectorizer, X)\nprint(selected_feature_names)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 901, "library_problem_id": 84, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 902, "code": ["<code>\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncorpus = [\n    'We are looking for Java developer',\n    'Frontend developer with knowledge in SQL and Jscript',\n    'And this is the third one.',\n    'Is this the first document?',\n]\n\n# Define your own vocabulary\nvocabulary = ['looking', 'Java', 'Frontend', 'developer', 'with', 'SQL', 'Jscript', 'and', 'this', 'first', 'document']\n\n# Create a CountVectorizer object with the custom vocabulary\nvectorizer = CountVectorizer(vocabulary=vocabulary)\n\n# Fit and transform the corpus\nX = vectorizer.fit_transform(corpus)\n\n# Get the feature names\nfeature_names = vectorizer.get_feature_names_out()\n\nprint(\"Feature names: \", feature_names)\nprint(\"X: \", X.toarray())\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 902, "library_problem_id": 85, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 903, "code": ["<code>\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncorpus = [\n    'We are looking for Java developer',\n    'Frontend developer with knowledge in SQL and Jscript',\n    'And this is the third one.',\n    'Is this the first document?',\n]\n\n# Create a custom vocabulary order\nvocabulary_order = ['developer', 'java', 'frontend', 'sql', 'jscript', 'first', 'document']\n\n# Create a CountVectorizer object with the custom vocabulary order\nvectorizer = CountVectorizer(vocabulary=vocabulary_order)\n\n# Fit and transform the corpus\nX = vectorizer.fit_transform(corpus)\n\n# Get the feature names\nfeature_names = vectorizer.get_feature_names_out()\n\nprint(feature_names)\nprint(X.toarray())\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 903, "library_problem_id": 86, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 904, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncorpus = [\n    'We are looking for Java developer',\n    'Frontend developer with knowledge in SQL and Jscript',\n    'And this is the third one.',\n    'Is this the first document?',\n]\n\n# Define the desired order of features\ndesired_order = ['looking', 'Java', 'Frontend', 'developer', 'with', 'SQL', 'Jscript', 'third', 'first']\n\n# Create a CountVectorizer object with the desired order\nvectorizer = CountVectorizer(vocabulary=desired_order)\n\n# Fit and transform the corpus\nX = vectorizer.fit_transform(corpus)\n\n# Get the feature names\nfeature_names = vectorizer.get_feature_names_out()\n\n# Convert the sparse matrix to a dense one\nX = X.toarray()\n\n# Print the feature names and the document-term matrix\nprint(\"Feature names:\", feature_names)\nprint(\"Document-term matrix:\")\nprint(X)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 904, "library_problem_id": 87, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 905, "code": ["<code>\n# Define the vocabulary in the desired order\nvocabulary = ['looking', 'for', 'java', 'developer', 'frontend', 'with', 'knowledge', 'sql', 'jscript', 'and', 'this', 'is', 'the', 'third', 'one', 'is', 'first', 'document']\n\n# Create a CountVectorizer with the custom vocabulary\nvectorizer = CountVectorizer(vocabulary=vocabulary)\n\n# Fit and transform the corpus\nX = vectorizer.fit_transform(corpus)\n\n# Get the feature names\nfeature_names = vectorizer.get_feature_names_out()\n\n# Convert the sparse matrix to a dense one\nX = X.toarray()\n\n# Print the feature names and the document-term matrix\nprint(\"Feature names:\", feature_names)\nprint(\"Document-term matrix:\")\nprint(X)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 905, "library_problem_id": 88, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 906, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndef get_slopes(df):\n    df = df.select_dtypes(include=[np.number]) # select only numeric columns\n    df = df.dropna(axis=1, how='all') # drop columns with all NaN values\n    df = df.fillna(0) # fill NaN values with 0\n\n    X = df.values # get the values of the dataframe\n    slopes = [] # list to store the slope coefficients\n\n    for i in range(X.shape[1]): # for each column\n        y = X[:, i] # get the column values\n        model = LinearRegression().fit(X, y) # fit the model\n        slopes.append(model.coef_[0]) # append the slope coefficient\n\n    return np.array(slopes) # return the slope coefficients as a numpy array\n\ndf1 = load_data()\nslopes = get_slopes(df1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 906, "library_problem_id": 89, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 907, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndef calculate_slopes(df):\n    slopes = np.zeros(df.shape[1])\n    for i in range(df.shape[1]):\n        # Ignore columns with NaN values\n        if df.iloc[:, i].isnull().any():\n            continue\n        X = df.index.values.reshape(-1, 1)\n        y = df.iloc[:, i].values.reshape(-1, 1)\n        linear_regressor = LinearRegression()\n        linear_regressor.fit(X, y)\n        slopes[i] = linear_regressor.coef_[0]\n    return slopes\n\ndf1 = load_data()\nslopes = calculate_slopes(df1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 907, "library_problem_id": 90, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 89}}
{"id": 908, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndf = load_data()\n\n# Initialize LabelEncoder\nle = LabelEncoder()\n\n# Fit and transform 'Sex' column\ndf['Sex'] = le.fit_transform(df['Sex'])\n\ntransformed_df = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 908, "library_problem_id": 91, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 909, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n# Assuming df is your DataFrame and 'Sex' is the column you want to transform\ndf = pd.DataFrame({'Sex': ['male', 'female', 'male', 'female', 'male']})\n\n# Initialize LabelEncoder\nle = LabelEncoder()\n\n# Fit and transform the 'Sex' column\ndf['Sex'] = le.fit_transform(df['Sex'])\n\ntransformed_df = df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 909, "library_problem_id": 92, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 910, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef Transform(df):\n    le = LabelEncoder()\n    df['Sex'] = le.fit_transform(df['Sex'])\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 910, "library_problem_id": 93, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 911, "code": ["<code>\n# Create an Elastic Net regressor\nen = linear_model.ElasticNet()\n\n# Train the model using the training sets\nen.fit(X_train, y_train)\n\n# Get the score of the model on the training set\ntraining_set_score = en.score(X_train, y_train)\n\n# Get the score of the model on the test set\ntest_set_score = en.score(X_test, y_test)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 911, "library_problem_id": 94, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 912, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Assuming np_array is a 2x2 numpy array\nnp_array = np.array([[1, 2], [3, 4]])\n\n# Reshape the array to 1D\nnp_array_reshaped = np_array.reshape(-1, 1)\n\n# Initialize MinMaxScaler\nscaler = MinMaxScaler()\n\n# Fit and transform the data\ntransformed = scaler.fit_transform(np_array_reshaped)\n\n# Reshape the transformed data back to 2D\ntransformed = transformed.reshape(2, 2)\n\nprint(transformed)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 912, "library_problem_id": 95, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 913, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Assuming np_array is your 3x3 numpy array\nnp_array = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n# Reshape the array to 1D\nnp_array_reshaped = np_array.reshape(-1, 1)\n\n# Initialize MinMaxScaler\nscaler = MinMaxScaler()\n\n# Fit and transform the data\ntransformed = scaler.fit_transform(np_array_reshaped)\n\n# Reshape the transformed data back to 3D\ntransformed = transformed.reshape(np_array.shape)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 913, "library_problem_id": 96, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 914, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\nnp_array = np.array([[1, 2], [3, 4]])\n\ndef Transform(a):\n    # reshape the array to 1D\n    a = a.reshape(-1, 1)\n    # initialize MinMaxScaler\n    scaler = MinMaxScaler()\n    # fit and transform the array\n    a = scaler.fit_transform(a)\n    # reshape the array back to 2D\n    a = a.reshape(2, 2)\n    return a\n\nnew_np_array = Transform(np_array)\nprint(new_np_array)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 914, "library_problem_id": 97, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 915, "code": ["<code>\n# Reshape the input data to a 2D array\nx_new = x.reshape(1, -1)\n\n# Use the trained model to predict the output\npredict = clf.predict(x_new)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 915, "library_problem_id": 98, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 916, "code": ["<code>\nfrom sklearn.preprocessing import LabelEncoder\n\n# Convert string data to numerical data\nle = LabelEncoder()\nnew_X = np.array(X)\nnew_X[:, 0] = le.fit_transform(new_X[:, 0])\n\n# Convert string labels to numerical labels\nnew_y = le.fit_transform(new_y)\n\n# Fit the model\nclf.fit(new_X, new_y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 916, "library_problem_id": 99, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 917, "code": ["<code>\nfrom sklearn.preprocessing import LabelEncoder\n\n# Convert string inputs to numerical values\nle = LabelEncoder()\nnew_X = np.array(X)\nfor i in range(len(new_X[0])):\n    new_X[:, i] = le.fit_transform(new_X[:, i])\n\n# Fit the classifier\nclf.fit(new_X, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 917, "library_problem_id": 100, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 918, "code": ["<code>\n# Convert the string data to numerical data using LabelEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nnew_X = np.array(X)\nnew_X[:, 0] = le.fit_transform(new_X[:, 0])\n\n# The classifier expects numerical data, so we need to convert the string data to numerical data\n# We can use LabelEncoder to do this\n\n# Fit the classifier\nclf.fit(new_X, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 918, "library_problem_id": 101, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 919, "code": ["<code>\n# Separate features (X) from the target variable (y)\nX = dataframe.drop(\"Class\", axis=1)\ny = dataframe[\"Class\"]\n\n# Initialize the logistic regression model\nlogReg = LogisticRegression()\n\n# Fit the model\nlogReg.fit(X, y)\n\n# Make predictions\npredict = logReg.predict(X)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 919, "library_problem_id": 102, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 920, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\nfilename = \"animalData.csv\"\ndataframe = pd.read_csv(filename, dtype='category')\n\n# Remove the name of the animal\ndataframe = dataframe.drop([\"Name\"], axis=1)\n\n# Change the hunter/scavenger to 0/1\ncleanup = {\"Class\": {\"Primary Hunter\": 0, \"Primary Scavenger\": 1}}\ndataframe.replace(cleanup, inplace=True)\n\n# Split the data into X (features) and y (target)\nX = dataframe.drop(\"Class\", axis=1)\ny = dataframe[\"Class\"]\n\n# Initialize the Logistic Regression model\nlogReg = LogisticRegression()\n\n# Fit the model to the data\nlogReg.fit(X, y)\n\n# Make predictions\npredict = logReg.predict(X)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 920, "library_problem_id": 103, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 102}}
{"id": 921, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Assuming that the dataframe is sorted by date\ndef split_train_test(df, test_size=0.2):\n    # Ensure that the dataframe is sorted by date\n    df = df.sort_values('date')\n    \n    # Split the dataframe into training and testing sets\n    train_df, test_df = train_test_split(df, test_size=test_size, shuffle=False)\n    \n    return train_df, test_df\n\nfeatures_dataframe = load_data()\ntrain_dataframe, test_dataframe = split_train_test(features_dataframe)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 921, "library_problem_id": 104, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 922, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Assuming that the data is loaded into a variable named 'features_dataframe'\nfeatures_dataframe = load_data()\n\n# Assuming that the date column is named 'date' and it is in datetime format\nfeatures_dataframe['date'] = pd.to_datetime(features_dataframe['date'])\n\n# Sort the dataframe by date\nfeatures_dataframe = features_dataframe.sort_values(by='date')\n\n# Split the dataframe into training and testing sets\n# 80% of the data will be used for training and the rest for testing\ntrain_dataframe, test_dataframe = train_test_split(features_dataframe, test_size=0.2, shuffle=False)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 922, "library_problem_id": 105, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 923, "code": ["<code>\n### BEGIN SOLUTION\n    # Assuming that the dataframe is sorted by date\n    # We can split the dataframe by a certain percentage\n    # Let's say 80% for training and 20% for testing\n    train_dataframe = features_dataframe.iloc[:int(0.8 * len(features_dataframe))]\n    test_dataframe = features_dataframe.iloc[int(0.8 * len(features_dataframe)):]\n\n    return train_dataframe, test_dataframe\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 923, "library_problem_id": 106, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 924, "code": ["<code>\ndf = df.groupby('Month').apply(lambda group: pd.DataFrame(scaler.fit_transform(group[['X2', 'X3']]), columns=['X2_scale', 'X3_scale']))\ndf = df.reset_index().drop(columns='level_1')\ndf\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 924, "library_problem_id": 107, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 925, "code": ["<code>\nHere is the completed function:\n\n```python\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\n\nmyData = pd.DataFrame({\n    'Month': [3, 3, 3, 3, 3, 3, 8, 8, 8, 8, 8, 8, 8],\n    'A1': [1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2],\n    'A2': [31, 13, 13, 13, 33, 33, 81, 38, 18, 38, 18, 18, 118],\n    'A3': [81, 38, 18, 38, 18, 18, 118, 31, 13, 13, 13, 33, 33],\n    'A4': [1, 1, 1, 1, 1, 1, 8, 8, 8, 8, 8, 8, 8],\n})\n\nscaler = MinMaxScaler()\n\n# Group by 'Month' and apply MinMaxScaler to 'A2' and 'A3'\nmyData['new_A2'] = myData.groupby('Month')['A2'].apply(lambda x: scaler.fit_transform(x.values.reshape(-1, 1)))\nmyData['\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 925, "library_problem_id": 108, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 107}}
{"id": 926, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Assuming the string is loaded into the variable 'text'\ntext = \"This is a sample text. It includes punctuation, capitalization, and numbers 123.\"\n\nvectorizer = CountVectorizer(token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', lowercase=False)\nvectorizer.fit([text])\n\nfeature_names = vectorizer.get_feature_names_out()\n\nprint(feature_names)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 926, "library_problem_id": 109, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 927, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef get_vocabulary(words):\n    vectorizer = CountVectorizer(lowercase=False)\n    vectorizer.fit_transform(words)\n    feature_names = vectorizer.get_feature_names_out()\n    return feature_names\n\nwords = load_data()\nfeature_names = get_vocabulary(words)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 927, "library_problem_id": 110, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 109}}
{"id": 928, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV\n\n# Assuming GridSearch_fitted is already fit\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\n\nprint(full_results)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 928, "library_problem_id": 111, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 929, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV\n\n# Assuming GridSearch_fitted is already fit\nfull_results = pd.DataFrame(GridSearch_fitted.cv_results_)\nfull_results = full_results.sort_values(by='mean_fit_time')\nprint(full_results)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 929, "library_problem_id": 112, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 930, "code": ["<code>\nimport joblib\n\n# Save the model in the file named \"sklearn_model\"\njoblib.dump(fitted_model, 'sklearn_model.pkl')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 930, "library_problem_id": 113, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 931, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Assuming df is your DataFrame and it has a column 'description'\n\n# Initialize TfidfVectorizer\ntfidf = TfidfVectorizer()\n\n# Fit and transform the 'description' column\ntfidf_matrix = tfidf.fit_transform(df['description'])\n\n# Compute the cosine similarity\ncosine_similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 931, "library_problem_id": 114, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 932, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Assuming optim is your optimizer\noptim = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# Variables to track the number of epochs without improvement and the current learning rate\nno_improvement_count = 0\ncurrent_lr = optim.param_groups[0]['lr']\n\nfor epoch in range(num_epochs):\n    # Training code here...\n\n    # Validation code here...\n\n    # If the validation loss has not decreased, reduce the learning rate\n    if validation_loss_not_decreased:\n        no_improvement_count += 1\n    else:\n        no_improvement_count = 0\n\n    if no_improvement_count >= 5:\n        for param_group in optim.param_groups:\n            current_lr = param_group['lr']\n            param_group['lr'] = current_lr / 2\n        no_improvement_count = 0\n\n    # Reset the validation loss not decreased flag\n    validation_loss_not_decreased = False\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 932, "library_problem_id": 0, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 933, "code": ["<code>\nimport torch\n\n# Assuming you have a model and data\nmodel = torch.nn.Linear(10, 1)\ncriterion = torch.nn.MSELoss()\ndata = torch.randn(100, 10)\ntarget = torch.randn(100, 1)\n\n# Initialize the optimizer with a learning rate\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# Define the number of epochs and the initial learning rate\nnum_epochs = 100\ninitial_lr = 0.01\n\n# Loop over the epochs\nfor epoch in range(num_epochs):\n    # Forward pass\n    outputs = model(data)\n    loss = criterion(outputs, target)\n    \n    # Backward pass and optimization\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    # Adjust the learning rate after each epoch\n    # Here, we're decreasing the learning rate by a factor of 0.9 after each epoch\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = initial_lr * (0.9 ** (epoch // 1))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 933, "library_problem_id": 1, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 934, "code": ["<code>\nimport torch\n\n# Assuming we have a simple model and a simple dataset\nmodel = torch.nn.Linear(10, 1)\ncriterion = torch.nn.MSELoss()\n\n# Initialize the optimizer with a starting learning rate\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# Training loop\nfor epoch in range(100):\n    optimizer.zero_grad()\n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\n    loss.backward()\n    \n    # Adjust the learning rate based on the loss value\n    if loss.item() > 0.5:\n        for param_group in optimizer.param_groups:\n            param_group['lr'] *= 0.1\n    \n    optimizer.step()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 934, "library_problem_id": 2, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 935, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Assuming you have a way to monitor the performance of your model\ndef monitor_performance():\n    # This is a placeholder, replace it with your own performance monitoring function\n    return np.random.uniform(0, 1)\n\n# Load your data\n# optim = load_data()\n\n# Initialize the learning rate\nlr = 0.005\n\n# Define the optimizer\noptim = torch.optim.SGD(model.parameters(), lr=lr)\n\n# Define the threshold for performance improvement\nthreshold = 0.95\n\nfor epoch in range(num_epochs):\n    # Train your model\n    train(model, optim)\n    \n    # Monitor the performance\n    performance = monitor_performance()\n    \n    if performance > threshold:\n        # If the performance is good, decrease the learning rate\n        lr *= 0.1\n        for param_group in optim.param_groups:\n            param_group['lr'] = lr\n        print(f\"Learning rate has been decreased to {lr}\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 935, "library_problem_id": 3, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 936, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom gensim.models import Word2Vec\nfrom gensim.test.utils import common_texts\n\n# Load data\nword2vec = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n\n# Create a PyTorch tensor from the weights of the gensim Word2Vec model\nweights = torch.FloatTensor(word2vec.wv.vectors)\n\n# Create an embedding layer with the same size as the weights\nembedding = torch.nn.Embedding.from_pretrained(weights)\n\n# Use the embedding layer to create embedded input\nembedded_input = embedding(input_Tensor)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 936, "library_problem_id": 4, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 937, "code": ["<code>\ndef get_embedded_input(input_Tensor):\n    # Create a PyTorch embedding layer with the same size as the word2vec model\n    embedding = torch.nn.Embedding(len(word2vec.wv.key_to_index), word2vec.wv.vector_size)\n\n    # Load the weights from the word2vec model into the embedding layer\n    embedding.weight.data.copy_(torch.from_numpy(word2vec.wv.vectors))\n\n    # Make sure the weights are not trainable\n    embedding.weight.requires_grad = False\n\n    # Use the embedding layer to get the embedded input\n    embedded_input = embedding(input_Tensor)\n\n    return embedded_input\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 937, "library_problem_id": 5, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 938, "code": ["<code>\nimport numpy as np\nimport torch\nimport pandas as pd\n\n# Assuming x is a PyTorch tensor\nx = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n# Convert PyTorch tensor to numpy array\nx_np = x.numpy()\n\n# Convert numpy array to pandas DataFrame\npx = pd.DataFrame(x_np)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 938, "library_problem_id": 6, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 939, "code": ["<code>\nimport numpy as np\nimport torch\nimport pandas as pd\n\n# Assuming x is a PyTorch tensor\nx = torch.tensor([[1, 2, 3], [4, 5, 6]])\n\n# Convert PyTorch tensor to numpy array\nx_np = x.numpy()\n\n# Convert numpy array to pandas DataFrame\npx = pd.DataFrame(x_np)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 939, "library_problem_id": 7, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 940, "code": ["<code>\nimport numpy as np\nimport torch\nimport pandas as pd\n\n# Assuming x is a PyTorch tensor\nx = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n# Convert PyTorch tensor to numpy array\nx_np = x.numpy()\n\n# Convert numpy array to pandas DataFrame\npx = pd.DataFrame(x_np)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 940, "library_problem_id": 8, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 941, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Assuming A_log is a boolean tensor and B is a tensor of the same size\n# Create a mask by applying the logical operation on A_log and B\nmask = torch.logical_and(A_log, B)\n\n# Use torch.masked_select to select elements based on the mask\nC = torch.masked_select(B, mask)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 941, "library_problem_id": 9, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 942, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Assuming A_logical is a ByteTensor and B is a PyTorch tensor\nA_logical = torch.ByteTensor([1, 0, 1, 0])\nB = torch.tensor([[1.0, 2.0, 3.0, 4.0],\n                  [5.0, 6.0, 7.0, 8.0],\n                  [9.0, 10.0, 11.0, 12.0],\n                  [13.0, 14.0, 15.0, 16.0]])\n\n# Convert ByteTensor to LongTensor\nA_long = A_logical.long()\n\n# Use LongTensor as index to select columns from B\nC = torch.index_select(B, 1, A_long)\n\nprint(C)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 942, "library_problem_id": 10, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 943, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Assuming A_log and B are your tensors\nA_log, B = load_data()\n\n# Create a tensor of ones with the same shape as A_log\nones = torch.ones_like(A_log)\n\n# Use advanced indexing to select only the columns where A_log is True\nC = B[:, A_log.bool()]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 943, "library_problem_id": 11, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 944, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Assuming A_log and B are your tensors\nA_log, B = load_data()\n\n# Convert B to a numpy array and convert all True values to 1 and False values to 0\nB_np = B.numpy().astype(int)\n\n# Use numpy's boolean indexing to select the columns of A_log where B is 0\nC = A_log[:, B_np.sum(axis=0) == 0]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 944, "library_problem_id": 12, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 945, "code": ["<code>\ndef solve(A_log, B):\n    # Convert the logical index to a ByteTensor\n    B_byte = torch.ByteTensor(B.numpy().astype(np.bool))\n\n    # Use the ByteTensor to slice the tensor\n    C = A_log[:, B_byte]\n\n    return C\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 945, "library_problem_id": 13, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 946, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Assuming A_log is a ByteTensor and B is a PyTorch tensor\nA_log, B = load_data()\n\n# Convert ByteTensor to numpy array\nA_np = A_log.numpy()\n\n# Use numpy array as index to select columns from B\nC = B[:, A_np]\n\n# Convert result back to PyTorch tensor\nC = torch.from_numpy(C)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 946, "library_problem_id": 14, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 947, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\ndef select_columns(tensor, index):\n    return torch.index_select(tensor, 1, index)\n\nidx, B = load_data()\nC = select_columns(B, idx)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 947, "library_problem_id": 15, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 948, "code": ["<code>\nimport pandas as pd\nimport torch\nimport numpy as np\n\n# Assuming x_array is a numpy array with dtype=object and nested numpy arrays of dtype=float16\nx_array = np.array([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]], dtype=object)\n\n# Convert the numpy array to a torch tensor\nx_tensor = torch.tensor(x_array)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 948, "library_problem_id": 16, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 949, "code": ["<code>\ndef array_to_tensor(array):\n    if isinstance(array, np.ndarray):\n        if array.dtype == object:\n            return torch.tensor([array_to_tensor(item) for item in array])\n        else:\n            return torch.tensor(array)\n    else:\n        return array\n\nx_tensor = array_to_tensor(x_array)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 949, "library_problem_id": 17, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 950, "code": ["<code>\ndef Convert(a):\n    # Convert the numpy array to a list\n    a_list = a.tolist()\n    # Convert the list to a PyTorch tensor\n    t = torch.tensor(a_list)\n    return t\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 950, "library_problem_id": 18, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 951, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Assuming lens is a list of sentence lengths\nlens = [5, 3, 2, 7, 1]\n\n# Calculate the maximum sentence length\nmax_len = max(lens)\n\n# Create a mask with 1s up to the sentence length and 0s afterward\nmask = torch.zeros(len(lens), max_len)\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n\nprint(mask)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 951, "library_problem_id": 19, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 952, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Assuming lens is a list of sentence lengths\nlens = [5, 3, 7, 2]\n\n# Find the maximum sentence length\nmax_len = max(lens)\n\n# Create a binary mask\nmask = torch.zeros(len(lens), max_len)\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n\nprint(mask)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 952, "library_problem_id": 20, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 953, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Assuming lens is a list of sentence lengths\nlens = [5, 3, 4, 2]\n\n# Find the maximum sentence length\nmax_len = max(lens)\n\n# Create a mask with the same shape as the longest sentence\nmask = torch.zeros(len(lens), max_len)\n\n# For each sentence, set the elements up to its length to 1\nfor i, length in enumerate(lens):\n    mask[i, :length] = 1\n\nprint(mask)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 953, "library_problem_id": 21, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 954, "code": ["<code>\ndef get_mask(lens):\n    max_len = max(lens)\n    mask = torch.zeros(len(lens), max_len)\n    for i, length in enumerate(lens):\n        mask[i, :length] = 1\n    return mask.long()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 954, "library_problem_id": 22, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 955, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Assuming Tensor_2D is a 2D tensor\nTensor_2D = torch.tensor([[1, 0], [0, 2], [3, 0], [0, 4]])\n\n# Create an empty list to store the 3D tensor\nTensor_3D = []\n\n# Iterate over each row in the 2D tensor\nfor i in range(Tensor_2D.shape[0]):\n    # Create a diagonal matrix from the row\n    diagonal_matrix = torch.diag(Tensor_2D[i])\n    # Append the diagonal matrix to the 3D tensor\n    Tensor_3D.append(diagonal_matrix)\n\n# Convert the list of 2D tensors to a 3D tensor\nTensor_3D = torch.stack(Tensor_3D)\n\nprint(Tensor_3D)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 955, "library_problem_id": 23, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 956, "code": ["<code>\ndef Convert(t):\n    # Create an empty list to store the diagonal matrices\n    diagonal_matrices = []\n\n    # Iterate over the rows of the input tensor\n    for row in t:\n        # Create a diagonal matrix from the current row\n        diagonal_matrix = torch.diag(row)\n\n        # Append the diagonal matrix to the list\n        diagonal_matrices.append(diagonal_matrix)\n\n    # Convert the list of diagonal matrices to a 3D tensor\n    result = torch.stack(diagonal_matrices)\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 956, "library_problem_id": 24, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 23}}
{"id": 957, "code": ["<code>\nimport torch\n\ndef stack_tensors(a, b):\n    if a.shape[1] != b.shape[1]:\n        max_len = max(a.shape[0], b.shape[0])\n        a = torch.nn.functional.pad(a, (0, 0, 0, max_len-a.shape[0]))\n        b = torch.nn.functional.pad(b, (0, 0, 0, max_len-b.shape[0]))\n    ab = torch.stack([a, b], dim=0)\n    return ab\n\na = torch.tensor([[1, 2, 3], [4, 5, 6]])\nb = torch.tensor([[7, 8, 9], [10, 11, 12]])\n\nab = stack_tensors(a, b)\nprint(ab)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 957, "library_problem_id": 25, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 958, "code": ["<code>\nimport torch\n\n# Assuming a and b are your tensors\na = torch.tensor([[1, 2, 3], [4, 5, 6]])\nb = torch.tensor([[7, 8, 9], [10, 11, 12]])\n\n# Use torch.cat to concatenate the tensors along the first dimension\nab = torch.cat((a, b), dim=0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 958, "library_problem_id": 26, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 959, "code": ["<code>\nif a.shape[0] != b.shape[0]:\n        max_len = max(a.shape[0], b.shape[0])\n        pad_size = max_len - a.shape[0]\n        a = torch.cat([a, torch.zeros(pad_size, *a.shape[1:]).to(a.device)], dim=0)\n        pad_size = max_len - b.shape[0]\n        b = torch.cat([b, torch.zeros(pad_size, *b.shape[1:]).to(b.device)], dim=0)\n    ab = torch.stack([a, b], dim=0)\n    return ab\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 959, "library_problem_id": 27, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 960, "code": ["<code>\nimport torch\n\ndef mask_tensor(tensor, lengths):\n    max_length = tensor.size(1)\n    device = tensor.device\n    lengths = lengths.unsqueeze(1).unsqueeze(2).expand(-1, max_length, -1).to(device)\n    arange = torch.arange(max_length).expand(len(lengths), max_length).to(device)\n    mask = arange < lengths\n    tensor[~mask] = 0\n\na = torch.rand((10, 1000, 96))\nlengths = torch.randint(1000, (10,))\nmask_tensor(a, lengths)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 960, "library_problem_id": 28, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 961, "code": ["<code>\nimport torch\n\n# Create a tensor of 2333\nfill_value = torch.tensor([2333], dtype=torch.float32)\n\n# Expand dimensions to match the sentence length dimension\nfill_value = fill_value.unsqueeze(0).unsqueeze(0)\n\n# Create a tensor of True values of the same shape as a\nmask = torch.arange(a.size(1)).expand_as(a) >= lengths.unsqueeze(1).unsqueeze(2)\n\n# Apply the mask to the fill_value\na[mask] = fill_value[mask]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 961, "library_problem_id": 29, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 28}}
{"id": 962, "code": ["<code>\nimport torch\n\n# Create a tensor with random values\na = torch.rand((10, 1000, 23))\n\n# Create a tensor of random lengths\nlengths = torch.randint(1000, (10,))\n\n# Create a mask with ones where the sentence length is less than the actual length\nmask = torch.arange(a.size(1))[None, :] < lengths[:, None]\n\n# Apply the mask to the tensor\na = a * mask[:, None, :]\n\nprint(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 962, "library_problem_id": 30, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 963, "code": ["<code>\nimport torch\n\n# Create a tensor of 2333\nfill_value = torch.tensor([2333]).unsqueeze(0).unsqueeze(0)\n\n# Expand the dimensions of lengths to match the shape of a\nlengths = lengths.unsqueeze(1).unsqueeze(2).expand_as(a)\n\n# Create a mask where the value is less than the length\nmask = torch.arange(a.size(1)).to(lengths.device) < lengths\n\n# Use mask to index into a and fill in the values\na = torch.where(mask, a, fill_value)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 963, "library_problem_id": 31, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 28}}
{"id": 964, "code": ["<code>\nimport torch\n\n# Assuming list_of_tensors is a list of PyTorch tensors\nlist_of_tensors = [torch.tensor([1, 2, 3]), torch.tensor([4, 5, 6]), torch.tensor([7, 8, 9])]\n\n# Convert the list of tensors into a single tensor\n# We need to use torch.stack to stack the tensors along a new dimension\ntensor_of_tensors = torch.stack(list_of_tensors)\n\nprint(tensor_of_tensors)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 964, "library_problem_id": 32, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 965, "code": ["<code>\nimport torch\n\ndef list_to_tensor(lst):\n    return torch.stack([torch.tensor(x) for x in lst])\n\n# Assuming the list contains tensors of varying shapes\nlst = [torch.tensor([1, 2, 3]), torch.tensor([4, 5]), torch.tensor([6])]\nnew_tensors = list_to_tensor(lst)\nprint(new_tensors)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 965, "library_problem_id": 33, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 966, "code": ["<code>\ndef Convert(lt):\n    # Convert the list of tensors to a single tensor\n    return torch.stack([torch.tensor(t) for t in lt])\n\n# Test the function\nlist_of_tensors = [np.array([1, 2, 3]), np.array([4, 5, 6]), np.array([7, 8, 9])]\ntensor = Convert(list_of_tensors)\nprint(tensor)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 966, "library_problem_id": 34, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 967, "code": ["<code>\nimport torch\n\n# Assuming list_of_tensors is a list of PyTorch tensors\nlist_of_tensors = [torch.tensor([1, 2, 3]), torch.tensor([4, 5, 6]), torch.tensor([7, 8, 9])]\n\n# Use torch.stack to stack the tensors in the list along a new dimension\ntensor_of_tensors = torch.stack(list_of_tensors)\n\nprint(tensor_of_tensors)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 967, "library_problem_id": 35, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 968, "code": ["<code>\nimport numpy as np\nimport torch\n\ndef select_from_tensor(t, idx):\n    assert type(t) == torch.Tensor\n    assert type(idx) == np.ndarray\n    result = torch.gather(t, 1, torch.from_numpy(idx).long().unsqueeze(1)).squeeze(1)\n    return result\n\nt = torch.tensor([[1, 2], [3, 4], [5, 6]])\nidx = np.array([0, 1, 0])\n\nresult = select_from_tensor(t, idx)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 968, "library_problem_id": 36, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 969, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\ndef select_from_tensor(t, idx):\n    assert type(t) == torch.Tensor\n    assert type(idx) == np.ndarray\n\n    result = torch.gather(t, 1, torch.from_numpy(idx).unsqueeze(1).long()).squeeze()\n\n    return result\n\n# Test the function\nt = torch.tensor([[1, 2], [3, 4], [5, 6]])\nidx = np.array([0, 1, 0])\n\nresult = select_from_tensor(t, idx)\nprint(result)  # Output: tensor([1, 4, 5])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 969, "library_problem_id": 37, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 970, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\ndef select_elements(t, idx):\n    # Convert the numpy array to a torch tensor\n    idx_tensor = torch.from_numpy(idx)\n\n    # Use torch.gather to select elements based on the indices\n    result = torch.gather(t, 1, idx_tensor.view(-1, 1))\n\n    return result.squeeze(1)\n\nt, idx = load_data()\nassert type(t) == torch.Tensor\nassert type(idx) == np.ndarray\n\nresult = select_elements(t, idx)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 970, "library_problem_id": 38, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 971, "code": ["<code>\nimport torch\n\ndef select_slices(ids, x):\n    # Use torch.gather to select slices\n    result = torch.gather(x, 1, ids.unsqueeze(-1).expand(-1, -1, 2))\n    return result[:, 0, :]\n\nids = torch.tensor([[0], [1], [2], [0], [1], [2], [0]])\nx = torch.tensor([[[1, 2]], [[3, 4]], [[5, 6]], [[7, 8]], [[9, 10]], [[11, 12]], [[13, 14]]])\n\nresult = select_slices(ids, x)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 971, "library_problem_id": 39, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 972, "code": ["<code>\nimport torch\n\n# Assuming ids and x are already defined\nids = torch.randint(0, 114, (30, 1))  # Example ids\nx = torch.randn(30, 3, 114)  # Example x\n\n# Use torch.gather to gather the slices\nresult = torch.gather(x, 2, ids.unsqueeze(2)).squeeze(2)\n\nprint(result.shape)  # Should print torch.Size([30, 114])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 972, "library_problem_id": 40, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 973, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Assuming ids and x are already defined\nids, x = load_data()\n\n# Convert ids to boolean tensor\nids_bool = torch.tensor(ids, dtype=torch.bool)\n\n# Use torch.masked_select to select the slices\nresult = torch.masked_select(x, ids_bool)\n\n# Reshape the result to (70,2)\nresult = result.view(70,2)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 973, "library_problem_id": 41, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 974, "code": ["<code>\nimport torch\n\n# Assuming softmax_output is the output of the logistic regression model\n# which is a tensor of size n x 3\nsoftmax_output = torch.tensor([[0.1, 0.2, 0.7],\n                               [0.3, 0.4, 0.3],\n                               [0.2, 0.3, 0.5]])\n\n# Use torch.max to get the index of the maximum value along the second dimension\n# This will give us the class with the highest probability for each input sample\ny = torch.max(softmax_output, dim=1)[1]\n\nprint(y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 974, "library_problem_id": 42, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 975, "code": ["<code>\nimport torch\n\n# Assuming softmax_output is a n x 3 tensor\nsoftmax_output = torch.tensor([[0.1, 0.6, 0.3],\n                               [0.2, 0.3, 0.5],\n                               [0.3, 0.4, 0.3],\n                               [0.4, 0.2, 0.4]])\n\n# Use torch.max to get the index of the maximum value along the second dimension (columns)\n# This will return a tuple where the first element is the values and the second element is the indices\n# We only need the indices, so we index into the tuple with [1]\ny = torch.max(softmax_output, dim=1)[1]\n\nprint(y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 975, "library_problem_id": 43, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 976, "code": ["<code>\nimport torch\n\ndef get_lowest_prob_class(softmax_output):\n    _, y = torch.max(softmax_output, dim=1)\n    return y.unsqueeze(1)\n\n# Test the function\nsoftmax_output = torch.tensor([[0.1, 0.4, 0.5], [0.2, 0.2, 0.6], [0.3, 0.3, 0.4]])\ny = get_lowest_prob_class(softmax_output)\nprint(y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 976, "library_problem_id": 44, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 977, "code": ["<code>\ndef solve(softmax_output):\n    # return the solution in this function\n    # y = solve(softmax_output)\n    ### BEGIN SOLUTION\n    y = torch.argmax(softmax_output, dim=1)\n    return y\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 977, "library_problem_id": 45, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 978, "code": ["<code>\nimport torch\n\ndef solve(softmax_output):\n    y = torch.argmax(softmax_output, dim=1)\n    return y\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 978, "library_problem_id": 46, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 979, "code": ["<code>\ndef custom_cross_entropy(input, target):\n    # One-hot encode the target\n    target_one_hot = torch.zeros_like(input).scatter_(1, target.unsqueeze(1), 1)\n\n    # Compute the cross-entropy loss\n    loss = -torch.sum(target_one_hot * torch.log(input + 1e-10)) / input.shape[0]\n\n    return loss\n\n# Test the function\ninput = Variable(torch.randn(2, 3, 2, 2))\ntarget = Variable(torch.LongTensor([[0, 1], [2, 0]]))\n\nloss = custom_cross_entropy(input, target)\nprint(loss)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 979, "library_problem_id": 47, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 980, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Assuming A and B are your tensors\nA, B = load_data()\n\n# Convert tensors to numpy arrays\nA = A.numpy()\nB = B.numpy()\n\n# Count the number of matching elements\ncnt_equal = np.sum(A == B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 980, "library_problem_id": 48, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 981, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Assuming A and B are 1D tensors\nA, B = load_data()\n\n# Convert the tensors to numpy arrays\nA = A.numpy()\nB = B.numpy()\n\n# Count the number of matching elements\ncnt_equal = np.sum(A == B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 981, "library_problem_id": 49, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 982, "code": ["<code>\nimport numpy as np\nimport torch\n\n# Assuming A and B are your tensors\nA, B = load_data()\n\n# Convert tensors to numpy arrays\nA_np = A.numpy()\nB_np = B.numpy()\n\n# Calculate the number of differing elements\ncnt_not_equal = np.count_nonzero(A_np != B_np)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 982, "library_problem_id": 50, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 48}}
{"id": 983, "code": ["<code>\ndef Count(A, B):\n    # Check if the tensors have the same size\n    assert A.shape == B.shape, \"Tensors must have the same size\"\n\n    # Use torch.eq to compare the tensors and count the number of matching elements\n    cnt_equal = torch.sum(torch.eq(A, B)).item()\n\n    return cnt_equal\n\n# Load your data\nA, B = load_data()\n\n# Count the number of matching elements\ncnt_equal = Count(A, B)\nprint(f\"Number of matching elements: {cnt_equal}\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 983, "library_problem_id": 51, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 984, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\ndef count_equal(A, B):\n    x = A.shape[0] // 2\n    cnt_equal = (A[-x:] == B[-x:]).sum().item()\n    return cnt_equal\n\nA, B = load_data()\ncnt_equal = count_equal(A, B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 984, "library_problem_id": 52, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 985, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Assuming A and B are your tensors\nA, B = load_data()\n\n# Get the last x elements of each tensor\nA_last_x = A[-x:]\nB_last_x = B[-x:]\n\n# Calculate the number of unequal elements\ncnt_not_equal = torch.sum(A_last_x != B_last_x)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 985, "library_problem_id": 53, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 986, "code": ["<code>\nimport torch\n\n# Load your data\n# a = load_data()\n# assert a.shape == (1, 3, 10, 40, 1)\n\n# Create a dummy tensor for the example\na = torch.randn(1, 3, 10, 40, 1)\n\nchunk_dim = 10\n\n# Split the tensor along the specified dimension\ntensors_list = [a[:, :, i:i+chunk_dim, :, :] for i in range(a.size(2) - chunk_dim + 1)]\n\n# Put tensors in order\ntensors_31 = torch.cat(tensors_list, dim=2)\n\nprint(tensors_31.shape)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 986, "library_problem_id": 54, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 987, "code": ["<code>\nimport torch\n\n# Load your data\n# a = load_data()\n# assert a.shape == (1, 3, 10, 40, 1)\n\n# Create a dummy tensor for the example\na = torch.randn(1, 3, 10, 40, 1)\n\nchunk_dim = 10\n\n# Calculate the number of chunks\nnum_chunks = a.size(chunk_dim)\n\n# Create a list to hold the chunks\ntensors_31 = []\n\n# Loop over the chunks\nfor i in range(num_chunks):\n    # Slice the tensor along the specified dimension\n    chunk = a.narrow(chunk_dim, i, 10)\n    \n    # Append the chunk to the list\n    tensors_31.append(chunk)\n\n# Check the shapes of the tensors in the list\nfor i, tensor in enumerate(tensors_31):\n    print(f\"Tensor {i+1} shape: {tensor.shape}\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 987, "library_problem_id": 55, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 988, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Assuming that the data is loaded and stored in the variables mask, clean_input_spectrogram, output\n\n# Use torch.where to replace the elements in output where mask is 1 with the corresponding elements from clean_input_spectrogram\noutput = torch.where(mask == 1, clean_input_spectrogram, output)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 988, "library_problem_id": 56, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 989, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Assuming that the data is loaded and stored in the variables mask, clean_input_spectrogram, and output\n\n# Create a mask tensor where the mask is 1 and the rest is 0\nmask_tensor = (mask != 0).float()\n\n# Use torch.mul to multiply the clean input spectrogram with the mask tensor\noutput = torch.mul(clean_input_spectrogram, mask_tensor.unsqueeze(-1)) + torch.mul(output, (1 - mask_tensor).unsqueeze(-1))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 989, "library_problem_id": 57, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 990, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\ndef signed_min(x, y):\n    abs_x = torch.abs(x)\n    abs_y = torch.abs(y)\n    mask_x = abs_x < abs_y\n    mask_y = ~mask_x\n    signed_min = torch.where(mask_x, x, y)\n    return signed_min\n\nx, y = load_data()\nsigned_min = signed_min(x, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 990, "library_problem_id": 58, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 991, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\ndef signed_max(x, y):\n    # Create a mask where x is greater than y\n    mask = x.abs() > y.abs()\n\n    # Use the mask to replace y with x where x is greater than y\n    y[mask] = x[mask]\n\n    return y\n\nx, y = load_data()\nsigned_max = signed_max(x, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 991, "library_problem_id": 59, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 992, "code": ["<code>\ndef solve(x, y):\n    # return the solution in this function\n    signed_min = torch.where(x.abs() < y.abs(), x, y)\n    return signed_min\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 992, "library_problem_id": 60, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 993, "code": ["<code>\nimport torch.nn.functional as F\n\n# Add softmax layer to the network\nMyNet = torch.nn.Sequential(torch.nn.Linear(4, 15),\n                            torch.nn.Sigmoid(),\n                            torch.nn.Linear(15, 3),\n                            torch.nn.Softmax(dim=1))\n\n# Load the model\nMyNet.load_state_dict(torch.load(\"my_model.pt\"))\n\n# Load the data\ninput = load_data()\nassert type(input) == torch.Tensor\n\n# Get the model's predictions\noutput = MyNet(input)\n\n# Use softmax to get the confidence scores\nconfidence_score = F.softmax(output, dim=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 993, "library_problem_id": 61, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 994, "code": ["<code>\nHere is the solution:\n\n```python\ndef combine_tensors(a, b):\n    # Check if the tensors are on the same device\n    if a.device != b.device:\n        raise ValueError(\"Tensors must be on the same device\")\n\n    # Check if the tensors are on the same type\n    if a.dtype != b.dtype:\n        raise ValueError(\"Tensors must be of the same type\")\n\n    # Check if the tensors have the same number of columns\n    if a.size(1) != b.size(1):\n        raise ValueError(\"Tensors must have the same number of columns\")\n\n    # Check if the tensors have the same number of rows\n    if a.size(0) != b.size(0):\n        raise ValueError(\"Tensors must have the same number of rows\")\n\n    # Calculate the overlap\n    overlap = a.size(1) // 2\n\n    # Combine the tensors\n    combined = torch.cat([a[:, :overlap], torch.mean(a[:, overlap:], dim=1, keepdim=True), b[:, overlap:]], dim=1)\n\n    return combined\n\n# Load your data\na = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\nb = torch.tensor([[11, 12, 13, 4, 5], [14, 15, 16, 9, 10]])\n\nresult = combine_t\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 994, "library_problem_id": 62, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 995, "code": ["<code>\ndef solve(a, b):\n    # Check if the tensors are on the same device\n    if a.device != b.device:\n        raise ValueError(\"Tensors must be on the same device\")\n\n    # Check if the tensors are on the same type\n    if a.dtype != b.dtype:\n        raise ValueError(\"Tensors must be of the same type\")\n\n    # Check if the tensors are 2D\n    if a.ndim != 2 or b.ndim != 2:\n        raise ValueError(\"Tensors must be 2D\")\n\n    # Check if the tensors have the same number of columns\n    if a.shape[1] != b.shape[1]:\n        raise ValueError(\"Tensors must have the same number of columns\")\n\n    # Check if the tensors have the same number of rows\n    if a.shape[0] != b.shape[0]:\n        raise ValueError(\"Tensors must have the same number of rows\")\n\n    # Calculate the average of overlapping columns\n    overlap = a[:, :1] * b[:, -1:]\n    overlap = overlap.mean(dim=0)\n\n    # Combine the tensors\n    result = torch.cat([a[:, :1], overlap, a[:, -1:]], dim=1)\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 995, "library_problem_id": 63, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 62}}
{"id": 996, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Assuming t is a PyTorch tensor\ndef add_border(t):\n    # Get the dimensions of the tensor\n    height, width = t.shape\n\n    # Create a new tensor of zeros with dimensions increased by 2 in both height and width\n    result = torch.zeros(height + 2, width + 2)\n\n    # Copy the original tensor into the center of the new tensor\n    result[1:-1, 1:-1] = t\n\n    return result\n\n# Load your data\nt = load_data()\n\n# Call the function\nresult = add_border(t)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 996, "library_problem_id": 64, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 997, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Assuming t is a 2x2 tensor\nt = torch.tensor([[1, 2], [3, 4]])\n\n# Create a 4x4 tensor with all elements as 0\nresult = torch.zeros(4, 4)\n\n# Place the original 2x2 tensor in the center of the new, larger tensor\nresult[1:3, 1:3] = t\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 997, "library_problem_id": 65, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 998, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Assuming that the data is loaded from a file\ndef load_data():\n    # Here we are creating a 2D tensor for the example\n    t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    return t\n\nt = load_data()\n\n# Create a new tensor with -1 values\nresult = torch.full((t.shape[0]+2, t.shape[1]+2), -1)\n\n# Copy the original tensor into the center of the new tensor\nresult[1:-1, 1:-1] = t\n\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 998, "library_problem_id": 66, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 999, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport torch\n\nhid_dim = 32\ndata = torch.randn(10, 2, 3, hid_dim)\ndata = data.view(10, 2 * 3, hid_dim)\nW = torch.randn(hid_dim)\n\n# Perform dot product\nresult = torch.bmm(data, W.unsqueeze(dim=1).unsqueeze(dim=-1)).squeeze(dim=-1)\n\nprint(result.shape)  # Should print: torch.Size([10, 2, 3])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 999, "library_problem_id": 67, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
