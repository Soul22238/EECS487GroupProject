{"id": 0, "code": ["<code>\n```python\n\"\"\"\nFunction to shuffle the rows of a DataFrame based on a given index list.\n\nThis function takes a DataFrame and a list of indices as input and returns a new DataFrame with rows reordered according to the specified list. The DataFrame is assumed to be read from a CSV file and initially sorted by a 'Type' column.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame with columns 'Col1', 'Col2', 'Col3', and 'Type'.\n    index_list (List[int]): A list of integers representing the new order of row indices.\n\nReturns:\n    pd.DataFrame: A new DataFrame with rows reordered according to the index_list.\n\nExample:\n    Given a DataFrame:\n        Col1  Col2  Col3  Type\n    0      1     2     3     1\n    1      4     5     6     1\n    2      7     8     9     2\n    3     10    11    12     2\n    4     13    14    15     3\n    5     16    17    18     3\n\n    And an index list: [2, 4, 0, 3, 1, 5]\n\n    The function will return:\n        Col1  Col2  Col3  Type\n    2      7     8     9     2\n    4     13    14    15     3\n    0      1     2     3     1\n    3     10    11    12     2\n    1      4     5     6     1\n    5     16    17    18     3\n\"\"\"\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 0, "library_problem_id": 0, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 1, "code": ["<code>\nimport pandas as pd\nfrom typing import List, Tuple\n\ndef shuffle_dataframe_and_count_type_differences(df: pd.DataFrame, shuffle_indices: List[int]) -> Tuple[pd.DataFrame, int]:\n    # Shuffle the DataFrame rows based on the provided indices\n    shuffled_df = df.iloc[shuffle_indices].reset_index(drop=True)\n    \n    # Count the number of rows where the 'Type' column differs from the original DataFrame\n    type_differences_count = (shuffled_df['Type'] != df['Type']).sum()\n    \n    return shuffled_df, type_differences_count\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 1, "library_problem_id": 1, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 2, "code": ["<code>\nimport pandas as pd\n\ndef refactor_dataframe(data: pd.DataFrame) -> pd.DataFrame:\n    def replace_values(column: pd.Series) -> pd.Series:\n        counts = column.value_counts()\n        frequent_values = counts[counts >= 2].index\n        return column.apply(lambda x: x if x in frequent_values else 'other')\n\n    result = data.copy()\n    for col in ['Qu1', 'Qu2', 'Qu3']:\n        result[col] = replace_values(result[col])\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 2, "library_problem_id": 2, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 3, "code": ["<code>\ndef refactor_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    # Define columns to process\n    columns_to_process = ['Qu1', 'Qu2', 'Qu3']\n    \n    # Create a copy of the DataFrame to avoid modifying the original\n    result_df = df.copy()\n    \n    # Iterate over each specified column\n    for column in columns_to_process:\n        # Calculate the value counts for the current column\n        value_counts = df[column].value_counts()\n        \n        # Determine which values have a count of 3 or more\n        frequent_values = value_counts[value_counts >= 3].index\n        \n        # Replace values not in frequent_values with 'other'\n        result_df[column] = df[column].apply(lambda x: x if x in frequent_values else 'other')\n    \n    return result_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 3, "library_problem_id": 3, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 4, "code": ["<code>\nimport pandas as pd\n\ndef transform_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    min_count = 2\n    result_df = df.copy()\n    \n    for column in result_df.columns:\n        value_counts = result_df[column].value_counts()\n        frequent_values = value_counts[value_counts >= min_count].index\n        result_df[column] = result_df[column].apply(lambda x: x if x in frequent_values else 'other')\n    \n    return result_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 4, "library_problem_id": 4, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 5, "code": ["<code>\n```python\n\"\"\"\nRefactor the values in a pandas DataFrame based on their frequency of occurrence.\n\nThe function processes a DataFrame with three columns: 'Qu1', 'Qu2', and 'Qu3'. \nValues in 'Qu1' are replaced with 'other' if their frequency is less than 3. \nValues in 'Qu2' and 'Qu3' are replaced with 'other' if their frequency is less than 2.\n\nArgs:\n    df (pd.DataFrame): A DataFrame with columns 'Qu1', 'Qu2', and 'Qu3'.\n\nReturns:\n    pd.DataFrame: A DataFrame with values replaced according to the specified conditions.\n\"\"\"\n\nimport pandas as pd\n\ndef refactor_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    # Define the frequency thresholds for each column\n    qu1_threshold = 3\n    qu2_qu3_threshold = 2\n\n    # Process 'Qu1' column\n    qu1_value_counts = df['Qu1'].value_counts()\n    df['Qu1'] = df['Qu1'].apply(lambda x: x if qu1_value_counts[x] >= qu1_threshold else 'other')\n\n    # Process 'Qu2' column\n    qu2_value_counts = df['Qu2'].value_counts()\n    df['Qu2'] = df['Qu2'].apply(lambda x: x if qu2_value_counts[x] >= qu2_qu3_threshold else 'other')\n\n    # Process 'Qu3' column\n    qu3_value_counts =\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 5, "library_problem_id": 5, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 6, "code": ["<code>\ndef modify_dataframe(df):\n    for col in df.columns:\n        if 'Qu1' in col:\n            df[col] = df[col].map(lambda x: x if x == 'apple' else 'other' if df[col].value_counts()[x] < 3 else x)\n        elif 'Qu3' in col:\n            df[col] = df[col].map(lambda x: x if df[col].value_counts()[x] >= 2 else 'other')\n    return df\n\nresult = modify_dataframe(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 6, "library_problem_id": 6, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 7, "code": ["<code>\ndef remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Removes duplicates from the DataFrame based on the 'url' column, retaining duplicates if 'keep_if_dup' is 'Yes'.\n\n    Args:\n    df (pd.DataFrame): A DataFrame containing columns 'id', 'url', and 'keep_if_dup'.\n\n    Returns:\n    pd.DataFrame: A DataFrame with duplicates removed according to the specified conditions.\n    \"\"\"\n    # Create a mask where 'keep_if_dup' is 'Yes'\n    mask = df['keep_if_dup'] == 'Yes'\n\n    # Use groupby to group by 'url' and apply the mask to keep the first occurrence of each 'url'\n    # if 'keep_if_dup' is 'Yes'\n    df = df.loc[df.groupby('url')['keep_if_dup'].apply(lambda x: x[x == 'Yes'].index[0] if x.nunique() == 1 else x.index).reset_index(drop=True)]\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 7, "library_problem_id": 7, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 8, "code": ["<code>\nimport pandas as pd\n\ndef remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n    # Separate rows where 'drop_if_dup' is 'No'\n    no_drop_df = df[df['drop_if_dup'] == 'No']\n    \n    # Separate rows where 'drop_if_dup' is 'Yes' and drop duplicates\n    yes_drop_df = df[df['drop_if_dup'] == 'Yes'].drop_duplicates(subset='url', keep='first')\n    \n    # Concatenate the two DataFrames and sort by 'id' to maintain original order\n    result_df = pd.concat([no_drop_df, yes_drop_df]).sort_values(by='id').reset_index(drop=True)\n    \n    return result_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 8, "library_problem_id": 8, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 7}}
{"id": 9, "code": ["<code>\ndef filter_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    # Separate rows where 'keep_if_dup' is 'Yes'\n    keep_yes = df[df['keep_if_dup'] == 'Yes']\n    \n    # Remove duplicates from rows where 'keep_if_dup' is 'No', keeping the last occurrence\n    keep_no = df[df['keep_if_dup'] == 'No'].drop_duplicates(subset='url', keep='last')\n    \n    # Concatenate the two DataFrames to get the final result\n    result = pd.concat([keep_yes, keep_no]).sort_index()\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 9, "library_problem_id": 9, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 7}}
{"id": 10, "code": ["<code>\nimport pandas as pd\nfrom typing import Dict, Any\n\ndef dataframe_to_nested_dict(df: pd.DataFrame) -> Dict[str, Any]:\n    nested_dict = {}\n    for _, row in df.iterrows():\n        current_level = nested_dict\n        for col in df.columns[:-2]:\n            current_level = current_level.setdefault(row[col], {})\n        current_level[row[df.columns[-2]]] = row[df.columns[-1]]\n    return nested_dict\n\ndf = pd.DataFrame({'name': ['A', 'A', 'B', 'C', 'B', 'A'],\n                   'v1': ['A1', 'A2', 'B1', 'C1', 'B2', 'A2'],\n                   'v2': ['A11', 'A12', 'B12', 'C11', 'B21', 'A21'],\n                   'v3': [1, 2, 3, 4, 5, 6]})\n\nresult = dataframe_to_nested_dict(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 10, "library_problem_id": 10, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 11, "code": ["<code>\nimport pandas as pd\n\ndef remove_timezone(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n    \"\"\"\n    Remove timezone information from a specified datetime column in a pandas DataFrame.\n\n    Args:\n        df (pd.DataFrame): The DataFrame containing the datetime column.\n        column_name (str): The name of the column from which to remove timezone information.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with the specified column's timezone information removed.\n    \"\"\"\n    # Convert the specified column to datetime if not already\n    df[column_name] = pd.to_datetime(df[column_name])\n\n    # Remove timezone information by converting to naive datetime\n    df[column_name] = df[column_name].dt.tz_localize(None)\n\n    return df\n\n# Example usage\ndf = pd.DataFrame({'datetime': ['2015-12-01 00:00:00-06:00', '2015-12-02 00:01:00-06:00', '2015-12-03 00:00:00-06:00']})\ndf = remove_timezone(df, 'datetime')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 11, "library_problem_id": 11, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 12, "code": ["<code>\nimport pandas as pd\n\ndef remove_timezone_info(df: pd.DataFrame) -> pd.DataFrame:\n    # Ensure the 'datetime' column is in datetime format\n    df['datetime'] = pd.to_datetime(df['datetime'])\n\n    # Remove timezone information by converting to naive datetime\n    df['datetime'] = df['datetime'].dt.tz_localize(None)\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 12, "library_problem_id": 12, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 13, "code": ["<code>\nimport pandas as pd\n\ndef process_datetime(df):\n    # Convert 'datetime' column to datetime objects and remove timezone information\n    df['datetime'] = pd.to_datetime(df['datetime']).dt.tz_localize(None)\n\n    # Format 'datetime' column to the desired string format\n    df['datetime'] = df['datetime'].dt.strftime('%d-%b-%Y %H:%M:%S')\n\n    # Sort DataFrame by 'datetime' column in ascending order\n    df = df.sort_values(by='datetime')\n\n    # Return the transformed DataFrame\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 13, "library_problem_id": 13, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 14, "code": ["<code>\ndef remove_timezone_and_sort(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Removes timezone information from the 'datetime' column in the DataFrame and sorts it in ascending order.\n\n    Args:\n        df (pd.DataFrame): A DataFrame containing a 'datetime' column with timezone information.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the 'datetime' column stripped of timezone info and sorted in ascending order.\n    \"\"\"\n    # Remove timezone information by converting to naive datetime\n    df['datetime'] = df['datetime'].dt.tz_localize(None)\n    \n    # Sort the DataFrame by 'datetime' column in ascending order\n    df = df.sort_values(by='datetime').reset_index(drop=True)\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 14, "library_problem_id": 14, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 11}}
{"id": 15, "code": ["<code>\nimport pandas as pd\n\ndef expand_message_column(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Transforms the input DataFrame by expanding the 'message' column into separate columns for each key-value pair.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame with columns 'name', 'status', 'number', and 'message'.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with the 'message' column expanded into separate columns for each key, \n                      filling missing values with the string 'none'.\n    \"\"\"\n    # Your implementation here\n    # Split the 'message' column into a list of key-value pairs\n    df['message'] = df['message'].str.strip('[]').str.split(', ')\n\n    # Convert the list of key-value pairs into a DataFrame\n    message_df = pd.DataFrame(df['message'].to_list(), index=df.index)\n\n    # Split each key-value pair into a separate column\n    message_df = message_df.apply(lambda x: x.str.split(': ').str[1])\n\n    # Fill missing values with 'none'\n    message_df.fillna('none', inplace=True)\n\n    # Concatenate the expanded DataFrame with the original DataFrame\n    result = pd.concat([df.drop('message', axis=1), message_df], axis=1)\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 15, "library_problem_id": 15, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 16, "code": ["<code>\ndef multiply_scores(df: pd.DataFrame, target_products: list, factor: float) -> pd.DataFrame:\n    # Create a boolean mask where 'product' is in target_products\n    mask = df['product'].isin(target_products)\n    \n    # Multiply 'score' by factor for rows where mask is True\n    df.loc[mask, 'score'] *= factor\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 16, "library_problem_id": 16, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 17, "code": ["<code>\nimport pandas as pd\nfrom typing import List\n\ndef adjust_scores(df: pd.DataFrame, products: List[int]) -> pd.DataFrame:\n    # Define the multiplication factor for scores not in the specified product list\n    factor = 10\n    \n    # Apply the multiplication factor to scores of products not in the list\n    df['score'] = df.apply(\n        lambda row: row['score'] * factor if row['product'] not in products else row['score'],\n        axis=1\n    )\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 17, "library_problem_id": 17, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 18, "code": ["<code>\ndef multiply_scores_in_ranges(df: pd.DataFrame, product_ranges: List[List[int]], constant: float) -> pd.DataFrame:\n    # Iterate over each product range\n    for start, end in product_ranges:\n        # Apply multiplication to 'score' for products within the current range\n        df.loc[df['product'].between(start, end), 'score'] *= constant\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 18, "library_problem_id": 18, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 19, "code": ["<code>\ndef min_max_normalize_scores(df: pd.DataFrame, products_to_normalize: List[int]) -> pd.DataFrame:\n    # Filter the DataFrame to get the scores for the specified products\n    mask = df['product'].isin(products_to_normalize)\n    scores_to_normalize = df.loc[mask, 'score']\n    \n    # Calculate the min and max of the scores to normalize\n    min_score = scores_to_normalize.min()\n    max_score = scores_to_normalize.max()\n    \n    # Apply Min-Max Normalization to the specified scores\n    df.loc[mask, 'score'] = (scores_to_normalize - min_score) / (max_score - min_score)\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 19, "library_problem_id": 19, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 20, "code": ["<code>\ndef convert_binary_to_categorical(df: pd.DataFrame) -> pd.DataFrame:\n    # Create a new column 'category' by using idxmax to find the column name with the max value (1) for each row\n    df['category'] = df.idxmax(axis=1)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 20, "library_problem_id": 20, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 21, "code": ["<code>\nimport pandas as pd\n\ndef add_category_column(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Add a 'category' column to the DataFrame based on binary columns.\n    \n    Args:\n    - df (pd.DataFrame): DataFrame with binary columns where 0 indicates the presence of a category.\n    \n    Returns:\n    - pd.DataFrame: DataFrame with an additional 'category' column.\n    \"\"\"\n    df['category'] = df.apply(lambda row: row.index[row == 0][0], axis=1)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 21, "library_problem_id": 21, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 22, "code": ["<code>\ndef convert_binary_to_categorical(df: pd.DataFrame) -> pd.DataFrame:\n    df['category'] = df.apply(lambda row: [col for col in df.columns if row[col] == 1], axis=1)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 22, "library_problem_id": 22, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 20}}
{"id": 23, "code": ["<code>\ndef transform_date_format(df: pd.DataFrame) -> pd.DataFrame:\n    df['Date'] = pd.to_datetime(df['Date'])\n    df['Date'] = df['Date'].dt.strftime('%b-%Y')\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 23, "library_problem_id": 23, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 24, "code": ["<code>\nimport pandas as pd\n\ndef format_dates(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Transforms the 'Date' column of the input DataFrame to the 'DD-MMM-YYYY' format.\n\n    Args:\n    df (pd.DataFrame): A DataFrame with a 'Date' column containing date strings in 'YYYY-MM-DD' format.\n\n    Returns:\n    pd.DataFrame: A DataFrame with the 'Date' column formatted as 'DD-MMM-YYYY'.\n    \"\"\"\n    # Convert 'Date' column to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n    \n    # Format 'Date' column to 'DD-MMM-YYYY'\n    df['Date'] = df['Date'].dt.strftime('%d-%b-%Y')\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 24, "library_problem_id": 24, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 25, "code": ["<code>\nimport pandas as pd\n\ndef format_dates_within_range(df: pd.DataFrame, date_range: list) -> pd.DataFrame:\n    \"\"\"\n    Filters the DataFrame to include only dates within the specified range and formats them.\n\n    Args:\n    - df (pd.DataFrame): DataFrame containing a 'Date' column with date strings.\n    - date_range (list): List of two date strings [start_date, end_date].\n\n    Returns:\n    - pd.DataFrame: DataFrame with a single 'Date' column formatted as \"DD-MMM-YYYY Weekday\".\n    \"\"\"\n    # Convert 'Date' column to datetime\n    df['Date'] = pd.to_datetime(df['Date'])\n    \n    # Convert date_range to datetime\n    start_date, end_date = pd.to_datetime(date_range)\n    \n    # Filter dates within the specified range\n    mask = (df['Date'] >= start_date) & (df['Date'] <= end_date)\n    filtered_df = df.loc[mask]\n    \n    # Format dates as \"DD-MMM-YYYY Weekday\"\n    filtered_df['Date'] = filtered_df['Date'].dt.strftime('%d-%b-%Y %A')\n    \n    return filtered_df.reset_index(drop=True)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 25, "library_problem_id": 25, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 23}}
{"id": 26, "code": ["<code>\ndef cyclic_shift_first_column(df: pd.DataFrame) -> pd.DataFrame:\n    # Ensure the DataFrame has at least one column\n    if df.shape[1] < 1:\n        raise ValueError(\"DataFrame must have at least one column.\")\n    \n    # Perform the cyclic shift on the first column\n    first_col = df.iloc[:, 0]\n    shifted_first_col = first_col.shift(1)\n    shifted_first_col.iloc[0] = first_col.iloc[-1]\n    \n    # Assign the shifted column back to the DataFrame\n    df.iloc[:, 0] = shifted_first_col\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 26, "library_problem_id": 26, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 27, "code": ["<code>\ndf['#1'] = df['#1'].shift(-1).fillna(df['#1'].iloc[0])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 27, "library_problem_id": 27, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 28, "code": ["<code>\ndef shift_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    # Shift the first column down by one position\n    first_col = df.iloc[:, 0].copy()\n    first_col_shifted = first_col.shift(1)\n    first_col_shifted.iloc[0] = first_col.iloc[-1]\n\n    # Shift the second column up by one position\n    second_col = df.iloc[:, 1].copy()\n    second_col_shifted = second_col.shift(-1)\n    second_col_shifted.iloc[-1] = second_col.iloc[0]\n\n    # Combine the shifted columns into a new DataFrame\n    shifted_df = pd.DataFrame({\n        df.columns[0]: first_col_shifted,\n        df.columns[1]: second_col_shifted\n    }, index=df.index)\n\n    return shifted_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 28, "library_problem_id": 28, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 29, "code": ["<code>\ndef minimize_r2_shift(df: pd.DataFrame) -> pd.DataFrame:\n    # Initialize variables\n    min_r2 = float('inf')\n    best_df = df.copy()\n\n    # Iterate over possible shifts\n    for _ in range(len(df)):\n        # Calculate R^2 value between the two columns\n        current_r2 = r2_score(df['#2'], df['#1'])\n\n        # Update the best DataFrame if the current R^2 is lower\n        if current_r2 < min_r2:\n            min_r2 = current_r2\n            best_df = df.copy()\n\n        # Perform the cyclic shift on the first column\n        df['#1'] = df['#1'].shift(1)\n        df['#1'].iloc[0] = df['#1'].iloc[-1]\n\n    return best_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 29, "library_problem_id": 29, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 30, "code": ["<code>\nimport pandas as pd\n\ndef rename_columns_with_suffix(df: pd.DataFrame, suffix: str) -> pd.DataFrame:\n    \"\"\"\n    Renames all columns in the DataFrame by appending a specified suffix to each column name.\n\n    Args:\n        df (pd.DataFrame): The DataFrame whose columns need to be renamed.\n        suffix (str): The suffix to append to each column name.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with renamed columns.\n    \"\"\"\n    df.columns = [col + suffix for col in df.columns]\n    return df\n\n# Example usage:\ndf = pd.DataFrame(\n    {'HeaderA': [476],\n     'HeaderB': [4365],\n     'HeaderC': [457]}\n)\n\ndf = rename_columns_with_suffix(df, 'X')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 30, "library_problem_id": 30, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 31, "code": ["<code>\ndef rename_columns_with_prefix(df: pd.DataFrame, prefix: str) -> pd.DataFrame:\n    \"\"\"\n    Renames all columns in the given DataFrame by adding a specified prefix to each column name.\n\n    Args:\n    df (pd.DataFrame): The DataFrame whose columns need to be renamed.\n    prefix (str): The prefix to add to each column name.\n\n    Returns:\n    pd.DataFrame: A new DataFrame with renamed columns.\n    \"\"\"\n    return df.rename(columns=lambda x: prefix + x)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 31, "library_problem_id": 31, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 30}}
{"id": 32, "code": ["<code>\ndef rename_columns(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Renames the columns of the given DataFrame according to the specified rules.\n\n    Args:\n    df (pd.DataFrame): The input DataFrame with columns to be renamed.\n\n    Returns:\n    pd.DataFrame: A new DataFrame with renamed columns.\n    \"\"\"\n    return df.rename(columns=lambda x: 'X' + x if not x.endswith('X') else 'X' + x)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 32, "library_problem_id": 32, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 30}}
{"id": 33, "code": ["<code>\ndef compute_grouped_means(df: pd.DataFrame) -> pd.DataFrame:\n    # Identify columns that start with 'val'\n    value_columns = [col for col in df.columns if col.startswith('val')]\n    \n    # Define aggregation dictionary\n    agg_dict = {'group_color': 'first'}\n    agg_dict.update({col: 'mean' for col in value_columns})\n    \n    # Group by 'group' and aggregate\n    result = df.groupby('group').agg(agg_dict)\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 33, "library_problem_id": 33, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 34, "code": ["<code>\ndef dynamic_grouped_sum(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculate the grouped sum for all columns in the DataFrame that contain 'val' in their names.\n\n    Args:\n    df (pd.DataFrame): The input DataFrame with a 'group' column and multiple 'val' columns.\n\n    Returns:\n    pd.DataFrame: A DataFrame with the grouped sum of all 'val' columns and the first occurrence of 'group_color'.\n    \"\"\"\n    # Identify columns containing 'val' in their names\n    value_columns = [col for col in df.columns if 'val' in col]\n\n    # Group by 'group' and aggregate\n    result = df.groupby('group').agg({**{\"group_color\": \"first\"}, **{col: \"sum\" for col in value_columns}})\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 34, "library_problem_id": 34, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 33}}
{"id": 35, "code": ["<code>\nimport pandas as pd\n\ndef aggregate_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    # Define aggregation functions based on column names\n    agg_funcs = {col: ('mean' if col.endswith('2') else 'sum') for col in df.columns if col.startswith('val')}\n    # Include 'group_color' to retain the first occurrence\n    agg_funcs['group_color'] = 'first'\n    \n    # Perform the aggregation\n    result = df.groupby('group').agg(agg_funcs)\n    \n    return result\n\n# Example usage\ndf = pd.DataFrame({\n    'group': ['A', 'A', 'A', 'B', 'B'],\n    'group_color': ['green', 'green', 'green', 'blue', 'blue'],\n    'val1': [5, 2, 3, 4, 5],\n    'val2': [4, 2, 8, 5, 7],\n    'val32': [4, 2, 8, 5, 7]\n})\n\nresult = aggregate_dataframe(df)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 35, "library_problem_id": 35, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 33}}
{"id": 36, "code": ["<code>\nimport pandas as pd\n\ndef calculate_mean(df: pd.DataFrame, row_indices: list, column_names: list) -> pd.Series:\n    \"\"\"\n    Calculate the mean of specified rows and columns in a DataFrame.\n\n    Args:\n    df (pd.DataFrame): The DataFrame containing the data.\n    row_indices (list): A list of row indices to include in the mean calculation.\n    column_names (list): A list of column names for which the mean should be calculated.\n\n    Returns:\n    pd.Series: A Series containing the mean of the specified rows for each specified column.\n    \"\"\"\n    # Select the specified rows and columns\n    selected_data = df.loc[row_indices, column_names]\n    # Calculate the mean for each column\n    result = selected_data.mean()\n    return result\n\n# Example usage\ndf = pd.DataFrame({'a': [1, 1, 1, 1], 'b': [2, 2, 1, 0], 'c': [3, 3, 1, 0], 'd': [0, 4, 6, 0], 'q': [5, 5, 1, 0]})\nrow_list = [0, 2, 3]\ncolumn_list = ['a', 'b', 'd']\nresult = calculate_mean(df, row_list, column_list)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 36, "library_problem_id": 36, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 37, "code": ["<code>\ndef sum_selected_rows_and_columns(df: pd.DataFrame, row_indices: list, column_names: list) -> pd.Series:\n    \"\"\"\n    Calculate the sum of specified columns for specified rows in a DataFrame.\n\n    Args:\n        df (pd.DataFrame): The DataFrame containing the data.\n        row_indices (list): A list of row indices to include in the summation.\n        column_names (list): A list of column names to sum over.\n\n    Returns:\n        pd.Series: A Series containing the sum of the specified columns for the specified rows.\n    \"\"\"\n    return df.loc[row_indices, column_names].sum()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 37, "library_problem_id": 37, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 38, "code": ["<code>\nimport pandas as pd\n\ndef calculate_and_filter_sum(df: pd.DataFrame, row_list: list, column_list: list) -> pd.Series:\n    \"\"\"\n    Calculate the sum of specified rows for given columns in a DataFrame and remove the column with the largest sum.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame.\n        row_list (list): List of row indices to include in the sum.\n        column_list (list): List of column names to include in the sum.\n\n    Returns:\n        pd.Series: A Series containing the sum of specified rows for each column, excluding the column with the largest sum.\n    \"\"\"\n    # Calculate the sum of specified rows for each column\n    result = df.loc[row_list, column_list].sum(axis=0)\n    \n    # Identify the column with the largest sum\n    largest_column = result.idxmax()\n    \n    # Remove the column with the largest sum\n    result = result.drop(largest_column)\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 38, "library_problem_id": 38, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 39, "code": ["<code>\ndef get_value_counts(df: DataFrame) -> Series:\n    \"\"\"\n    Calculate the frequency of each unique value in every column of the DataFrame.\n\n    Args:\n    df (DataFrame): A Pandas DataFrame with multiple columns.\n\n    Returns:\n    Series: A Pandas Series with a multi-level index where the first level is the column name\n            and the second level is the unique value. The values of the Series are the counts\n            of each unique value in the respective column.\n    \"\"\"\n    # Your implementation here\n    return df.apply(lambda x: x.value_counts()).unstack()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 39, "library_problem_id": 39, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 40, "code": ["<code>\ndef count_null_values(df: pd.DataFrame) -> pd.Series:\n    # Calculate the count of 'null' values in each column\n    null_counts = df.apply(lambda col: (col == 'null').sum()).astype(float)\n    # Replace 0 counts with NaN for clarity\n    null_counts[null_counts == 0] = float('nan')\n    return null_counts\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 40, "library_problem_id": 40, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 41, "code": ["<code>\ndef generate_value_counts_report(df: pd.DataFrame) -> str:\n    \"\"\"\n    Generate a report of value counts for each column in the DataFrame.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame with multiple columns.\n\n    Returns:\n        str: A formatted string representing the value counts for each column.\n    \"\"\"\n    result = \"\"\n    for col in df.columns:\n        result += f\"---- {col} ---\\n{df[col].value_counts().to_string()}\\n\"\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 41, "library_problem_id": 41, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 39}}
{"id": 42, "code": ["<code>\nimport pandas as pd\n\ndef merge_header_rows(file_path: str, sheet_name: str) -> pd.DataFrame:\n    # Load the Excel file and parse the specified sheet\n    xl = pd.ExcelFile(file_path)\n    df = xl.parse(sheet_name)\n    \n    # Drop any unwanted columns, e.g., columns with 'Unnamed' in their name\n    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n    \n    # Merge the first two rows to form a new header\n    new_header = df.iloc[0].fillna('') + ' ' + df.iloc[1].fillna('')\n    new_header = new_header.str.strip().str.replace(' +', ' ', regex=True)\n    \n    # Assign the new header to the DataFrame\n    df.columns = new_header\n    \n    # Drop the first two rows as they are now part of the header\n    df = df.drop([0, 1]).reset_index(drop=True)\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 42, "library_problem_id": 42, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 43, "code": ["<code>\n```python\n\"\"\"\nFunction to merge the first two rows of a DataFrame to create a single header row.\n\nThis function reads an Excel file, processes the specified sheet, and merges the first two rows\nto form a new header. The resulting DataFrame will have the merged header and the remaining data.\n\nArgs:\n    file_path (str): The path to the Excel file.\n    sheet_name (str): The name of the sheet to parse.\n\nReturns:\n    pd.DataFrame: A DataFrame with the merged header and data.\n\nExample:\n    >>> df = merge_first_two_rows_as_header(\"nanonose.xls\", \"Sheet1\")\n    >>> print(df.head())\n\"\"\"\n\nimport pandas as pd\n\ndef merge_first_two_rows_as_header(file_path: str, sheet_name: str) -> pd.DataFrame:\n    # Load the Excel file and parse the specified sheet\n    xl = pd.ExcelFile(file_path)\n    df = xl.parse(sheet_name)\n    \n    # Drop any unnecessary columns, such as those with 'Unnamed' in their name\n    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n    \n    # Merge the first two rows to create a new header\n    new_header = df.iloc[0].combine_first(df.iloc[1])\n    \n    # Set the new header and drop the original first two rows\n    df.columns = new_header\n    df = df.drop([0, 1]).reset_index(drop=True)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 43, "library_problem_id": 43, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 44, "code": ["<code>\n```python\n\"\"\"\nFunction to reorder DataFrame rows by moving non-null values to the left.\n\nThis function takes a pandas DataFrame and reorders each row such that all non-null\nvalues are shifted to the left, while null values are moved to the right. The function\nis optimized for large DataFrames by avoiding the use of the apply method, which can be\nslow for large datasets.\n\nArgs:\n    df (pd.DataFrame): A pandas DataFrame with any number of rows and columns.\n\nReturns:\n    pd.DataFrame: A new DataFrame with the same shape as the input, where each row's\n    non-null values are shifted to the left.\n\nExample:\n    >>> df = pd.DataFrame([[0.0, 1.0, 2.0], [np.nan, 1.0, 2.0], [np.nan, np.nan, 2.0]])\n    >>> result = reorder_dataframe_rows(df)\n    >>> print(result)\n         0    1    2\n    0  0.0  1.0  2.0\n    1  1.0  2.0  NaN\n    2  2.0  NaN  NaN\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\n\ndef reorder_dataframe_rows(df: pd.DataFrame) -> pd.DataFrame:\n    # Convert DataFrame to a NumPy array for efficient processing\n    array = df.to_numpy()\n    \n    # Create a mask for non-null values\n    not_null_mask = ~np.isnan(array)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 44, "library_problem_id": 44, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 45, "code": ["<code>\ndef shift_non_nulls_to_right(df: pd.DataFrame) -> pd.DataFrame:\n    # Apply a function to each row that shifts non-null values to the right\n    return df.apply(lambda row: pd.Series(sorted(row, key=pd.isna)), axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 45, "library_problem_id": 45, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 44}}
{"id": 46, "code": ["<code>\ndef move_nans_to_top(df: pd.DataFrame) -> pd.DataFrame:\n    # Apply a function to each column that sorts values by NaN status\n    return df.apply(lambda col: pd.Series(\n        np.concatenate([col[col.isna()], col[col.notna()]])), axis=0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 46, "library_problem_id": 46, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 44}}
{"id": 47, "code": ["<code>\ndef aggregate_below_threshold(df: pd.DataFrame, threshold: int) -> pd.DataFrame:\n    # Identify rows with 'value' below the threshold\n    below_threshold = df['value'] < threshold\n    \n    # Calculate the sum of values below the threshold\n    sum_below_threshold = df.loc[below_threshold, 'value'].sum()\n    \n    # Create a new DataFrame excluding rows below the threshold\n    df_above_threshold = df.loc[~below_threshold]\n    \n    # Append the aggregated row with label 'X' and the calculated sum\n    df_result = df_above_threshold.append(pd.DataFrame({'value': [sum_below_threshold]}, index=['X']))\n    \n    return df_result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 47, "library_problem_id": 47, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 48, "code": ["<code>\ndef aggregate_dataframe(df: pd.DataFrame, threshold: int) -> pd.DataFrame:\n    \"\"\"\n    Aggregate rows in a DataFrame where the 'value' exceeds a specified threshold.\n    \n    Args:\n        df (pd.DataFrame): The input DataFrame with an index and a single column 'value'.\n        threshold (int): The threshold value for aggregation.\n        \n    Returns:\n        pd.DataFrame: A new DataFrame with aggregated rows replaced by their average.\n    \"\"\"\n    # Identify rows with 'value' greater than the threshold\n    above_threshold = df[df['value'] > threshold]\n    \n    # Calculate the average of these rows\n    average_value = above_threshold['value'].mean()\n    \n    # Create a new DataFrame for rows with 'value' less than or equal to the threshold\n    below_threshold = df[df['value'] <= threshold]\n    \n    # Append the average row with label 'X'\n    result = below_threshold.append(pd.DataFrame({'value': [average_value]}, index=['X']))\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 48, "library_problem_id": 48, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 47}}
{"id": 49, "code": ["<code>\ndef aggregate_outside_section(df: pd.DataFrame, section_left: int, section_right: int) -> pd.DataFrame:\n    # Identify rows where the 'value' is outside the range [section_left, section_right]\n    outside_section = df[(df['value'] < section_left) | (df['value'] > section_right)]\n    \n    # Calculate the average of these values\n    avg_value = outside_section['value'].mean()\n    \n    # Create a new row labeled 'X' with this average value\n    df = df[~((df['value'] < section_left) | (df['value'] > section_right))]\n    df = df.append(pd.Series(avg_value, index=['X']))\n    \n    # Ensure the returned DataFrame is sorted by the index\n    df = df.sort_index()\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 49, "library_problem_id": 49, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 47}}
{"id": 50, "code": ["<code>\ndef add_inverse_columns(df: pd.DataFrame) -> pd.DataFrame:\n    result_df = df.copy()\n    for column in df.columns:\n        inverse_values = 1 / df[column]\n        new_column_name = f\"inv_{column}\"\n        result_df[new_column_name] = inverse_values\n    return result_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 50, "library_problem_id": 50, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 51, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef add_exponential_columns(df: pd.DataFrame) -> pd.DataFrame:\n    result_df = df.copy()\n\n    for column in df.columns:\n        exp_values = np.exp(df[column])\n        result_df[f'exp_{column}'] = exp_values\n\n    return result_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 51, "library_problem_id": 51, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 52, "code": ["<code>\ndef add_inverse_columns(df: pd.DataFrame) -> pd.DataFrame:\n    result_df = df.copy()\n    for col in df.columns:\n        inverse_col = df[col].apply(lambda x: 1/x if x != 0 else 0)\n        result_df[f'inv_{col}'] = inverse_col\n    return result_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 52, "library_problem_id": 52, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 53, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef add_sigmoid_columns(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Adds sigmoid-transformed columns to the DataFrame.\n\n    Args:\n    - df (pd.DataFrame): The input DataFrame with numerical columns.\n\n    Returns:\n    - pd.DataFrame: The DataFrame with additional sigmoid-transformed columns.\n    \"\"\"\n    sigmoid_df = df.apply(lambda x: 1 / (1 + np.exp(-x)))\n    sigmoid_df.columns = [f\"sigmoid_{col}\" for col in df.columns]\n    return pd.concat([df, sigmoid_df], axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 53, "library_problem_id": 53, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 50}}
{"id": 54, "code": ["<code>\nmin_index = df.idxmin()\nmax_index = df.loc[:min_index].idxmax()\nresult = max_index\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 54, "library_problem_id": 54, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 55, "code": ["<code>\ndef find_first_max_after_min(df: pd.DataFrame) -> pd.Series:\n    def first_max_after_min(series: pd.Series) -> pd.Timestamp:\n        min_idx = series.idxmin()\n        max_after_min = series[min_idx:].idxmax()\n        return max_after_min\n\n    result = df.apply(first_max_after_min)\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 55, "library_problem_id": 55, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 56, "code": ["<code>\nimport pandas as pd\n\ndef expand_dates_with_defaults(df: pd.DataFrame) -> pd.DataFrame:\n    # Ensure 'dt' column is of datetime type\n    df['dt'] = pd.to_datetime(df['dt'])\n    \n    # Determine the minimum and maximum dates across all users\n    min_date = df['dt'].min()\n    max_date = df['dt'].max()\n    \n    # Create a complete date range from min_date to max_date\n    full_date_range = pd.date_range(start=min_date, end=max_date)\n    \n    # Create a DataFrame with all combinations of users and the full date range\n    users = df['user'].unique()\n    expanded_df = pd.DataFrame([(user, date) for user in users for date in full_date_range], columns=['user', 'dt'])\n    \n    # Merge the original DataFrame with the expanded DataFrame\n    merged_df = pd.merge(expanded_df, df, on=['user', 'dt'], how='left')\n    \n    # Fill missing 'val' values with 0\n    merged_df['val'].fillna(0, inplace=True)\n    \n    # Convert 'val' to integer type\n    merged_df['val'] = merged_df['val'].astype(int)\n    \n    return merged_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 56, "library_problem_id": 56, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 57, "code": ["<code>\nimport pandas as pd\n\ndef expand_date_range(df: pd.DataFrame) -> pd.DataFrame:\n    df['dt'] = pd.to_datetime(df['dt'])\n    min_date = df['dt'].min()\n    max_date = df['dt'].max()\n    all_dates = pd.date_range(start=min_date, end=max_date)\n\n    expanded_dfs = []\n    for user in df['user'].unique():\n        user_df = df[df['user'] == user].set_index('dt')\n        user_df = user_df.reindex(all_dates, fill_value=0).reset_index()\n        user_df['user'] = user\n        expanded_dfs.append(user_df)\n\n    result = pd.concat(expanded_dfs).reset_index(drop=True)\n    result.columns = ['dt', 'val', 'user']\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 57, "library_problem_id": 57, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 56}}
{"id": 58, "code": ["<code>\n```python\ndef expand_date_range_and_fill(df: pd.DataFrame, fill_value: int) -> pd.DataFrame:\n    # Ensure 'dt' column is of datetime type\n    df['dt'] = pd.to_datetime(df['dt'])\n    \n    # Create a list to store the expanded DataFrames for each user\n    expanded_dfs = []\n    \n    # Iterate over each unique user\n    for user in df['user'].unique():\n        # Filter the DataFrame for the current user\n        user_df = df[df['user'] == user]\n        \n        # Determine the minimum and maximum dates for the current user\n        min_date = user_df['dt'].min()\n        max_date = user_df['dt'].max()\n        \n        # Create a complete date range from min_date to max_date\n        full_date_range = pd.date_range(start=min_date, end=max_date)\n        \n        # Reindex the user DataFrame to include the full date range\n        user_df = user_df.set_index('dt').reindex(full_date_range).fillna({'val': fill_value})\n        \n        # Reset the index to turn the date range back into a column\n        user_df = user_df.reset_index().rename(columns={'index': 'dt'})\n        \n        # Add the 'user' column back\n        user_df['user'] = user\n        \n        # Append the expanded DataFrame for the current user to the list\n        expanded_dfs.append(user_df)\n    \n    # Con\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 58, "library_problem_id": 58, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 59, "code": ["<code>\n```python\ndef expand_dates_and_fill_values(df: pd.DataFrame) -> pd.DataFrame:\n    # Convert 'dt' column to datetime\n    df['dt'] = pd.to_datetime(df['dt'])\n    \n    # Initialize an empty DataFrame to store results\n    result = pd.DataFrame()\n    \n    # Iterate over each user group\n    for user, group in df.groupby('user'):\n        # Determine the date range for the current user\n        min_date, max_date = group['dt'].min(), group['dt'].max()\n        \n        # Create a complete date range\n        all_dates = pd.date_range(start=min_date, end=max_date)\n        \n        # Determine the maximum value for the current user\n        max_val = group['val'].max()\n        \n        # Create a DataFrame with all dates and fill with the maximum value\n        user_df = pd.DataFrame({'dt': all_dates, 'user': user, 'val': max_val})\n        \n        # Merge with the original group to fill in actual values\n        user_df = user_df.merge(group, on=['dt', 'user'], how='left', suffixes=('', '_orig'))\n        \n        # Fill missing 'val' values with the maximum value\n        user_df['val'] = user_df['val_orig'].combine_first(user_df['val'])\n        \n        # Drop the original 'val' column used for merging\n        user_df.drop(columns='val_orig', inplace=True)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 59, "library_problem_id": 59, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 60, "code": ["<code>\n```python\nimport pandas as pd\n\ndef transform_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Transforms the input DataFrame by expanding the date range for each user and filling\n    the 'val' column with the maximum value for each user.\n\n    Parameters:\n    df (pd.DataFrame): Input DataFrame with columns 'user', 'dt', and 'val'.\n\n    Returns:\n    pd.DataFrame: Transformed DataFrame with expanded date range and formatted dates.\n    \"\"\"\n    # Convert 'dt' column to datetime\n    df['dt'] = pd.to_datetime(df['dt'])\n\n    # Find the overall minimum and maximum dates\n    min_date = df['dt'].min()\n    max_date = df['dt'].max()\n\n    # Create a date range from min_date to max_date\n    all_dates = pd.date_range(start=min_date, end=max_date)\n\n    # Initialize a list to store the expanded DataFrame\n    expanded_rows = []\n\n    # Iterate over each unique user\n    for user in df['user'].unique():\n        # Filter the DataFrame for the current user\n        user_df = df[df['user'] == user]\n\n        # Find the maximum 'val' for the current user\n        max_val = user_df['val'].max()\n\n        # Create a DataFrame with all dates for the current user\n        user_dates_df = pd.DataFrame({'dt': all_dates, 'user': user, 'val': max_val})\n\n        # Update 'val' for\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 60, "library_problem_id": 60, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 56}}
{"id": 61, "code": ["<code>\ndef replace_names_with_ids(df: pd.DataFrame) -> pd.DataFrame:\n    # Create a mapping from unique names to unique integer IDs\n    name_to_id = {name: idx for idx, name in enumerate(df['name'].unique(), start=1)}\n    \n    # Replace the 'name' column with the corresponding integer IDs\n    df['name'] = df['name'].map(name_to_id)\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 61, "library_problem_id": 61, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 62, "code": ["<code>\ndef replace_with_unique_ids(df: pd.DataFrame) -> pd.DataFrame:\n    # Create a mapping from each unique 'name' to a unique ID\n    unique_id_map = {name: idx + 1 for idx, name in enumerate(df['name'].unique())}\n    \n    # Replace the values in column 'a' using the mapping\n    df['a'] = df['name'].map(unique_id_map)\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 62, "library_problem_id": 62, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 61}}
{"id": 63, "code": ["<code>\ndef replace_names_with_ids(df: pd.DataFrame) -> pd.DataFrame:\n    # Create a mapping from unique names to unique integer IDs\n    name_to_id = {name: idx for idx, name in enumerate(df['name'].unique(), start=1)}\n    \n    # Replace the 'name' column with the corresponding integer IDs\n    df['name'] = df['name'].map(name_to_id)\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 63, "library_problem_id": 63, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 64, "code": ["<code>\ndef transform_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    # Create a new column 'ID' by factorizing the combination of 'name' and 'a'\n    df['ID'] = pd.factorize(df[['name', 'a']].apply(tuple, axis=1))[0] + 1\n    \n    # Select and return the required columns\n    return df[['ID', 'b', 'c']]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 64, "library_problem_id": 64, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 61}}
{"id": 65, "code": ["<code>\n```python\n\"\"\"\nRefactor a table with date columns into a normalized format with separate date and value columns.\n\nGiven a DataFrame with columns representing dates and corresponding values for each user, \ntransform the DataFrame to have a single 'date' column and a 'value' column, while retaining \nother columns such as 'user' and 'someBool'.\n\nArgs:\n    df (pd.DataFrame): A pandas DataFrame with the following columns:\n        - 'user': Identifier for each user.\n        - Date columns (e.g., '01/12/15', '02/12/15'): Columns containing values for each date.\n        - 'someBool': A boolean column associated with each user.\n\nReturns:\n    pd.DataFrame: A transformed DataFrame with the following columns:\n        - 'user': Identifier for each user.\n        - 'date': Date corresponding to the value.\n        - 'value': Value associated with the date.\n        - 'someBool': The original boolean column.\n\nExample:\n    Input DataFrame:\n        user    01/12/15    02/12/15 someBool\n        u1      100         300      True\n        u2      200        -100      False\n        u3     -50          200      True\n\n    Output DataFrame:\n        user    date       value   someBool\n        u1      01/12/15   100     True\n        u1      02/12/15   300     True\n        u2      01/12/15   200\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 65, "library_problem_id": 65, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 66, "code": ["<code>\ndef transform_table(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Transform the input DataFrame by melting specified columns into 'others' and 'value' columns.\n\n    Args:\n    df (pd.DataFrame): Input DataFrame with columns ['user', '01/12/15', '02/12/15', 'someBool'].\n\n    Returns:\n    pd.DataFrame: Transformed DataFrame with columns ['user', '01/12/15', 'others', 'value'].\n    \"\"\"\n    # Melt the DataFrame to transform '02/12/15' and 'someBool' into 'others' and 'value'\n    transformed_df = df.melt(id_vars=['user', '01/12/15'], \n                             value_vars=['02/12/15', 'someBool'], \n                             var_name='others', \n                             value_name='value')\n    return transformed_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 66, "library_problem_id": 66, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 65}}
{"id": 67, "code": ["<code>\n```python\n\"\"\"\nRefactor the given table by transforming date columns into two separate columns: 'date' and 'value'.\n\nThe input table has the following structure:\n- Columns: 'user', date columns (e.g., '01/12/15', '02/12/15'), and 'someBool'.\n- Each date column contains numerical values or None.\n\nThe desired output table should have the following structure:\n- Columns: 'user', 'date', 'value', 'someBool'.\n- Each row represents a unique combination of 'user' and 'date', with corresponding 'value' and 'someBool'.\n\nSteps:\n1. Convert the wide-format table into a long-format table using pandas.\n2. Use the pandas `melt` function to achieve this transformation.\n3. Ensure that the 'date' column contains the original date column names.\n4. Retain the 'user' and 'someBool' columns as they are.\n\nArgs:\n    None\n\nReturns:\n    pd.DataFrame: A DataFrame with columns ['user', 'date', 'value', 'someBool'].\n\nPython Version:\n    This solution is compatible with Python 3.x and requires the pandas library.\n\nExample:\n    Input:\n        user    01/12/15    02/12/15 someBool\n        u1      100         None      True\n        u2      200        -100      False\n        u3     None          200      True\n\n    Output:\n        user    date       value   someBool\n        u1      01/12/15   10\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 67, "library_problem_id": 67, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 65}}
{"id": 68, "code": ["<code>\ndef select_subset(df: pd.DataFrame, filter_column: str, filter_value: float, selected_columns: List[str]) -> np.ndarray:\n    # Filter rows where the value in 'filter_column' is greater than 'filter_value'\n    filtered_df = df[df[filter_column] > filter_value]\n    \n    # Select the specified columns from the filtered DataFrame\n    selected_df = filtered_df[selected_columns]\n    \n    # Convert the resulting DataFrame to a numpy array\n    result_array = selected_df.to_numpy()\n    \n    return result_array\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 68, "library_problem_id": 68, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 69, "code": ["<code>\ndef select_and_convert(df: pd.DataFrame, filter_column: str, threshold: float, selected_columns: List[str]) -> np.ndarray:\n    # Filter rows where the value in filter_column is greater than the threshold\n    filtered_df = df[df[filter_column] > threshold]\n    \n    # Select the specified columns from the filtered DataFrame\n    selected_df = filtered_df[selected_columns]\n    \n    # Convert the selected DataFrame to a numpy array\n    result_array = selected_df.to_numpy()\n    \n    return result_array\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 69, "library_problem_id": 69, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 70, "code": ["<code>\n```python\ndef select_and_convert_to_numpy(df: pd.DataFrame, column_filter: str, threshold: float, selected_columns: list) -> np.ndarray:\n    \"\"\"\n    Selects a subset of rows and columns from a pandas DataFrame based on a condition and converts the result to a numpy array.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame from which to select data.\n        column_filter (str): The column name to apply the filter condition on.\n        threshold (float): The threshold value to filter rows. Only rows where the value in `column_filter` is greater than this threshold are selected.\n        selected_columns (list): A list of column names to include in the final selection.\n\n    Returns:\n        np.ndarray: A numpy array containing the selected rows and columns from the DataFrame.\n\n    Example:\n        >>> df = pd.DataFrame(np.random.rand(4, 5), columns=list('abcde'))\n        >>> result = select_and_convert_to_numpy(df, 'c', 0.5, ['b', 'e'])\n        >>> print(result)\n        # Output will be a numpy array with rows where column 'c' > 0.5 and only columns 'b' and 'e' included.\n    \"\"\"\n    # Filter the DataFrame to include only rows where the value in `column_filter` is greater than `threshold`\n    filtered_df = df[df[column_filter] > threshold]\n    \n    # Select the specified columns from the filtered DataFrame\n    selected_df = filtered_df[selected_columns\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 70, "library_problem_id": 70, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 71, "code": ["<code>\ndef filter_and_compute_sum(df: pd.DataFrame, columns: list = ['b', 'e']) -> pd.DataFrame:\n    # Filter rows where the value in column 'c' is greater than 0.5\n    filtered_df = df[df['c'] > 0.5]\n    \n    # Select the specified columns from the filtered DataFrame\n    selected_columns_df = filtered_df[columns]\n    \n    # Compute the sum of the selected columns for each row and add it as a new column 'sum'\n    selected_columns_df['sum'] = selected_columns_df.sum(axis=1)\n    \n    return selected_columns_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 71, "library_problem_id": 71, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 72, "code": ["<code>\ndef select_subset(df: pd.DataFrame, column_filter: str = 'c', threshold: float = 0.5, selected_columns: list = ['b', 'e']) -> pd.DataFrame:\n    \"\"\"\n    Select a subset of rows and columns from a pandas DataFrame based on specified conditions.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame from which to select rows and columns.\n        column_filter (str): The column name used to filter rows. Default is 'c'.\n        threshold (float): The threshold value for filtering rows. Only rows where the value in `column_filter` is greater than this threshold will be selected. Default is 0.5.\n        selected_columns (list): A list of column names to include in the output DataFrame. Default is ['b', 'e'].\n\n    Returns:\n        pd.DataFrame: A DataFrame containing only the rows where the value in `column_filter` is greater than `threshold`, and only the columns specified in `selected_columns`.\n\n    Example:\n        >>> df = pd.DataFrame(np.random.rand(4, 5), columns=list('abcde'))\n        >>> select_subset(df)\n    \"\"\"\n    # Filter rows where the value in `column_filter` is greater than `threshold`\n    filtered_rows = df[df[column_filter] > threshold]\n    \n    # Select only the columns specified in `selected_columns`\n    result = filtered_rows[selected_columns]\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 72, "library_problem_id": 72, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 73, "code": ["<code>\n```python\nimport pandas as pd\n\ndef remove_overlapping_rows(df: pd.DataFrame, X: int) -> pd.DataFrame:\n    \"\"\"\n    Remove overlapping rows from the DataFrame based on a specified number of days.\n\n    Args:\n    - df (pd.DataFrame): The input DataFrame with columns 'ID', 'date', and 'close'.\n    - X (int): The number of days to determine overlap between rows.\n\n    Returns:\n    - pd.DataFrame: A DataFrame with non-overlapping rows.\n    \"\"\"\n    # Convert 'date' column to datetime format\n    df['date'] = pd.to_datetime(df['date'], format='%m/%d/%y')\n    \n    # Sort DataFrame by date\n    df = df.sort_values(by='date').reset_index(drop=True)\n    \n    # Initialize a list to keep track of non-overlapping indices\n    non_overlapping_indices = []\n    \n    # Iterate over the DataFrame\n    for i in range(len(df)):\n        if not non_overlapping_indices or (df.loc[i, 'date'] - df.loc[non_overlapping_indices[-1], 'date']).days > X:\n            non_overlapping_indices.append(i)\n    \n    # Return the DataFrame with non-overlapping rows\n    return df.loc[non_overlapping_indices].reset_index(drop=True)\n\n# Example usage\ndf = pd.DataFrame({'ID\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 73, "library_problem_id": 73, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 74, "code": ["<code>\n```python\nimport pandas as pd\nfrom datetime import timedelta\n\ndef remove_overlapping_rows(df: pd.DataFrame, X: int) -> pd.DataFrame:\n    \"\"\"\n    Remove overlapping rows from a DataFrame based on a specified number of weeks.\n\n    Args:\n    df (pd.DataFrame): The input DataFrame with columns 'ID', 'date', and 'close'.\n    X (int): The number of weeks within which rows are considered overlapping.\n\n    Returns:\n    pd.DataFrame: A DataFrame with overlapping rows removed.\n    \"\"\"\n    # Convert 'date' column to datetime\n    df['date'] = pd.to_datetime(df['date'], format='%m/%d/%y')\n    \n    # Sort DataFrame by date\n    df = df.sort_values(by='date')\n    \n    # Initialize a list to store indices of non-overlapping rows\n    non_overlapping_indices = []\n    \n    # Iterate over rows to identify non-overlapping rows\n    last_date = None\n    for index, row in df.iterrows():\n        if last_date is None or (row['date'] - last_date).days > X * 7:\n            non_overlapping_indices.append(index)\n            last_date = row['date']\n    \n    # Return DataFrame with non-overlapping rows\n    return df.loc[non_overlapping_indices].reset_index(drop=True)\n\n# Example usage\ndf = pd.DataFrame({'ID': [1,\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 74, "library_problem_id": 74, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 73}}
{"id": 75, "code": ["<code>\nimport pandas as pd\nfrom datetime import timedelta\n\ndef filter_overlapping_rows(df: pd.DataFrame, X: int) -> pd.DataFrame:\n    # Convert 'date' column to datetime\n    df['date'] = pd.to_datetime(df['date'], format='%m/%d/%y')\n    \n    # Sort DataFrame by 'date'\n    df = df.sort_values(by='date')\n    \n    # Initialize list to hold non-overlapping rows\n    filtered_rows = []\n    last_date = None\n    \n    # Iterate through DataFrame\n    for index, row in df.iterrows():\n        if last_date is None or (row['date'] - last_date).days > X * 7:\n            filtered_rows.append(row)\n            last_date = row['date']\n    \n    # Create a new DataFrame from the filtered rows\n    filtered_df = pd.DataFrame(filtered_rows)\n    \n    # Format 'date' column as 'DD-MMM-YYYY'\n    filtered_df['date'] = filtered_df['date'].dt.strftime('%d-%b-%Y')\n    \n    return filtered_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 75, "library_problem_id": 75, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 73}}
{"id": 76, "code": ["<code>\ndef bin_and_average(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Transforms the input DataFrame by binning every 3 rows and calculating the average for each bin.\n\n    Args:\n    df (pd.DataFrame): A DataFrame with a single column of numerical data.\n\n    Returns:\n    pd.DataFrame: A DataFrame containing the average values of every 3-row bin.\n\n    Example:\n    >>> df = pd.DataFrame({'col1': [2, 1, 3, 1, 0]})\n    >>> bin_and_average(df)\n       col1\n    0   2.0\n    1   0.5\n    \"\"\"\n    df['bin'] = df.index // 3\n    result = df.groupby('bin').mean().reset_index(drop=True)\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 76, "library_problem_id": 76, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 77, "code": ["<code>\ndef bin_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Bins every 3 consecutive rows of the input DataFrame and calculates their mean.\n\n    Args:\n        df (pd.DataFrame): A DataFrame with a single column 'col1'.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the mean of every 3 consecutive rows.\n    \"\"\"\n    result = df['col1'].groupby(df.index // 3).mean().reset_index(drop=True)\n    return result.to_frame()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 77, "library_problem_id": 77, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 78, "code": ["<code>\nimport pandas as pd\n\ndef aggregate_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    result = df['col1'].iloc[::4].sum()\n    return pd.DataFrame({'col1': [result]})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 78, "library_problem_id": 78, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 79, "code": ["<code>\ndef bin_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Bins the DataFrame by averaging every 3 rows from the end to the start.\n\n    Args:\n        df (pd.DataFrame): A DataFrame with a single column 'col1' containing numerical values.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with a single column 'col1' containing the averaged values.\n    \"\"\"\n    # Reverse the DataFrame\n    df = df.iloc[::-1].reset_index(drop=True)\n    \n    # Group by every 3 rows and calculate the average\n    df = df.groupby(df.index // 3).mean().reset_index(drop=True)\n    \n    # Reverse the DataFrame back to its original order\n    df = df.iloc[::-1].reset_index(drop=True)\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 79, "library_problem_id": 79, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 76}}
{"id": 80, "code": ["<code>\ndef transform_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    result = []\n    i = 0\n    while i < len(df):\n        if i + 3 <= len(df):\n            # Sum every 3 rows\n            sum_value = df['col1'].iloc[i:i+3].sum()\n            result.append(sum_value)\n            i += 3\n        if i + 2 <= len(df):\n            # Average every 2 rows\n            avg_value = df['col1'].iloc[i:i+2].mean()\n            result.append(avg_value)\n            i += 2\n    return pd.DataFrame({'col1': result})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 80, "library_problem_id": 80, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 81, "code": ["<code>\nimport pandas as pd\n\ndef transform_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    result = []\n    n = len(df)\n    i = n - 1\n    \n    while i >= 0:\n        if i >= 2:\n            # Calculate sum of 3 rows\n            sum_value = df.iloc[i]['col1'] + df.iloc[i-1]['col1'] + df.iloc[i-2]['col1']\n            result.append(sum_value)\n            i -= 3\n        elif i >= 1:\n            # Calculate average of 2 rows\n            avg_value = (df.iloc[i]['col1'] + df.iloc[i-1]['col1']) / 2\n            result.append(avg_value)\n            i -= 2\n        else:\n            # If only one row is left, just append it\n            result.append(df.iloc[i]['col1'])\n            i -= 1\n\n    # Reverse the result to match the order from the start to the end\n    result.reverse()\n    \n    return pd.DataFrame({'col1': result})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 81, "library_problem_id": 81, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 76}}
{"id": 82, "code": ["<code>\nimport pandas as pd\n\n# Initialize the DataFrame\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns=['A'])\n\n# Fill zeros with the previous non-zero value\ndf['A'] = df['A'].replace(0, method='ffill')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 82, "library_problem_id": 82, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 83, "code": ["<code>\nimport pandas as pd\n\ndef fill_zeros_with_next_nonzero(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Fill zeros in the DataFrame with the next non-zero value in the column 'A'.\n\n    Args:\n    df (pd.DataFrame): A DataFrame with a single column 'A' containing integers.\n\n    Returns:\n    pd.DataFrame: A DataFrame where each zero in column 'A' is replaced by the next non-zero value.\n    \"\"\"\n    # Your implementation here\n    df['A'] = df['A'].replace(0, method='bfill')\n    return df\n\n# Example usage\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns=['A'])\ndf = fill_zeros_with_next_nonzero(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 83, "library_problem_id": 83, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 82}}
{"id": 84, "code": ["<code>\ndef fill_zeros_with_max(df: pd.DataFrame) -> pd.DataFrame:\n    df['B'] = df['A'].replace(0, method='ffill')\n    df['C'] = df['A'].replace(0, method='bfill')\n    df['A'] = df[['B', 'C']].min(axis=1)\n    df.drop(['B', 'C'], axis=1, inplace=True)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 84, "library_problem_id": 84, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 85, "code": ["<code>\ndef transform_duration_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    # Extract the numerical part from 'duration' and convert to integer\n    df['number'] = df['duration'].str.extract(r'(\\d+)').astype(int)\n    \n    # Extract the time unit part from 'duration'\n    df['time'] = df['duration'].str.extract(r'([a-zA-Z]+)')\n    \n    # Map time units to equivalent days\n    time_to_days = {'year': 365, 'month': 30, 'week': 7, 'day': 1}\n    df['time_days'] = df['time'].map(time_to_days)\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 85, "library_problem_id": 85, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 86, "code": ["<code>\ndef transform_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Transform the input DataFrame by separating numbers from time units and adding a column for time in days.\n\n    Args:\n    df (pd.DataFrame): Input DataFrame with a 'duration' column containing mixed time units and numbers.\n\n    Returns:\n    pd.DataFrame: Transformed DataFrame with additional columns 'time', 'number', and 'time_day'.\n    \"\"\"\n    # Extract the time unit using regex\n    df['time'] = df['duration'].str.extract(r'([a-zA-Z]+)')\n    \n    # Extract the number using regex\n    df['number'] = df['duration'].str.extract(r'(\\d+)').astype(int)\n    \n    # Map the time unit to the corresponding number of days\n    time_to_days = {'year': 365, 'month': 30, 'week': 7, 'day': 1}\n    df['time_day'] = df['time'].map(time_to_days)\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 86, "library_problem_id": 86, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 87, "code": ["<code>\nimport pandas as pd\n\ndef transform_duration_data(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Transforms a DataFrame by separating the numeric and time unit components\n    from the 'duration' column into new columns 'number' and 'time'. Additionally,\n    it creates a 'time_days' column that converts the time units into equivalent days.\n\n    Args:\n        df (pd.DataFrame): A DataFrame containing a 'duration' column with strings\n                           representing a number followed by a time unit (e.g., '7 year').\n\n    Returns:\n        pd.DataFrame: A DataFrame with the original index and new columns:\n                      - 'number': The numeric part extracted from 'duration'.\n                      - 'time': The time unit part extracted from 'duration'.\n                      - 'time_days': The equivalent number of days for each time unit.\n    \"\"\"\n    # Extract numeric part from 'duration' and convert to integer\n    df['number'] = df['duration'].str.extract(r'(\\d+)').astype(int)\n    \n    # Extract time unit part from 'duration'\n    df['time'] = df['duration'].str.extract(r'([a-zA-Z]+)')\n    \n    # Map time units to their equivalent in days\n    time_to_days = {'year': 365, 'month': 30, 'week': 7, 'day': 1}\n    df['time_days'] = df['time'].map(time_to_days)\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 87, "library_problem_id": 87, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 88, "code": ["<code>\n```python\n\"\"\"\nRefactor the given DataFrame by separating the numeric values from the time units in the 'duration' column.\nCreate two new columns: 'time' and 'number', where 'time' contains the time unit and 'number' contains the numeric value.\nAdditionally, create a 'time_day' column that converts the time units into days based on the following conversions:\n- year: 365 days\n- month: 30 days\n- week: 7 days\n- day: 1 day\n\nArgs:\n    df (pd.DataFrame): A DataFrame containing a single column 'duration' with mixed time units and numbers.\n\nReturns:\n    pd.DataFrame: A DataFrame with three new columns: 'time', 'number', and 'time_day'.\n\"\"\"\n\nimport pandas as pd\n\n# Initialize the DataFrame\ndf = pd.DataFrame({'duration': ['year 7', 'day2', 'week 4', 'month 8']}, index=list(range(1, 5)))\n\n# Extract the time unit and number into separate columns\ndf['time'] = df['duration'].str.extract(r'([a-zA-Z]+)')\ndf['number'] = df['duration'].str.extract(r'(\\d+)').astype(int)\n\n# Define a mapping from time units to days\ntime_to_days = {\n    'year': 365,\n    'month': 30,\n    'week': 7,\n    'day': 1\n}\n\n# Calculate the 'time_day' column by converting time units to days and multip\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 88, "library_problem_id": 88, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 89, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndf1 = pd.DataFrame({'A': [1, 1, 1],\n                    'B': [2, 2, 2],\n                    'C': [3, 3, 3],\n                    'D': [4, 4, 4],\n                    'E': [5, 5, 5],\n                    'F': [6, 6, 6],\n                    'Postset': ['yes', 'no', 'yes']})\n\ndf2 = pd.DataFrame({'A': [1, 1, 1],\n                    'B': [2, 2, 2],\n                    'C': [3, 3, 3],\n                    'D': [4, 4, 4],\n                    'E': [5, 5, 5],\n                    'F': [6, 4, 6],\n                    'Preset': ['yes', 'yes', 'yes']})\n\ncolumns_check_list = ['A', 'B', 'C', 'D', 'E', 'F']\n\n# Solution\nresult = np.all([df1[col] == df2[col] for col in columns_check_list], axis=0)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 89, "library_problem_id": 89, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 90, "code": ["<code>\ndef compare_dataframes(df1: pd.DataFrame, df2: pd.DataFrame, columns_check_list: List[str]) -> List[bool]:\n    \"\"\"\n    Compare two DataFrames row-wise for equality across specified columns.\n\n    Args:\n        df1 (pd.DataFrame): The first DataFrame to compare.\n        df2 (pd.DataFrame): The second DataFrame to compare.\n        columns_check_list (List[str]): A list of column names to check for equality.\n\n    Returns:\n        List[bool]: A list of boolean values indicating row-wise equality across specified columns.\n    \"\"\"\n    # Create a boolean DataFrame indicating equality for each specified column\n    equality_df = df1[columns_check_list] == df2[columns_check_list]\n    \n    # Determine row-wise equality across all specified columns\n    result = equality_df.all(axis=1).tolist()\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 90, "library_problem_id": 90, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 89}}
{"id": 91, "code": ["<code>\nimport pandas as pd\n\n# Create the MultiIndex from tuples\nindex = pd.MultiIndex.from_tuples([('abc', '3/1/1994'), ('abc', '9/1/1994'), ('abc', '3/1/1995')],\n                                  names=('id', 'date'))\n\n# Construct the DataFrame\ndf = pd.DataFrame({'x': [100, 90, 80], 'y': [7, 8, 9]}, index=index)\n\n# Solution to convert date strings to datetime objects\ndf.index = df.index.set_levels([df.index.levels[0], pd.to_datetime(df.index.levels[1])], level='date')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 91, "library_problem_id": 91, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 92, "code": ["<code>\n```python\nimport pandas as pd\n\ndef convert_multiindex_dates(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Convert the date strings in the second level of a multi-index DataFrame to datetime objects.\n\n    Args:\n    df (pd.DataFrame): A DataFrame with a multi-index where the second level contains date strings.\n\n    Returns:\n    pd.DataFrame: A DataFrame with the second level of the multi-index converted to datetime objects.\n    \"\"\"\n    # Extract the current multi-index levels\n    index_names = df.index.names\n    index_values = df.index.to_frame(index=False)\n    \n    # Convert the second level (date strings) to datetime objects\n    index_values[index_names[1]] = pd.to_datetime(index_values[index_names[1]])\n    \n    # Reconstruct the multi-index with the converted datetime objects\n    new_index = pd.MultiIndex.from_frame(index_values)\n    \n    # Assign the new multi-index back to the DataFrame\n    df.index = new_index\n    \n    return df\n\n# Example usage\nindex = pd.MultiIndex.from_tuples([('abc', '3/1/1994'), ('abc', '9/1/1994'), ('abc', '3/1/1995')],\n                                  names=('name', 'datetime'))\ndf = pd.DataFrame({'fee': [100, 90, 80], 'credits': [7, 8,\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 92, "library_problem_id": 92, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 93, "code": ["<code>\ndef transform_dataframe_to_array(df: pd.DataFrame) -> np.ndarray:\n    # Extract the date index and parse it into Timestamps\n    parsed_dates = pd.to_datetime(df.index.get_level_values('date'))\n    \n    # Combine the parsed dates with the 'x' and 'y' columns into a NumPy array\n    result_array = np.column_stack((parsed_dates, df['x'].values, df['y'].values))\n    \n    return result_array\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 93, "library_problem_id": 93, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 94, "code": ["<code>\ndef transform_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    # Parse the 'date' index level to datetime\n    df.index = df.index.set_levels([pd.to_datetime(df.index.levels[0]), df.index.levels[1]])\n    \n    # Swap the index levels\n    df = df.swaplevel('date', 'id')\n    \n    # Sort the index to ensure proper order\n    df = df.sort_index()\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 94, "library_problem_id": 94, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 91}}
{"id": 95, "code": ["<code>\ndef reshape_wide_to_long(df: pd.DataFrame) -> pd.DataFrame:\n    # Pivot the DataFrame to long format\n    df_long = pd.melt(df, id_vars=['Country', 'Variable'], var_name='year', value_name='value')\n\n    # Pivot the DataFrame to have separate columns for each variable\n    df_long = df_long.pivot_table(index=['Country', 'year'], columns='Variable', values='value').reset_index()\n\n    return df_long\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 95, "library_problem_id": 95, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 96, "code": ["<code>\nimport pandas as pd\n\ndef reshape_data(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Reshape a DataFrame from wide to long format.\n\n    Args:\n        df (pd.DataFrame): A DataFrame in wide format with columns for each year and rows for each country and variable.\n\n    Returns:\n        pd.DataFrame: A DataFrame in long format with columns 'Country', 'year', 'var1', 'var2', etc., and rows for each year in descending order.\n    \"\"\"\n    # Melt the DataFrame to convert it from wide to long format\n    df_melt = df.melt(id_vars=['Country', 'Variable'], var_name='year')\n\n    # Convert the 'year' column to integer type\n    df_melt['year'] = df_melt['year'].astype(int)\n\n    # Pivot the DataFrame to convert it from long to wide format\n    df_pivot = df_melt.pivot_table(index=['Country', 'year'], columns='Variable', values='value')\n\n    # Sort the DataFrame by 'year' in descending order\n    df_pivot = df_pivot.sort_values(by='year', ascending=False)\n\n    return df_pivot.reset_index()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 96, "library_problem_id": 96, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 97, "code": ["<code>\ndef filter_rows_by_absolute_value(df: pd.DataFrame) -> pd.DataFrame:\n    # Identify columns prefixed with 'Value'\n    value_columns = [col for col in df.columns if col.startswith('Value')]\n    \n    # Filter rows where the absolute value of all 'Value' columns is less than 1\n    filtered_df = df[df[value_columns].abs().lt(1).all(axis=1)]\n    \n    return filtered_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 97, "library_problem_id": 97, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 98, "code": ["<code>\n\"\"\"\nFilter rows in a DataFrame based on the absolute values of specific columns.\n\nThis function processes a DataFrame to filter and return rows where the absolute value of any column \nwith a name prefixed by 'Value' exceeds a threshold of 1. The DataFrame can have an arbitrary number \nof such columns, and the function dynamically identifies and processes them.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing various columns, including those prefixed with 'Value'.\n\nReturns:\n    pd.DataFrame: A filtered DataFrame containing only the rows where the absolute value of any 'Value' \n    prefixed column is greater than 1.\n\"\"\"\n\nimport pandas as pd\n\n# Identify columns with names prefixed by 'Value'\nvalue_columns = [col for col in df.columns if col.startswith('Value')]\n\n# Filter rows where the absolute value of any 'Value' column is greater than 1\ndf_filtered = df[df[value_columns].abs().gt(1).any(axis=1)]\n\nreturn df_filtered\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 98, "library_problem_id": 98, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 97}}
{"id": 99, "code": ["<code>\n\"\"\"\nFilter a DataFrame to retain rows where the absolute value of any column \nprefixed with 'Value_' exceeds 1, and rename these columns by removing the 'Value_' prefix.\n\nArgs:\n    df (pd.DataFrame): A DataFrame containing columns with names prefixed by 'Value_' \n                       and other columns.\n\nReturns:\n    pd.DataFrame: A filtered DataFrame with rows meeting the condition and columns \n                  renamed by removing the 'Value_' prefix.\n\"\"\"\n\nimport pandas as pd\n\n# Identify columns with 'Value_' prefix\nvalue_columns = [col for col in df.columns if col.startswith('Value_')]\n\n# Filter rows where the absolute value of any 'Value_' column exceeds 1\nfiltered_df = df[df[value_columns].abs().gt(1).any(axis=1)]\n\n# Rename columns by removing the 'Value_' prefix\nfiltered_df = filtered_df.rename(columns=lambda x: x.replace('Value_', '') if x.startswith('Value_') else x)\n\n# Assign the solution to the variable 'df'\ndf = filtered_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 99, "library_problem_id": 99, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 100, "code": ["<code>\ndef replace_ampersand(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Replace all occurrences of '&AMP;' with '&' in all columns of a pandas DataFrame.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing string values with '&AMP;'.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with '&AMP;' replaced by '&' in all string columns.\n    \"\"\"\n    return df.applymap(lambda x: x.replace('&AMP;', '&') if isinstance(x, str) else x)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 100, "library_problem_id": 100, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 101, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'A': ['Good &LT bad', 'BB', 'CC', 'DD', 'Good &LT; bad'], 'B': range(5), 'C': ['Good &LT; bad'] * 5})\n\ndef replace_lt_in_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Replace all occurrences of the substring '&LT;' with the character '<' in a pandas DataFrame.\n\n    Args:\n    df (pd.DataFrame): The input DataFrame containing string data where '&LT;' needs to be replaced.\n\n    Returns:\n    pd.DataFrame: A new DataFrame with '&LT;' replaced by '<' in all string entries.\n    \"\"\"\n    return df.applymap(lambda x: x.replace('&LT;', '<') if isinstance(x, str) else x)\n\ndf = replace_lt_in_dataframe(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 101, "library_problem_id": 101, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 102, "code": ["<code>\n\"\"\"\nFunction to replace occurrences of '&AMP;' with '&' in all columns of a pandas DataFrame.\n\nThis function iterates over each column of the DataFrame and applies a string replacement operation\nto substitute '&AMP;' with '&' wherever it appears in any string within the DataFrame.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing string data where '&AMP;' needs to be replaced.\n\nReturns:\n    pd.DataFrame: A new DataFrame with '&AMP;' replaced by '&' in all string entries.\n\nExample:\n    Given a DataFrame with a column 'Title' containing the value 'Good &AMP; bad',\n    the function will return a DataFrame where this value is replaced with 'Good & bad'.\n\"\"\"\n\nimport pandas as pd\n\nexample_df = pd.DataFrame({\n    'A': ['Good &AMP; bad', 'BB', 'CC', 'DD', 'Good &AMP; bad'],\n    'B': range(5),\n    'C': ['Good &AMP; bad'] * 5\n})\n\ndef replace_ampersand(df: pd.DataFrame) -> pd.DataFrame:\n    # Apply string replacement to each element in the DataFrame\n    return df.applymap(lambda x: x.replace('&AMP;', '&') if isinstance(x, str) else x)\n\n# Example usage:\n# result_df = replace_ampersand(example_df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 102, "library_problem_id": 102, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 100}}
{"id": 103, "code": ["<code>\ndef replace_html_entities(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Replaces HTML-like entities '&AMP;', '&LT;', and '&GT;' with their corresponding characters '&', '<', and '>'\n    in all string columns of the given DataFrame.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing string columns with HTML-like entities.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with the specified replacements made in all string columns.\n    \"\"\"\n    # Define the replacement mapping\n    replacements = {\n        '&AMP;': '&',\n        '&LT;': '<',\n        '&GT;': '>'\n    }\n    \n    # Apply replacements to all string columns\n    return df.applymap(lambda x: x.replace('&AMP;', '&').replace('&LT;', '<').replace('&GT;', '>') if isinstance(x, str) else x)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 103, "library_problem_id": 103, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 104, "code": ["<code>\ndef replace_and_evaluate(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Replaces all occurrences of '&AMP;' with '&' in all columns of the DataFrame.\n    Evaluates logical expressions formed by '&' and appends the result to the string.\n\n    Args:\n    df (pd.DataFrame): The input DataFrame containing strings with '&AMP;'.\n\n    Returns:\n    pd.DataFrame: A new DataFrame with '&AMP;' replaced by '&' and logical expressions evaluated.\n    \"\"\"\n    def evaluate_expression(expr: str) -> str:\n        \"\"\"\n        Evaluates a logical expression containing '&' and returns the expression with its result.\n\n        Args:\n        expr (str): The expression to evaluate.\n\n        Returns:\n        str: The expression with the evaluated result appended.\n        \"\"\"\n        try:\n            # Split the expression by '&' and evaluate if possible\n            left, right = map(int, expr.split(' & '))\n            result = left & right\n            return f\"{expr} = {result}\"\n        except (ValueError, TypeError):\n            # Return the original expression if it cannot be evaluated\n            return expr\n\n    # Replace '&AMP;' with '&' and evaluate expressions in all columns\n    return df.applymap(lambda x: evaluate_expression(x.replace('&AMP;', '&')) if isinstance(x, str) else x)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 104, "library_problem_id": 104, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 105, "code": ["<code>\ndef split_names(df: pd.DataFrame) -> pd.DataFrame:\n    def split_name(name: str) -> tuple:\n        # Split the name by spaces\n        parts = name.split(' ')\n        # Check if there is exactly one space\n        if len(parts) == 2:\n            return parts[0], parts[1]\n        else:\n            return name, None\n\n    # Apply the split_name function to each name in the DataFrame\n    df[['first_name', 'last_name']] = df['name'].apply(split_name).apply(pd.Series)\n    return df[['first_name', 'last_name']]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 105, "library_problem_id": 105, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 106, "code": ["<code>\ndef split_names(df: pd.DataFrame) -> pd.DataFrame:\n    def split_name(name: str) -> tuple:\n        # Split the name by spaces\n        parts = name.split(' ')\n        # Check the number of parts\n        if len(parts) == 2:\n            return parts[0], parts[1]\n        else:\n            return name, ''\n\n    # Apply the split_name function to the 'name' column\n    df[['1_name', '2_name']] = df['name'].apply(split_name).apply(pd.Series)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 106, "library_problem_id": 106, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 105}}
{"id": 107, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef split_name(name: str) -> tuple:\n    parts = name.split()\n    if len(parts) > 2:\n        return parts[0], ' '.join(parts[1:-1]), parts[-1]\n    elif len(parts) == 2:\n        return parts[0], np.nan, parts[1]\n    else:\n        return name, np.nan, np.nan\n\n# Create the DataFrame\nname_df = pd.DataFrame({'name': ['Jack Fine', 'Kim Q. Danger', 'Jane 114 514 Smith', 'Zhongli']})\n\n# Apply the split_name function to the 'name' column\nname_df[['first_name', 'middle_name', 'last_name']] = name_df['name'].apply(split_name).apply(pd.Series)\n\nname_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 107, "library_problem_id": 107, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 105}}
{"id": 108, "code": ["<code>\ndef join_dataframes_on_nearest_previous_timestamp(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:\n    # Ensure the 'Timestamp' columns are of datetime type\n    df1['Timestamp'] = pd.to_datetime(df1['Timestamp'])\n    df2['Timestamp'] = pd.to_datetime(df2['Timestamp'])\n\n    # Merge df1 and df2 using a backward fill method to find the nearest previous timestamp\n    merged_df = pd.merge_asof(df2.sort_values('Timestamp'), \n                              df1.sort_values('Timestamp'), \n                              on='Timestamp', \n                              direction='backward')\n\n    return merged_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 108, "library_problem_id": 108, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 109, "code": ["<code>\ndef merge_dataframes_on_nearest_timestamp(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:\n    df1['Timestamp'] = pd.to_datetime(df1['Timestamp'])\n    df2['Timestamp'] = pd.to_datetime(df2['Timestamp'])\n    \n    # Merge df1 and df2 on the nearest preceding or equal timestamp\n    result = pd.merge_asof(df1.sort_values('Timestamp'), \n                           df2.sort_values('Timestamp'), \n                           on='Timestamp', \n                           direction='backward')\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 109, "library_problem_id": 109, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 108}}
{"id": 110, "code": ["<code>\ndef compute_state_column(df: pd.DataFrame) -> pd.DataFrame:\n    df['state'] = df.apply(\n        lambda row: row['col1'] if row['col2'] <= 50 and row['col3'] <= 50 else max(row['col1'], row['col2'], row['col3']),\n        axis=1\n    )\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 110, "library_problem_id": 110, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 111, "code": ["<code>\ndef calculate_state(df: pd.DataFrame) -> pd.DataFrame:\n    df['state'] = df.apply(lambda row: row['col1'] if row['col2'] > 50 and row['col3'] > 50 else row['col1'] + row['col2'] + row['col3'], axis=1)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 111, "library_problem_id": 111, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 110}}
{"id": 112, "code": ["<code>\ndef find_non_integers(df: pd.DataFrame, column_name: str) -> List[Any]:\n    error_values = []\n    for value in df[column_name]:\n        # Check if the value is not an integer\n        if not isinstance(value, int):\n            error_values.append(value)\n    return error_values\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 112, "library_problem_id": 112, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 113, "code": ["<code>\ndef extract_integers(df: pd.DataFrame, column_name: str) -> List[int]:\n    integer_values = []\n    for value in df[column_name]:\n        # Check if the value is an integer\n        if isinstance(value, int):\n            integer_values.append(value)\n    return integer_values\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 113, "library_problem_id": 113, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 112}}
{"id": 114, "code": ["<code>\ndef find_non_integer_values(df: pd.DataFrame) -> list:\n    non_integer_values = []\n    \n    for value in df['Field1']:\n        # Check if the value is not an integer\n        if not isinstance(value, int):\n            non_integer_values.append(value)\n    \n    return non_integer_values\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 114, "library_problem_id": 114, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 112}}
{"id": 115, "code": ["<code>\n```python\nimport pandas as pd\n\ndef calculate_percentage(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculate the percentage contribution of each value to its respective category total.\n\n    Args:\n    df (pd.DataFrame): A DataFrame with a 'cat' column and several value columns (e.g., 'val1', 'val2', etc.).\n\n    Returns:\n    pd.DataFrame: A DataFrame with the same structure as the input, where each value is replaced by its percentage\n                  contribution to the sum of its respective category row.\n\n    Example:\n    Input:\n    | cat | val1 | val2 | val3 | val4 |\n    |-----|------|------|------|------|\n    | A   | 7    | 10   | 0    | 19   |\n    | B   | 10   | 2    | 1    | 14   |\n    | C   | 5    | 15   | 6    | 16   |\n\n    Output:\n    | cat | val1  | val2  | val3  | val4  |\n    |-----|-------|-------|-------|-------|\n    | A   | 0.194 | 0.278 | 0.0   | 0.528 |\n    | B   | 0.370 | 0.074 | 0.037 | 0.519 |\n    | C   | 0.119 | 0.357 | 0.143 | 0.381 |\n    \"\"\"\n    # Calculate the sum of each row excluding the 'cat' column\n    row_sums = df.iloc[:, 1\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 115, "library_problem_id": 115, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 116, "code": ["<code>\nimport pandas as pd\n\ndef compute_percentage_contributions(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculate the percentage contribution of each category to the total of each numerical column.\n\n    Args:\n        df (pd.DataFrame): A DataFrame containing a 'cat' column and several numerical columns.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the same structure, where numerical values are replaced by their percentage contributions.\n    \"\"\"\n    # Calculate the sum of each numerical column\n    column_totals = df.iloc[:, 1:].sum()\n\n    # Divide each value by the corresponding column total to get the percentage\n    percentage_df = df.copy()\n    percentage_df.iloc[:, 1:] = df.iloc[:, 1:].div(column_totals)\n\n    return percentage_df\n\n# Example usage\ndf = pd.DataFrame({'cat': ['A', 'B', 'C'],\n                   'val1': [7, 10, 5],\n                   'val2': [10, 2, 15],\n                   'val3': [0, 1, 6],\n                   'val4': [19, 14, 16]})\n\nresult_df = compute_percentage_contributions(df)\nprint(result_df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 116, "library_problem_id": 116, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 115}}
{"id": 117, "code": ["<code>\ndef extract_rows(df: pd.DataFrame, row_names: List[str]) -> pd.DataFrame:\n    return df.loc[row_names]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 117, "library_problem_id": 117, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 118, "code": ["<code>\n```python\nimport pandas as pd\nimport io\n\ndef extract_rows(data: str, row_indices: list) -> pd.DataFrame:\n    \"\"\"\n    Extract specific rows from a DataFrame using a list of row indices.\n\n    Args:\n        data (str): A string representation of the DataFrame with whitespace as delimiter.\n        row_indices (list): A list of row indices to extract from the DataFrame.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing only the specified rows.\n\n    Example:\n        data = \\\"\\\"\\\"\n        rs    alias  chrome  poston\n        TP3      A/C      0    3\n        TP7      A/T      0    7\n        TP12     T/A      0   12\n        TP15     C/A      0   15\n        TP18     C/T      0   18\n        \\\"\\\"\\\"\n        row_indices = ['TP3', 'TP18']\n        result = extract_rows(data, row_indices)\n    \"\"\"\n    df = pd.read_csv(io.StringIO(data), delim_whitespace=True).set_index('rs')\n    return df.loc[row_indices]\n\n# Example usage\ndata = \"\"\"\nrs    alias  chrome  poston\nTP3      A/C      0    3\nTP7      A/T      0    7\nTP12     T/A      0   12\nTP15     C/A      0   15\nTP18     C/T      0   1\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 118, "library_problem_id": 118, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 117}}
{"id": 119, "code": ["<code>\ndef delete_rows(df: pd.DataFrame, rows_to_delete: List[str]) -> pd.DataFrame:\n    # Ensure the DataFrame index is set to the row names\n    if not df.index.is_unique:\n        raise ValueError(\"DataFrame index must be unique.\")\n    \n    # Create a new DataFrame excluding the specified rows\n    result_df = df.drop(index=rows_to_delete, errors='ignore')\n    \n    return result_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 119, "library_problem_id": 119, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 117}}
{"id": 120, "code": ["<code>\n```python\nimport pandas as pd\nfrom typing import List\n\ndef extract_unique_rows(df: pd.DataFrame, row_names: List[str]) -> pd.DataFrame:\n    \"\"\"\n    Extracts unique rows from a Pandas DataFrame based on a list of row names.\n    \n    This function retrieves rows from the DataFrame `df` using the list `row_names`.\n    It ensures that each row is included only once in the result, even if the row\n    name appears multiple times in `row_names`. The order of rows in the result\n    corresponds to the first occurrence of each row name in `row_names`.\n\n    Args:\n        df (pd.DataFrame): The DataFrame from which rows are to be extracted. The DataFrame\n                           is expected to have its index set to the row names.\n        row_names (List[str]): A list of row names to extract from the DataFrame. The list\n                               may contain duplicates, but only the first occurrence of each\n                               row name will be included in the result.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the unique rows specified in `row_names`, \n                      ordered according to their first appearance in `row_names`.\n\n    Example:\n        # Given DataFrame `df`:\n        #     alleles  chrom  pos strand  assembly#  center  protLSID  assayLSID  \n        # rs#\n        # TP3      A/C      0    3      +        NaN     NaN       NaN        NaN\n        # TP7      A/T      0    7      +\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 120, "library_problem_id": 120, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 117}}
{"id": 121, "code": ["<code>\ndef calculate_nearest_neighbors(df: pd.DataFrame) -> pd.DataFrame:\n    results = []\n\n    # Group by each time point\n    for time, group in df.groupby('time'):\n        # Extract coordinates\n        coords = group[['x', 'y']].values\n        cars = group['car'].values\n\n        # Calculate pairwise distances\n        distances = cdist(coords, coords, metric='euclidean')\n\n        # Find nearest neighbor for each car\n        for i, car in enumerate(cars):\n            # Exclude the distance to itself by setting it to infinity\n            distances[i, i] = np.inf\n            nearest_idx = np.argmin(distances[i])\n            nearest_car = cars[nearest_idx]\n            nearest_distance = distances[i, nearest_idx]\n\n            results.append({\n                'time': time,\n                'car': car,\n                'nearest_neighbour': nearest_car,\n                'euclidean_distance': nearest_distance\n            })\n\n    # Convert results to DataFrame\n    result_df = pd.DataFrame(results)\n    return result_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 121, "library_problem_id": 121, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 122, "code": ["<code>\n```python\nimport numpy as np\n\n# Calculate pairwise Euclidean distances between cars for each time point\ndf['coords'] = list(zip(df.x, df.y))\ndf = df.groupby('time').apply(lambda group: pd.DataFrame(\n    {\n        'car1': group['car'],\n        'car2': group['car'],\n        'distance': group['coords'].apply(lambda coord1: np.sqrt(\n            group['coords'].apply(lambda coord2: (coord2[0]-coord1[0])**2 + (coord2[1]-coord1[1])**2\n        ).min())\n    }\n)).reset_index(drop=True)\n\n# Identify the farthest neighbor for each car\ndf = df.groupby('car1').apply(lambda group: group[group.distance == group.distance.max()].iloc[0]).reset_index(drop=True)\n\n# Rename columns\ndf.rename(columns={'car1': 'car', 'car2': 'farthest_neighbor'}, inplace=True)\n\n# Compute the average of these distances for each time point\ndf = df.groupby('time').distance.mean().reset_index()\ndf.rename(columns={'distance': 'average_distance'}, inplace=True)\n\n# Join with original dataframe\ndf = df.merge(df[['time', 'car', 'farthest_neighbor', 'average_distance']], on='time')\n\n# Return only the columns we need\ndf = df[['time', 'car', 'farthest_\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 122, "library_problem_id": 122, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 123, "code": ["<code>\ndef concatenate_non_nan_values(df: pd.DataFrame) -> pd.DataFrame:\n    # List of columns to concatenate\n    columns_to_concatenate = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\n    \n    # Apply a lambda function to concatenate non-NaN values across specified columns\n    df['keywords_all'] = df[columns_to_concatenate].apply(\n        lambda row: ','.join(row.dropna()), axis=1\n    )\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 123, "library_problem_id": 123, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 124, "code": ["<code>\ndef concatenate_non_nan_values(df: pd.DataFrame) -> pd.DataFrame:\n    # List of columns to concatenate\n    columns_to_concatenate = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\n    \n    # Apply a lambda function to concatenate non-NaN values for each row\n    df['keywords_all'] = df[columns_to_concatenate].apply(\n        lambda row: '-'.join(row.dropna()), axis=1\n    )\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 124, "library_problem_id": 124, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 125, "code": ["<code>\ndef concatenate_keywords(df: pd.DataFrame) -> pd.DataFrame:\n    # Identify all columns that start with 'keywords_' to be concatenated\n    keyword_columns = [col for col in df.columns if col.startswith('keywords_')]\n    \n    # Create 'keywords_all' by concatenating non-NaN values row-wise\n    df['keywords_all'] = df[keyword_columns].apply(\n        lambda row: '-'.join(row.dropna().astype(str)), axis=1\n    )\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 125, "library_problem_id": 125, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 126, "code": ["<code>\ndef concatenate_keywords(df: pd.DataFrame) -> pd.DataFrame:\n    # Define the keyword columns to process\n    keyword_columns = ['keywords_0', 'keywords_1', 'keywords_2', 'keywords_3']\n    \n    # Concatenate non-NaN values from the keyword columns for each row\n    df['keywords_all'] = df[keyword_columns].apply(\n        lambda row: '-'.join(row.dropna()[::-1]), axis=1\n    )\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 126, "library_problem_id": 126, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 127, "code": ["<code>\ndef modify_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Randomly selects 20% of the rows from the input DataFrame and sets the 'Quantity' column\n    of these rows to zero. The selection is reproducible using random_state=0.\n\n    Args:\n    - df (pd.DataFrame): The input DataFrame with columns 'UserId', 'ProductId', and 'Quantity'.\n\n    Returns:\n    - pd.DataFrame: The modified DataFrame with the 'Quantity' column set to zero for the selected rows.\n    \"\"\"\n    # Calculate the number of rows to sample (20% of the DataFrame)\n    n = int(len(df) * 0.2)\n\n    # Randomly sample the rows and get their indices\n    sampled_indices = df.sample(n=n, random_state=0).index\n\n    # Set the 'Quantity' column of the sampled rows to zero\n    df.loc[sampled_indices, 'Quantity'] = 0\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 127, "library_problem_id": 127, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 128, "code": ["<code>\nimport pandas as pd\n\ndef modify_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Randomly selects 20% of the rows from the input DataFrame, sets the 'ProductId' of these rows to zero,\n    and retains the original indexes of the altered rows.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame with columns 'UserId', 'ProductId', and 'Quantity'.\n\n    Returns:\n        pd.DataFrame: The modified DataFrame with 20% of 'ProductId' values set to zero.\n    \"\"\"\n    # Calculate the number of rows to sample (20% of the total rows)\n    num_rows_to_sample = int(len(df) * 0.2)\n\n    # Randomly sample the rows and get their indices\n    sampled_indices = df.sample(n=num_rows_to_sample, random_state=0).index\n\n    # Set the 'ProductId' of the sampled rows to zero\n    df.loc[sampled_indices, 'ProductId'] = 0\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 128, "library_problem_id": 128, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 127}}
{"id": 129, "code": ["<code>\nimport numpy as np\n\n# Calculate 20% of the rows for each user and round up to the nearest integer if necessary\nnum_rows_to_zero = df.groupby('UserId').size() * 0.2\nnum_rows_to_zero = num_rows_to_zero.apply(lambda x: int(np.ceil(x)))\n\n# Randomly select 20% of the rows for each user and set the 'Quantity' value to zero\nfor user, num_rows in num_rows_to_zero.items():\n    rows_to_zero = df[df['UserId'] == user].sample(n=num_rows, random_state=0)\n    df.loc[rows_to_zero.index, 'Quantity'] = 0\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 129, "library_problem_id": 129, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 127}}
{"id": 130, "code": ["<code>\ndef find_duplicates_with_index(df: pd.DataFrame) -> pd.DataFrame:\n    # Identify duplicate rows, keeping the first occurrence\n    duplicate_bool = df.duplicated(subset=df.columns, keep='first')\n    \n    # Extract the duplicate rows\n    duplicates = df.loc[duplicate_bool]\n    \n    # Map each duplicate to the index of its first occurrence\n    first_occurrence_index = df.loc[~duplicate_bool].index\n    duplicates['index_original'] = duplicates.apply(\n        lambda row: first_occurrence_index[df.loc[~duplicate_bool].eq(row).all(axis=1)].min(), axis=1\n    )\n    \n    return duplicates\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 130, "library_problem_id": 130, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 131, "code": ["<code>\n```python\nimport pandas as pd\n\ndef find_duplicates_with_last_index(df: pd.DataFrame, subset: list) -> pd.DataFrame:\n    \"\"\"\n    Identify duplicate rows in a DataFrame and add a column indicating the index of the last occurrence of each duplicate.\n\n    Args:\n    - df (pd.DataFrame): The input DataFrame to search for duplicates.\n    - subset (list): List of column names to consider for identifying duplicates.\n\n    Returns:\n    - pd.DataFrame: A DataFrame containing duplicate rows with an additional column 'index_original' indicating the index of the last duplicate.\n    \"\"\"\n    # Identify duplicates, keeping the last occurrence\n    duplicate_bool = df.duplicated(subset=subset, keep='last')\n    \n    # Filter the DataFrame to get only the duplicate rows\n    duplicates = df.loc[duplicate_bool]\n    \n    # Map each duplicate to the index of its last occurrence\n    last_indices = df.loc[duplicate_bool, :].index.to_series().map(df.loc[duplicate_bool, :].index)\n    \n    # Add the 'index_original' column to the duplicates DataFrame\n    duplicates['index_original'] = last_indices.values\n    \n    return duplicates\n\n# Example usage\ndf = pd.DataFrame(data=[[1,2],[3,4],[1,2],[1,4],[1,2]], columns=['col1','col2'])\nresult = find_duplicates_with_last_index(df, subset=\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 131, "library_problem_id": 131, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 130}}
{"id": 132, "code": ["<code>\n```python\nimport pandas as pd\n\ndef find_duplicate_rows_with_original_index(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Identifies duplicate rows in a pandas DataFrame and adds a column indicating\n    the index of the first occurrence of each duplicate row.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame to search for duplicate rows. \n                           It must contain at least one column.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing only the duplicate rows, with an \n                      additional column 'index_original' that indicates the index \n                      of the first occurrence of each duplicate row.\n\n    Example:\n        >>> df = pd.DataFrame(data=[[1,2],[3,4],[1,2],[1,4],[1,2]], columns=['col1','col2'])\n        >>> find_duplicate_rows_with_original_index(df)\n           col1  col2  index_original\n        2     1     2               0\n        4     1     2               0\n    \"\"\"\n    # Identify duplicate rows, keeping the first occurrence\n    duplicate_bool = df.duplicated(subset=df.columns.tolist(), keep='first')\n    \n    # Extract duplicate rows\n    duplicates = df.loc[duplicate_bool]\n    \n    # Map each duplicate to the index of its first occurrence\n    duplicates['index_original'] = duplicates.apply(\n        lambda row: df[(df == row).all(axis=1)].index[0], axis=1\n    )\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 132, "library_problem_id": 132, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 130}}
{"id": 133, "code": ["<code>\nimport pandas as pd\nfrom typing import List\n\ndef find_duplicates_with_index(df: pd.DataFrame, subset: List[str]) -> pd.DataFrame:\n    # Identify duplicate rows based on the specified subset of columns\n    duplicate_bool = df.duplicated(subset=subset, keep='first')\n    \n    # Select the duplicate rows\n    duplicates = df.loc[duplicate_bool]\n    \n    # Add a column indicating the index of the first occurrence of each duplicate\n    duplicates['index_original'] = df.loc[duplicate_bool, subset].apply(\n        lambda row: df[(df[subset] == row).all(axis=1)].index[0], axis=1\n    )\n    \n    return duplicates\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 133, "library_problem_id": 133, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 134, "code": ["<code>\nimport pandas as pd\nfrom typing import List\n\ndef find_duplicates_with_index(df: pd.DataFrame, subset: List[str]) -> pd.DataFrame:\n    # Identify duplicate rows based on the specified subset of columns, keeping the last occurrence\n    duplicate_bool = df.duplicated(subset=subset, keep='last')\n    \n    # Extract the duplicate rows\n    duplicates = df.loc[duplicate_bool]\n    \n    # Add a column 'index_original' indicating the index of the last duplicate\n    duplicates['index_original'] = df.loc[duplicate_bool, :].index\n    \n    return duplicates\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 134, "library_problem_id": 134, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 130}}
{"id": 135, "code": ["<code>\ndef get_max_count_rows(df: pd.DataFrame) -> pd.DataFrame:\n    return df.loc[df.groupby(['Sp', 'Mt'])['count'].idxmax()]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 135, "library_problem_id": 135, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 136, "code": ["<code>\ndef find_max_count_rows(df: pd.DataFrame) -> pd.DataFrame:\n    return df.loc[df.groupby(['Sp', 'Mt'])['count'].idxmax()]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 136, "library_problem_id": 136, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 135}}
{"id": 137, "code": ["<code>\ndef find_min_count_rows(df: pd.DataFrame) -> pd.DataFrame:\n    # Group the DataFrame by ['Sp', 'Mt'] and find the minimum 'count' value for each group\n    min_count = df.groupby(['Sp', 'Mt'])['count'].transform('min')\n    \n    # Filter the DataFrame to include only rows where 'count' is equal to the minimum 'count' in its group\n    result = df[df['count'] == min_count]\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 137, "library_problem_id": 137, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 135}}
{"id": 138, "code": ["<code>\ndef find_max_count_rows(df: pd.DataFrame, group_by_columns: List[str]) -> pd.DataFrame:\n    # Group the DataFrame by the specified columns and find the maximum 'count' in each group\n    max_count_per_group = df.groupby(group_by_columns)['count'].transform('max')\n    \n    # Filter the DataFrame to include only rows where 'count' equals the maximum 'count' in its group\n    result_df = df[df['count'] == max_count_per_group]\n    \n    return result_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 138, "library_problem_id": 138, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 135}}
{"id": 139, "code": ["<code>\nimport pandas as pd\n\ndef filter_dataframe(df: pd.DataFrame, filter_list: list) -> pd.DataFrame:\n    \"\"\"\n    Filters the DataFrame to return rows where the 'Category' column matches any value in the filter_list.\n\n    Args:\n        df (pd.DataFrame): The DataFrame to be filtered. Must contain a 'Category' column.\n        filter_list (list): A list of category values to filter by.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing only the rows where 'Category' is in filter_list.\n    \"\"\"\n    return df[df['Category'].isin(filter_list)]\n\ndf = pd.DataFrame({\"Category\": ['Foo', 'Bar', 'Cho', 'Foo'], 'Index': [1, 2, 3, 4]})\nfilter_list = ['Foo', 'Bar']\nresult = filter_dataframe(df, filter_list)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 139, "library_problem_id": 139, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 140, "code": ["<code>\nimport pandas as pd\n\n# Create a DataFrame with 'Category' and 'Index' columns\ndf = pd.DataFrame({\"Category\": ['Foo', 'Bar', 'Cho', 'Foo'], 'Index': [1, 2, 3, 4]})\n\n# Define a list of categories to filter out\nfilter_list = ['Foo', 'Bar']\n\n# Use the DataFrame's query method to filter out specified categories\n# The '@' symbol is used to reference a Python variable inside the query string\nresult = df.query(\"Category not in @filter_list\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 140, "library_problem_id": 140, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 139}}
{"id": 141, "code": ["<code>\ndef melt_multi_level_columns(df: pd.DataFrame) -> pd.DataFrame:\n    # Extract the unique combinations of column levels\n    value_vars = [tuple(col) for col in df.columns]\n\n    # Melt the DataFrame using the extracted value_vars\n    melted_df = pd.melt(df, value_vars=value_vars)\n\n    return melted_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 141, "library_problem_id": 141, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 142, "code": ["<code>\ndef melt_multi_level_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    # Ensure the DataFrame has three levels in its columns\n    if df.columns.nlevels != 3:\n        raise ValueError(\"DataFrame must have three levels in its columns\")\n\n    # Generate a list of tuples for value_vars, where each tuple contains elements from each column level\n    value_vars = [(level_0, level_1, level_2) \n                  for level_0 in df.columns.levels[0]\n                  for level_1 in df.columns.levels[1]\n                  for level_2 in df.columns.levels[2]]\n\n    # Melt the DataFrame using the generated value_vars\n    melted_df = pd.melt(df, value_vars=value_vars, var_name=['variable_0', 'variable_1', 'variable_2'], value_name='value')\n\n    return melted_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 142, "library_problem_id": 142, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 141}}
{"id": 143, "code": ["<code>\ndef compute_cumulative_sum(df: pd.DataFrame) -> pd.DataFrame:\n    df['cumsum'] = df.groupby('id')['val'].cumsum()\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 143, "library_problem_id": 143, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 144, "code": ["<code>\ndef compute_running_sum(df: pd.DataFrame) -> pd.DataFrame:\n    # Calculate the cumulative sum of 'val' for each 'id'\n    df['cumsum'] = df.groupby('id')['val'].cumsum()\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 144, "library_problem_id": 144, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 145, "code": ["<code>\nimport pandas as pd\n\ndef compute_running_cumsum(df: pd.DataFrame) -> pd.DataFrame:\n    # Ensure 'val' column is numeric for cumulative sum calculation\n    df['val'] = pd.to_numeric(df['val'], errors='coerce')\n\n    # Group by 'id' and calculate the cumulative sum of 'val' for each group\n    df['cumsum'] = df.groupby('id')['val'].cumsum()\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 145, "library_problem_id": 145, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 143}}
{"id": 146, "code": ["<code>\ndef compute_running_max(df: pd.DataFrame) -> pd.DataFrame:\n    df['cummax'] = df.groupby('id')['val'].cummax()\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 146, "library_problem_id": 146, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 147, "code": ["<code>\ndef calculate_running_sum(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculate the running sum of 'val' for each 'id' in the DataFrame.\n    If the running sum becomes negative, reset it to zero.\n\n    Args:\n    - df (pd.DataFrame): Input DataFrame with columns 'id', 'val', and 'stuff'.\n\n    Returns:\n    - pd.DataFrame: DataFrame with an additional column 'cumsum' representing the running sum.\n    \"\"\"\n    def running_sum(group: pd.Series) -> pd.Series:\n        cumsum = group.cumsum()\n        cumsum[cumsum < 0] = 0\n        return cumsum\n\n    df['cumsum'] = df.groupby('id')['val'].apply(running_sum).reset_index(drop=True)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 147, "library_problem_id": 147, "library": "Pandas", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 143}}
{"id": 148, "code": ["<code>\ndef grouped_sum_with_nan(df: pd.DataFrame, group_col: str, sum_col: str) -> pd.Series:\n    \"\"\"\n    Compute the sum of 'sum_col' for each group in 'group_col', returning NaN for any group containing NaN values.\n\n    Args:\n    df (pd.DataFrame): The input DataFrame.\n    group_col (str): The column name to group by.\n    sum_col (str): The column name to sum.\n\n    Returns:\n    pd.Series: A Series containing the sum for each group, with NaN for groups containing NaN values.\n    \"\"\"\n    return df.groupby(group_col)[sum_col].apply(lambda x: x.sum() if x.notna().all() else np.nan)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 148, "library_problem_id": 148, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 149, "code": ["<code>\ndef group_sum_with_nan(df: pd.DataFrame, group_col: str, sum_col: str) -> pd.Series:\n    \"\"\"\n    Calculate the sum of values in the specified sum_col of a DataFrame, grouped by group_col.\n    If any group contains NaN in sum_col, the sum for that group should be NaN.\n\n    Args:\n    df (pd.DataFrame): The DataFrame containing the data.\n    group_col (str): The column name to group by.\n    sum_col (str): The column name whose values are to be summed.\n\n    Returns:\n    pd.Series: A Series with the sum of values for each group, with NaN for groups containing NaN values.\n    \"\"\"\n    grouped = df.groupby(group_col)[sum_col]\n    sums = grouped.sum()\n    has_nan = grouped.apply(lambda x: x.isnull().any())\n    sums[has_nan] = np.nan\n    return sums\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 149, "library_problem_id": 149, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 148}}
{"id": 150, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\n# Sample data\ndata = {\n    'l': ['left', 'right', 'left', 'right', 'left', 'right'],\n    'r': ['right', 'left', 'right', 'left', 'right', 'left'],\n    'v': [-1, 1, -1, 1, -1, np.nan]\n}\ndf = pd.DataFrame(data)\n\n# Implement the solution here\nresult = df.groupby('l').apply(lambda group: group['v'].sum() if not group['v'].isna().any() else np.nan).reset_index()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 150, "library_problem_id": 150, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 151, "code": ["<code>\ndef determine_relationships(df: pd.DataFrame) -> List[str]:\n    relationships = []\n    columns = df.columns\n\n    for i in range(len(columns)):\n        for j in range(len(columns)):\n            if i != j:\n                col1 = columns[i]\n                col2 = columns[j]\n\n                unique_col1 = df[col1].nunique()\n                unique_col2 = df[col2].nunique()\n                unique_pairs = df[[col1, col2]].drop_duplicates().shape[0]\n\n                if unique_col1 == unique_col2 == unique_pairs:\n                    relationship = 'one-to-one'\n                elif unique_col1 == unique_pairs:\n                    relationship = 'one-to-many'\n                elif unique_col2 == unique_pairs:\n                    relationship = 'many-to-one'\n                else:\n                    relationship = 'many-to-many'\n\n                relationships.append(f'{col1} {col2} {relationship}')\n\n    return relationships\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 151, "library_problem_id": 151, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 152, "code": ["<code>\ndef determine_relationships(df: pd.DataFrame) -> list[str]:\n    relationships = []\n    columns = df.columns\n\n    for i in range(len(columns)):\n        for j in range(len(columns)):\n            if i != j:\n                col1 = df[columns[i]]\n                col2 = df[columns[j]]\n\n                unique_col1 = col1.nunique()\n                unique_col2 = col2.nunique()\n                combined_unique = df[[columns[i], columns[j]]].drop_duplicates()\n\n                if unique_col1 == combined_unique.shape[0] and unique_col2 == combined_unique.shape[0]:\n                    relationship = 'one-to-one'\n                elif unique_col1 == combined_unique.shape[0]:\n                    relationship = 'one-to-many'\n                elif unique_col2 == combined_unique.shape[0]:\n                    relationship = 'many-to-one'\n                else:\n                    relationship = 'many-to-many'\n\n                relationships.append(f\"{columns[i]} {columns[j]} {relationship}\")\n\n    return relationships\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 152, "library_problem_id": 152, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 153, "code": ["<code>\ndef determine_relationships(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Determine the type of relationship between each pair of columns in the DataFrame.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame with multiple columns.\n\n    Returns:\n        pd.DataFrame: A DataFrame indicating the type of relationship between each pair of columns.\n    \"\"\"\n    relationship_types = {\n        'one-to-one': lambda x: len(x) == len(x.unique()),\n        'one-to-many': lambda x: len(x) != len(x.unique()),\n        'many-to-one': lambda x: len(x) != len(x.unique()),\n        'many-to-many': lambda x: len(x) != len(x.unique())\n    }\n\n    relationships = pd.DataFrame(index=df.columns, columns=df.columns)\n\n    for column1 in df.columns:\n        for column2 in df.columns:\n            if column1 == column2:\n                relationships.loc[column1, column2] = np.nan\n            else:\n                for relationship, condition in relationship_types.items():\n                    if condition(df[column1]) and condition(df[column2]):\n                        relationships.loc[column1, column2] = relationship\n                        break\n\n    return relationships\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 153, "library_problem_id": 153, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 154, "code": ["<code>\ndef determine_relationships(df: pd.DataFrame) -> pd.DataFrame:\n    def relationship_type(col1, col2):\n        unique_pairs = df[[col1, col2]].drop_duplicates()\n        unique_col1 = unique_pairs[col1].nunique()\n        unique_col2 = unique_pairs[col2].nunique()\n        \n        if unique_col1 == unique_col2 == len(unique_pairs):\n            return 'one-2-one'\n        elif unique_col1 == len(unique_pairs):\n            return 'one-2-many'\n        elif unique_col2 == len(unique_pairs):\n            return 'many-2-one'\n        else:\n            return 'many-2-many'\n    \n    columns = df.columns\n    relationship_df = pd.DataFrame(index=columns, columns=columns)\n    \n    for col1 in columns:\n        for col2 in columns:\n            if col1 == col2:\n                relationship_df.loc[col1, col2] = float('nan')\n            else:\n                relationship_df.loc[col1, col2] = relationship_type(col1, col2)\n    \n    return relationship_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 154, "library_problem_id": 154, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 155, "code": ["<code>\ndef deduplicate_records(df: pd.DataFrame) -> pd.DataFrame:\n    # Normalize the 'firstname', 'lastname', and 'email' fields\n    df_normalized = df.copy()\n    df_normalized[['firstname', 'lastname', 'email']] = df_normalized[['firstname', 'lastname', 'email']].applymap(\n        lambda x: x.lower().replace(\" \", \"\") if isinstance(x, str) else x\n    )\n    \n    # Sort by 'bank' to prioritize non-NaN values\n    df_sorted = df_normalized.sort_values(by='bank', na_position='last')\n    \n    # Drop duplicates, keeping the first occurrence (with bank account if available)\n    unique_indices = df_sorted.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first').index\n    \n    # Return the deduplicated DataFrame\n    return df.loc[unique_indices].reset_index(drop=True)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 155, "library_problem_id": 155, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 155}}
{"id": 156, "code": ["<code>\ndef convert_columns_to_float(df: pd.DataFrame) -> pd.DataFrame:\n    # Replace commas in the DataFrame to handle thousand separators\n    df_cleaned = df.apply(lambda x: x.str.replace(',', '') if x.dtype == 'object' else x)\n    \n    # Convert the cleaned DataFrame to float, coercing errors to NaN\n    df_converted = df_cleaned.apply(pd.to_numeric, errors='coerce')\n    \n    return df_converted\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 156, "library_problem_id": 156, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 156}}
{"id": 157, "code": ["<code>\nimport pandas as pd\n\n# Sample DataFrame\ndf = pd.DataFrame({\n    'Survived': [0, 1, 1, 1, 0],\n    'SibSp': [1, 1, 0, 1, 0],\n    'Parch': [0, 0, 0, 0, 1]\n})\n\n# Define the conditions for grouping\nhas_family_condition = (df['SibSp'] > 0) | (df['Parch'] > 0)\nno_family_condition = (df['SibSp'] == 0) & (df['Parch'] == 0)\n\n# Create a new column 'FamilyStatus' based on the conditions\ndf['FamilyStatus'] = 'No Family'\ndf.loc[has_family_condition, 'FamilyStatus'] = 'Has Family'\n\n# Group by 'FamilyStatus' and calculate the mean of 'Survived'\nresult = df.groupby('FamilyStatus')['Survived'].mean()\n\n# Output the result\nresult\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 157, "library_problem_id": 157, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 158, "code": ["<code>\nimport pandas as pd\n\n# Sample DataFrame\ndf = pd.DataFrame({\n    'Survived': [0, 1, 1, 1, 0],\n    'SibSp': [1, 1, 0, 1, 0],\n    'Parch': [0, 0, 0, 0, 1]\n})\n\n# Define the conditions for grouping\nhas_family_condition = (df['Survived'] > 0) | (df['Parch'] > 0)\nno_family_condition = (df['Survived'] == 0) & (df['Parch'] == 0)\n\n# Create a new column 'FamilyStatus' based on the conditions\ndf['FamilyStatus'] = 'No Family'\ndf.loc[has_family_condition, 'FamilyStatus'] = 'Has Family'\n\n# Group by 'FamilyStatus' and calculate the mean of 'SibSp'\nresult = df.groupby('FamilyStatus')['SibSp'].mean()\n\n# Output the result\nresult\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 158, "library_problem_id": 158, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 157}}
{"id": 159, "code": ["<code>\ndef categorize_and_calculate_means(df: pd.DataFrame) -> pd.Series:\n    # Define conditions for each family group\n    conditions = [\n        ((df['SibSp'] == 1) & (df['Parch'] == 1), \"Has Family\"),\n        ((df['SibSp'] == 0) & (df['Parch'] == 0), \"No Family\"),\n        ((df['SibSp'] == 0) & (df['Parch'] == 1), \"New Family\"),\n        ((df['SibSp'] == 1) & (df['Parch'] == 0), \"Old Family\")\n    ]\n\n    # Initialize a new column 'FamilyGroup' with default value\n    df['FamilyGroup'] = None\n\n    # Assign family group labels based on conditions\n    for condition, label in conditions:\n        df.loc[condition, 'FamilyGroup'] = label\n\n    # Calculate the mean of 'Survived' for each family group\n    result = df.groupby('FamilyGroup')['Survived'].mean()\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 159, "library_problem_id": 159, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 157}}
{"id": 160, "code": ["<code>\nimport pandas as pd\n\n# Create the DataFrame\ndf = pd.DataFrame({\n    'cokey': [11168155, 11168155, 11168155, 11168156, 11168156],\n    'A': [18, 0, 56, 96, 0],\n    'B': [56, 18, 96, 152, 96]\n})\n\n# Sort the DataFrame by 'A' within each 'cokey' group\nresult = df.sort_values(by=['cokey', 'A']).set_index(['cokey', df.groupby('cokey').cumcount()])\n\n# Display the result\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 160, "library_problem_id": 160, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 161, "code": ["<code>\ndef sort_values_in_groups(df):\n    return df.groupby('cokey', group_keys=False).apply(lambda x: x.sort_values('A', ascending=False))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 161, "library_problem_id": 161, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 160}}
{"id": 162, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\n# Sample DataFrame with tuple column headers\nl = [('A', 'a'), ('A', 'b'), ('B', 'a'), ('B', 'b')]\nnp.random.seed(1)\ndf = pd.DataFrame(np.random.randn(5, 4), columns=l)\n\n# Solution: Transform tuple column headers into a MultiIndex\nmulti_index = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Lower'])\ndf.columns = multi_index\n\n# Output the transformed DataFrame\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 162, "library_problem_id": 162, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 162}}
{"id": 163, "code": ["<code>\ndef transform_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Transforms a DataFrame with tuple-based column headers into a MultiIndex column format.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame with tuple-based column headers.\n\n    Returns:\n        pd.DataFrame: The transformed DataFrame with MultiIndex columns.\n    \"\"\"\n    # Convert tuple-based columns to MultiIndex\n    df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps', 'Middle', 'Lower'])\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 163, "library_problem_id": 163, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 162}}
{"id": 164, "code": ["<code>\nimport pandas as pd\n\ndef transform_to_multiindex(df: pd.DataFrame) -> pd.DataFrame:\n    # Extract the tuple elements into separate lists for each level of the MultiIndex\n    level_0 = [col[0] for col in df.columns]\n    level_1 = [col[1] for col in df.columns]\n    level_2 = [col[2] for col in df.columns]\n\n    # Create a MultiIndex from the extracted levels\n    multi_index = pd.MultiIndex.from_tuples(zip(level_0, level_1, level_2), names=['Caps', 'Middle', 'Lower'])\n\n    # Assign the MultiIndex to the DataFrame columns\n    df.columns = multi_index\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 164, "library_problem_id": 164, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 162}}
{"id": 165, "code": ["<code>\ndef construct_dataframe(unique_counts: tuple) -> pd.DataFrame:\n    unique_values, counts = unique_counts\n    return pd.DataFrame({'birdType': unique_values, 'birdCount': counts})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 165, "library_problem_id": 165, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 166, "code": ["<code>\ndef calculate_group_stats(df: pd.DataFrame, group_col: str, target_col: str) -> pd.DataFrame:\n    grouped = df.groupby(group_col)[target_col]\n    mean_series = grouped.mean()\n    std_series = grouped.std()\n    result = pd.DataFrame({'mean': mean_series, 'std': std_series})\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 166, "library_problem_id": 166, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 166}}
{"id": 167, "code": ["<code>\nimport pandas as pd\n\ndef calculate_group_statistics(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculate the mean and standard deviation of column 'a' for each group in column 'b'.\n\n    Args:\n        df (pd.DataFrame): A DataFrame with at least two columns 'a' and 'b'.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the mean and standard deviation of 'a' for each unique value in 'b'.\n                      The index should be the unique values of 'b', and the columns should be 'mean' and 'std'.\n    \"\"\"\n    # Group the DataFrame by column 'b'\n    grouped = df.groupby('b')['a']\n    \n    # Calculate mean and standard deviation for each group\n    result = grouped.agg(['mean', 'std'])\n    \n    return result\n\n# Example usage\ndf = pd.DataFrame({'a': [12, 13, 23, 22, 23, 24, 30, 35, 55], 'b': [1, 1, 1, 2, 2, 2, 3, 3, 3]})\nresult = calculate_group_statistics(df)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 167, "library_problem_id": 167, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 166}}
{"id": 168, "code": ["<code>\ndef compute_softmax_minmax(df: pd.DataFrame) -> pd.DataFrame:\n    # Define a function to compute softmax\n    def softmax(x: pd.Series) -> pd.Series:\n        e_x = np.exp(x - np.max(x))\n        return e_x / e_x.sum()\n\n    # Define a function to compute min-max normalization\n    def min_max_normalize(x: pd.Series) -> pd.Series:\n        return (x - x.min()) / (x.max() - x.min())\n\n    # Apply the functions to each group in column 'a'\n    df['softmax'] = df.groupby('a')['b'].transform(softmax)\n    df['min-max'] = df.groupby('a')['b'].transform(min_max_normalize)\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 168, "library_problem_id": 168, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 166}}
{"id": 169, "code": ["<code>\ndef remove_zero_rows_and_columns(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Remove rows and columns from the DataFrame where all elements are zero.\n\n    Args:\n    df (pd.DataFrame): The input DataFrame.\n\n    Returns:\n    pd.DataFrame: A DataFrame with zero-only rows and columns removed.\n    \"\"\"\n    # Remove rows where all elements are zero\n    df = df.loc[~(df == 0).all(axis=1)]\n    # Remove columns where all elements are zero\n    df = df.loc[:, ~(df == 0).all(axis=0)]\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 169, "library_problem_id": 169, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 169}}
{"id": 170, "code": ["<code>\nimport pandas as pd\n\ndef filter_zero_sum(df: pd.DataFrame) -> pd.DataFrame:\n    # Remove rows with sum equal to zero\n    df_filtered_rows = df.loc[~(df.sum(axis=1) == 0)]\n    \n    # Remove columns with sum equal to zero\n    df_filtered = df_filtered_rows.loc[:, ~(df_filtered_rows.sum(axis=0) == 0)]\n    \n    return df_filtered\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 170, "library_problem_id": 170, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 169}}
{"id": 171, "code": ["<code>\nimport pandas as pd\nfrom typing import Tuple\n\ndef filter_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Filters the input DataFrame by removing rows and columns that contain the maximum value of 2.\n\n    Args:\n    df (pd.DataFrame): The input DataFrame to be filtered.\n\n    Returns:\n    pd.DataFrame: A new DataFrame with rows and columns removed where the maximum value is 2.\n    \"\"\"\n    # Identify columns to keep (those without the maximum value of 2)\n    columns_to_keep = df.columns[df.max() < 2]\n    \n    # Identify rows to keep (those without the maximum value of 2)\n    rows_to_keep = df[df.max(axis=1) < 2].index\n    \n    # Filter the DataFrame\n    filtered_df = df.loc[rows_to_keep, columns_to_keep]\n    \n    return filtered_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 171, "library_problem_id": 171, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 169}}
{"id": 172, "code": ["<code>\nimport pandas as pd\n\ndef zero_out_max_value_rows_columns(df: pd.DataFrame) -> pd.DataFrame:\n    # Identify rows and columns containing the maximum value of 2\n    rows_with_max = df.eq(2).any(axis=1)\n    cols_with_max = df.eq(2).any(axis=0)\n    \n    # Set identified rows and columns to zero\n    df.loc[rows_with_max, :] = 0\n    df.loc[:, cols_with_max] = 0\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 172, "library_problem_id": 172, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 169}}
{"id": 173, "code": ["<code>\ndef sort_series_by_value_and_index(series: pd.Series) -> pd.Series:\n    return series.sort_values(ascending=True).sort_index(kind='mergesort')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 173, "library_problem_id": 173, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 174, "code": ["<code>\ndef sort_series(series: pd.Series) -> pd.DataFrame:\n    # Convert the Series to a DataFrame\n    df = series.reset_index()\n    df.columns = ['index', 'value']\n    \n    # Sort by 'value' in ascending order, then by 'index' alphabetically\n    df_sorted = df.sort_values(by=['value', 'index'], ascending=[True, True]).reset_index(drop=True)\n    \n    return df_sorted\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 174, "library_problem_id": 174, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 173}}
{"id": 175, "code": ["<code>\ndef filter_numeric_rows(dataframe: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Filters the DataFrame to include only rows where the values in column 'A' are numeric.\n\n    Args:\n    dataframe (pd.DataFrame): The input DataFrame with mixed data types in column 'A'.\n\n    Returns:\n    pd.DataFrame: A DataFrame containing only the rows where column 'A' has numeric values.\n    \"\"\"\n    # Use the `pd.to_numeric` function with `errors='coerce'` to convert non-numeric values to NaN\n    numeric_mask = pd.to_numeric(dataframe['A'], errors='coerce').notna()\n    # Filter the DataFrame using the mask\n    return dataframe[numeric_mask]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 175, "library_problem_id": 175, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 175}}
{"id": 176, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'A': [1, 2, 's', 3, 'b'],\n                   'B': ['green', 'red', 'blue', 'yellow', 'black']})\n\ndef filter_string_rows(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Filters the DataFrame to return only the rows where the values in column 'A' are strings.\n\n    Args:\n    df (pd.DataFrame): The input DataFrame with columns 'A' and 'B'.\n\n    Returns:\n    pd.DataFrame: A DataFrame containing only the rows where the values in column 'A' are strings.\n    \"\"\"\n    # Logical condition: Check if the type of each element in column 'A' is a string\n    string_rows = df['A'].apply(lambda x: isinstance(x, str))\n    # Filter the DataFrame based on the condition\n    return df[string_rows]\n\nresult = filter_string_rows(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 176, "library_problem_id": 176, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 175}}
{"id": 177, "code": ["<code>\ndef find_max_count_rows(df: pd.DataFrame, group_by_columns: List[str], target_column: str) -> pd.DataFrame:\n    # Group the DataFrame by the specified columns\n    grouped = df.groupby(group_by_columns)\n    \n    # Find the maximum value of the target column within each group\n    max_values = grouped[target_column].transform('max')\n    \n    # Filter the DataFrame to include only rows where the target column equals the maximum value within its group\n    result = df[df[target_column] == max_values]\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 177, "library_problem_id": 177, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 177}}
{"id": 178, "code": ["<code>\ndef find_max_count_rows(df: pd.DataFrame) -> pd.DataFrame:\n    # Group the DataFrame by 'Sp' and 'Mt' columns\n    grouped = df.groupby(['Sp', 'Mt'])\n    \n    # Find the maximum 'count' value for each group\n    max_count = grouped['count'].transform('max')\n    \n    # Filter the DataFrame to include only rows with the maximum 'count' value in their group\n    result = df[df['count'] == max_count]\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 178, "library_problem_id": 178, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 177}}
{"id": 179, "code": ["<code>\ndef find_min_count_rows(df: pd.DataFrame) -> pd.DataFrame:\n    # Group the DataFrame by 'Sp' and 'Mt' columns\n    grouped = df.groupby(['Sp', 'Mt'])\n    \n    # Find the minimum 'count' value for each group\n    min_count = grouped['count'].transform('min')\n    \n    # Filter the DataFrame to include only rows where 'count' equals the minimum 'count' in its group\n    result = df[df['count'] == min_count]\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 179, "library_problem_id": 179, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 180, "code": ["<code>\ndef find_max_count_rows(df: pd.DataFrame) -> pd.DataFrame:\n    # Group the DataFrame by 'Sp' and 'Value' columns\n    grouped = df.groupby(['Sp', 'Value'])\n    \n    # Find the maximum 'count' value for each group\n    max_count = grouped['count'].transform('max')\n    \n    # Filter rows where 'count' equals the maximum 'count' in their respective group\n    result = df[df['count'] == max_count]\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 180, "library_problem_id": 180, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 177}}
{"id": 181, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef map_dict_to_dataframe_column(df: pd.DataFrame, mapping_dict: dict, key_column: str, target_column: str) -> pd.DataFrame:\n    df[target_column] = df[key_column].map(mapping_dict).fillna(df[target_column])\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 181, "library_problem_id": 181, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 181}}
{"id": 182, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef map_dict_to_dataframe(data_dict: dict, df: pd.DataFrame, default_date: str) -> pd.DataFrame:\n    # Map the dictionary values to the 'Date' column based on the 'Member' column\n    df['Date'] = df['Member'].map(data_dict)\n    # Fill NaN values in 'Date' with the default date\n    df['Date'].fillna(default_date, inplace=True)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 182, "library_problem_id": 182, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 181}}
{"id": 183, "code": ["<code>\ndef map_dict_to_dataframe(dict: dict, df: pd.DataFrame) -> pd.DataFrame:\n    # Create a copy of the DataFrame to avoid modifying the original\n    updated_df = df.copy()\n    \n    # Map the dictionary values to the 'Date' column based on matching 'Member' keys\n    updated_df['Date'] = updated_df['Member'].map(dict).fillna(updated_df['Date'])\n    \n    return updated_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 183, "library_problem_id": 183, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 181}}
{"id": 184, "code": ["<code>\n```python\nimport pandas as pd\nimport numpy as np\n\ndef map_dates(date_dict: dict, df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Map dates from a dictionary to a DataFrame column based on matching keys.\n\n    Args:\n    - date_dict (dict): Dictionary with 'Member' keys and date string values in 'MM/DD/YYYY' format.\n    - df (pd.DataFrame): DataFrame with 'Member', 'Group', and 'Date' columns.\n\n    Returns:\n    - pd.DataFrame: DataFrame with 'Date' column populated based on dictionary mapping or default date.\n    \"\"\"\n    default_date = '17-Aug-1926'\n    for i, member in enumerate(df['Member']):\n        if member in date_dict:\n            date = pd.to_datetime(date_dict[member], format='%m/%d/%Y').strftime('%d-%b-%Y')\n            df.at[i, 'Date'] = date\n        else:\n            df.at[i, 'Date'] = default_date\n    return df\n\ndate_dict = {'abc':'1/2/2003', 'def':'1/5/2017', 'ghi':'4/10/2013'}\ndf = pd.DataFrame({'Member':['xyz', 'uvw', 'abc', 'def', 'ghi'], 'Group':['A', 'B', 'A', 'B', 'B'], 'Date':[np.nan, np.nan\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 184, "library_problem_id": 184, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 181}}
{"id": 185, "code": ["<code>\ndef transform_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    # Convert 'Date' column to datetime\n    df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\n    \n    # Calculate daily counts\n    df['Count_d'] = df['Date'].map(df.groupby('Date').size())\n    \n    # Calculate monthly counts\n    df['Count_m'] = df['Date'].map(df.groupby([df['Date'].dt.year, df['Date'].dt.month]).size())\n    \n    # Calculate yearly counts\n    df['Count_y'] = df['Date'].map(df.groupby(df['Date'].dt.year).size())\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 185, "library_problem_id": 185, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 186, "code": ["<code>\nimport pandas as pd\n\ndef transform_dataframe(data: dict) -> pd.DataFrame:\n    \"\"\"\n    Transforms the input data into a DataFrame with additional count columns.\n\n    Args:\n        data (dict): A dictionary containing 'Date' and 'Val' keys with lists of strings.\n\n    Returns:\n        pd.DataFrame: A DataFrame with additional columns for daily, monthly, yearly, and value counts.\n    \"\"\"\n    # Create DataFrame from input data\n    df = pd.DataFrame(data=data)\n    \n    # Convert 'Date' column to datetime format\n    df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\n    \n    # Calculate daily counts\n    df['Count_d'] = df.groupby('Date')['Date'].transform('size')\n    \n    # Calculate monthly counts\n    df['Count_m'] = df.groupby([df['Date'].dt.year, df['Date'].dt.month])['Date'].transform('size')\n    \n    # Calculate yearly counts\n    df['Count_y'] = df.groupby(df['Date'].dt.year)['Date'].transform('size')\n    \n    # Calculate value counts\n    df['Count_Val'] = df.groupby(['Date', 'Val'])['Val'].transform('size')\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 186, "library_problem_id": 186, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 185}}
{"id": 187, "code": ["<code>\ndef transform_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    # Convert 'Date' column to datetime format\n    df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\n    \n    # Count occurrences per day\n    df['Count_d'] = df.groupby('Date')['Date'].transform('size')\n    \n    # Count occurrences per month\n    df['Count_m'] = df.groupby(df['Date'].dt.to_period('M'))['Date'].transform('size')\n    \n    # Count occurrences per year\n    df['Count_y'] = df.groupby(df['Date'].dt.year)['Date'].transform('size')\n    \n    # Count occurrences per weekday\n    df['Count_w'] = df.groupby(df['Date'].dt.weekday)['Date'].transform('size')\n    \n    # Count occurrences per value\n    df['Count_Val'] = df.groupby('Val')['Val'].transform('size')\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 187, "library_problem_id": 187, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 185}}
{"id": 188, "code": ["<code>\ndef count_zero_nonzero_values(df: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame):\n    \"\"\"\n    Count zero and non-zero values for each numerical column in the DataFrame, grouped by date.\n\n    Args:\n    - df (pd.DataFrame): Input DataFrame with a 'Date' column and numerical columns.\n\n    Returns:\n    - zero_counts (pd.DataFrame): DataFrame with counts of zero values for each column, grouped by date.\n    - non_zero_counts (pd.DataFrame): DataFrame with counts of non-zero values for each column, grouped by date.\n    \"\"\"\n    # Ensure 'Date' is the index\n    df_grouped = df.set_index('Date')\n    \n    # Count zero values\n    zero_counts = df_grouped.eq(0).groupby('Date').sum()\n    \n    # Count non-zero values\n    non_zero_counts = df_grouped.ne(0).groupby('Date').sum()\n    \n    return zero_counts, non_zero_counts\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 188, "library_problem_id": 188, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 189, "code": ["<code>\n# Group by 'Date' and count even and odd values for each column\neven_counts = df.groupby('Date').apply(lambda x: (x % 2 == 0).sum())\nodd_counts = df.groupby('Date').apply(lambda x: (x % 2 != 0).sum())\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 189, "library_problem_id": 189, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 188}}
{"id": 190, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef create_custom_pivot_table() -> pd.DataFrame:\n    # Set a random seed for reproducibility\n    np.random.seed(1)\n    \n    # Create the DataFrame with specified columns and random data\n    df = pd.DataFrame({\n        'A': ['one', 'one', 'two', 'three'] * 6,\n        'B': ['A', 'B', 'C'] * 8,\n        'C': ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 4,\n        'D': np.random.randn(24),\n        'E': np.random.randn(24)\n    })\n    \n    # Create the pivot table with different aggregation functions for each column\n    result = pd.pivot_table(\n        df,\n        values={'D': np.sum, 'E': np.mean},\n        index=['B'],\n        aggfunc={'D': np.sum, 'E': np.mean}\n    )\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 190, "library_problem_id": 190, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 190}}
{"id": 191, "code": ["<code>\nresult = pd.pivot_table(\n    df,\n    values={'D': 'sum', 'E': 'mean'},\n    index=['B'],\n    aggfunc={'D': np.sum, 'E': np.mean}\n)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 191, "library_problem_id": 191, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 192, "code": ["<code>\nresult = pd.pivot_table(\n    df,\n    values={'D': 'sum', 'E': 'mean'},\n    index=['B']\n)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 192, "library_problem_id": 192, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 190}}
{"id": 193, "code": ["<code>\nresult = pd.pivot_table(df, values=['D', 'E'], index=['B'], aggfunc={'D': np.max, 'E': np.min})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 193, "library_problem_id": 193, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 190}}
{"id": 194, "code": ["<code>\nimport dask.dataframe as dd\n\n# Convert the pandas DataFrame to a Dask DataFrame\nddf = dd.from_pandas(df, npartitions=2)\n\n# Define a function to split the 'var2' column\ndef split_var2(df):\n    return df.assign(var2=df['var2'].str.split(',')).explode('var2')\n\n# Apply the function to the Dask DataFrame\nresult = ddf.map_partitions(split_var2).compute()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 194, "library_problem_id": 194, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 195, "code": ["<code>\nresult = df.assign(var2=df['var2'].str.split(',')).explode('var2')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 195, "library_problem_id": 195, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 194}}
{"id": 196, "code": ["<code>\nimport dask.dataframe as dd\n\n# Read the CSV file into a Dask DataFrame\ndf = dd.read_csv('input.csv')\n\n# Split 'var2' into multiple rows\nresult = df.assign(var2=df['var2'].str.split('-')).explode('var2')\n\n# Compute the result\nresult = result.compute()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 196, "library_problem_id": 196, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 194}}
{"id": 197, "code": ["<code>\nimport pandas as pd\n\ndef count_special_characters(string: str) -> int:\n    special_char_count = 0\n    for char in string:\n        if not char.isalpha():\n            special_char_count += 1\n    return special_char_count\n\ndf = pd.read_csv('inn.txt', sep='\\t')\ndf['new'] = df['str'].apply(count_special_characters)\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 197, "library_problem_id": 197, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 197}}
{"id": 198, "code": ["<code>\nimport pandas as pd\n\ndef count_alpha_chars(string: str) -> int:\n    alpha_count = 0\n    for char in string:\n        if char.isalpha():\n            alpha_count += 1\n    return alpha_count\n\ndf = pd.DataFrame({'str': ['Aa', 'Bb', '?? ?', '###', '{}xxa;']})\n\ndf['new'] = df['str'].apply(count_alpha_chars)\n\nprint(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 198, "library_problem_id": 198, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 197}}
{"id": 199, "code": ["<code>\ndef split_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    # Split the 'row' column into two separate columns using space as the delimiter\n    df[['fips', 'location']] = df['row'].str.split(' ', 1, expand=True)\n    return df[['fips', 'location']]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 199, "library_problem_id": 199, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 199}}
{"id": 200, "code": ["<code>\nimport pandas as pd\n\n# Create the initial DataFrame\ndf = pd.DataFrame({'row': ['114 AAAAAA', '514 ENENEN',\n                           '1926 HAHAHA', '0817 O-O,O-O',\n                           '998244353 TTTTTT']})\n\n# Solution: Split the 'row' column into 'fips' and 'row' columns\ndf[['fips', 'row']] = df['row'].str.split(' ', 1, expand=True)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 200, "library_problem_id": 200, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 199}}
{"id": 201, "code": ["<code>\ndef split_dataframe_column(df: pd.DataFrame) -> pd.DataFrame:\n    # Split the 'row' column into three parts: 'fips', 'medi', and 'row'\n    df[['fips', 'medi', 'row']] = df['row'].str.extract(r'(^\\d{5})\\s+(.+)', expand=True)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 201, "library_problem_id": 201, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 199}}
{"id": 202, "code": ["<code>\nimport pandas as pd\nfrom typing import Any\n\ndef calculate_cumulative_average(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculate the cumulative average for each row in the DataFrame, ignoring zero values.\n\n    Args:\n    df (pd.DataFrame): A DataFrame with a 'Name' column and several year columns containing integer values.\n\n    Returns:\n    pd.DataFrame: A DataFrame with the same structure as the input, where each year column contains the cumulative average\n                  of non-zero values for each row up to that year.\n    \"\"\"\n    # Your implementation here\n    df_non_zero = df.replace(0, pd.np.nan)\n    df_cumulative_average = df_non_zero.expanding(axis=1).mean().fillna(0)\n    return df_cumulative_average\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 202, "library_problem_id": 202, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 203, "code": ["<code>\ndef calculate_cumulative_average(df: pd.DataFrame) -> pd.DataFrame:\n    columns = df.columns[1:]\n    for column in columns:\n        df[column] = df[column].replace(0, np.nan)\n        df[column] = df[column].cumsum() / df[column].count()\n        df[column].fillna(0, inplace=True)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 203, "library_problem_id": 203, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 202}}
{"id": 204, "code": ["<code>\nimport pandas as pd\n\ndef calculate_cumulative_average(df: pd.DataFrame) -> pd.DataFrame:\n    # Copy the DataFrame to avoid modifying the original data\n    result_df = df.copy()\n\n    # Iterate over each row in the DataFrame\n    for index, row in result_df.iterrows():\n        cumulative_sum = 0\n        count_non_zero = 0\n        \n        # Iterate over each column starting from the second one (year columns)\n        for col in df.columns[1:]:\n            value = row[col]\n            \n            # Only consider non-zero values for the cumulative average\n            if value != 0:\n                cumulative_sum += value\n                count_non_zero += 1\n            \n            # Calculate the cumulative average if there are non-zero values\n            if count_non_zero > 0:\n                result_df.at[index, col] = cumulative_sum / count_non_zero\n            else:\n                result_df.at[index, col] = 0\n\n    return result_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 204, "library_problem_id": 204, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 202}}
{"id": 205, "code": ["<code>\nimport pandas as pd\nfrom typing import Any\n\ndef calculate_cumulative_average(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculate the cumulative average for each row in the DataFrame from the last column to the first,\n    ignoring zero values in the calculation.\n\n    Args:\n    df (pd.DataFrame): The input DataFrame with a 'Name' column and year columns.\n\n    Returns:\n    pd.DataFrame: A DataFrame with the same structure as the input, but with cumulative averages calculated.\n    \"\"\"\n    # Copy the DataFrame to avoid modifying the original\n    result_df = df.copy()\n\n    # Iterate over each row in the DataFrame\n    for index, row in result_df.iterrows():\n        # Initialize variables to store cumulative sum and count of non-zero elements\n        cumulative_sum = 0\n        count_non_zero = 0\n\n        # Iterate over the columns in reverse order, excluding the 'Name' column\n        for col in df.columns[1:][::-1]:\n            value = row[col]\n            if value != 0:\n                cumulative_sum += value\n                count_non_zero += 1\n            # Calculate the cumulative average if count_non_zero is greater than zero\n            if count_non_zero > 0:\n                result_df.at[index, col] = cumulative_sum / count_non_zero\n\n    return result_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 205, "library_problem_id": 205, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 206, "code": ["<code>\ndef label_close_difference(df: pd.DataFrame) -> pd.DataFrame:\n    # Initialize the 'label' column with zeros\n    df['label'] = 0\n    \n    # Calculate the difference between consecutive 'Close' values\n    # Assign 1 if the difference is positive, otherwise 0\n    df.loc[1:, 'label'] = (df['Close'].diff() > 0).astype(int)\n    \n    # Set the label of the first row to 1\n    df.loc[0, 'label'] = 1\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 206, "library_problem_id": 206, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 206}}
{"id": 207, "code": ["<code>\ndef add_trend_labels(df: pd.DataFrame) -> pd.DataFrame:\n    df['label'] = df['Close'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n    df.loc[0, 'label'] = 1\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 207, "library_problem_id": 207, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 206}}
{"id": 208, "code": ["<code>\nimport pandas as pd\n\ndef transform_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Transforms the input DataFrame by formatting 'DateTime', calculating differences in 'Close',\n    and assigning labels based on the difference.\n\n    Args:\n    df (pd.DataFrame): Input DataFrame with 'DateTime' and 'Close' columns.\n\n    Returns:\n    pd.DataFrame: Transformed DataFrame with 'DateTime', 'Close', and 'label' columns.\n    \"\"\"\n    # Convert 'DateTime' to datetime format and then to 'DD-MMM-YYYY' string format\n    df['DateTime'] = pd.to_datetime(df['DateTime']).dt.strftime('%d-%b-%Y')\n    \n    # Calculate the difference between consecutive 'Close' values\n    df['label'] = df['Close'].diff().fillna(0).apply(lambda x: 1 if x > 0 else (0 if x == 0 else -1))\n    \n    # Set the label of the first row to 1\n    df.loc[0, 'label'] = 1\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 208, "library_problem_id": 208, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 206}}
{"id": 209, "code": ["<code>\nimport pandas as pd\n\ndef calculate_train_durations(id, arrival_time, departure_time):\n    # Create DataFrame from input lists\n    df = pd.DataFrame({'id': id, 'arrival_time': arrival_time, 'departure_time': departure_time})\n\n    # Convert '0' to NaT and strings to datetime64[ns]\n    df['arrival_time'] = pd.to_datetime(df['arrival_time'].replace(\"0\", pd.NaT))\n    df['departure_time'] = pd.to_datetime(df['departure_time'])\n\n    # Calculate the duration between consecutive departure and arrival times\n    df['Duration'] = df['arrival_time'].shift(-1) - df['departure_time']\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 209, "library_problem_id": 209, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 210, "code": ["<code>\ndef calculate_train_durations(id: list, arrival_time: list, departure_time: list) -> pd.DataFrame:\n    # Create DataFrame\n    df = pd.DataFrame({'id': id, 'arrival_time': arrival_time, 'departure_time': departure_time})\n    \n    # Convert '0' to NaT and strings to datetime\n    df['arrival_time'] = pd.to_datetime(df['arrival_time'].replace(\"0\", pd.NaT))\n    df['departure_time'] = pd.to_datetime(df['departure_time'])\n    \n    # Calculate duration in seconds\n    df['Duration'] = (df['arrival_time'] - df['departure_time'].shift(1)).dt.total_seconds()\n    \n    # Set duration to NaN for the first entry of each train ID\n    df.loc[df['id'] != df['id'].shift(1), 'Duration'] = pd.NA\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 210, "library_problem_id": 210, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 209}}
{"id": 211, "code": ["<code>\ndef calculate_train_durations(id: list[str], arrival_time: list[str], departure_time: list[str]) -> pd.DataFrame:\n    # Create DataFrame from input lists\n    df = pd.DataFrame({\n        'id': id,\n        'arrival_time': arrival_time,\n        'departure_time': departure_time\n    })\n\n    # Convert '0' to NaT and parse datetime columns\n    df['arrival_time'] = pd.to_datetime(df['arrival_time'].replace('0', pd.NaT))\n    df['departure_time'] = pd.to_datetime(df['departure_time'])\n\n    # Calculate duration in seconds between consecutive departure and arrival times\n    df['Duration'] = (df['arrival_time'] - df['departure_time'].shift(1)).dt.total_seconds()\n\n    # Format datetime columns to 'DD-MMM-YYYY HH:MM:SS'\n    df['arrival_time'] = df['arrival_time'].dt.strftime('%d-%b-%Y %H:%M:%S')\n    df['departure_time'] = df['departure_time'].dt.strftime('%d-%b-%Y %H:%M:%S')\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 211, "library_problem_id": 211, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 209}}
{"id": 212, "code": ["<code>\nimport pandas as pd\n\n# Create the DataFrame\ndf = pd.DataFrame({'key1': ['a', 'a', 'b', 'b', 'a', 'c'],\n                   'key2': ['one', 'two', 'one', 'two', 'one', 'two']})\n\n# Solution: Group by 'key1' and count occurrences of 'one' in 'key2'\nresult = df[df['key2'] == 'one'].groupby('key1').size().reset_index(name='count')\n\n# Ensure all 'key1' values are present, even if count is zero\nresult = result.set_index('key1').reindex(df['key1'].unique(), fill_value=0).reset_index()\n\n# Output the result\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 212, "library_problem_id": 212, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 212}}
{"id": 213, "code": ["<code>\nimport pandas as pd\n\ndf = pd.DataFrame({'key1': ['a', 'a', 'b', 'b', 'a', 'c'],\n                   'key2': ['one', 'two', 'one', 'two', 'one', 'two']})\n\nresult = df[df['key2'] == 'two'].groupby('key1').size().reset_index(name='count')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 213, "library_problem_id": 213, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 212}}
{"id": 214, "code": ["<code>\ndef count_key2_ending_with_e(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Groups the DataFrame by 'key1' and counts the occurrences in 'key2' where the string ends with 'e'.\n\n    Args:\n    df (pd.DataFrame): A DataFrame with columns 'key1' and 'key2'.\n\n    Returns:\n    pd.DataFrame: A DataFrame with columns 'key1' and 'count', where 'count' represents the number of times\n                  a string in 'key2' ends with 'e' for each group in 'key1'.\n    \"\"\"\n    # Solution implementation\n    df['endswith_e'] = df['key2'].apply(lambda x: x.endswith('e'))\n    result = df.groupby('key1')['endswith_e'].sum().reset_index(name='count')\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 214, "library_problem_id": 214, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 212}}
{"id": 215, "code": ["<code>\n```python\nimport pandas as pd\n\ndef get_min_max_dates(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Extracts the minimum and maximum dates from the index of a pandas DataFrame.\n\n    Args:\n    df (pd.DataFrame): A DataFrame with a Date index.\n\n    Returns:\n    tuple: A tuple containing the minimum and maximum dates as strings.\n    \"\"\"\n    # Ensure the index is of datetime type for accurate comparison\n    df.index = pd.to_datetime(df.index)\n    \n    # Calculate the minimum and maximum dates\n    min_date = df.index.min()\n    max_date = df.index.max()\n    \n    return min_date.strftime('%Y-%m-%d'), max_date.strftime('%Y-%m-%d')\n\n# Example usage\ndf = pd.DataFrame({'value':[10000,2000,2000,200,5,70,200,5,25,0.02,12,0.022]},\n                  index=['2014-03-13','2014-03-21','2014-03-27','2014-03-17','2014-03-17','2014-03-17','2014-03-21','2014-03-27','2014-03-27','2014-0\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 215, "library_problem_id": 215, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 216, "code": ["<code>\n```python\nimport pandas as pd\n\ndef calculate_mode_and_median_dates(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Calculate the mode and median dates from the index of a DataFrame.\n\n    Args:\n    df (pd.DataFrame): A DataFrame with dates as its index.\n\n    Returns:\n    tuple: A tuple containing the mode date and the median date.\n    \"\"\"\n    # Convert index to a pandas Series\n    date_index = df.index.to_series()\n\n    # Calculate mode date\n    mode_date = date_index.mode().iloc[0]\n\n    # Calculate median date\n    median_date = date_index.sort_values().iloc[len(date_index) // 2]\n\n    return mode_date, median_date\n\ndf = pd.DataFrame({'value': [10000, 2000, 2000, 200, 5, 70, 200, 5, 25, 0.02, 12, 11, 0.022]},\n                  index=['2014-03-13', '2014-03-21', '2014-03-27', '2014-03-17', '2014-03-17', '2014-03-17',\n                         '2014-03-21', '2014-03-27', '2014-0\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 216, "library_problem_id": 216, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 215}}
{"id": 217, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef filter_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    mask = (df['closing_price'] >= 99) & (df['closing_price'] <= 101)\n    filtered_df = df[mask]\n    return filtered_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 217, "library_problem_id": 217, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 217}}
{"id": 218, "code": ["<code>\n# Create a boolean mask for values outside the range 99 to 101\nmask = (df['closing_price'] < 99) | (df['closing_price'] > 101)\n\n# Apply the mask to filter the DataFrame and assign to result\nresult = df[mask]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 218, "library_problem_id": 218, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 217}}
{"id": 219, "code": ["<code>\ndef filter_minimum_rows(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Filters the DataFrame to retain rows with the minimum value in the 'diff' column\n    for each group defined by the 'item' column, while preserving all other columns.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame with columns 'item', 'diff', and others.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing rows with the minimum 'diff' value for each 'item'.\n    \"\"\"\n    # Identify the minimum 'diff' value for each 'item'\n    min_diff = df.groupby('item')['diff'].transform('min')\n    \n    # Filter the DataFrame to retain rows with the minimum 'diff' value\n    result = df[df['diff'] == min_diff]\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 219, "library_problem_id": 219, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 219}}
{"id": 220, "code": ["<code>\ndef parse_strings_after_last_underscore(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n    df[column_name] = df[column_name].apply(lambda x: x.rsplit('_', 1)[0] if '_' in x else x)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 220, "library_problem_id": 220, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 220}}
{"id": 221, "code": ["<code>\ndef parse_strings_after_last_underscore(df: pd.DataFrame) -> pd.DataFrame:\n    df['SOURCE_NAME'] = df['SOURCE_NAME'].apply(lambda x: x.split('_')[-1] if '_' in x else x)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 221, "library_problem_id": 221, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 220}}
{"id": 222, "code": ["<code>\ndef parse_strings_after_last_underscore(df: pd.DataFrame) -> pd.Series:\n    def remove_after_last_underscore(s: str) -> str:\n        # Find the position of the last underscore\n        last_underscore_pos = s.rfind('_')\n        # If an underscore is found, return the substring before it; otherwise, return the original string\n        return s[:last_underscore_pos] if last_underscore_pos != -1 else s\n\n    # Apply the function to each string in the 'SOURCE_NAME' column\n    return df['SOURCE_NAME'].apply(remove_after_last_underscore)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 222, "library_problem_id": 222, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 220}}
{"id": 223, "code": ["<code>\ndef fill_nan_with_alternating_values(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n    # Identify the indices of NaN values in the specified column\n    nan_indices = df[df[column_name].isna()].index\n    num_nans = len(nan_indices)\n    \n    # Calculate the number of NaNs to fill with '0' and '1'\n    num_zeros = num_nans // 2\n    num_ones = num_nans - num_zeros\n    \n    # Fill the first half of NaNs with '0'\n    df.loc[nan_indices[:num_zeros], column_name] = 0\n    \n    # Fill the second half of NaNs with '1'\n    df.loc[nan_indices[num_zeros:], column_name] = 1\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 223, "library_problem_id": 223, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 223}}
{"id": 224, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef fill_nan_values(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n    # Calculate the number of NaN values in the column\n    nan_count = df[column_name].isna().sum()\n    \n    # Calculate the number of NaN values to fill with each specified value\n    first_fill_count = nan_count // 3\n    second_fill_count = nan_count // 3\n    third_fill_count = nan_count - first_fill_count - second_fill_count\n    \n    # Create a list of fill values based on the calculated counts\n    fill_values = [0] * first_fill_count + [0.5] * second_fill_count + [1] * third_fill_count\n    \n    # Fill NaN values in the column with the specified values\n    df[column_name] = df[column_name].fillna(pd.Series(fill_values, index=df.index[df[column_name].isna()]))\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 224, "library_problem_id": 224, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 223}}
{"id": 225, "code": ["<code>\ndef fill_nan_with_binary(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n    # Count the number of NaN values in the specified column\n    nan_count = df[column_name].isna().sum()\n    \n    # Calculate the number of 0s and 1s to fill\n    num_zeros = nan_count // 2\n    num_ones = nan_count - num_zeros\n    \n    # Create a list of values to fill NaNs with\n    fill_values = [0] * num_zeros + [1] * num_ones\n    \n    # Fill NaN values with the specified pattern\n    df[column_name] = df[column_name].fillna(pd.Series(fill_values))\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 225, "library_problem_id": 225, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 223}}
{"id": 226, "code": ["<code>\ndef create_tuple_dataframe(dataframes: List[pd.DataFrame]) -> pd.DataFrame:\n    # Ensure all DataFrames have the same shape and columns\n    if not all(df.shape == dataframes[0].shape for df in dataframes):\n        raise ValueError(\"All DataFrames must have the same shape.\")\n    if not all(df.columns.equals(dataframes[0].columns) for df in dataframes):\n        raise ValueError(\"All DataFrames must have the same columns.\")\n\n    # Create a new DataFrame with tuple elements\n    tuple_data = [\n        [tuple(df.iloc[i, j] for df in dataframes) for j in range(dataframes[0].shape[1])]\n        for i in range(dataframes[0].shape[0])\n    ]\n    \n    return pd.DataFrame(tuple_data, columns=dataframes[0].columns)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 226, "library_problem_id": 226, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 226}}
{"id": 227, "code": ["<code>\nimport pandas as pd\n\ndef create_tuple_dataframe(*dataframes: pd.DataFrame) -> pd.DataFrame:\n    return pd.concat(dataframes, axis=1).apply(lambda row: tuple(row[i] for i in range(len(dataframes))), axis=1).unstack().to_frame().T\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 227, "library_problem_id": 227, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 226}}
{"id": 228, "code": ["<code>\ndef create_tuple_dataframe(*dfs: pd.DataFrame) -> pd.DataFrame:\n    # Determine the maximum number of rows among all DataFrames\n    max_rows = max(df.shape[0] for df in dfs)\n    \n    # Initialize a list to store the tuples for each column\n    tuple_data = {col: [] for col in dfs[0].columns}\n    \n    # Iterate over each row index up to max_rows\n    for i in range(max_rows):\n        for col in dfs[0].columns:\n            # Create a tuple for each column from the current row of each DataFrame\n            tuple_data[col].append(tuple(\n                df[col].iloc[i] if i < df.shape[0] else np.nan for df in dfs\n            ))\n    \n    # Create a new DataFrame from the tuple data\n    return pd.DataFrame(tuple_data)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 228, "library_problem_id": 228, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 226}}
{"id": 229, "code": ["<code>\nimport pandas as pd\n\ndef count_views_by_bins(df: pd.DataFrame, bins: list) -> pd.DataFrame:\n    \"\"\"\n    Count the number of views within specified bins for each user.\n\n    Args:\n    - df (pd.DataFrame): DataFrame containing 'username', 'post_id', and 'views'.\n    - bins (list): List of integers defining the bin edges.\n\n    Returns:\n    - pd.DataFrame: DataFrame with 'username' as index and columns representing view count bins.\n    \"\"\"\n    # Create a categorical column based on the bins\n    df['view_bins'] = pd.cut(df['views'], bins)\n\n    # Group by 'username' and 'view_bins' and count occurrences\n    result = df.groupby(['username', 'view_bins']).size().unstack(fill_value=0)\n\n    return result\n\n# Example usage\ndf = pd.DataFrame({'username': ['john', 'john', 'john', 'john', 'jane', 'jane', 'jane', 'jane'],\n                   'post_id': [1, 2, 3, 4, 7, 8, 9, 10],\n                   'views': [3, 23, 44, 82, 5, 25, 46, 56]})\nbins = [1, 10, 25, 50, 100]\nresult = count_views_by_bins(df, bins)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 229, "library_problem_id": 229, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 229}}
{"id": 230, "code": ["<code>\ndef count_views_by_bins(df: pd.DataFrame, bins: list) -> pd.DataFrame:\n    # Create a categorical column based on the bins\n    df['view_bins'] = pd.cut(df['views'], bins=bins, right=True)\n\n    # Group by 'username' and 'view_bins', then count the occurrences\n    grouped = df.groupby(['username', 'view_bins']).size().unstack(fill_value=0)\n\n    # Ensure the columns are ordered according to the bins\n    grouped = grouped.reindex(columns=pd.IntervalIndex.from_breaks(bins, closed='right'), fill_value=0)\n\n    return grouped\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 230, "library_problem_id": 230, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 231, "code": ["<code>\nimport pandas as pd\n\ndef count_views_by_bins(df: pd.DataFrame, bins: list) -> pd.DataFrame:\n    \"\"\"\n    Count the number of views within specified bins for each user.\n\n    Args:\n    df (pd.DataFrame): DataFrame containing 'username', 'post_id', and 'views'.\n    bins (list): List of integers defining the bin edges.\n\n    Returns:\n    pd.DataFrame: DataFrame with usernames as index and bin ranges as columns, showing counts of views in each bin.\n    \"\"\"\n    # Categorize views into bins\n    df['view_bins'] = pd.cut(df['views'], bins)\n    \n    # Group by username and view_bins, then count occurrences\n    result = df.groupby(['username', 'view_bins']).size().unstack(fill_value=0)\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 231, "library_problem_id": 231, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 229}}
{"id": 232, "code": ["<code>\nimport pandas as pd\n\ndef merge_rows_to_single_string(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Merges all rows of a DataFrame's 'text' column into a single string separated by commas.\n    \n    Args:\n    df (pd.DataFrame): A DataFrame with a single column 'text' containing string values.\n    \n    Returns:\n    pd.DataFrame: A DataFrame with a single row and a single column 'text', where the value is a \n                  comma-separated string of all input strings.\n    \"\"\"\n    # Ensure the input DataFrame has the expected structure\n    assert 'text' in df.columns, \"DataFrame must have a 'text' column\"\n    \n    # Merge all text entries into a single comma-separated string\n    merged_text = ', '.join(df['text'])\n    \n    # Create a new DataFrame with the merged text\n    result_df = pd.DataFrame({'text': [merged_text]})\n    \n    return result_df\n\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = merge_rows_to_single_string(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 232, "library_problem_id": 232, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 232}}
{"id": 233, "code": ["<code>\ndef concatenate_dataframe_rows(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Concatenates all strings in the 'text' column of the input DataFrame into a single string,\n    with each original string separated by a hyphen ('-'). Returns a new DataFrame with the\n    concatenated string in a single row.\n\n    Args:\n    df (pd.DataFrame): A DataFrame with a single column 'text' containing string data.\n\n    Returns:\n    pd.DataFrame: A new DataFrame with one row and one column 'text', containing the concatenated string.\n    \"\"\"\n    df['text'] = df['text'].apply('-'.join)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 233, "library_problem_id": 233, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 234, "code": ["<code>\ndef merge_rows_to_single_row(df: pd.DataFrame) -> pd.DataFrame:\n    concatenated_text = ', '.join(df['text'][::-1])\n    return pd.DataFrame({'text': [concatenated_text]})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 234, "library_problem_id": 234, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 235, "code": ["<code>\nimport pandas as pd\n\ndef merge_rows_to_series(df: pd.DataFrame) -> pd.Series:\n    result = pd.Series([', '.join(df['text'])], name='text')\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 235, "library_problem_id": 235, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 232}}
{"id": 236, "code": ["<code>\nimport pandas as pd\n\ndef concatenate_reverse_order(df: pd.DataFrame) -> pd.Series:\n    \"\"\"\n    Concatenates the strings in the 'text' column of the DataFrame in reverse order,\n    separated by hyphens, and returns the result as a pandas Series.\n\n    Args:\n    df (pd.DataFrame): A DataFrame with a single column 'text' containing string values.\n\n    Returns:\n    pd.Series: A Series with a single element, where the strings from the 'text' column\n               are concatenated in reverse order and separated by hyphens.\n    \"\"\"\n    # Reverse the order of the 'text' column and join with hyphens\n    concatenated_text = '-'.join(df['text'][::-1])\n    # Return as a Series\n    return pd.Series([concatenated_text], name='text')\n\ndf = pd.DataFrame({'text': ['abc', 'def', 'ghi', 'jkl']})\nresult = concatenate_reverse_order(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 236, "library_problem_id": 236, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 232}}
{"id": 237, "code": ["<code>\nimport pandas as pd\n\ndef concatenate_and_fill(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:\n    # Merge df2 with df1 on 'id' to fill 'city' and 'district' columns in df2\n    df2_filled = df2.merge(df1[['id', 'city', 'district']], on='id', how='left')\n    \n    # Concatenate df1 and the filled df2\n    result = pd.concat([df1, df2_filled], axis=0, ignore_index=True)\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 237, "library_problem_id": 237, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 237}}
{"id": 238, "code": ["<code>\nimport pandas as pd\n\ndef process_dataframes(df1, df2):\n    # Merge DataFrames on 'id', filling missing 'city' and 'district' from df1\n    merged_df = pd.concat([df1, df2], axis=0, ignore_index=True)\n    merged_df = merged_df.sort_values(by=['id', 'date'], ascending=[True, True])\n\n    # Fill missing 'city' and 'district' values\n    merged_df[['city', 'district']] = merged_df[['city', 'district']].fillna(method='ffill')\n\n    # Format 'date' column to 'DD-MMM-YYYY'\n    merged_df['date'] = pd.to_datetime(merged_df['date']).dt.strftime('%d-%b-%Y')\n\n    return merged_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 238, "library_problem_id": 238, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 239, "code": ["<code>\n```python\nimport pandas as pd\n\ndef concatenate_and_fill_dataframes(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Concatenates two DataFrames based on the 'id' column, fills missing 'city' and 'district' values in df2\n    using values from df1, and sorts the resulting DataFrame by 'id' and 'date'.\n\n    Args:\n    - df1 (pd.DataFrame): The first DataFrame containing columns 'id', 'city', 'district', 'date', and 'value'.\n    - df2 (pd.DataFrame): The second DataFrame containing columns 'id', 'date', and 'value'.\n\n    Returns:\n    - pd.DataFrame: A DataFrame with concatenated rows from df1 and df2, with missing 'city' and 'district'\n      values in df2 filled from df1, and sorted by 'id' and 'date'.\n    \"\"\"\n    # Concatenate the two DataFrames\n    concatenated_df = pd.concat([df1, df2], axis=0, ignore_index=True)\n    \n    # Fill missing 'city' and 'district' values in df2 using values from df1\n    concatenated_df['city'] = concatenated_df.groupby('id')['city'].ffill()\n    concatenated_df['district'] = concatenated_df.groupby('id')['district'].ffill()\n    \n    # Sort the DataFrame by 'id' and 'date'\n    concatenated_df['date'] = pd.to_datetime\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 239, "library_problem_id": 239, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 237}}
{"id": 240, "code": ["<code>\ndef merge_dataframes(df_left: pd.DataFrame, df_right: pd.DataFrame) -> pd.DataFrame:\n    # Merge the DataFrames on column 'A' using an outer join\n    merged_df = pd.merge(df_left, df_right, how='outer', on='A', suffixes=('_left', '_right'))\n    \n    # Overwrite values in 'B_left' with 'B_right' where 'B_right' is not null\n    merged_df['B'] = merged_df['B_right'].combine_first(merged_df['B_left'])\n    \n    # Select the relevant columns and preserve the order of `df_left`\n    result_df = merged_df[['A', 'B']].sort_index(kind='mergesort')\n    \n    return result_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 240, "library_problem_id": 240, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 240}}
{"id": 241, "code": ["<code>\ndef merge_dataframes(C: pd.DataFrame, D: pd.DataFrame) -> pd.DataFrame:\n    # Perform an outer merge on column 'A'\n    merged_df = pd.merge(C, D, how='outer', on='A', suffixes=('_C', '_D'))\n    \n    # Fill NaN values in 'B_C' with values from 'B_D'\n    merged_df['B'] = merged_df['B_C'].combine_first(merged_df['B_D'])\n    \n    # Select only the necessary columns\n    result = merged_df[['A', 'B']]\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 241, "library_problem_id": 241, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 240}}
{"id": 242, "code": ["<code>\ndef merge_dataframes(C: pd.DataFrame, D: pd.DataFrame) -> pd.DataFrame:\n    # Merge DataFrames on column 'A' using an outer join\n    merged_df = pd.merge(C, D, how='outer', on='A', suffixes=('_C', '_D'))\n    \n    # Overwrite 'B' values from C with those from D where 'A' matches\n    merged_df['B'] = merged_df['B_D'].combine_first(merged_df['B_C'])\n    \n    # Determine if 'A' is present in both DataFrames\n    merged_df['duplicated'] = merged_df['B_C'].notna() & merged_df['B_D'].notna()\n    \n    # Select relevant columns and maintain order\n    result_df = merged_df[['A', 'B', 'duplicated']]\n    \n    return result_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 242, "library_problem_id": 242, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 240}}
{"id": 243, "code": ["<code>\nimport pandas as pd\n\ndef aggregate_transactions(df: pd.DataFrame) -> pd.Series:\n    df = df.sort_values(by=['user', 'time'])\n    return df.groupby('user')[['time', 'amount']].apply(lambda x: x.values.tolist())\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 243, "library_problem_id": 243, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 243}}
{"id": 244, "code": ["<code>\ndef aggregate_transactions(df: pd.DataFrame) -> pd.DataFrame:\n    # Sort the DataFrame by 'user' and 'time' to ensure transactions are ordered by time for each user\n    df_sorted = df.sort_values(by=['user', 'time'])\n    \n    # Group by 'user' and aggregate 'time' and 'amount' into a list of tuples\n    result = df_sorted.groupby('user').apply(\n        lambda group: list(zip(group['time'], group['amount']))\n    ).reset_index(name='amount-time-tuple')\n    \n    # Set 'user' as the index of the resulting DataFrame\n    result.set_index('user', inplace=True)\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 244, "library_problem_id": 244, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 243}}
{"id": 245, "code": ["<code>\ndef aggregate_and_sort_transactions(df: pd.DataFrame) -> pd.DataFrame:\n    # Group by 'user', sort by 'time', and aggregate into a list of tuples (time, amount)\n    result = df.sort_values(by=['user', 'time']).groupby('user').apply(\n        lambda x: list(zip(x['time'], x['amount']))\n    ).reset_index(name='amount-time-tuple')\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 245, "library_problem_id": 245, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 243}}
{"id": 246, "code": ["<code>\ndef expand_series_to_dataframe(series: pd.Series) -> pd.DataFrame:\n    \"\"\"\n    Transforms a pandas Series with numpy array values into a DataFrame.\n\n    Args:\n    - series (pd.Series): A pandas Series where each value is a numpy array.\n\n    Returns:\n    - pd.DataFrame: A DataFrame with the arrays expanded into columns, maintaining the original Series index.\n    \"\"\"\n    return pd.DataFrame(series.tolist(), index=series.index)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 246, "library_problem_id": 246, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 246}}
{"id": 247, "code": ["<code>\ndf = pd.DataFrame(series.to_list(), index=series.index).reset_index().rename(columns={'index':'name'})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 247, "library_problem_id": 247, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 246}}
{"id": 248, "code": ["<code>\ndef find_columns_containing_substring(df: pd.DataFrame, substring: str) -> List[str]:\n    # Initialize an empty list to store column names that contain the substring\n    matching_columns = []\n    \n    # Iterate over each column name in the DataFrame\n    for column in df.columns:\n        # Check if the substring is present in the column name\n        if substring in column:\n            # If present, add the column name to the list\n            matching_columns.append(column)\n    \n    # Return the list of matching column names\n    return matching_columns\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 248, "library_problem_id": 248, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 248}}
{"id": 249, "code": ["<code>\ndef find_columns_containing_substring(df: pd.DataFrame, substring: str) -> List[str]:\n    return [col for col in df.columns if substring in col and col != substring]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 249, "library_problem_id": 249, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 248}}
{"id": 250, "code": ["<code>\ndef rename_columns_containing_substring(df: pd.DataFrame, substring: str) -> pd.DataFrame:\n    # Initialize a counter for renaming\n    counter = 1\n    \n    # Create a dictionary to map old column names to new names\n    rename_dict = {}\n    \n    # Iterate over each column name in the DataFrame\n    for col in df.columns:\n        # Check if the substring is present in the column name\n        if substring in col:\n            # Map the old column name to a new sequential name\n            rename_dict[col] = f\"{substring}{counter}\"\n            # Increment the counter for the next match\n            counter += 1\n    \n    # Rename the columns in the DataFrame using the mapping dictionary\n    renamed_df = df.rename(columns=rename_dict)\n    \n    return renamed_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 250, "library_problem_id": 250, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 248}}
{"id": 251, "code": ["<code>\ndef transform_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Transforms a DataFrame by splitting lists in the 'codes' column into separate columns.\n\n    Args:\n    - df (pd.DataFrame): A DataFrame with a single column 'codes' containing lists of integers.\n\n    Returns:\n    - pd.DataFrame: A new DataFrame with each list element in 'codes' as a separate column, filled with NaN where applicable.\n    \"\"\"\n    # Get the maximum length of the lists in the 'codes' column\n    max_len = max(df['codes'].apply(len))\n\n    # Create a new DataFrame with NaN values\n    new_df = pd.DataFrame(np.nan, index=df.index, columns=['code_' + str(i) for i in range(max_len)])\n\n    # Iterate over the 'codes' column and fill the new DataFrame with the list elements\n    for i, row in df.iterrows():\n        for j, code in enumerate(row['codes']):\n            new_df.loc[i, 'code_' + str(j)] = code\n\n    return new_df\n\nresult = transform_dataframe(df)\n```\nEND SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 251, "library_problem_id": 251, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 251}}
{"id": 252, "code": ["<code>\ndef split_codes_to_columns(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Splits the lists in the 'codes' column of the DataFrame into separate columns.\n\n    Args:\n    df (pd.DataFrame): A DataFrame with a single column 'codes', where each entry is a list of numerical codes.\n\n    Returns:\n    pd.DataFrame: A DataFrame with each list element in 'codes' split into separate columns, filled with NaN where necessary.\n    \"\"\"\n    # Create a new DataFrame with the maximum length of the lists in the 'codes' column\n    max_len = max([len(lst) for lst in df['codes']])\n    new_df = pd.DataFrame({f'code_{i+1}': [np.nan]*max_len for i in range(max_len)})\n\n    # Fill the new DataFrame with the values from the 'codes' column\n    for i, lst in enumerate(df['codes']):\n        for j, val in enumerate(lst):\n            new_df.loc[i, f'code_{j+1}'] = val\n\n    return new_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 252, "library_problem_id": 252, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 251}}
{"id": 253, "code": ["<code>\ndef transform_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    # Ensure each list in 'codes' is sorted\n    df['codes'] = df['codes'].apply(sorted)\n    \n    # Determine the maximum length of lists in 'codes'\n    max_length = df['codes'].apply(len).max()\n    \n    # Split lists into separate columns and fill missing values with NaN\n    codes_expanded = df['codes'].apply(lambda x: pd.Series(x + [float('nan')] * (max_length - len(x))))\n    \n    # Rename columns to 'code_1', 'code_2', ..., 'code_n'\n    codes_expanded.columns = [f'code_{i+1}' for i in range(max_length)]\n    \n    return codes_expanded\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 253, "library_problem_id": 253, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 251}}
{"id": 254, "code": ["<code>\nimport pandas as pd\nimport ast\n\ndef concatenate_list_column(df: pd.DataFrame, column_name: str) -> list:\n    concatenated_list = []\n    for item in df[column_name]:\n        # Convert the string representation of a list to an actual list\n        list_item = ast.literal_eval(item)\n        # Extend the concatenated list with the elements of the current list\n        concatenated_list.extend(list_item)\n    return concatenated_list\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 254, "library_problem_id": 254, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 254}}
{"id": 255, "code": ["<code>\nimport pandas as pd\nfrom typing import List\nfrom ast import literal_eval\n\ndef concatenate_reversed_lists(df: pd.DataFrame, column_name: str) -> str:\n    \"\"\"\n    Reverses each list in the specified DataFrame column and concatenates them into a single string.\n\n    Args:\n    - df (pd.DataFrame): The DataFrame containing the column with list values.\n    - column_name (str): The name of the column containing lists to be processed.\n\n    Returns:\n    - str: A single string with all reversed lists concatenated, separated by commas.\n    \"\"\"\n    # Initialize an empty list to store reversed lists\n    reversed_lists = []\n\n    # Iterate over each row in the specified column\n    for list_str in df[column_name]:\n        # Convert the string representation of the list to an actual list\n        actual_list = literal_eval(list_str)\n        \n        # Reverse the list and extend the reversed_lists\n        reversed_lists.extend(reversed(actual_list))\n    \n    # Join all elements in reversed_lists into a single comma-separated string\n    result = ','.join(map(str, reversed_lists))\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 255, "library_problem_id": 255, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 256, "code": ["<code>\nimport pandas as pd\n\ndef concatenate_lists_to_string(df: pd.DataFrame, column_name: str) -> str:\n    # Extract the column with list values\n    list_column = df[column_name]\n    \n    # Flatten the list of lists into a single list of integers\n    flattened_list = [item for sublist in list_column for item in sublist]\n    \n    # Convert the list of integers to a comma-separated string\n    result = ','.join(map(str, flattened_list))\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 256, "library_problem_id": 256, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 254}}
{"id": 257, "code": ["<code>\ndef bin_and_average_time_series(df: pd.DataFrame, bin_size: str) -> pd.DataFrame:\n    # Ensure 'Time' column is datetime\n    df['Time'] = pd.to_datetime(df['Time'])\n    \n    # Set 'Time' as the index\n    df.set_index('Time', inplace=True)\n    \n    # Resample the data into bins and calculate the mean for each bin\n    binned_df = df.resample(bin_size).mean().reset_index()\n    \n    return binned_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 257, "library_problem_id": 257, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 257}}
{"id": 258, "code": ["<code>\ndef bin_and_sum_time_series(df: pd.DataFrame, interval: str) -> pd.DataFrame:\n    # Ensure 'Time' column is in datetime format\n    df['Time'] = pd.to_datetime(df['Time'])\n    \n    # Set 'Time' as the DataFrame index\n    df.set_index('Time', inplace=True)\n    \n    # Resample the data into specified intervals and sum the values\n    binned_df = df.resample(interval).sum()\n    \n    # Reset index to convert 'Time' back to a column\n    binned_df.reset_index(inplace=True)\n    \n    return binned_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 258, "library_problem_id": 258, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 257}}
{"id": 259, "code": ["<code>\nimport pandas as pd\n\ndef rank_datetimes(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Rank datetime entries within a DataFrame based on their timestamps, grouped by 'ID'.\n\n    Args:\n    df (pd.DataFrame): A DataFrame containing 'ID' and 'TIME' columns.\n\n    Returns:\n    pd.DataFrame: The original DataFrame with an additional 'RANK' column indicating the rank of each 'TIME' entry within its 'ID' group.\n    \"\"\"\n    # Ensure 'TIME' column is in datetime format\n    df['TIME'] = pd.to_datetime(df['TIME'])\n\n    # Rank 'TIME' within each 'ID' group\n    df['RANK'] = df.groupby('ID')['TIME'].rank(ascending=True)\n\n    return df\n\n# Example usage\ndf = pd.DataFrame({\n    'ID': ['01', '01', '01', '02', '02'],\n    'TIME': ['2018-07-11 11:12:20', '2018-07-12 12:00:23', '2018-07-13 12:00:00', '2019-09-11 11:00:00', '2019-09-12 12:00:00']\n})\n\ndf = rank_datetimes(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 259, "library_problem_id": 259, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 259}}
{"id": 260, "code": ["<code>\ndef rank_datetime_entries(df: pd.DataFrame) -> pd.DataFrame:\n    # Convert 'TIME' column to datetime format\n    df['TIME'] = pd.to_datetime(df['TIME'], errors='raise')\n    \n    # Rank 'TIME' within each 'ID' group in descending order\n    df['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 260, "library_problem_id": 260, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 259}}
{"id": 261, "code": ["<code>\nimport pandas as pd\n\n# Create the DataFrame\ndf = pd.DataFrame({\n    'ID': ['01', '01', '01', '02', '02'],\n    'TIME': ['2018-07-11 11:12:20', '2018-07-12 12:00:23', '2018-07-13 12:00:00', '2019-09-11 11:00:00', '2019-09-12 12:00:00']\n})\n\n# Convert 'TIME' to datetime format\ndf['TIME'] = pd.to_datetime(df['TIME'])\n\n# Rank 'TIME' in descending order for each 'ID'\ndf['RANK'] = df.groupby('ID')['TIME'].rank(ascending=False)\n\n# Format 'TIME' as \"DD-MMM-YYYY DDD HH:MM:SS\"\ndf['TIME'] = df['TIME'].dt.strftime('%d-%b-%Y %a %H:%M:%S')\n\n# The DataFrame 'df' now contains the ranked and formatted 'TIME' column\ndf\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 261, "library_problem_id": 261, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 259}}
{"id": 262, "code": ["<code>\ndef filter_dataframe_by_series(df: pd.DataFrame, filt: pd.Series) -> pd.DataFrame:\n    return df.loc[filt.index.intersection(df.index.get_level_values(0))]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 262, "library_problem_id": 262, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 262}}
{"id": 263, "code": ["<code>\ndef filter_dataframe_by_series(df: pd.DataFrame, filt: pd.Series) -> pd.DataFrame:\n    # Ensure the index level to filter by is the same as the Series index\n    index_level_to_filter = df.index.get_level_values(0)\n    \n    # Apply the boolean series to filter the DataFrame\n    filtered_df = df[index_level_to_filter.isin(filt.index[filt])]\n    \n    return filtered_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 263, "library_problem_id": 263, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 262}}
{"id": 264, "code": ["<code>\ndef find_differing_columns(df: pd.DataFrame, row_index1: int, row_index2: int) -> pd.Index:\n    # Extract the two rows to compare\n    row1 = df.iloc[row_index1]\n    row2 = df.iloc[row_index2]\n    \n    # Identify columns where values differ, treating NaNs as equal\n    differing_columns = row1.index[~((row1 == row2) | (row1.isna() & row2.isna()))]\n    \n    return differing_columns\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 264, "library_problem_id": 264, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 264}}
{"id": 265, "code": ["<code>\n```python\n\"\"\"\nFunction to compare two rows in a pandas DataFrame and identify columns where the values are considered equal.\nNaN values in the same location are treated as equal, similar to pandas.DataFrame.equals.\n\nArgs:\n    df (pd.DataFrame): The DataFrame containing the data.\n    row_index_1 (int): The index of the first row to compare.\n    row_index_2 (int): The index of the second row to compare.\n\nReturns:\n    pd.Index: An Index object containing the column names where the values in the two rows are considered equal.\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\n\ndef compare_rows(df: pd.DataFrame, row_index_1: int, row_index_2: int) -> pd.Index:\n    # Extract the two rows to compare\n    row1 = df.iloc[row_index_1]\n    row2 = df.iloc[row_index_2]\n    \n    # Identify columns where values are equal or both are NaN\n    equal_columns = row1.index[(row1 == row2) | (row1.isna() & row2.isna())]\n    \n    return equal_columns\n\n# Example usage\nnp.random.seed(10)\ndf = pd.DataFrame(np.random.randint(0, 20, (10, 10)).astype(float), columns=[\"c%d\" % d for d in range(10)])\ndf.where(np.random.randint(0, 2, df.shape).astype(bool),\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 265, "library_problem_id": 265, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 266, "code": ["<code>\ndef find_differing_columns(df: pd.DataFrame, row1: int, row2: int) -> list:\n    differing_columns = []\n    for col in df.columns:\n        val1, val2 = df.at[row1, col], df.at[row2, col]\n        if not ((pd.isna(val1) and pd.isna(val2)) or (val1 == val2)):\n            differing_columns.append(col)\n    return differing_columns\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 266, "library_problem_id": 266, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 264}}
{"id": 267, "code": ["<code>\ndef compare_rows(df: pd.DataFrame, row_index_1: int, row_index_2: int) -> list:\n    row1 = df.iloc[row_index_1]\n    row2 = df.iloc[row_index_2]\n    differing_columns = []\n\n    for col in df.columns:\n        val1 = row1[col]\n        val2 = row2[col]\n        if not (pd.isna(val1) and pd.isna(val2)) and val1 != val2:\n            differing_columns.append((val1, val2))\n\n    return differing_columns\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 267, "library_problem_id": 267, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 264}}
{"id": 268, "code": ["<code>\nimport pandas as pd\n\ndef dataframe_to_series(df: pd.DataFrame) -> pd.Series:\n    # Check if the DataFrame contains the required columns\n    if 'Date' not in df.columns or 'Value' not in df.columns:\n        raise ValueError(\"DataFrame must contain 'Date' and 'Value' columns.\")\n    \n    # Convert 'Date' column to datetime format\n    df['Date'] = pd.to_datetime(df['Date'])\n    \n    # Create a Series with 'Date' as the index and 'Value' as the data\n    ts = pd.Series(df['Value'].values, index=df['Date'])\n    \n    return ts\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 268, "library_problem_id": 268, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 268}}
{"id": 269, "code": ["<code>\ndef transform_dataframe_to_single_row(df: pd.DataFrame) -> pd.DataFrame:\n    new_data = {}\n    for index, row in df.iterrows():\n        for col in df.columns:\n            new_col_name = f\"{col}_{index + 1}\"\n            new_data[new_col_name] = row[col]\n    result_df = pd.DataFrame([new_data])\n    return result_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 269, "library_problem_id": 269, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 269}}
{"id": 270, "code": ["<code>\ndef flatten_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    # Create a dictionary to hold the new column names and values\n    data = {}\n    \n    # Iterate over each row in the DataFrame\n    for row_index in range(df.shape[0]):\n        # Iterate over each column in the DataFrame\n        for col_name in df.columns:\n            # Construct the new column name by appending the row index\n            new_col_name = f\"{col_name}_{row_index}\"\n            # Assign the value to the new column name in the dictionary\n            data[new_col_name] = df.at[row_index, col_name]\n    \n    # Convert the dictionary to a DataFrame with a single row\n    return pd.DataFrame([data])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 270, "library_problem_id": 270, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 269}}
{"id": 271, "code": ["<code>\ndf['dogs'] = df['dogs'].apply(lambda x: round(x, 2) if pd.notna(x) else x)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 271, "library_problem_id": 271, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 271}}
{"id": 272, "code": ["<code>\ndef round_dataframe(df: pd.DataFrame, decimals: int) -> pd.DataFrame:\n    return df.apply(lambda col: col.round(decimals) if col.dtype == 'float64' else col)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 272, "library_problem_id": 272, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 271}}
{"id": 273, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\n# Set random seed for reproducibility\nnp.random.seed(10)\n\n# Create a DataFrame with random integer values\ndata = {}\nfor i in [chr(x) for x in range(65, 91)]:\n    data['Col ' + i] = np.random.randint(1, 100, 10)\ndf = pd.DataFrame(data)\n\n# Define the list of column names to sum\nlist_of_my_columns = ['Col A', 'Col E', 'Col Z']\n\n# Calculate the sum of specified columns for each row\ndf['Sum'] = df[list_of_my_columns].sum(axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 273, "library_problem_id": 273, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 273}}
{"id": 274, "code": ["<code>\n# Calculate the row-wise average of specified columns\ndf['Avg'] = df[list_of_my_columns].mean(axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 274, "library_problem_id": 274, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 273}}
{"id": 275, "code": ["<code>\ndef compute_statistics(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n    \"\"\"\n    Compute row-wise statistical measures (average, minimum, maximum, median) for specified columns in a DataFrame.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data.\n        columns (list): A list of column names for which to compute the statistics.\n\n    Returns:\n        pd.DataFrame: A DataFrame with additional columns ['Avg', 'Min', 'Max', 'Median'] representing the computed statistics.\n    \"\"\"\n    # Calculate the average, minimum, maximum, and median for each row\n    df['Avg'] = df[columns].mean(axis=1)\n    df['Min'] = df[columns].min(axis=1)\n    df['Max'] = df[columns].max(axis=1)\n    df['Median'] = df[columns].median(axis=1)\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 275, "library_problem_id": 275, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 273}}
{"id": 276, "code": ["<code>\ndef sort_multiindex_dataframe(df: pd.DataFrame, sort_level: str) -> pd.DataFrame:\n    # Sort the DataFrame by the specified index level using a stable sort\n    sorted_df = df.sort_index(level=sort_level, sort_remaining=False)\n    return sorted_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 276, "library_problem_id": 276, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 276}}
{"id": 277, "code": ["<code>\ndef sort_dataframe_by_vim(df: pd.DataFrame) -> pd.DataFrame:\n    # Sort the DataFrame by 'VIM' column in ascending order\n    # Use 'time' index as a secondary sort key to maintain original order for same 'VIM' values\n    sorted_df = df.sort_values(by=['VIM', 'time'], ascending=[True, True], kind='mergesort')\n    return sorted_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 277, "library_problem_id": 277, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 276}}
{"id": 278, "code": ["<code>\ndef remove_days(df: pd.DataFrame, days_to_remove: list) -> pd.DataFrame:\n    # Convert the list of days to remove into a set of datetime objects for faster lookup\n    days_to_remove_set = set(pd.to_datetime(days_to_remove))\n    \n    # Filter the DataFrame by checking if the date part of the index is not in the days_to_remove_set\n    filtered_df = df[~df.index.normalize().isin(days_to_remove_set)]\n    \n    return filtered_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 278, "library_problem_id": 278, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 278}}
{"id": 279, "code": ["<code>\ndef process_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n    # Define the days to be removed\n    days_to_remove = ['2020-02-17', '2020-02-18']\n    \n    # Remove specified days\n    df = df[~df.index.normalize().isin(pd.to_datetime(days_to_remove))]\n    \n    # Format the 'Date' index to include the day of the week\n    df.index = df.index.strftime('%d-%b-%Y %A')\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 279, "library_problem_id": 279, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 278}}
{"id": 280, "code": ["<code>\nimport pandas as pd\n\ndef extract_significant_correlations(corr_matrix: pd.DataFrame, threshold: float = 0.3) -> pd.DataFrame:\n    # Mask the upper triangle and diagonal of the correlation matrix to avoid duplicate pairs and self-correlation\n    mask = corr_matrix.where(~pd.np.tril(pd.np.ones(corr_matrix.shape)).astype(bool))\n    \n    # Stack the masked matrix to get a Series of correlation values with multi-index\n    stacked_corr = mask.stack()\n    \n    # Filter the Series to include only values above the specified threshold\n    significant_corr = stacked_corr[stacked_corr > threshold]\n    \n    # Convert the filtered Series to a DataFrame with appropriate column name\n    result_df = significant_corr.reset_index()\n    result_df.columns = ['Col1', 'Col2', 'Pearson Correlation Coefficient']\n    \n    # Set the multi-index for the resulting DataFrame\n    result_df.set_index(['Col1', 'Col2'], inplace=True)\n    \n    return result_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 280, "library_problem_id": 280, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 280}}
{"id": 281, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef extract_high_correlations(corr_matrix: pd.DataFrame, threshold: float = 0.3) -> pd.Series:\n    # Mask the diagonal to avoid self-correlation\n    mask = pd.DataFrame(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool), index=corr_matrix.index, columns=corr_matrix.columns)\n    \n    # Apply the mask and filter values above the threshold\n    filtered_values = corr_matrix.where(mask).stack()\n    return filtered_values[filtered_values > threshold]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 281, "library_problem_id": 281, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 280}}
{"id": 282, "code": ["<code>\ndef rename_last_column(df: pd.DataFrame, new_name: str) -> pd.DataFrame:\n    # Copy the DataFrame to avoid modifying the original\n    df_copy = df.copy()\n    \n    # Get the current list of column names\n    columns = df_copy.columns.tolist()\n    \n    # Rename the last column\n    columns[-1] = new_name\n    \n    # Assign the new column names back to the DataFrame\n    df_copy.columns = columns\n    \n    return df_copy\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 282, "library_problem_id": 282, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 282}}
{"id": 283, "code": ["<code>\ndef rename_first_column(df: pd.DataFrame, new_name: str) -> pd.DataFrame:\n    # Create a copy of the DataFrame to avoid modifying the original\n    df_copy = df.copy()\n    \n    # Rename the first column\n    df_copy.columns = [new_name] + list(df_copy.columns[1:])\n    \n    return df_copy\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 283, "library_problem_id": 283, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 282}}
{"id": 284, "code": ["<code>\ndef calculate_frequent_values(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculate the most frequent binary value and its count for each row in the DataFrame.\n\n    Args:\n        df (pd.DataFrame): A DataFrame containing binary values in columns.\n\n    Returns:\n        pd.DataFrame: The original DataFrame with two additional columns:\n                      - 'frequent': The most frequent binary value in each row.\n                      - 'freq_count': The count of the most frequent binary value in each row.\n    \"\"\"\n    # Calculate the most frequent value and its count for each row\n    df['frequent'] = df.mode(axis=1)[0]\n    df['freq_count'] = df.apply(lambda row: (row == row['frequent']).sum(), axis=1)\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 284, "library_problem_id": 284, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 284}}
{"id": 285, "code": ["<code>\nimport pandas as pd\n\ndef compute_frequent_values(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Computes the most frequent value and its count for each row in the DataFrame.\n\n    Args:\n    df (pd.DataFrame): A DataFrame containing integer values across multiple columns.\n\n    Returns:\n    pd.DataFrame: The original DataFrame with two additional columns:\n                  - 'frequent': The most frequent value in each row.\n                  - 'freq_count': The count of the most frequent value in each row.\n    \"\"\"\n    # Compute the most frequent value and its count for each row\n    df['frequent'] = df.mode(axis=1)[0]\n    df['freq_count'] = df.apply(lambda row: (row == row['frequent']).sum(), axis=1)\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 285, "library_problem_id": 285, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 284}}
{"id": 286, "code": ["<code>\nimport pandas as pd\nfrom collections import Counter\n\ndef add_frequent_columns(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Add 'frequent' and 'freq_count' columns to the DataFrame, indicating the most frequent value(s) in each row\n    and their frequency count.\n\n    Args:\n    df (pd.DataFrame): A DataFrame containing integer values across multiple columns.\n\n    Returns:\n    pd.DataFrame: The original DataFrame with two additional columns:\n                  - 'frequent': A list of the most frequent value(s) in each row.\n                  - 'freq_count': The count of the most frequent value(s) in each row.\n    \"\"\"\n    def find_frequent_values(row):\n        count = Counter(row)\n        max_freq = max(count.values())\n        frequent_values = [val for val, freq in count.items() if freq == max_freq]\n        return frequent_values, max_freq\n\n    df['frequent'], df['freq_count'] = zip(*df.apply(find_frequent_values, axis=1))\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 286, "library_problem_id": 286, "library": "Pandas", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 284}}
{"id": 287, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef calculate_grouped_means(df: pd.DataFrame) -> pd.DataFrame:\n    # Replace 'NULL' with NaN for proper numerical operations\n    df['bar'] = df['bar'].replace('NULL', np.nan).astype(float)\n    \n    # Group by 'id1' and 'id2' and calculate the mean of 'foo' and 'bar'\n    result = df.groupby(['id1', 'id2'])[['foo', 'bar']].mean()\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 287, "library_problem_id": 287, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 287}}
{"id": 288, "code": ["<code>\ndef calculate_grouped_means(df: pd.DataFrame) -> pd.DataFrame:\n    # Replace 'NULL' with 0 in the 'bar' column\n    df['bar'] = df['bar'].replace('NULL', 0).astype(float)\n    \n    # Group by 'id1' and 'id2' and calculate the mean for 'foo' and 'bar'\n    result = df.groupby(['id1', 'id2'], as_index=True)[['foo', 'bar']].mean()\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 288, "library_problem_id": 288, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 287}}
{"id": 289, "code": ["<code>\n# Perform the merge operation\nresult = pd.merge(df_a, df_b[['EntityNum', 'a_col']], on='EntityNum')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 289, "library_problem_id": 289, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 289}}
{"id": 290, "code": ["<code>\nimport pandas as pd\n\ndef merge_dataframes(df_a: pd.DataFrame, df_b: pd.DataFrame) -> pd.DataFrame:\n    # Merge the DataFrames on 'EntityNum' and select only the required columns\n    result = pd.merge(df_a, df_b[['EntityNum', 'b_col']], on='EntityNum')[['EntityNum', 'foo', 'b_col']]\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 290, "library_problem_id": 290, "library": "Pandas", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 289}}
{"id": 291, "code": ["<code>\ndef get_array_dimensions(a: np.ndarray) -> tuple:\n    \"\"\"\n    Determine the dimensions of a given NumPy array.\n\n    Parameters:\n    - a (np.ndarray): The input NumPy array.\n\n    Returns:\n    - tuple: A tuple representing the dimensions of the array.\n    \"\"\"\n    result = a.shape\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 291, "library_problem_id": 0, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 292, "code": ["<code>\nimport numpy as np\n\ndef remove_nan_values(arr: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Removes all NaN values from the input numpy array.\n\n    Parameters:\n    arr (np.ndarray): A numpy array containing numerical values and NaN values.\n\n    Returns:\n    np.ndarray: A new numpy array with all NaN values removed.\n    \"\"\"\n    return arr[~np.isnan(arr)]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 292, "library_problem_id": 1, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 293, "code": ["<code>\nimport numpy as np\n\ndef replace_nan_with_inf(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Replace all NaN values in the input NumPy array with np.inf.\n\n    Parameters:\n    x (np.ndarray): A NumPy array of floats, potentially containing NaN values.\n\n    Returns:\n    np.ndarray: A new NumPy array where all NaN values are replaced with np.inf.\n    \"\"\"\n    # Use np.isnan to create a boolean mask identifying NaN values\n    nan_mask = np.isnan(x)\n    \n    # Replace NaN values with np.inf using the boolean mask\n    x[nan_mask] = np.inf\n    \n    return x\n\n# Example usage:\nx = np.array([1400, 1500, 1600, np.nan, np.nan, np.nan, 1700])\nx = replace_nan_with_inf(x)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 293, "library_problem_id": 2, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 294, "code": ["<code>\nimport numpy as np\n\ndef remove_nan_and_convert_to_list(x: np.ndarray) -> list:\n    result = []\n    for sub_array in x:\n        filtered_sub_array = [value for value in sub_array if not np.isnan(value)]\n        result.append(filtered_sub_array)\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 294, "library_problem_id": 3, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 1}}
{"id": 295, "code": ["<code>\nimport numpy as np\n\ndef one_hot_encode(a: np.ndarray) -> np.ndarray:\n    # Determine the number of columns for the one-hot encoding\n    num_columns = np.max(a) + 1\n    \n    # Create a 2D array of zeros with shape (len(a), num_columns)\n    b = np.zeros((len(a), num_columns), dtype=int)\n    \n    # Use numpy advanced indexing to set the appropriate elements to 1\n    b[np.arange(len(a)), a] = 1\n    \n    return b\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 295, "library_problem_id": 4, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 296, "code": ["<code>\ndef one_hot_encode(a):\n    max_val = np.max(a)\n    b = np.zeros((len(a), max_val+1))\n    b[np.arange(len(a)), a] = 1\n    return b\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 296, "library_problem_id": 5, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 297, "code": ["<code>\n```python\n\"\"\"\nFunction to convert a 1D numpy integer array into a 2D one-hot encoded array.\n\nThis function takes a 1D numpy array 'a' of integers and returns a 2D numpy array 'b' where each row corresponds to a one-hot encoded representation of the elements in 'a'. The one-hot encoding is determined by the range of values in 'a', with the leftmost column corresponding to the smallest value in 'a' and the rightmost column corresponding to the largest value in 'a'.\n\nParameters:\n    a (np.ndarray): A 1D numpy array of integers.\n\nReturns:\n    np.ndarray: A 2D numpy array representing the one-hot encoded version of 'a'.\n\nExample:\n    Given a 1D array a = np.array([-1, 0, 3]), the function will return:\n    np.array([[1, 0, 0, 0, 0],\n              [0, 1, 0, 0, 0],\n              [0, 0, 0, 0, 1]])\n\nConstraints:\n    - The function should only use numpy operations for efficiency.\n    - The one-hot encoding should be based on the range of values in 'a'.\n\n\"\"\"\nimport numpy as np\n\ndef one_hot_encode(a: np.ndarray) -> np.ndarray:\n    # Determine the range of values in 'a'\n    min_val = np.min(a)\n    max_val = np.max(a)\n    \n    # Calculate the number of columns for the one-hot encoding\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 297, "library_problem_id": 6, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 298, "code": ["<code>\n# Determine the unique elements in `a` and sort them in ascending order\nunique_elements = np.sort(np.unique(a))\n\n# Create a 2D array `b` with shape (len(a), len(unique_elements))\nb = np.zeros((len(a), len(unique_elements)))\n\n# For each element in `a`, set the corresponding position in `b` to 1 based on the sorted order of unique elements\nb[np.arange(len(a))[:, None], a[None, :] >= unique_elements] = 1\n\nreturn b\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 298, "library_problem_id": 7, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 299, "code": ["<code>\ndef one_hot_encode_2d_array(a: np.ndarray) -> np.ndarray:\n    # Flatten the input array to a 1D array\n    flat_a = a.flatten()\n    \n    # Determine the number of unique integers in the array\n    num_classes = np.max(flat_a) + 1\n    \n    # Create the one-hot encoded array\n    b = np.eye(num_classes, dtype=int)[flat_a]\n    \n    return b\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 299, "library_problem_id": 8, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 300, "code": ["<code>\ndef calculate_percentile(data: np.ndarray, percentile: float) -> float:\n    # Ensure the percentile is within the valid range\n    if not (0 <= percentile <= 100):\n        raise ValueError(\"Percentile must be between 0 and 100.\")\n    \n    # Calculate and return the desired percentile\n    return np.percentile(data, percentile)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 300, "library_problem_id": 9, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 301, "code": ["<code>\nimport numpy as np\n\ndef vec2matrix(array: np.ndarray, ncol: int) -> np.ndarray:\n    if len(array) % ncol != 0:\n        raise ValueError(\"The total number of elements in the array must be divisible by ncol.\")\n    return array.reshape(-1, ncol)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 301, "library_problem_id": 10, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 302, "code": ["<code>\nimport numpy as np\n\ndef vec2matrix(array: np.ndarray, nrow: int) -> np.ndarray:\n    if len(array) % nrow != 0:\n        raise ValueError(\"The total number of elements in the array must be divisible by the number of rows.\")\n    ncol = len(array) // nrow\n    return array.reshape(nrow, ncol)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 302, "library_problem_id": 11, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 303, "code": ["<code>\nimport numpy as np\n\ndef vec2matrix(arr: np.ndarray, ncol: int) -> np.ndarray:\n    # Calculate the number of rows by using integer division\n    nrow = len(arr) // ncol\n    \n    # Reshape the array, discarding excess elements\n    return arr[:nrow * ncol].reshape(nrow, ncol)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 303, "library_problem_id": 12, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 304, "code": ["<code>\ndef vec2matrix(array: np.ndarray, ncol: int) -> np.ndarray:\n    # Reverse the array\n    reversed_array = array[::-1]\n    \n    # Calculate the number of rows for the 2D array\n    nrow = len(reversed_array) // ncol\n    \n    # Reshape the reversed array into a 2D array with the specified number of columns\n    return reversed_array[:nrow * ncol].reshape(nrow, ncol)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 304, "library_problem_id": 13, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 10}}
{"id": 305, "code": ["<code>\nimport numpy as np\n\ndef shift_array(array: np.ndarray, shift: int) -> np.ndarray:\n    result = np.empty_like(array)\n    if shift > 0:\n        result[:shift] = np.nan\n        result[shift:] = array[:-shift]\n    elif shift < 0:\n        result[shift:] = np.nan\n        result[:shift] = array[-shift:]\n    else:\n        result[:] = array\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 305, "library_problem_id": 14, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 306, "code": ["<code>\nimport numpy as np\n\ndef shift_array(arr: np.ndarray, shift: int) -> np.ndarray:\n    shifted_arr = np.full_like(arr, np.nan)\n    if shift > 0:\n        shifted_arr[:, shift:] = arr[:, :-shift]\n    elif shift < 0:\n        shifted_arr[:, :shift] = arr[:, -shift:]\n    else:\n        shifted_arr = arr\n    return shifted_arr\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 306, "library_problem_id": 15, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 307, "code": ["<code>\nimport numpy as np\nfrom typing import List\n\ndef shift(array: np.ndarray, shifts: List[int]) -> np.ndarray:\n    # Initialize an array of the same shape as the input array filled with np.nan\n    result = np.full_like(array, np.nan, dtype=float)\n    \n    # Iterate over each row and corresponding shift value\n    for i, (row, shift_value) in enumerate(zip(array, shifts)):\n        if shift_value > 0:\n            # Right shift: fill from the start of the row\n            result[i, shift_value:] = row[:-shift_value]\n        elif shift_value < 0:\n            # Left shift: fill from the end of the row\n            result[i, :shift_value] = row[-shift_value:]\n        else:\n            # No shift: copy the row as is\n            result[i] = row\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 307, "library_problem_id": 16, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 14}}
{"id": 308, "code": ["<code>\ndef generate_reproducible_array(seed: int) -> np.ndarray:\n    np.random.seed(seed)\n    return np.random.randint(3, size=(100, 2000)) - 1\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 308, "library_problem_id": 17, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 309, "code": ["<code>\nimport numpy as np\n\ndef find_largest_value_raveled_index(a: np.ndarray) -> int:\n    # Step 1: Identify the largest value in the array\n    largest_value = np.max(a)\n    \n    # Step 2: Determine the multi-dimensional index of this largest value\n    largest_value_index = np.where(a == largest_value)\n    \n    # Step 3: Convert this multi-dimensional index to a raveled index in C order\n    raveled_index = np.ravel_multi_index(largest_value_index, a.shape, mode='C')\n    \n    return raveled_index\n\n# Example usage\na = np.array([[10, 50, 30], [60, 20, 40]])\nresult = find_largest_value_raveled_index(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 309, "library_problem_id": 18, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 310, "code": ["<code>\ndef find_raveled_index_of_min(a: np.ndarray) -> int:\n    # Flatten the array in C order and find the index of the minimum value\n    return np.argmin(a.ravel(order='C'))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 310, "library_problem_id": 19, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 311, "code": ["<code>\ndef find_max_index(a: np.ndarray) -> tuple:\n    return np.unravel_index(a.argmax(order='F'), a.shape, order='F')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 311, "library_problem_id": 20, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 312, "code": ["<code>\nimport numpy as np\n\ndef find_largest_value_index(a: np.ndarray) -> tuple:\n    # Step 1: Find the index of the largest value in the flattened array\n    flat_index = np.argmax(a)\n    \n    # Step 2: Convert the flat index to a multi-dimensional index in C order\n    unraveled_index = np.unravel_index(flat_index, a.shape)\n    \n    return unraveled_index\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 312, "library_problem_id": 21, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 313, "code": ["<code>\n\"\"\"\nFunction to find the raveled index of the largest value in a multi-dimensional NumPy array.\n\nThis function takes a multi-dimensional NumPy array as input and returns the raveled index \nof the largest value in the array. The raveled index is computed in C order, which means \nthat the array is flattened row-major (row by row).\n\nParameters:\n    a (np.ndarray): A multi-dimensional NumPy array from which to find the largest value's index.\n\nReturns:\n    int: The raveled index of the largest value in the array.\n\nExample:\n    >>> import numpy as np\n    >>> example_a = np.array([[10, 50, 30], [60, 20, 40]])\n    >>> f(example_a)\n    3  # The largest value is 60, which is at index 3 when the array is raveled in C order.\n\"\"\"\nimport numpy as np\n\ndef f(a: np.ndarray = np.array([[10, 50, 30], [60, 20, 40]])) -> int:\n    # Find the index of the largest value in the array\n    max_index = np.argmax(a)\n    # Return the raveled index of the largest value\n    return max_index\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 313, "library_problem_id": 22, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 18}}
{"id": 314, "code": ["<code>\nimport numpy as np\n\ndef find_second_largest_unraveled_index(a: np.ndarray) -> tuple:\n    # Flatten the array to 1D\n    flat_a = a.flatten()\n    \n    # Get unique values and sort them in descending order\n    unique_values = np.unique(flat_a)\n    sorted_values = np.sort(unique_values)[::-1]\n    \n    # Identify the second largest value\n    second_largest_value = sorted_values[1]\n    \n    # Find the index of the second largest value in the flattened array\n    second_largest_index = np.where(flat_a == second_largest_value)[0][0]\n    \n    # Convert the flat index to unraveled index in C order\n    unraveled_index = np.unravel_index(second_largest_index, a.shape, order='C')\n    \n    return unraveled_index\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 314, "library_problem_id": 23, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 18}}
{"id": 315, "code": ["<code>\nimport numpy as np\n\ndef remove_nan_columns(arr: np.ndarray) -> np.ndarray:\n    # Identify columns with NaN values\n    nan_columns = np.any(np.isnan(arr), axis=0)\n    # Remove columns with NaN values\n    result = arr[:, ~nan_columns]\n    return result\n\n# Example usage\na = np.array([[np.nan, 2., 3., np.nan],\n              [1., 2., 3., 9.]])\nresult = remove_nan_columns(a)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 315, "library_problem_id": 24, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 316, "code": ["<code>\n\"\"\"\nFunction to remove rows containing NaN values from a numpy array.\n\nThis function takes a numpy array as input and returns a new array with all rows containing \nat least one NaN value removed. The function utilizes numpy's capabilities to efficiently \nidentify and filter out rows with NaN values.\n\nArgs:\n    a (np.ndarray): A 2D numpy array from which rows containing NaN values need to be removed.\n\nReturns:\n    np.ndarray: A new 2D numpy array with rows containing NaN values removed.\n\nExample:\n    >>> import numpy as np\n    >>> a = np.array([[np.nan, 2., 3., np.nan],\n    ...               [1., 2., 3., 9.]])\n    >>> result = remove_nan_rows(a)\n    >>> print(result)\n    array([[1., 2., 3., 9.]])\n\nLogical Conditions:\n- A row is considered for removal if any element in the row is NaN.\n- The function uses numpy's isnan() to identify NaN values and logical_not() to filter them out.\n\"\"\"\n\nimport numpy as np\n\ndef remove_nan_rows(a: np.ndarray) -> np.ndarray:\n    return a[~np.isnan(a).any(axis=1)]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 316, "library_problem_id": 25, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 24}}
{"id": 317, "code": ["<code>\nimport numpy as np\n\ndef convert_to_numpy_array(a: list[list[int]]) -> np.ndarray:\n    result = np.array(a)\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 317, "library_problem_id": 26, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 318, "code": ["<code>\nimport numpy as np\nfrom typing import List\n\ndef reorder_columns(a: np.ndarray, permutation: List[int]) -> np.ndarray:\n    return a[:, permutation]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 318, "library_problem_id": 27, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 319, "code": ["<code>\nimport numpy as np\n\ndef reorder_matrices(a: np.ndarray, permutation: list[int]) -> np.ndarray:\n    \"\"\"\n    Reorders the matrices in a 3D NumPy array according to a specified permutation.\n\n    Args:\n    a (np.ndarray): A 3D NumPy array where each element along the first axis is a 2D matrix.\n    permutation (list[int]): A list of integers representing the new order of the matrices.\n\n    Returns:\n    np.ndarray: A new 3D NumPy array with matrices reordered according to the permutation.\n    \"\"\"\n    return a[permutation]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 319, "library_problem_id": 28, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 27}}
{"id": 320, "code": ["<code>\nimport numpy as np\n\ndef find_min_index(arr: np.ndarray) -> tuple[int, int]:\n    # Flatten the array to find the index of the minimum value\n    min_index_flat = np.argmin(arr)\n    # Convert the flattened index to a 2D index (row, column)\n    min_index_2d = np.unravel_index(min_index_flat, arr.shape)\n    return min_index_2d\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 320, "library_problem_id": 29, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 321, "code": ["<code>\nimport numpy as np\n\ndef find_max_index(a: np.ndarray) -> tuple:\n    # Flatten the array to find the index of the maximum value\n    max_index_flat = np.argmax(a)\n    # Convert the flat index to a 2D index (row, column)\n    row_index, column_index = np.unravel_index(max_index_flat, a.shape)\n    return (row_index, column_index)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 321, "library_problem_id": 30, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 29}}
{"id": 322, "code": ["<code>\nresult = []\n    min_value = np.min(array)\n    for row_index, row in enumerate(array):\n        for column_index, value in enumerate(row):\n            if value == min_value:\n                result.append([row_index, column_index])\n    result.sort(key=lambda x: (x[0], x[1]))\n    return result\n</code>\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 322, "library_problem_id": 31, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 29}}
{"id": 323, "code": ["<code>\nimport numpy as np\n\ndef calculate_sine_in_degrees(degree: float) -> float:\n    radians = np.radians(degree)  # Convert degrees to radians\n    sine_value = np.sin(radians)  # Calculate sine of the angle in radians\n    return sine_value\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 323, "library_problem_id": 32, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 324, "code": ["<code>\nimport numpy as np\n\ndef compute_cosine_in_degrees(degree: float) -> float:\n    radians = np.radians(degree)  # Step 1: Convert degrees to radians\n    cosine_value = np.cos(radians)  # Step 2: Compute cosine in radians\n    return cosine_value  # Step 3: Return the result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 324, "library_problem_id": 33, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 325, "code": ["<code>\nimport numpy as np\n\ndef classify_angle(angle: float) -> int:\n    # Calculate sine assuming the angle is in degrees\n    sine_degrees = np.sin(np.radians(angle))\n    \n    # Calculate sine assuming the angle is in radians\n    sine_radians = np.sin(angle)\n    \n    # Compare sine values and determine if the angle is in degrees or radians\n    if sine_degrees > sine_radians:\n        return 0  # Angle is in degrees\n    else:\n        return 1  # Angle is in radians\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 325, "library_problem_id": 34, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 326, "code": ["<code>\nimport numpy as np\n\ndef calculate_angle_from_sine(sine_value: float) -> float:\n    \"\"\"\n    Calculate the angle in degrees corresponding to a given sine value.\n\n    Args:\n    sine_value (float): The sine value for which the corresponding angle is to be calculated. \n                        The value should be within the range [-1, 1].\n\n    Returns:\n    float: The angle in degrees corresponding to the given sine value, within the range [-90, 90].\n\n    Raises:\n    ValueError: If the sine_value is not within the range [-1, 1].\n\n    Example:\n    >>> calculate_angle_from_sine(1.0)\n    90.0\n    >>> calculate_angle_from_sine(-1.0)\n    -90.0\n    >>> calculate_angle_from_sine(0.0)\n    0.0\n    \"\"\"\n    # Ensure the sine_value is within the valid range\n    if not -1 <= sine_value <= 1:\n        raise ValueError(\"sine_value must be within the range [-1, 1].\")\n\n    # Calculate the angle in radians using arcsin\n    angle_radians = np.arcsin(sine_value)\n\n    # Convert the angle from radians to degrees\n    angle_degrees = np.degrees(angle_radians)\n\n    return angle_degrees\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 326, "library_problem_id": 35, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 327, "code": ["<code>\nimport numpy as np\n\ndef pad_array_with_zeros(array: np.ndarray, target_length: int) -> np.ndarray:\n    if target_length < len(array):\n        raise ValueError(\"Target length must be greater than or equal to the length of the input array.\")\n    return np.pad(array, (0, target_length - len(array)))\n\ndef pad_to_nearest_multiple(array: np.ndarray, multiple: int) -> np.ndarray:\n    if multiple <= 0 or not isinstance(multiple, int):\n        raise ValueError(\"Multiple must be a positive integer.\")\n    return np.pad(array, (0, -len(array) % multiple))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 327, "library_problem_id": 36, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 328, "code": ["<code>\n```python\ndef pad(array: np.ndarray, target_length: int) -> np.ndarray:\n    \"\"\"\n    Pads a given NumPy array with zeros at the end to achieve a specified target length.\n\n    Args:\n        array (np.ndarray): The input array to be padded.\n        target_length (int): The desired length of the array after padding.\n\n    Returns:\n        np.ndarray: A new array padded with zeros to the specified length.\n\n    Conditions:\n        - If the target_length is less than or equal to the current length of the array, \n          the array should remain unchanged.\n        - If the target_length is greater than the current length, append zeros to the end \n          until the target length is reached.\n    \"\"\"\n    if target_length <= len(array):\n        return array\n    else:\n        return np.pad(array, (0, target_length - len(array)))\n\n# Example usage\nA = np.array([1, 2, 3, 4, 5])\ntarget_length = 8\nresult = pad(A, target_length)\nprint(result)  # Output: [1, 2, 3, 4, 5, 0, 0, 0]\n\n# Additional requirement\ntarget_length = 1342\nresult = pad(A, target_length)\nprint(result)  # Output: [1, 2, 3, 4, 5, ..., 0, 0] (1024 zeros)\n\ntarget_length = 3000\nresult = pad(A, target\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 328, "library_problem_id": 37, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 329, "code": ["<code>\nimport numpy as np\n\ndef raise_array_to_power(a: np.ndarray, power: int) -> np.ndarray:\n    return np.power(a, power)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 329, "library_problem_id": 38, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 330, "code": ["<code>\ndef f(a: np.ndarray = np.arange(4).reshape(2, 2), power: int = 5) -> np.ndarray:\n    # Use numpy's power function for elementwise exponentiation\n    return np.power(a, power)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 330, "library_problem_id": 39, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 331, "code": ["<code>\nfrom fractions import Fraction\n\nnumerator = 98\ndenominator = 42\n\n# Reduce the fraction\nfraction = Fraction(numerator, denominator)\n\n# Convert the reduced fraction to a tuple and store it in the variable `result`\nresult = (fraction.numerator, fraction.denominator)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 331, "library_problem_id": 40, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 332, "code": ["<code>\ndef reduce_fraction(numerator: int, denominator: int) -> tuple[int, int]:\n    \"\"\"\n    Reduce a fraction to its simplest form.\n\n    Args:\n    numerator (int): The numerator of the fraction.\n    denominator (int): The denominator of the fraction.\n\n    Returns:\n    tuple[int, int]: A tuple containing the reduced numerator and denominator.\n    \"\"\"\n    # Calculate the greatest common divisor of the numerator and denominator\n    gcd = np.gcd(numerator, denominator)\n    \n    # Divide both numerator and denominator by the gcd to get the reduced fraction\n    reduced_numerator = numerator // gcd\n    reduced_denominator = denominator // gcd\n    \n    return (reduced_numerator, reduced_denominator)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 332, "library_problem_id": 41, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 40}}
{"id": 333, "code": ["<code>\nimport numpy as np\n\ndef reduce_fraction(numerator: int, denominator: int) -> tuple:\n    \"\"\"\n    Reduces a fraction to its simplest form.\n\n    Args:\n    numerator (int): The numerator of the fraction.\n    denominator (int): The denominator of the fraction.\n\n    Returns:\n    tuple: A tuple (int, int) representing the reduced fraction.\n           If the denominator is zero, returns (NaN, NaN).\n    \"\"\"\n    if denominator == 0:\n        return (np.nan, np.nan)\n    else:\n        gcd = np.gcd(numerator, denominator)\n        return (numerator // gcd, denominator // gcd)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 333, "library_problem_id": 42, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 40}}
{"id": 334, "code": ["<code>\nimport numpy as np\n\ndef elementwise_average(arrays: list[np.ndarray]) -> np.ndarray:\n    \"\"\"\n    Calculate the element-wise average of a list of numpy ndarrays.\n\n    Args:\n    arrays (list[np.ndarray]): A list of numpy arrays of the same shape.\n\n    Returns:\n    np.ndarray: A numpy array containing the element-wise average of the input arrays.\n\n    Preconditions:\n    - All arrays in the input list must have the same shape.\n    - The input list must contain at least one array.\n\n    Example:\n    >>> a = np.array([10, 20, 30])\n    >>> b = np.array([30, 20, 20])\n    >>> c = np.array([50, 20, 40])\n    >>> elementwise_average([a, b, c])\n    array([30., 20., 30.])\n    \"\"\"\n    # Solution implementation goes here\n    return np.mean(arrays, axis=0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 334, "library_problem_id": 43, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 43}}
{"id": 335, "code": ["<code>\nimport numpy as np\n\ndef elementwise_maximum(*arrays: np.ndarray) -> np.ndarray:\n    return np.maximum.reduce(arrays)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 335, "library_problem_id": 44, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 43}}
{"id": 336, "code": ["<code>\ndef get_anti_diagonal(a: np.ndarray) -> np.ndarray:\n    n = a.shape[0]\n    return np.array([a[i, n - i - 1] for i in range(n)])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 336, "library_problem_id": 45, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 337, "code": ["<code>\nimport numpy as np\n\ndef get_top_right_diagonal(array: np.ndarray) -> np.ndarray:\n    rows, cols = array.shape\n    # Calculate the indices for the top-right diagonal\n    indices = (np.arange(rows), np.arange(cols - 1, -1, -1))\n    return array[indices]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 337, "library_problem_id": 46, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 338, "code": ["<code>\nimport numpy as np\n\ndef get_top_right_diagonal(arr: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Extracts the diagonal of a 2D NumPy array starting from the top-right corner.\n\n    Args:\n        arr (np.ndarray): A 2D NumPy array from which the diagonal is to be extracted.\n\n    Returns:\n        np.ndarray: A 1D NumPy array containing the elements of the diagonal starting from the top-right corner.\n    \"\"\"\n    n_rows, n_cols = arr.shape\n    return arr[np.arange(n_rows), np.arange(n_cols-1, -1, -1)]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 338, "library_problem_id": 47, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 339, "code": ["<code>\nimport numpy as np\n\ndef get_bottom_left_diagonal(a: np.ndarray) -> np.ndarray:\n    m, n = a.shape\n    min_dim = min(m, n)\n    return np.array([a[i, n - i - 1] for i in range(min_dim)])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 339, "library_problem_id": 48, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 45}}
{"id": 340, "code": ["<code>\ndef flatten_array(X):\n    return X.flatten().tolist()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 340, "library_problem_id": 49, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 341, "code": ["<code>\ndef flatten_array_in_c_order(X: np.ndarray) -> list:\n    \"\"\"\n    Flattens a two-dimensional NumPy array into a one-dimensional list in 'C' order.\n\n    Args:\n        X (np.ndarray): A two-dimensional NumPy array.\n\n    Returns:\n        list: A one-dimensional list containing all elements of `X` in row-major order.\n    \"\"\"\n    return X.flatten().tolist()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 341, "library_problem_id": 50, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 49}}
{"id": 342, "code": ["<code>\nresult = X.flatten().tolist()\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 342, "library_problem_id": 51, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 343, "code": ["<code>\nresult = list(X.flatten('F'))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 343, "library_problem_id": 52, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 49}}
{"id": 344, "code": ["<code>\nimport numpy as np\n\ndef convert_string_to_numpy_array(mystr: str) -> np.ndarray:\n    # Convert each character in the string to an integer and create a numpy array\n    return np.array([int(char) for char in mystr])\n\n# Example usage\nmystr = \"100110\"\nresult = convert_string_to_numpy_array(mystr)\nassert np.array_equal(result, np.array([1, 0, 0, 1, 1, 0]))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 344, "library_problem_id": 53, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 53}}
{"id": 345, "code": ["<code>\ndef process_column(array: np.ndarray, col_index: int, multiplier: float) -> np.ndarray:\n    \"\"\"\n    Multiplies the specified column of the input array by a given multiplier and computes the cumulative sum of that column.\n\n    Parameters:\n    array (np.ndarray): A 2D NumPy array representing the dataset.\n    col_index (int): The index of the column to be processed.\n    multiplier (float): The scalar value to multiply the column by.\n\n    Returns:\n    np.ndarray: A 1D NumPy array containing the cumulative sum of the modified column.\n    \"\"\"\n    # Extract the specified column\n    column = array[:, col_index]\n\n    # Multiply the elements of the column by the multiplier\n    column *= multiplier\n\n    # Compute the cumulative sum of the modified column\n    cumulative_sum = np.cumsum(column)\n\n    return cumulative_sum\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 345, "library_problem_id": 54, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 346, "code": ["<code>\nimport numpy as np\n\ndef process_row_of_array(array: np.ndarray, row_index: int, scalar: float) -> np.ndarray:\n    \"\"\"\n    Processes a specific row of a 2D NumPy array by multiplying it with a scalar and computing its cumulative sum.\n\n    Args:\n        array (np.ndarray): A 2D NumPy array containing the dataset.\n        row_index (int): The index of the row to be processed.\n        scalar (float): The scalar value to multiply with the specified row.\n\n    Returns:\n        np.ndarray: A 1-dimensional array containing the cumulative sum of the processed row.\n    \"\"\"\n    # Extract the specified row from the array\n    selected_row = array[row_index]\n    \n    # Multiply the row by the scalar\n    multiplied_row = selected_row * scalar\n    \n    # Calculate the cumulative sum of the multiplied row\n    cumulative_sum = np.cumsum(multiplied_row)\n    \n    return cumulative_sum\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 346, "library_problem_id": 55, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 347, "code": ["<code>\nimport numpy as np\n\ndef process_row(a, row, divide_number):\n    # Divide each element of the specified row by the divisor\n    divided_row = a[row] / divide_number\n    \n    # Compute the product of the elements in the modified row\n    result = np.prod(divided_row)\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 347, "library_problem_id": 56, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 54}}
{"id": 348, "code": ["<code>\nm, n = a.shape\n    result = []\n    independent_set = []\n    for i in range(m):\n        independent_set.append(a[i])\n        for j in range(i+1, m):\n            if np.all(a[j] != a[i]):\n                independent_set.append(a[j])\n        if len(independent_set) > len(result):\n            result = independent_set\n        independent_set = []\n    return np.array(result)\n    # END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 348, "library_problem_id": 57, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 349, "code": ["<code>\ndef get_row_length(a: np.ndarray) -> int:\n    \"\"\"\n    Calculate the length of a row in a 2D NumPy array.\n\n    Args:\n        a (np.ndarray): A 2D NumPy array.\n\n    Returns:\n        int: The length of the row (number of columns) in the array.\n    \"\"\"\n    return a.shape[1]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 349, "library_problem_id": 58, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 350, "code": ["<code>\nimport numpy as np\nimport scipy.stats\n\ndef weighted_two_tailed_t_test(a: np.ndarray, b: np.ndarray) -> float:\n    # Calculate sample statistics\n    mean_a, mean_b = np.mean(a), np.mean(b)\n    std_a, std_b = np.std(a, ddof=1), np.std(b, ddof=1)\n    n_a, n_b = len(a), len(b)\n    \n    # Calculate the t-statistic\n    t_statistic = (mean_a - mean_b) / np.sqrt((std_a**2 / n_a) + (std_b**2 / n_b))\n    \n    # Calculate degrees of freedom\n    df = n_a + n_b - 2\n    \n    # Calculate the two-tailed p-value\n    p_value = 2 * scipy.stats.t.sf(np.abs(t_statistic), df)\n    \n    return p_value\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 350, "library_problem_id": 59, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 351, "code": ["<code>\nimport numpy as np\nfrom scipy import stats\n\ndef weighted_two_tailed_t_test(a: np.ndarray, b: np.ndarray) -> float:\n    # Remove NaN values from both datasets\n    a = a[~np.isnan(a)]\n    b = b[~np.isnan(b)]\n\n    # Calculate the means and standard deviations of the cleaned datasets\n    mean_a = np.sum(a) / len(a)\n    mean_b = np.sum(b) / len(b)\n    std_a = np.sqrt(np.sum((a - mean_a) ** 2) / len(a))\n    std_b = np.sqrt(np.sum((b - mean_b) ** 2) / len(b))\n\n    # Perform the two-tailed t-test, considering the sample sizes as weights\n    t_stat, p_val = stats.ttest_ind(a, b, equal_var=False, nan_policy='omit')\n\n    # Return the p-value of the test\n    return p_val\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 351, "library_problem_id": 60, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 352, "code": ["<code>\n```python\nimport numpy as np\nimport scipy.stats\n\ndef calculate_p_value(mean1: float, var1: float, nobs1: int, mean2: float, var2: float, nobs2: int) -> float:\n    \"\"\"\n    Calculate the p-value for a two-tailed t-test given summary statistics of two samples.\n\n    Args:\n    - mean1 (float): Mean of sample 1.\n    - var1 (float): Variance of sample 1.\n    - nobs1 (int): Number of observations in sample 1.\n    - mean2 (float): Mean of sample 2.\n    - var2 (float): Variance of sample 2.\n    - nobs2 (int): Number of observations in sample 2.\n\n    Returns:\n    - float: The p-value of the two-tailed t-test.\n    \"\"\"\n    # Calculate the pooled standard deviation\n    pooled_std = np.sqrt(((nobs1 - 1) * var1 + (nobs2 - 1) * var2) / (nobs1 + nobs2 - 2))\n    \n    # Calculate the t-statistic\n    t_statistic = (mean1 - mean2) / (pooled_std * np.sqrt(1/nobs1 + 1/nobs2))\n    \n    # Calculate the degrees of freedom\n    df = nobs1 + nobs2 - 2\n    \n    # Calculate the p-value for the two-tailed test\n    p_value = 2 * scipy.stats.t.sf(np.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 352, "library_problem_id": 61, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 59}}
{"id": 353, "code": ["<code>\n```python\n\"\"\"\nFunction to compute the complement of two 2D numpy arrays A and B.\n\nThe task is to remove elements from array A that are present in array B. \nThis operation is analogous to finding the complement in set theory, \ndenoted as A - B. The function should return the elements of A that \nare not in B, preserving the original order of A.\n\nParameters:\n    A (np.ndarray): A 2D numpy array from which elements will be removed.\n    B (np.ndarray): A 2D numpy array containing elements to be removed from A.\n\nReturns:\n    np.ndarray: A 2D numpy array containing elements of A that are not in B.\n\nExample:\n    A = np.asarray([[1, 1, 1], [1, 1, 2], [1, 1, 3], [1, 1, 4]])\n    B = np.asarray([[0, 0, 0], [1, 0, 2], [1, 0, 3], [1, 0, 4], [1, 1, 0], [1, 1, 1], [1, 1, 4]])\n    output = complement_2d_arrays(A, B)\n    # output should be: [[1, 1, 2], [1, 1, 3]]\n\"\"\"\n\nimport numpy as np\n\ndef complement_2d_arrays(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n    # Convert B to a set of tuples for efficient\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 353, "library_problem_id": 62, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 354, "code": ["<code>\ndef symmetric_difference(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n    # Convert arrays to sets of tuples for set operations\n    set_A = set(map(tuple, A))\n    set_B = set(map(tuple, B))\n    \n    # Compute symmetric difference\n    diff_A = set_A - set_B  # Elements in A but not in B\n    diff_B = set_B - set_A  # Elements in B but not in A\n    \n    # Convert back to numpy arrays and concatenate results\n    result_A = np.array(list(diff_A))\n    result_B = np.array(list(diff_B))\n    \n    # Concatenate results, ensuring original order is preserved\n    result = np.vstack((result_A, result_B))\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 354, "library_problem_id": 63, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 62}}
{"id": 355, "code": ["<code>\nimport numpy as np\n\ndef sort_b_by_a(a: np.ndarray, b: np.ndarray, axis: int) -> np.ndarray:\n    \"\"\"\n    Sorts the array `b` based on the sorted order of array `a` along the specified axis.\n\n    Parameters:\n    a (np.ndarray): 3D array to determine the sort order.\n    b (np.ndarray): 3D array to be sorted.\n    axis (int): Axis along which to sort.\n\n    Returns:\n    np.ndarray: Sorted array `b`.\n    \"\"\"\n    # Get the indices that would sort `a` along the specified axis\n    sort_indices = np.argsort(a, axis=axis)\n    \n    # Use advanced indexing to sort `b` according to `sort_indices`\n    sorted_b = np.take_along_axis(b, sort_indices, axis=axis)\n    \n    return sorted_b\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 355, "library_problem_id": 64, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 356, "code": ["<code>\nimport numpy as np\n\ndef sort_array_by_another(a: np.ndarray, b: np.ndarray, axis: int) -> np.ndarray:\n    # Obtain the indices that would sort `a` along the specified axis\n    sort_indices = np.argsort(a, axis=axis)\n    \n    # Use advanced indexing to sort `b` using the obtained indices\n    b_sorted = np.take_along_axis(b, sort_indices, axis=axis)\n    \n    return b_sorted\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 356, "library_problem_id": 65, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 357, "code": ["<code>\nimport numpy as np\n\ndef sort_b_by_a(a: np.ndarray, b: np.ndarray, axis: int) -> np.ndarray:\n    \"\"\"\n    Sorts the array `b` based on the values of `a` along the specified axis in decreasing order.\n\n    Parameters:\n    a (np.ndarray): The array whose values determine the sort order.\n    b (np.ndarray): The array to be sorted.\n    axis (int): The axis along which to sort.\n\n    Returns:\n    np.ndarray: The sorted array `b`.\n    \"\"\"\n    # Get the indices that would sort `a` along the specified axis in decreasing order\n    sort_indices = np.argsort(a, axis=axis)[::-1]\n\n    # Use advanced indexing to sort `b` according to `sort_indices`\n    sorted_b = np.take_along_axis(b, sort_indices, axis=axis)\n\n    return sorted_b\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 357, "library_problem_id": 66, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 358, "code": ["<code>\nimport numpy as np\n\ndef sort_matrices_by_sum(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n    # Calculate the sum of each matrix in the 3D array `a`\n    sums = np.sum(a, axis=(1, 2))\n    \n    # Get the indices that would sort the sums in descending order\n    sorted_indices = np.argsort(-sums)\n    \n    # Use the sorted indices to rearrange the matrices in `b`\n    sorted_b = b[sorted_indices]\n    \n    return sorted_b\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 358, "library_problem_id": 67, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 64}}
{"id": 359, "code": ["<code>\nimport numpy as np\n\ndef remove_third_column(arr: np.ndarray) -> np.ndarray:\n    # Use NumPy's delete function to remove the third column (index 2)\n    return np.delete(arr, 2, axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 359, "library_problem_id": 68, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 360, "code": ["<code>\nimport numpy as np\n\ndef delete_row(arr: np.ndarray, row_index: int) -> np.ndarray:\n    if not (0 <= row_index < arr.shape[0]):\n        raise IndexError(\"row_index is out of bounds for the array.\")\n    return np.delete(arr, row_index, axis=0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 360, "library_problem_id": 69, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 361, "code": ["<code>\nimport numpy as np\n\ndef remove_columns(arr: np.ndarray, columns_to_remove: list) -> np.ndarray:\n    return np.delete(arr, columns_to_remove, axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 361, "library_problem_id": 70, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 362, "code": ["<code>\nimport numpy as np\nfrom typing import List\n\ndef delete_columns(arr: np.ndarray, del_col: List[int]) -> np.ndarray:\n    # Filter out out-of-bound indices\n    valid_indices = [i for i in del_col if i < arr.shape[1]]\n    # Delete the valid columns from the array\n    return np.delete(arr, valid_indices, axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 362, "library_problem_id": 71, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 68}}
{"id": 363, "code": ["<code>\nimport numpy as np\n\ndef insert_into_numpy_array(arr: np.ndarray, position: int, element: int) -> np.ndarray:\n    return np.insert(arr, position, element)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 363, "library_problem_id": 72, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 364, "code": ["<code>\nimport numpy as np\n\ndef insert_row(array: np.ndarray, pos: int, element: list) -> np.ndarray:\n    \"\"\"\n    Insert a row into a 2-dimensional NumPy array at a specified position.\n\n    Args:\n    array (np.ndarray): The original 2-dimensional array.\n    pos (int): The index at which the new row should be inserted. \n               Must satisfy 0 <= pos <= len(array).\n    element (list): The row to be inserted, represented as a list. \n                    Must have the same number of elements as the existing rows in the array.\n\n    Returns:\n    np.ndarray: A new 2-dimensional array with the specified row inserted.\n    \"\"\"\n    return np.insert(array, pos, element, axis=0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 364, "library_problem_id": 73, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 365, "code": ["<code>\nimport numpy as np\n\ndef insert_element(a: np.ndarray, pos: int, element) -> np.ndarray:\n    # Ensure the position is within the valid range\n    if not (0 <= pos <= len(a)):\n        raise ValueError(\"Position must be within the range of the array length.\")\n    \n    # Use numpy's insert function to add the element at the specified position\n    return np.insert(a, pos, element)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 365, "library_problem_id": 74, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 72}}
{"id": 366, "code": ["<code>\nimport numpy as np\n\ndef insert_rows(a: np.ndarray, element: np.ndarray, pos: list[int]) -> np.ndarray:\n    # Initialize an empty list to hold the new rows\n    result = []\n    # Initialize a counter for the position list\n    element_index = 0\n    # Iterate over the original array 'a' with index\n    for i in range(len(a) + len(element)):\n        # Check if the current index matches the next position in 'pos'\n        if element_index < len(pos) and i == pos[element_index]:\n            # Append the corresponding row from 'element'\n            result.append(element[element_index])\n            # Move to the next position in 'pos'\n            element_index += 1\n        else:\n            # Append the current row from 'a'\n            result.append(a[i - element_index])\n    # Convert the result list back to a NumPy array\n    return np.array(result)\n\n# Example usage\na = np.array([[1, 2], [3, 4]])\nelement = np.array([[3, 5], [6, 6]])\npos = [1, 2]\na = insert_rows(a, element, pos)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 366, "library_problem_id": 75, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 72}}
{"id": 367, "code": ["<code>\nimport numpy as np\n\ndef deep_copy_array_of_arrays(pairs):\n    \"\"\"\n    Create a deep copy of an array of numpy arrays with varying shapes.\n\n    Args:\n    - pairs (List[Tuple[int, int]]): A list of tuples where each tuple contains two integers representing \n      the dimensions of the sub-arrays to be created.\n\n    Returns:\n    - List[np.ndarray]: A list of numpy arrays, each being a deep copy of the corresponding sub-array in the \n      original array_of_arrays.\n    \"\"\"\n    # Create the original array of arrays\n    array_of_arrays = [np.arange(a * b).reshape(a, b) for (a, b) in pairs]\n    \n    # Perform a deep copy of the array of arrays\n    result = [np.copy(arr) for arr in array_of_arrays]\n    \n    return result\n\n# Example usage\npairs = [(2, 3), (3, 4), (4, 5)]\nresult = deep_copy_array_of_arrays(pairs)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 367, "library_problem_id": 76, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 368, "code": ["<code>\ndef are_all_rows_equal(arr: np.ndarray) -> bool:\n    # Check if all rows are equal by comparing each row to the first row\n    return np.all(arr == arr[0])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 368, "library_problem_id": 77, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 369, "code": ["<code>\nresult = np.all(arr[:, 1:] == arr[:, :-1], axis=0)\n    return np.all(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 369, "library_problem_id": 78, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Semantic", "perturbation_origin_id": 77}}
{"id": 370, "code": ["<code>\n\"\"\"\nFunction to determine if all rows in a 2D numpy array are identical.\n\nThis function checks whether every row in a given 2D numpy array is equal to the first row.\nIt provides a more efficient and idiomatic approach using numpy operations, avoiding the mixing\nof Python lists with numpy arrays.\n\nArgs:\n    a (np.ndarray): A 2D numpy array where the equality of all rows is to be tested.\n\nReturns:\n    bool: True if all rows in the array are identical, False otherwise.\n\nExample:\n    >>> import numpy as np\n    >>> example_a = np.repeat(np.arange(1, 6).reshape(1, -1), 3, axis=0)\n    >>> f(example_a)\n    True\n\"\"\"\n\nimport numpy as np\n\ndef f(a: np.ndarray = np.repeat(np.arange(1, 6).reshape(1, -1), 3, axis=0)) -> bool:\n    # Check if all rows are equal to the first row\n    return np.all(a == a[0], axis=1).all()\n\n# Example usage\nexample_a = np.repeat(np.arange(1, 6).reshape(1, -1), 3, axis=0)\nresult = f(example_a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 370, "library_problem_id": 79, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Surface", "perturbation_origin_id": 77}}
{"id": 371, "code": ["<code>\ndef integrate_2d_samples(x: np.ndarray, y: np.ndarray, z: np.ndarray) -> float:\n    dx = x[1] - x[0]\n    dy = y[1] - y[0]\n\n    # Create weight matrices\n    wx = np.array([[1, 4, 1], [4, 16, 4], [1, 4, 1]]) * dx**2 / 36\n    wy = np.array([[1, 4, 1], [4, 16, 4], [1, 4, 1]]) * dy**2 / 36\n\n    # Element-wise multiplication of the weight matrices with the function values\n    weighted_x = wx * z\n    weighted_xy = np.matmul(weighted_x, wy)\n\n    # Sum the weighted function values to obtain the integral\n    integral = np.sum(weighted_xy)\n\n    return integral\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 371, "library_problem_id": 80, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 372, "code": ["<code>\ndef integrate_2d_samples(x: np.ndarray, y: np.ndarray, z: np.ndarray) -> float:\n    \"\"\"\n    Perform a 2D numerical integration over a rectangular grid using Simpson's rule.\n\n    Args:\n        x (np.ndarray): 1D array of x-coordinates.\n        y (np.ndarray): 1D array of y-coordinates.\n        z (np.ndarray): 2D array of function values at each (x, y) grid point.\n\n    Returns:\n        float: The computed integral over the 2D grid.\n    \"\"\"\n    # Calculate the grid spacing in x and y\n    dx = x[1] - x[0]\n    dy = y[1] - y[0]\n\n    # Construct the weight matrix for Simpson's rule\n    wx = np.array([[1, 4, 1], [4, 16, 4], [1, 4, 1]]) * (dx / 3.0)\n    wy = np.array([[1, 4, 1], [4, 16, 4], [1, 4, 1]]) * (dy / 3.0)\n\n    # Multiply the weight matrix by the function values matrix and sum the result\n    integral = np.sum(wx * wy * z)\n\n    return integral\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 372, "library_problem_id": 81, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 373, "code": ["<code>\n```python\nimport numpy as np\nfrom typing import Tuple\n\ndef ecdf(x: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute the empirical cumulative distribution function (ECDF) for a given array.\n\n    Args:\n        x (np.ndarray): A 1D array of numerical data for which the ECDF is to be computed.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: A tuple containing two arrays:\n            - The sorted data points from the input array `x`.\n            - The ECDF values corresponding to each sorted data point.\n\n    Example:\n        >>> x = np.array([93.5, 93, 60.8, 94.5, 82, 87.5, 91.5])\n        >>> sorted_x, ecdf_values = ecdf(x)\n        >>> sorted_x\n        array([60.8, 82. , 87.5, 91.5, 93. , 93.5, 94.5])\n        >>> ecdf_values\n        array([0.14285714, 0.28571429, 0.42857143, 0.57142857, 0.71428571, 0.85714286, 1.        ])\n    \"\"\"\n    # Sort the data\n    sorted_x = np.sort(x\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 373, "library_problem_id": 82, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 374, "code": ["<code>\ndef ecdf(x):\n    \"\"\"\n    Compute the empirical cumulative distribution function (ECDF) for a given array.\n\n    Args:\n        x (np.ndarray): A 1-dimensional array of numerical values.\n\n    Returns:\n        np.ndarray: An array representing the cumulative sum of the normalized input array.\n    \"\"\"\n    # Sort the input array\n    x_sorted = np.sort(x)\n    \n    # Compute the ECDF values\n    ecdf_values = np.arange(1, len(x) + 1) / len(x)\n    \n    # Interpolate the ECDF values for the evaluation array\n    ecdf_eval = np.interp(eval, x_sorted, ecdf_values)\n    \n    return ecdf_eval\n\n# Compute the ECDF values for the sample data and evaluation array\nresult = ecdf(np.concatenate((grades, eval)))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 374, "library_problem_id": 83, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 375, "code": ["<code>\n```python\ndef ecdf(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the empirical cumulative distribution function (ECDF) for an array `x`.\n\n    Parameters:\n    x (np.ndarray): A 1D array of numerical values.\n\n    Returns:\n    np.ndarray: The ECDF values corresponding to the sorted elements of `x`.\n    \"\"\"\n    # Normalize `x` to sum to 1\n    x = x / np.sum(x)\n    # Compute the cumulative sum\n    return np.cumsum(x)\n\n# Example data\ngrades = np.array([93.5, 93, 60.8, 94.5, 82, 87.5, 91.5, 99.5, 86, 93.5, 92.5, 78, 76, 69, 94.5,\n                   89.5, 92.8, 78, 65.5, 98, 98.5, 92.3, 95.5, 76, 91, 95, 61])\n\n# Threshold for ECDF\nthreshold = 0.5\n\n# Variables to store the solution\nlow, high = None, None\n\n# Begin solution implementation\necdf_values = ecdf(grades)\nfor i in range(len(ecdf_values) - 1):\n    if ecdf_values[i] < threshold and threshold <= ecdf_\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 375, "library_problem_id": 84, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 82}}
{"id": 376, "code": ["<code>\nimport numpy as np\n\ndef generate_random_binary_array(size: int, one_ratio: float) -> np.ndarray:\n    num_ones = int(size * one_ratio)\n    num_zeros = size - num_ones\n    array = np.array([1] * num_ones + [0] * num_zeros)\n    np.random.shuffle(array)\n    return array\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 376, "library_problem_id": 85, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 377, "code": ["<code>\nimport torch\nimport numpy as np\n\ndef convert_tensor_to_numpy(tensor: torch.Tensor) -> np.ndarray:\n    return tensor.numpy()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 377, "library_problem_id": 86, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 378, "code": ["<code>\nimport torch\nimport numpy as np\n\n# Create a NumPy array\na = np.ones(5)\n\n# Convert the NumPy array to a PyTorch tensor\na_pt = torch.from_numpy(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 378, "library_problem_id": 87, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 86}}
{"id": 379, "code": ["<code>\nimport tensorflow as tf\nimport numpy as np\n\ndef tensor_to_numpy(tensor: tf.Tensor) -> np.ndarray:\n    # Convert the TensorFlow tensor to a NumPy array\n    return tensor.numpy()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 379, "library_problem_id": 88, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 380, "code": ["<code>\nimport tensorflow as tf\nimport numpy as np\n\n# Create a NumPy array with shape [2, 3, 4] filled with ones\na = np.ones([2, 3, 4])\n\n# Convert the NumPy array to a TensorFlow tensor\na_tf = tf.convert_to_tensor(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 380, "library_problem_id": 89, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 88}}
{"id": 381, "code": ["<code>\n```python\n\"\"\"\nFunction: get_decreasing_order_indices\n\nDescription:\n    This function takes a numpy array or a Python list as input and returns a list of indices that represent the order of elements sorted in decreasing order. The function identifies the indices of the elements in the original array such that if the array were sorted in descending order, these indices would correspond to the positions of the elements.\n\nParameters:\n    arr (Union[np.ndarray, List[int]]): A numpy array or a Python list containing numerical elements.\n\nReturns:\n    List[int]: A list of indices representing the order of elements in the input array sorted in decreasing order.\n\nExample:\n    Given the input array:\n        a = np.array([4, 1, 0, 8, 5, 2])\n    The function should return:\n        [3, 4, 0, 5, 1, 2]\n    Explanation:\n        - The element '8' is the largest and is at index 3.\n        - The element '5' is the second largest and is at index 4.\n        - The element '4' is the third largest and is at index 0.\n        - The element '2' is the fourth largest and is at index 5.\n        - The element '1' is the fifth largest and is at index 1.\n        - The element '0' is the smallest and is at index 2.\n\nConstraints:\n    - The input array should contain comparable numerical elements.\n    - The function should handle both numpy arrays and Python lists.\n\nImplementation:\n\"\"\"\nimport numpy as np\n\ndef get_decreasing\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 381, "library_problem_id": 90, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 382, "code": ["<code>\nimport numpy as np\nfrom typing import List, Union\n\ndef get_sorted_indices(arr: Union[np.ndarray, List[int]]) -> List[int]:\n    # Convert input to numpy array if it is a list\n    if isinstance(arr, list):\n        arr = np.array(arr)\n    # Use numpy's argsort to get indices of sorted elements\n    sorted_indices = np.argsort(arr)\n    return sorted_indices.tolist()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 382, "library_problem_id": 91, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 383, "code": ["<code>\nimport numpy as np\n\ndef get_top_n_indexes(arr: np.ndarray, N: int) -> list:\n    \"\"\"\n    Find the indexes of the N largest elements in a numpy array or Python list, sorted in decreasing order of their values.\n\n    Args:\n    arr (np.ndarray): A numpy array or Python list of numerical values.\n    N (int): The number of top elements to find.\n\n    Returns:\n    list: A list of indexes of the N largest elements, sorted in decreasing order of their values.\n    \"\"\"\n    # Convert the array to a list of tuples where each tuple is (value, index)\n    arr_with_indexes = list(enumerate(arr))\n\n    # Sort the list in decreasing order of the values\n    arr_with_indexes.sort(key=lambda x: x[1], reverse=True)\n\n    # Return the indexes of the top N elements\n    return [index for value, index in arr_with_indexes[:N]]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 383, "library_problem_id": 92, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 90}}
{"id": 384, "code": ["<code>\nimport numpy as np\n\ndef matrix_power(A: np.ndarray, n: int) -> np.ndarray:\n    if len(A.shape) != 2 or A.shape[0] != A.shape[1]:\n        raise ValueError(\"Input array must be a 2-dimensional square matrix.\")\n    if n < 0:\n        raise ValueError(\"Power must be a non-negative integer.\")\n    if n == 0:\n        # Any non-zero square matrix raised to the power of 0 is the identity matrix\n        return np.eye(A.shape[0])\n    if n == 1:\n        # Any square matrix raised to the power of 1 is the matrix itself\n        return A\n\n    # Initialize result as the identity matrix\n    result = np.eye(A.shape[0])\n\n    # Multiply result by A n times\n    for _ in range(n):\n        result = np.matmul(result, A)\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 384, "library_problem_id": 93, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 93}}
{"id": 385, "code": ["<code>\ndef extract_patches(array: np.ndarray) -> np.ndarray:\n    # Ensure the input array has even dimensions for proper patch extraction\n    rows, cols = array.shape\n    assert rows % 2 == 0 and cols % 2 == 0, \"Array dimensions must be even.\"\n\n    # Calculate the number of patches along each dimension\n    num_patches_row = rows // 2\n    num_patches_col = cols // 2\n\n    # Initialize a list to store the patches\n    patches = []\n\n    # Iterate over the array to extract 2x2 patches\n    for i in range(num_patches_row):\n        for j in range(num_patches_col):\n            # Extract the 2x2 patch\n            patch = array[i*2:(i+1)*2, j*2:(j+1)*2]\n            patches.append(patch)\n\n    # Convert the list of patches to a 3D numpy array\n    return np.array(patches)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 385, "library_problem_id": 94, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 386, "code": ["<code>\nimport numpy as np\nfrom typing import List\n\ndef extract_patches(a: np.ndarray) -> List[np.ndarray]:\n    \"\"\"\n    Extracts overlapping 2x2 patches from a 2D numpy array using a sliding window approach.\n\n    Args:\n    a (np.ndarray): A 2D numpy array of shape (m, n) where m, n >= 2.\n\n    Returns:\n    List[np.ndarray]: A list of 2x2 numpy arrays representing the extracted patches.\n    \"\"\"\n    result = []\n    # Iterate over the array with a sliding window of size 2x2\n    for i in range(a.shape[0] - 1):\n        for j in range(a.shape[1] - 1):\n            # Extract the 2x2 patch\n            patch = a[i:i+2, j:j+2]\n            result.append(patch)\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 386, "library_problem_id": 95, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 387, "code": ["<code>\nimport numpy as np\n\ndef extract_patches(a: np.ndarray) -> np.ndarray:\n    # Calculate the number of patches along each dimension\n    num_patches_row = a.shape[0] // 2\n    num_patches_col = a.shape[1] // 2\n    \n    # Initialize an empty list to store the patches\n    patches = []\n    \n    # Iterate over the array to extract 2x2 patches\n    for i in range(num_patches_row):\n        for j in range(num_patches_col):\n            # Extract the current 2x2 patch\n            patch = a[i*2:(i+1)*2, j*2:(j+1)*2]\n            # Append the patch to the list of patches\n            patches.append(patch)\n    \n    # Convert the list of patches to a numpy array\n    return np.array(patches)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 387, "library_problem_id": 96, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 388, "code": ["<code>\nimport numpy as np\n\ndef extract_patches(array: np.ndarray, patch_size: int) -> list:\n    patches = []\n    for i in range(0, array.shape[0] - patch_size + 1, patch_size):\n        for j in range(0, array.shape[1] - patch_size + 1, patch_size):\n            patch = array[i:i + patch_size, j:j + patch_size]\n            patches.append(patch)\n    return patches\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 388, "library_problem_id": 97, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 389, "code": ["<code>\ndef reconstruct_array(a: np.ndarray) -> np.ndarray:\n    # Begin solution\n    return np.concatenate(np.concatenate(a, axis=1), axis=0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 389, "library_problem_id": 98, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 390, "code": ["<code>\nimport numpy as np\n\ndef extract_patches(array: np.ndarray, patch_size: int) -> np.ndarray:\n    # Calculate the number of patches in each dimension\n    n_patches_x = array.shape[0] // patch_size\n    n_patches_y = array.shape[1] // patch_size\n\n    # Reshape the array to have patches as the first dimension\n    array_reshaped = np.reshape(array[:n_patches_x*patch_size, :n_patches_y*patch_size], \n                                (n_patches_x, patch_size, n_patches_y, patch_size))\n\n    # Transpose the array to have patches as the second dimension\n    array_transposed = np.transpose(array_reshaped, (0, 2, 1, 3))\n\n    # Flatten the array to have patches as the first dimension\n    array_flattened = np.reshape(array_transposed, (-1, patch_size, patch_size))\n\n    return array_flattened\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 390, "library_problem_id": 99, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 94}}
{"id": 391, "code": ["<code>\nimport numpy as np\n\ndef extract_columns_in_range(array: np.ndarray, start_col: int, end_col: int) -> np.ndarray:\n    \"\"\"\n    Extracts columns from a 2D NumPy array within a specified inclusive range.\n\n    Parameters:\n    array (np.ndarray): The input 2D array.\n    start_col (int): The starting column index (inclusive).\n    end_col (int): The ending column index (inclusive).\n\n    Returns:\n    np.ndarray: A subarray containing the columns from start_col to end_col, inclusive.\n    \"\"\"\n    return array[:, start_col:end_col + 1]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 391, "library_problem_id": 100, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 392, "code": ["<code>\nimport numpy as np\n\ndef extract_rows_in_range(array: np.ndarray, start: int, end: int) -> np.ndarray:\n    \"\"\"\n    Extracts rows from a 2D NumPy array within the specified range.\n\n    Parameters:\n    - array (np.ndarray): The input 2D array.\n    - start (int): The starting index of the range (inclusive).\n    - end (int): The ending index of the range (exclusive).\n\n    Returns:\n    - np.ndarray: A subarray containing the specified rows.\n    \"\"\"\n    return array[start:end]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 392, "library_problem_id": 101, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 100}}
{"id": 393, "code": ["<code>\n```python\nimport numpy as np\n\ndef extract_columns_in_range(array: np.ndarray, low: int, high: int) -> np.ndarray:\n    \"\"\"\n    Extracts columns from the input 2D NumPy array where the column indices fall within the specified range [low, high].\n    \n    Parameters:\n    - array (np.ndarray): A 2D NumPy array from which columns are to be extracted.\n    - low (int): The inclusive lower bound of the column range.\n    - high (int): The inclusive upper bound of the column range.\n    \n    Returns:\n    - np.ndarray: A 2D NumPy array containing the columns from the input array that fall within the specified range.\n    \n    Notes:\n    - If 'high' exceeds the number of columns in the array, it should be adjusted to the last column index.\n    - The function assumes that 'low' is a valid index within the array's column range.\n    \"\"\"\n    # Ensure 'high' does not exceed the last column index\n    high = min(high, array.shape[1] - 1)\n    \n    # Extract and return the subarray within the specified column range\n    return array[:, low:high + 1]\n\n# Example Usage\na = np.array([[ 0,  1,  2,  3, 5, 6, 7, 8],\n              [ 4,  5,  6,  7, 5, 3, 2, 5],\n              [ 8,  9, 10, 11, 4, 5, 3, 5\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 393, "library_problem_id": 102, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 100}}
{"id": 394, "code": ["<code>\nimport numpy as np\n\ndef from_string(input_string: str) -> np.ndarray:\n    # Remove brackets and split the string into individual numbers\n    cleaned_string = input_string.replace('[', '').replace(']', '').strip()\n    # Convert the cleaned string into a NumPy array\n    array = np.fromstring(cleaned_string, sep=' ')\n    # Reshape the array to the intended 2D structure\n    return array.reshape(-1, 2)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 394, "library_problem_id": 103, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 395, "code": ["<code>\nimport numpy as np\n\ndef generate_log_uniform_samples(n: int, min_value: float, max_value: float, base: float) -> np.ndarray:\n    \"\"\"\n    Generate samples from a log-uniform distribution.\n\n    Parameters:\n    - n (int): Number of samples to generate.\n    - min_value (float): Minimum value of the range.\n    - max_value (float): Maximum value of the range.\n    - base (float): Base of the logarithm for transformation.\n\n    Returns:\n    - np.ndarray: Array of log-uniformly distributed samples.\n    \"\"\"\n    log_min = np.log(min_value) / np.log(base)\n    log_max = np.log(max_value) / np.log(base)\n    uniform_samples = np.random.uniform(log_min, log_max, n)\n    log_uniform_samples = np.power(base, uniform_samples)\n    return log_uniform_samples\n\n# Example usage\nn = 10000\nmin_value = 1\nmax_value = np.e\nbase = np.e\nresult = generate_log_uniform_samples(n, min_value, max_value, base)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 395, "library_problem_id": 104, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 396, "code": ["<code>\nimport numpy as np\n\ndef generate_log_uniform_samples(n: int, min_exp: float, max_exp: float, base: float = np.e) -> np.ndarray:\n    \"\"\"\n    Generate samples from a log-uniform distribution.\n\n    Parameters:\n    - n (int): The number of samples to generate.\n    - min_exp (float): The minimum exponent value.\n    - max_exp (float): The maximum exponent value.\n    - base (float): The base of the logarithm. Default is the natural logarithm base (e).\n\n    Returns:\n    - np.ndarray: An array of n samples log-uniformly distributed between base**min_exp and base**max_exp.\n    \"\"\"\n    # Generate uniform samples in the exponent range\n    uniform_samples = np.random.uniform(min_exp, max_exp, n)\n    # Convert to log-uniform distribution\n    log_uniform_samples = base ** uniform_samples\n    return log_uniform_samples\n\n# Example usage\nmin_exp = 0\nmax_exp = 1\nn = 10000\nresult = generate_log_uniform_samples(n, min_exp, max_exp)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 396, "library_problem_id": 105, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 104}}
{"id": 397, "code": ["<code>\nimport numpy as np\n\ndef generate_log_uniform_samples(min: float = 1, max: float = np.e, n: int = 10000) -> np.ndarray:\n    if min <= 0:\n        raise ValueError(\"Parameter 'min' must be greater than 0.\")\n    if max <= min:\n        raise ValueError(\"Parameter 'max' must be greater than 'min'.\")\n    if n <= 0:\n        raise ValueError(\"Parameter 'n' must be a positive integer.\")\n    \n    # Generate uniform samples in the log space\n    log_min = np.log(min)\n    log_max = np.log(max)\n    uniform_samples = np.random.uniform(log_min, log_max, n)\n    \n    # Transform back to the original space\n    log_uniform_samples = np.exp(uniform_samples)\n    \n    return log_uniform_samples\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 397, "library_problem_id": 106, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 398, "code": ["<code>\nimport numpy as np\nimport pandas as pd\n\ndef compute_series_B(A, a, b):\n    # Initialize series B\n    B = pd.Series(index=A.index, dtype=float)\n\n    # Compute B using the recursive formula\n    B[0] = a * A[0]\n    for t in range(1, len(A)):\n        B[t] = a * A[t] + b * B[t-1]\n\n    return B\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 398, "library_problem_id": 107, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 399, "code": ["<code>\ndef compute_recursive_series(A: pd.Series, a: float, b: float, c: float) -> pd.Series:\n    \"\"\"\n    Compute a new time-series B from the given series A using the specified recursive formula.\n\n    Args:\n        A (pd.Series): The input time-series containing numerical values.\n        a (float): The coefficient for the current value of A.\n        b (float): The coefficient for the previous value of B.\n        c (float): The coefficient for the value of B two steps back.\n\n    Returns:\n        pd.Series: The computed time-series B.\n\n    The recursive formula is defined as:\n    - B[0] = a * A[0]\n    - B[1] = a * A[1] + b * B[0]\n    - For t >= 2, B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n    \"\"\"\n    B = pd.Series(index=A.index, dtype=float)\n    B[0] = a * A[0]\n    if len(A) > 1:\n        B[1] = a * A[1] + b * B[0]\n    for t in range(2, len(A)):\n        B[t] = a * A[t] + b * B[t-1] + c * B[t-2]\n    return B\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 399, "library_problem_id": 108, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 107}}
{"id": 400, "code": ["<code>\nimport numpy as np\n\ndef create_empty_numpy_array() -> np.ndarray:\n    \"\"\"\n    Creates an empty numpy array with a shape of (0,).\n\n    Returns:\n        np.ndarray: An empty numpy array with shape (0,).\n    \"\"\"\n    result = np.array([], dtype=float)  # Initialize an empty array with shape (0,)\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 400, "library_problem_id": 109, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 401, "code": ["<code>\nimport numpy as np\n\ndef create_empty_numpy_array() -> np.ndarray:\n    \"\"\"\n    Creates an empty NumPy array with a shape of (3, 0).\n\n    Returns:\n        np.ndarray: An empty NumPy array with 3 rows and 0 columns.\n    \"\"\"\n    result = np.empty((3, 0))\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 401, "library_problem_id": 110, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 109}}
{"id": 402, "code": ["<code>\ndef sub2ind(dims: tuple, subscripts: tuple) -> int:\n    # Ensure the number of dimensions matches the number of subscripts\n    assert len(dims) == len(subscripts), \"Dimensions and subscripts must have the same length.\"\n    \n    # Calculate the linear index using Fortran-style order\n    linear_index = 0\n    multiplier = 1\n    for dim_size, subscript in zip(dims, subscripts):\n        assert 0 <= subscript < dim_size, \"Subscript out of bounds.\"\n        linear_index += subscript * multiplier\n        multiplier *= dim_size\n    \n    return linear_index\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 402, "library_problem_id": 111, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 403, "code": ["<code>\ndef sub2ind(dims: tuple, index: tuple) -> int:\n    # Calculate the linear index using the row-major order formula\n    linear_index = index[0] * (dims[1] * dims[2]) + index[1] * dims[2] + index[2]\n    return linear_index\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 403, "library_problem_id": 112, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 404, "code": ["<code>\nimport numpy as np\nimport pandas as pd\n\ndef create_dataframe(index, columns):\n    # Create a structured numpy array with two fields: one for integers and one for floats\n    values = np.zeros((len(index),), dtype=[('a', 'int32'), ('b', 'float32'), ('c', 'float32')])\n\n    # Convert the structured array to a DataFrame\n    df = pd.DataFrame(values, index=index)\n\n    # Assign column names\n    df.columns = columns\n\n    # Verify the DataFrame's data types\n    assert df.dtypes['a'] == np.int32\n    assert df.dtypes['b'] == np.float32\n    assert df.dtypes['c'] == np.float32\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 404, "library_problem_id": 113, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 405, "code": ["<code>\nimport numpy as np\n\ndef numpy_accumarray(values: np.ndarray, indices: np.ndarray) -> np.ndarray:\n    return np.add.reduceat(values, np.where(np.diff(indices))[0] + 1).astype(int)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 405, "library_problem_id": 114, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 406, "code": ["<code>\nimport numpy as np\n\ndef compute_group_maximum(a: np.ndarray, index: np.ndarray) -> np.ndarray:\n    unique_indices = np.unique(index)\n    max_values = np.zeros(len(unique_indices))\n    \n    for i, unique_index in enumerate(unique_indices):\n        group_indices = np.where(index == unique_index)\n        group_values = a[group_indices]\n        max_values[i] = np.max(group_values)\n    \n    return max_values\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 406, "library_problem_id": 115, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 407, "code": ["<code>\nimport numpy as np\n\ndef accumarray_numpy(a: np.ndarray, accmap: np.ndarray) -> np.ndarray:\n    # Convert negative indices to Python-style negative list indices\n    accmap[accmap < 0] = len(a) + accmap[accmap < 0]\n\n    # Use NumPy's bincount function to accumulate elements based on indices\n    result = np.bincount(accmap, weights=a)\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 407, "library_problem_id": 116, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 114}}
{"id": 408, "code": ["<code>\nimport numpy as np\n\ndef compute_minimum_by_index(a: np.ndarray, index: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the minimum values of elements in array `a` grouped by their corresponding indices in `index`.\n\n    Parameters:\n    a (np.ndarray): The input array of elements.\n    index (np.ndarray): The array of indices, which may include negative values.\n\n    Returns:\n    np.ndarray: An array containing the minimum values for each unique index in `index`.\n    \"\"\"\n    result = np.empty(len(np.unique(index)), dtype=int)\n    for i, unique_index in enumerate(np.unique(index)):\n        if unique_index < 0:\n            result[i] = np.min(a[index[index>=0]+unique_index])\n        else:\n            result[i] = np.min(a[index==unique_index])\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 408, "library_problem_id": 117, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 114}}
{"id": 409, "code": ["<code>\ndef elementwise_function(element_1: int, element_2: int) -> int:\n    \"\"\"Applies a specified operation to two integer elements.\"\"\"\n    return element_1 + element_2\n\n# Define input arrays\nx = np.array([[2, 2, 2],\n              [2, 2, 2],\n              [2, 2, 2]])\n\ny = np.array([[3, 3, 3],\n              [3, 3, 3],\n              [3, 3, 1]])\n\n# Compute the result using vectorized operations\nz = elementwise_function(x, y).tolist()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 409, "library_problem_id": 118, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 410, "code": ["<code>\n```python\nimport numpy as np\nfrom typing import List, Tuple\n\ndef sample_tuples_with_probability(\n    tuples: List[Tuple[int, int]], \n    probabilities: List[float], \n    num_samples: int\n) -> List[Tuple[int, int]]:\n    \"\"\"\n    Randomly samples tuples from a given list based on specified probabilities.\n\n    Args:\n        tuples (List[Tuple[int, int]]): A list of tuples to sample from.\n        probabilities (List[float]): A list of probabilities corresponding to each tuple.\n        num_samples (int): The number of samples to draw.\n\n    Returns:\n        List[Tuple[int, int]]: A list of sampled tuples.\n\n    Raises:\n        ValueError: If the length of tuples and probabilities do not match or if the sum of probabilities is not 1.\n    \"\"\"\n    if len(tuples) != len(probabilities):\n        raise ValueError(\"The length of tuples and probabilities must match.\")\n    if not np.isclose(sum(probabilities), 1):\n        raise ValueError(\"The sum of probabilities must be 1.\")\n\n    # Convert list of tuples to a numpy array for compatibility with np.random.choice\n    tuple_array = np.array(tuples)\n    \n    # Use np.random.choice with the correct parameters\n    sampled_indices = np.random.choice(len(tuple_array), size=num_samples, p=probabilities)\n    sampled_tuples = tuple\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 410, "library_problem_id": 119, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 411, "code": ["<code>\n```python\nimport numpy as np\n\ndef zero_pad_slice(array: np.ndarray, row_slice: slice, col_slice: slice) -> np.ndarray:\n    \"\"\"\n    Slices the input 2D array with zero padding if the slice exceeds the array's dimensions.\n\n    Parameters:\n    - array (np.ndarray): The input 2D array to be sliced.\n    - row_slice (slice): The slice object defining the row range.\n    - col_slice (slice): The slice object defining the column range.\n\n    Returns:\n    - np.ndarray: A 2D array of the specified slice size, zero-padded if necessary.\n    \"\"\"\n    # Determine the size of the desired slice\n    row_start, row_stop, _ = row_slice.indices(array.shape[0])\n    col_start, col_stop, _ = col_slice.indices(array.shape[1])\n    row_size = row_stop - row_start\n    col_size = col_stop - col_start\n\n    # Initialize a zero-padded array of the desired slice size\n    result = np.zeros((row_size, col_size))\n\n    # Calculate the valid slicing range within the array boundaries\n    valid_row_start = max(row_start, 0)\n    valid_row_stop = min(row_stop, array.shape[0])\n    valid_col_start = max(col_start, 0)\n    valid_col_stop = min(col_stop, array.shape[1])\n\n    # Calculate the position to\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 411, "library_problem_id": 120, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 412, "code": ["<code>\nimport numpy as np\n\ndef remove_negative_elements(arr: np.ndarray) -> np.ndarray:\n    # Create a boolean mask where True indicates non-negative elements\n    non_negative_mask = arr >= 0\n    # Use the mask to filter the array and return the result\n    return arr[non_negative_mask]\n\n# Example usage\nx = np.array([-2, -1.4, -1.1, 0, 1.2, 2.2, 3.1, 4.4, 8.3, 9.9, 10, 14, 16.2])\nresult = remove_negative_elements(x)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 412, "library_problem_id": 121, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 413, "code": ["<code>\nimport numpy as np\n\ndef remove_real_numbers(arr: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Remove all real numbers from a given complex number array.\n\n    Args:\n    arr (np.ndarray): A NumPy array containing both real and complex numbers.\n\n    Returns:\n    np.ndarray: A NumPy array containing only the complex numbers from the input array.\n    \"\"\"\n    return arr[np.iscomplex(arr)]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 413, "library_problem_id": 122, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 414, "code": ["<code>\ndef compute_bin_means(data: np.ndarray, bin_size: int) -> np.ndarray:\n    if not isinstance(bin_size, int) or bin_size <= 0:\n        raise ValueError(\"bin_size must be a positive integer.\")\n    if data.ndim != 1:\n        raise ValueError(\"data must be a 1-dimensional array.\")\n\n    # Calculate the number of complete bins\n    num_bins = len(data) // bin_size\n\n    # Reshape the data to form complete bins and calculate the mean of each bin\n    reshaped_data = data[:num_bins * bin_size].reshape(num_bins, bin_size)\n    bin_means = reshaped_data.mean(axis=1)\n\n    return bin_means\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 414, "library_problem_id": 123, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 123}}
{"id": 415, "code": ["<code>\n```python\nimport numpy as np\n\ndef calculate_bin_maximums(data: np.ndarray, bin_size: int) -> np.ndarray:\n    \"\"\"\n    Partition the input numpy array into bins of specified size and compute the maximum value for each bin.\n\n    Args:\n        data (np.ndarray): A 1D numpy array containing time series data.\n        bin_size (int): The size of each bin. Must be a positive integer.\n\n    Returns:\n        np.ndarray: A 1D numpy array containing the maximum values of each bin.\n                    The length of this array is floor(len(data) / bin_size).\n\n    Example:\n        For data = np.array([4, 2, 5, 6, 7, 5, 4, 3, 5, 7]) and bin_size = 2:\n        - Bins: [(4, 2), (5, 6), (7, 5), (4, 3), (5, 7)]\n        - Maximums: [4, 6, 7, 4, 7]\n\n        For data = np.array([4, 2, 5, 6, 7, 5, 4, 3, 5, 7]) and bin_size = 3:\n        - Bins: [(4, 2, 5), (6, 7, 5), (4, 3, 5)]\n        - Maximums: [5, 7, 5]\n    \"\"\"\n    # Calculate the number of bins\n    num_bins = len(\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 415, "library_problem_id": 124, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 416, "code": ["<code>\nimport numpy as np\n\ndef bin_and_compute_mean(data: np.ndarray, bin_size: int) -> np.ndarray:\n    # Calculate the number of complete bins that can be formed\n    num_bins = data.shape[1] // bin_size\n    \n    # Reshape the data to form bins and calculate the mean along the new axis\n    bin_means = np.mean(data[:, :num_bins * bin_size].reshape(data.shape[0], num_bins, bin_size), axis=2)\n    \n    return bin_means\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 416, "library_problem_id": 125, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 417, "code": ["<code>\n```python\n\"\"\"\nFunction to partition a numpy array containing time series data into equal-sized bins starting from the end of the array, \nand calculate the mean of each bin. If the last partition is smaller than the specified bin size, it is dropped.\n\nArgs:\n    data (np.ndarray): A 1D numpy array containing the time series data.\n    bin_size (int): The size of each bin. Must be a positive integer.\n\nReturns:\n    np.ndarray: A 1D numpy array containing the mean of each bin.\n\nExample:\n    Given data = np.array([4, 2, 5, 6, 7, 5, 4, 3, 5, 7]) and bin_size = 3,\n    The function will return np.array([5, 5.33, 4.33]).\n\"\"\"\n\nimport numpy as np\n\ndef calculate_bin_means(data: np.ndarray, bin_size: int) -> np.ndarray:\n    # Ensure bin_size is a positive integer\n    if bin_size <= 0:\n        raise ValueError(\"bin_size must be a positive integer\")\n    \n    # Calculate the number of complete bins that can be formed\n    num_complete_bins = len(data) // bin_size\n    \n    # Reverse the data to start binning from the end\n    reversed_data = data[::-1]\n    \n    # Create bins and calculate their means\n    bin_means = [\n        np.mean(reversed_data[i * bin_size:(i + 1) * bin_size])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 417, "library_problem_id": 126, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 123}}
{"id": 418, "code": ["<code>\ndef bin_and_calculate_mean(data: np.ndarray, bin_size: int) -> np.ndarray:\n    # Calculate the number of bins\n    num_bins = data.shape[1] // bin_size\n\n    # Create an empty list to store the bins\n    bins = []\n\n    # Loop over each row in the data\n    for row in data:\n        # Create a list to store the bins for this row\n        row_bins = []\n\n        # Loop over the row in chunks of bin_size\n        for i in range(num_bins * bin_size, 0, -bin_size):\n            # Append the mean of the bin to the row_bins list\n            row_bins.append(np.mean(row[i-bin_size:i]))\n\n        # Append the row_bins list to the bins list\n        bins.append(row_bins)\n\n    # Return the bins as a numpy array\n    return np.array(bins)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 418, "library_problem_id": 127, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 419, "code": ["<code>\ndef calculate_binned_means(data: np.ndarray, bin_size: int) -> np.ndarray:\n    # Calculate the number of bins\n    num_bins = data.shape[1] // bin_size\n\n    # Create a list to store the means of each bin\n    bin_means = []\n\n    # Iterate over each row\n    for row in data:\n        # Create a list to store the means of each bin in the current row\n        row_means = []\n\n        # Iterate over each bin in the current row\n        for i in range(num_bins):\n            # Get the start and end indices of the current bin\n            start = i * bin_size\n            end = (i + 1) * bin_size\n\n            # Get the current bin and compute its mean\n            bin = row[start:end]\n            mean = np.mean(bin)\n\n            # Append the mean to the list of means for the current row\n            row_means.append(mean)\n\n        # Append the list of means for the current row to the list of means for all rows\n        bin_means.append(row_means)\n\n    # Convert the list of means for all rows to a numpy array and return it\n    return np.array(bin_means)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 419, "library_problem_id": 128, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 123}}
{"id": 420, "code": ["<code>\ndef smoothclamp(x, x_min, x_max):\n    t = (x - x_min) / (x_max - x_min) # Normalize x to the range [0, 1]\n    t = max(0, min(1, t)) # Clamp t to the range [0, 1]\n    t = t * t * (3 - 2 * t) # Apply smoothing polynomial\n    return x_min + t * (x_max - x_min) # Scale and shift the result to the range [x_min, x_max]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 420, "library_problem_id": 129, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 421, "code": ["<code>\ndef smoothclamp(x, x_min, x_max, N):\n    t = np.clip((x - x_min) / (x_max - x_min), 0.0, 1.0)\n    return x_min + (x_max - x_min) * (6*t**5 - 15*t**4 + 10*t**3) if N == 5 else x_min + (x_max - x_min) * t\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 421, "library_problem_id": 130, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 129}}
{"id": 422, "code": ["<code>\n```python\n\"\"\"\nProblem:\nDetermine if it's possible to perform circular cross-correlation or auto-correlation on 1D arrays using existing functions from numpy, scipy, or matplotlib. The goal is to achieve a periodic circular correlation where the second array 'b' is treated as circular and slides over the fixed array 'a'. This is different from zero-padded correlation, which is the default behavior of numpy.correlate() and matplotlib.pyplot.xcorr.\n\nExample:\nGiven an array [1, 2, 3, 4], a circular correlation with a lag of 1 would result in [2, 3, 4, 1] due to the periodic assumption, as opposed to [2, 3, 4, 0] with zero-padding.\n\nObjective:\nFind a method or workaround to perform periodic circular correlation on arrays 'a' and 'b', where 'b' is the circular sliding array and 'a' is the fixed array.\n\nSolution:\nImplement a function to perform circular cross-correlation using numpy.fft for Fourier transforms, which inherently handle periodicity.\n\nArgs:\n    a (np.ndarray): The fixed 1D array.\n    b (np.ndarray): The 1D array to be circularly correlated with 'a'.\n\nReturns:\n    np.ndarray: The circular cross-correlation result of arrays 'a' and 'b'.\n\nExample Usage:\n    a = np.array([1, 2, 3, 4])\n    b = np.array([5, 4, 3, 2])\n    result = circular_cross_\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 422, "library_problem_id": 131, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 423, "code": ["<code>\ndef dataframe_to_3d_array(df: pd.DataFrame) -> np.ndarray:\n    # Ensure the DataFrame is sorted by 'major' and 'timestamp'\n    df_sorted = df.sort_index(level=['major', 'timestamp'])\n\n    # Extract unique categories and timestamps\n    categories = df_sorted.index.get_level_values('major').unique()\n    timestamps = df_sorted.index.get_level_values('timestamp').unique()\n\n    # Initialize an empty array with the desired shape\n    result_array = np.empty((df.shape[1], len(categories), len(timestamps)))\n\n    # Populate the array\n    for i, col in enumerate(df.columns):\n        for j, category in enumerate(categories):\n            result_array[i, j, :] = df_sorted.loc[category][col].values\n\n    return result_array\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 423, "library_problem_id": 132, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 424, "code": ["<code>\ndef dataframe_to_3d_array(df: pd.DataFrame) -> np.ndarray:\n    # Ensure the DataFrame is sorted by the MultiIndex for consistent reshaping\n    df = df.sort_index()\n\n    # Extract unique levels from the MultiIndex\n    categories = df.index.get_level_values('major').unique()\n    timestamps = df.index.get_level_values('timestamp').unique()\n\n    # Reshape the DataFrame values into a 3D NumPy array\n    num_categories = len(categories)\n    num_columns = df.shape[1]\n    num_timestamps = len(timestamps)\n\n    # Reshape the DataFrame values into the desired shape\n    array_3d = df.values.reshape(num_categories, num_timestamps, num_columns).transpose(0, 2, 1)\n\n    return array_3d\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 424, "library_problem_id": 133, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 132}}
{"id": 425, "code": ["<code>\nimport numpy as np\n\ndef convert_to_binary_matrix(integers: np.ndarray, m: int) -> np.ndarray:\n    # Ensure input is a numpy array of integers\n    if not isinstance(integers, np.ndarray) or integers.dtype.kind not in {'i', 'u'}:\n        raise ValueError(\"Input must be a numpy array of integers.\")\n    \n    # Ensure m is a positive integer\n    if not isinstance(m, int) or m <= 0:\n        raise ValueError(\"The length 'm' must be a positive integer.\")\n    \n    # Ensure each integer is within the valid range\n    if np.any(integers < 0) or np.any(integers >= 2**m):\n        raise ValueError(\"Each integer must satisfy 0 <= integer < 2**m.\")\n    \n    # Convert each integer to a binary string of length m and then to a numpy array\n    binary_matrix = np.array([list(f\"{num:0{m}b}\") for num in integers], dtype=int)\n    \n    return binary_matrix\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 425, "library_problem_id": 134, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 426, "code": ["<code>\ndef convert_to_binary_matrix(integers: np.ndarray, m: int) -> np.ndarray:\n    binary_matrix = []\n    for num in integers:\n        if num >= 0:\n            binary_str = format(num, f'0{m}b')\n        else:\n            binary_str = format((1 << m) + num, f'0{m}b')\n        binary_matrix.append([int(bit) for bit in binary_str[-m:]])\n    return np.array(binary_matrix)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 426, "library_problem_id": 135, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 427, "code": ["<code>\nimport numpy as np\n\ndef convert_and_xor(nums: list[int], m: int) -> np.ndarray:\n    # Convert each integer to a binary numpy array of length m\n    binary_arrays = np.array([np.array(list(np.binary_repr(num, width=m)), dtype=int) for num in nums])\n    \n    # Compute the XOR of all binary arrays\n    xor_result = np.bitwise_xor.reduce(binary_arrays, axis=0)\n    \n    return xor_result\n\n# Example usage\na = np.array([1, 2, 3, 4, 5])\nm = 6\nresult = convert_and_xor(a, m)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 427, "library_problem_id": 136, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 134}}
{"id": 428, "code": ["<code>\nimport numpy as np\n\ndef calculate_third_standard_deviation_interval(arr: np.ndarray) -> tuple[float, float]:\n    \"\"\"\n    Calculate the third standard deviation interval for a given NumPy array.\n\n    Args:\n    arr (np.ndarray): A one-dimensional NumPy array of numerical values.\n\n    Returns:\n    tuple[float, float]: A tuple containing two float values representing the start and end of the third standard deviation interval (\u03bc-3\u03c3, \u03bc+3\u03c3).\n\n    Mathematical Definitions:\n    - \u03bc (mu) is the mean of the array.\n    - \u03c3 (sigma) is the standard deviation of the array.\n    - The third standard deviation interval is defined as (\u03bc-3\u03c3, \u03bc+3\u03c3).\n    \"\"\"\n    mean = np.mean(arr)\n    std_dev = np.std(arr)\n    start_value = mean - 3 * std_dev\n    end_value = mean + 3 * std_dev\n    return (start_value, end_value)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 428, "library_problem_id": 137, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 429, "code": ["<code>\nimport numpy as np\n\ndef calculate_second_standard_deviation_interval(arr: np.ndarray) -> tuple[float, float]:\n    mean = np.mean(arr)\n    std_dev = np.std(arr)\n    lower_bound = mean - 2 * std_dev\n    upper_bound = mean + 2 * std_dev\n    return (lower_bound, upper_bound)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 429, "library_problem_id": 138, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 137}}
{"id": 430, "code": ["<code>\n\"\"\"\nCalculate the third standard deviation interval for a given numpy array.\n\nThis function computes the mean (\u03bc) and standard deviation (\u03c3) of the input array\nand returns a tuple representing the interval (\u03bc-3\u03c3, \u03bc+3\u03c3), which corresponds to\nthe range covering the third standard deviation from the mean.\n\nArgs:\n    a (np.ndarray): A numpy array of numerical values.\n\nReturns:\n    tuple: A tuple containing two float values representing the start and end of\n           the third standard deviation interval (\u03bc-3\u03c3, \u03bc+3\u03c3).\n\nExample:\n    >>> import numpy as np\n    >>> example_a = np.array([0, 1, 2, 5, 6, 7, 8, 8, 8, 10, 29, 32, 45])\n    >>> calculate_third_std_dev_interval(example_a)\n    (-19.632, 43.632)\n\"\"\"\n\nimport numpy as np\n\ndef calculate_third_std_dev_interval(a: np.ndarray) -> tuple:\n    # Calculate the mean (\u03bc) of the array\n    mean = np.mean(a)\n    \n    # Calculate the standard deviation (\u03c3) of the array\n    std_dev = np.std(a)\n    \n    # Calculate the start and end of the third standard deviation interval\n    start = mean - 3 * std_dev\n    end = mean + 3 * std_dev\n    \n    # Return the interval as a tuple\n    return (start, end)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 430, "library_problem_id": 139, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 137}}
{"id": 431, "code": ["<code>\nimport numpy as np\n\ndef detect_outliers(a: np.ndarray) -> np.ndarray:\n    # Calculate the mean (\u03bc) of the array\n    mean = np.mean(a)\n    \n    # Calculate the standard deviation (\u03c3) of the array\n    std_dev = np.std(a)\n    \n    # Define the 2nd standard deviation interval (\u03bc-2\u03c3, \u03bc+2\u03c3)\n    lower_bound = mean - 2 * std_dev\n    upper_bound = mean + 2 * std_dev\n    \n    # Determine outliers: values outside the interval are considered outliers\n    outliers = (a < lower_bound) | (a > upper_bound)\n    \n    return outliers\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 431, "library_problem_id": 140, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 137}}
{"id": 432, "code": ["<code>\nimport numpy as np\nimport numpy.ma as ma\n\ndef compute_percentile_excluding_nodata(data: np.ndarray, percentile: float) -> float:\n    # Define the NoData value\n    nodata_value = -3.40282347e+38\n    \n    # Create a masked array, masking NoData values and any value less than 0\n    masked_data = ma.masked_where((data == nodata_value) | (data < 0), data)\n    \n    # Compute the percentile from the masked array\n    result = np.percentile(masked_data.compressed(), percentile)\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 432, "library_problem_id": 141, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 141}}
{"id": 433, "code": ["<code>\nimport numpy as np\n\ndef zero_out_row_col(a: np.ndarray, zero_rows: int, zero_cols: int) -> np.ndarray:\n    a[zero_rows, :] = 0\n    a[:, zero_cols] = 0\n    return a\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 433, "library_problem_id": 142, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 142}}
{"id": 434, "code": ["<code>\nimport numpy as np\n\ndef zero_out_rows_and_columns(a: np.ndarray, zero_rows: list[int], zero_cols: list[int]) -> np.ndarray:\n    # Create a copy of the original array to avoid modifying it\n    result = np.copy(a)\n    \n    # Zero out the specified rows\n    for row in zero_rows:\n        result[row, :] = 0\n    \n    # Zero out the specified columns\n    for col in zero_cols:\n        result[:, col] = 0\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 434, "library_problem_id": 143, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 435, "code": ["<code>\nimport numpy as np\n\ndef zero_out_row_and_column(a: np.ndarray) -> np.ndarray:\n    # Set all elements in the second row (index 1) to zero\n    a[1, :] = 0\n    # Set all elements in the first column (index 0) to zero\n    a[:, 0] = 0\n    return a\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 435, "library_problem_id": 144, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 142}}
{"id": 436, "code": ["<code>\nimport numpy as np\n\ndef create_max_mask(array: np.ndarray, axis: int) -> np.ndarray:\n    max_values = np.max(array, axis=axis, keepdims=True)\n    mask = array == max_values\n    return mask\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 436, "library_problem_id": 145, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 145}}
{"id": 437, "code": ["<code>\nimport numpy as np\n\ndef create_min_mask(a: np.ndarray, axis: int) -> np.ndarray:\n    min_values = np.min(a, axis=axis, keepdims=True)\n    mask = a == min_values\n    return mask\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 437, "library_problem_id": 146, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 145}}
{"id": 438, "code": ["<code>\nimport numpy as np\n\ndef calculate_pearson_correlation(post: list[int], distance: list[int]) -> float:\n    if len(post) != len(distance):\n        raise ValueError(\"The input lists must be of equal length.\")\n    \n    correlation_coefficient = np.corrcoef(post, distance)[0, 1]\n    \n    return correlation_coefficient\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 438, "library_problem_id": 147, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 439, "code": ["<code>\nimport numpy as np\n\ndef construct_3d_array(X: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Constructs a 3D array from the given M x N matrix X.\n\n    Parameters:\n    X (np.ndarray): A 2D numpy array of shape (M, N).\n\n    Returns:\n    np.ndarray: A 3D numpy array of shape (N, M, M) where each slice [i, :, :] is the\n                matrix product of the i-th column of X and its transpose.\n    \"\"\"\n    M, N = X.shape\n    X_T = X.T\n    result = np.einsum('ij,ik->ijk', X, X_T)\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 439, "library_problem_id": 148, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 440, "code": ["<code>\ndef restore_matrix(Y: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Restore the original M x N matrix X from the given 3D array Y.\n\n    Parameters:\n    - Y (np.ndarray): A 3D numpy array of shape (N, M, M).\n\n    Returns:\n    - np.ndarray: The restored M x N matrix X.\n    \"\"\"\n    # Determine the dimensions of the original matrix X\n    N, M, _ = Y.shape\n    \n    # Initialize the matrix X\n    X = np.zeros((M, N))\n    \n    # Restore each column vector xi of X\n    for i in range(N):\n        # Extract the i-th M x M matrix from Y\n        Yi = Y[i]\n        \n        # Compute the i-th column vector xi by taking the square root of the diagonal elements\n        xi = np.sqrt(np.diag(Yi))\n        \n        # Assign the column vector xi to the i-th column of X\n        X[:, i] = xi\n    \n    return X\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 440, "library_problem_id": 149, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 148}}
{"id": 441, "code": ["<code>\nimport numpy as np\n\ndef contains_number(arr: np.ndarray, number: int) -> bool:\n    # Check if the number is present in the array using numpy's built-in functionality\n    return number in arr\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 441, "library_problem_id": 150, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 442, "code": ["<code>\nC = np.setdiff1d(A, B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 442, "library_problem_id": 151, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 443, "code": ["<code>\nimport numpy as np\n\ndef filter_array(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n    # Use numpy's isin function to create a boolean mask of elements in A that are in B\n    mask = np.isin(A, B)\n    # Use the mask to filter A\n    C = A[mask]\n    return C\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 443, "library_problem_id": 152, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 151}}
{"id": 444, "code": ["<code>\nimport numpy as np\n\ndef filter_array(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n    # Initialize a boolean mask with False values\n    mask = np.zeros_like(A, dtype=bool)\n    \n    # Iterate over the intervals defined by B\n    for i in range(len(B) - 1):\n        # Update the mask for elements in the interval [B[i], B[i+1])\n        mask |= (A >= B[i]) & (A < B[i+1])\n    \n    # Apply the mask to A to get the filtered array\n    C = A[mask]\n    return C\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 444, "library_problem_id": 153, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 151}}
{"id": 445, "code": ["<code>\ndef rank_highest_to_lowest(values: list) -> np.ndarray:\n    # Compute ranks using rankdata with 'max' method, then reverse the ranks\n    return len(values) - rankdata(values, method='max').astype(int) + 1\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 445, "library_problem_id": 154, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 446, "code": ["<code>\ndef descending_rank(a: List[int]) -> np.ndarray:\n    # Convert the list to a numpy array for efficient operations\n    a_array = np.array(a)\n    \n    # Get the sorted indices in descending order\n    sorted_indices = np.argsort(-a_array)\n    \n    # Initialize an array to store the ranks\n    ranks = np.empty_like(sorted_indices)\n    \n    # Assign ranks based on sorted indices\n    ranks[sorted_indices] = np.arange(len(a_array))\n    \n    return ranks\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 446, "library_problem_id": 155, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 154}}
{"id": 447, "code": ["<code>\n```python\n\"\"\"\nFunction to compute a 'highest to lowest' ranking of a list of values.\n\nThis function takes a list of numerical values and returns an array where each element is replaced by its rank in descending order. \nThe highest value receives the rank of 1, the second highest receives 2, and so on. Ties are assigned the average of the ranks they span.\n\nExample:\nFor input list [1, 2, 3, 4, 3, 2, 3, 4], the function should return [7, 6, 3, 1, 3, 6, 3, 1].\n\nArgs:\n    a (list of int or float): A list of numerical values to be ranked.\n\nReturns:\n    numpy.ndarray: An array of ranks corresponding to the input list, ranked from highest to lowest.\n\nMathematical/Logical Conditions:\n- The function uses the `rankdata` method from `scipy.stats` to compute ranks in ascending order.\n- The ranks are then adjusted to reflect descending order by subtracting each rank from the maximum rank plus one.\n\nExample Usage:\n    import numpy as np\n    from scipy.stats import rankdata\n\n    example_a = [1, 2, 3, 4, 3, 2, 3, 4]\n    result = f(example_a)\n    print(result)  # Output: array([7, 6, 3, 1, 3, 6, 3, 1])\n\"\"\"\n\nimport numpy as np\nfrom scipy.stats import rankdata\n\ndef f(a: list\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 447, "library_problem_id": 156, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 154}}
{"id": 448, "code": ["<code>\n```python\n\"\"\"\nFunction to combine two 2D numpy arrays representing x and y distances into a single 3D numpy array of tuples.\n\nThis function takes two 2D numpy arrays, `x_dists` and `y_dists`, where each element represents the x or y distance \nbetween points. The function returns a 3D numpy array where each element is a tuple containing the corresponding \nx and y distances from the input arrays.\n\nArgs:\n    x_dists (np.ndarray): A 2D numpy array of shape (n, n) representing x distances between points.\n    y_dists (np.ndarray): A 2D numpy array of shape (n, n) representing y distances between points.\n\nReturns:\n    np.ndarray: A 3D numpy array of shape (n, n, 2) where each element is a tuple (x_dist, y_dist).\n\nExample:\n    Given:\n    x_dists = np.array([[ 0, -1, -2],\n                        [ 1,  0, -1],\n                        [ 2,  1,  0]])\n\n    y_dists = np.array([[ 0, 1, -2],\n                        [-1,  0, 1],\n                        [-2,  1,  0]])\n\n    The function returns:\n    np.array([[[ 0,  0], [-1, 1], [-2, -2]],\n              [[ 1, -1], [ 0, 0], [-1, 1]],\n              [[ 2, -2], [ 1,\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 448, "library_problem_id": 157, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 157}}
{"id": 449, "code": ["<code>\ndef combine_distances(x_dists: np.ndarray, y_dists: np.ndarray) -> np.ndarray:\n    # Ensure the input arrays have the same shape\n    assert x_dists.shape == y_dists.shape, \"Input arrays must have the same shape.\"\n    \n    # Stack the arrays along the third dimension to create tuples of (x, y) distances\n    dists = np.dstack((x_dists, y_dists))\n    \n    return dists\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 449, "library_problem_id": 158, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 157}}
{"id": 450, "code": ["<code>\nimport numpy as np\n\ndef slice_and_flatten_array(seed: int, shape: tuple, second: list, third: list) -> np.ndarray:\n    np.random.seed(seed)\n    A = np.random.random(shape)\n    result = A[:, second, :][:, :, third].flatten()\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 450, "library_problem_id": 159, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 159}}
{"id": 451, "code": ["<code>\nimport numpy as np\n\ndef create_custom_4d_array() -> np.ndarray:\n    \"\"\"\n    Creates a 4-dimensional numpy array of zeros with specified lengths for each dimension.\n\n    Returns:\n        np.ndarray: A 4-dimensional array of zeros with dimensions (20, 10, 10, 2).\n    \"\"\"\n    arr = np.zeros((20, 10, 10, 2))\n    return arr\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 451, "library_problem_id": 160, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 160}}
{"id": 452, "code": ["<code>\ndef normalize_rows_l1(X: np.ndarray) -> np.ndarray:\n    # Calculate the L1 norm for each row\n    l1_norms = np.sum(np.abs(X), axis=1)\n    \n    # Reshape l1_norms to allow broadcasting for division\n    l1_norms = l1_norms.reshape(-1, 1)\n    \n    # Normalize each row by its L1 norm\n    normalized_X = X / l1_norms\n    \n    return normalized_X\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 452, "library_problem_id": 161, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 161}}
{"id": 453, "code": ["<code>\nimport numpy as np\nfrom numpy import linalg as LA\n\ndef l2_normalize_rows(X: np.ndarray) -> np.ndarray:\n    # Calculate the L2 norm of each row\n    norms = LA.norm(X, ord=2, axis=1, keepdims=True)\n    # Normalize each row by its L2 norm\n    normalized_X = X / norms\n    return normalized_X\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 453, "library_problem_id": 162, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 454, "code": ["<code>\nimport numpy as np\n\ndef normalize_rows_linf(X: np.ndarray) -> np.ndarray:\n    # Calculate the L\u221e norm for each row, which is the maximum absolute value in the row\n    linf_norms = np.max(np.abs(X), axis=1, keepdims=True)\n    \n    # Normalize each row by its L\u221e norm\n    normalized_X = X / linf_norms\n    \n    return normalized_X\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 454, "library_problem_id": 163, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 161}}
{"id": 455, "code": ["<code>\nimport numpy as np\nimport pandas as pd\n\ndef categorize_paths(df: pd.DataFrame) -> pd.DataFrame:\n    conditions = [\n        df[\"properties_path\"].str.contains('blog', na=False),\n        df[\"properties_path\"].str.contains('credit-card-readers/|machines|poss|team|transaction_fees', na=False),\n        df[\"properties_path\"].str.contains('signup|sign-up|create-account|continue|checkout', na=False),\n        df[\"properties_path\"].str.contains('complete', na=False),\n        df[\"properties_path\"].isin(['/za/', '/']),\n        df[\"properties_path\"].str.contains('promo', na=False)\n    ]\n    \n    choices = [\"blog\", \"info_pages\", \"signup\", \"completed\", \"home_page\", \"promo\"]\n    \n    df[\"page_type\"] = np.select(conditions, choices, default=np.nan)\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 455, "library_problem_id": 164, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 164}}
{"id": 456, "code": ["<code>\nimport numpy as np\n\ndef compute_distance_matrix(a: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the symmetric distance matrix for a given set of points.\n\n    Args:\n    a (np.ndarray): A NumPy array of shape (n, m) representing n points in m-dimensional space.\n\n    Returns:\n    np.ndarray: A symmetric distance matrix of shape (n, n) where element (i, j) is the Euclidean distance between point i and point j.\n    \"\"\"\n    # Subtract each row from every other row\n    diff = a[:, np.newaxis, :] - a[np.newaxis, :, :]\n    \n    # Square the differences\n    diff_squared = diff ** 2\n    \n    # Sum the squared differences along the last axis (columns)\n    sum_diff_squared = diff_squared.sum(axis=-1)\n    \n    # Take the square root of the sum to get the Euclidean distance\n    distance = np.sqrt(sum_diff_squared)\n    \n    # Make the matrix symmetric by taking the minimum of the current value and the transposed value\n    result = np.minimum(distance, distance.T)\n    \n    # Set the diagonal to zero\n    np.fill_diagonal(result, 0)\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 456, "library_problem_id": 165, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 165}}
{"id": 457, "code": ["<code>\nimport numpy as np\n\ndef calculate_pairwise_distances(data: np.ndarray) -> np.ndarray:\n    n = data.shape[0]\n    distance_matrix = np.zeros((n, n))\n    for i in range(n):\n        for j in range(i, n):\n            distance = np.linalg.norm(data[i] - data[j])\n            distance_matrix[i, j] = distance\n            distance_matrix[j, i] = distance\n    return distance_matrix\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 457, "library_problem_id": 166, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 165}}
{"id": 458, "code": ["<code>\nimport numpy as np\n\ndef compute_pairwise_distances(a: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the pairwise Euclidean distances between points in the input array.\n\n    Parameters:\n    a (np.ndarray): A 2D array of shape (n, m) containing n points each of dimension m.\n\n    Returns:\n    np.ndarray: A 2D array of shape (n, n) representing the upper triangular matrix of pairwise distances.\n    \"\"\"\n    # Calculate the squared norms of each point\n    squared_norms = np.sum(a**2, axis=1)\n    \n    # Compute the distance matrix using the formula:\n    # dist[i, j] = sqrt(squared_norms[i] + squared_norms[j] - 2 * np.dot(a[i], a[j]))\n    dist_matrix = np.sqrt(squared_norms[:, np.newaxis] + squared_norms - 2 * np.dot(a, a.T))\n    \n    # Return the upper triangular part of the distance matrix\n    return np.triu(dist_matrix)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 458, "library_problem_id": 167, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 165}}
{"id": 459, "code": ["<code>\nimport numpy as np\n\ndef calculate_mean_of_strings(string_numbers: list[str]) -> float:\n    numeric_array = np.asarray(string_numbers, dtype=float)\n    mean_value = np.mean(numeric_array)\n    return mean_value\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 459, "library_problem_id": 168, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 168}}
{"id": 460, "code": ["<code>\nimport numpy as np\n\ndef calculate_mean(A: list[str]) -> float:\n    \"\"\"\n    Calculate the mean of a list of numeric strings, including 'inf', using NumPy.\n\n    Args:\n    A (list of str): A list of strings representing numbers, where 'inf' is a valid entry.\n\n    Returns:\n    float: The mean of the numbers in the list, computed without explicit loops.\n    \"\"\"\n    # Convert the list of strings to a NumPy array of floats\n    NA = np.array(A, dtype=float)\n    \n    # Replace 'inf' with infinity\n    NA[NA == np.inf] = np.inf\n    \n    # Calculate and return the mean of the array\n    return np.mean(NA)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 460, "library_problem_id": 169, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 168}}
{"id": 461, "code": ["<code>\n\"\"\"\nFunction to calculate the mean of a list containing numeric strings and 'np.inf'.\n\nThis function takes a list of strings, where each string represents a number or 'np.inf',\nand calculates the mean of the list after converting the strings to their appropriate\nnumeric types. The function handles the conversion of 'np.inf' to the numpy infinity\nconstant and other numeric strings to floats.\n\nArgs:\n    A (List[str]): A list of strings, each representing a number or 'np.inf'.\n\nReturns:\n    float: The mean of the converted list.\n\nExample:\n    >>> A = ['np.inf', '33.33', '33.33', '33.37']\n    >>> calculate_mean(A)\n    inf\n\"\"\"\n\nimport numpy as np\n\ndef calculate_mean(A: list) -> float:\n    # Convert each element in the list to a float, handling 'np.inf' appropriately\n    converted_A = [np.inf if x == 'np.inf' else float(x) for x in A]\n    \n    # Calculate and return the mean of the converted list\n    return np.mean(converted_A)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 461, "library_problem_id": 170, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 168}}
{"id": 462, "code": ["<code>\ndef remove_adjacent_duplicates_and_zeros(arr: np.ndarray) -> np.ndarray:\n    result = []\n    for i in range(len(arr)):\n        if arr[i] != 0 and (i == 0 or arr[i] != arr[i-1]):\n            result.append(arr[i])\n    return np.array(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 462, "library_problem_id": 171, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 171}}
{"id": 463, "code": ["<code>\nimport numpy as np\n\ndef remove_adjacent_duplicates_and_zeros(arr: np.ndarray) -> np.ndarray:\n    # Initialize an empty list to store the result\n    result = []\n    # Iterate over the array, keeping track of the previous non-zero value\n    previous_value = None\n    for value in arr.flatten():\n        # Check if the current value is non-zero and different from the previous value\n        if value != 0 and value != previous_value:\n            result.append(value)\n            previous_value = value\n    # Convert the result list back to a numpy array with shape (m, 1)\n    return np.array(result).reshape(-1, 1)\n\n# Example usage\na = np.array([0, 0, 1, 1, 1, 2, 2, 0, 1, 3, 3, 3]).reshape(-1, 1)\nresult = remove_adjacent_duplicates_and_zeros(a)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 463, "library_problem_id": 172, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 171}}
{"id": 464, "code": ["<code>\nimport numpy as np\nimport pandas as pd\n\nlat = np.array([[10, 20, 30],\n                [20, 11, 33],\n                [21, 20, 10]])\n\nlon = np.array([[100, 102, 103],\n                [105, 101, 102],\n                [100, 102, 103]])\n\nval = np.array([[17, 2, 11],\n                [86, 84, 1],\n                [9, 5, 10]])\n\n# Flatten the arrays and create the DataFrame\ndf = pd.DataFrame({\n    'lat': lat.flatten(),\n    'lon': lon.flatten(),\n    'val': val.flatten()\n})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 464, "library_problem_id": 173, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 173}}
{"id": 465, "code": ["<code>\ndef create_dataframe(lat: np.ndarray, lon: np.ndarray, val: np.ndarray) -> pd.DataFrame:\n    # Flatten the input arrays to create a single list of values for each column\n    lat_flat = lat.flatten()\n    lon_flat = lon.flatten()\n    val_flat = val.flatten()\n    \n    # Create a pandas DataFrame using the flattened arrays\n    df = pd.DataFrame({'lat': lat_flat, 'lon': lon_flat, 'val': val_flat})\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 465, "library_problem_id": 174, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 173}}
{"id": 466, "code": ["<code>\nimport numpy as np\nimport pandas as pd\n\ndef create_dataframe_with_maximum(lat: np.ndarray, lon: np.ndarray, val: np.ndarray) -> pd.DataFrame:\n    # Flatten the arrays to ensure each corresponding element is aligned in the same row\n    lat_flat = lat.flatten()\n    lon_flat = lon.flatten()\n    val_flat = val.flatten()\n\n    # Create a DataFrame from the flattened arrays\n    df = pd.DataFrame({\n        'lat': lat_flat,\n        'lon': lon_flat,\n        'val': val_flat\n    })\n\n    # Calculate the maximum value for each row and add it as a new column\n    df['maximum'] = df[['lat', 'lon', 'val']].max(axis=1)\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 466, "library_problem_id": 175, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 173}}
{"id": 467, "code": ["<code>\nimport numpy as np\nfrom typing import Tuple, List\n\ndef moving_window_2d(array: np.ndarray, window_size: Tuple[int, int]) -> List[np.ndarray]:\n    # Get the shape of the input array\n    array_shape = array.shape\n\n    # Get the window dimensions\n    window_height, window_width = window_size\n\n    # Initialize an empty list to store the window views\n    window_views = []\n\n    # Iterate over the array in steps of the window size\n    for i in range(0, array_shape[0], window_height):\n        for j in range(0, array_shape[1], window_width):\n            # Get the current window view\n            window_view = array[i:i+window_height, j:j+window_width]\n\n            # Check if the window view is fully within the array\n            if window_view.shape == window_size:\n                # If so, append the view to the list\n                window_views.append(window_view)\n            else:\n                # If not, check if the window view overlaps the array boundaries\n                if i + window_height > array_shape[0] or j + window_width > array_shape[1]:\n                    # If so, slice the view to the overlapping portion\n                    window_views.append(window_view[:min(window_height, array_shape[0]-i), :min(window_width, array_shape[1]-j)])\n\n    return window_views\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 467, "library_problem_id": 176, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 176}}
{"id": 468, "code": ["<code>\nimport numpy as np\nfrom typing import List, Tuple\n\ndef moving_window_with_edge_effects(grid: np.ndarray, window_size: Tuple[int, int]) -> List[np.ndarray]:\n    # Initialize the list to store the resulting windows\n    result = []\n\n    # Get the dimensions of the grid and the window\n    grid_height, grid_width = grid.shape\n    window_height, window_width = window_size\n\n    # Iterate over each possible center position of the window in the grid\n    for i in range(grid_height):\n        for j in range(grid_width):\n            # Calculate the window boundaries, ensuring they do not exceed grid boundaries\n            start_row = max(0, i - window_height // 2)\n            end_row = min(grid_height, i + window_height // 2 + 1)\n            start_col = max(0, j - window_width // 2)\n            end_col = min(grid_width, j + window_width // 2 + 1)\n\n            # Extract the window from the grid and append to the result list\n            window = grid[start_row:end_row, start_col:end_col]\n            result.append(window)\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 468, "library_problem_id": 177, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 176}}
{"id": 469, "code": ["<code>\ndef compute_complex_mean(arr: np.ndarray) -> complex:\n    if len(arr) == 0:\n        raise ValueError(\"Input array cannot be empty.\")\n\n    real_part = arr.real\n    imag_part = arr.imag\n\n    real_mean = np.mean(real_part[np.isfinite(real_part)])\n    imag_mean = np.mean(imag_part[np.isfinite(imag_part)])\n\n    if np.isinf(real_mean):\n        real_mean = np.inf if np.all(np.isinf(real_part)) else -np.inf\n\n    if np.isinf(imag_mean):\n        imag_mean = np.inf if np.all(np.isinf(imag_part)) else -np.inf\n\n    return complex(real_mean, imag_mean)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 469, "library_problem_id": 178, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 178}}
{"id": 470, "code": ["<code>\n\"\"\"\nFunction to compute the mean of an array of complex numbers, handling complex infinities.\n\nThis function addresses the issue of computing the mean of an array containing complex numbers,\nincluding complex infinities, using numpy. The standard numpy.mean function may return unexpected\nresults when complex infinities are involved, particularly in the imaginary part.\n\nArgs:\n    a (np.ndarray): An array of complex numbers. Default is np.array([1 + 0j, 2 + 3j, np.inf + 0j]).\n\nReturns:\n    complex: The mean of the array, with special handling for complex infinities.\n\nNotes:\n    - If the array contains complex infinities, the mean is computed by considering only the finite\n      parts of the array. If all elements are infinite, the mean will be infinity.\n    - The function avoids the RuntimeWarning encountered in numpy when dealing with complex infinities.\n\"\"\"\n\nimport numpy as np\n\ndef compute_complex_mean(a: np.ndarray = np.array([1 + 0j, 2 + 3j, np.inf + 0j])) -> complex:\n    # Filter out complex infinities from the array\n    finite_elements = a[np.isfinite(a)]\n    \n    # Check if there are any finite elements to compute the mean\n    if finite_elements.size > 0:\n        mean_value = np.mean(finite_elements)\n    else:\n        # If all elements are infinite, return infinity\n        mean_value = np.inf + 0j\n    \n    return mean_value\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 470, "library_problem_id": 179, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 178}}
{"id": 471, "code": ["<code>\ndef slice_last_dimension(Z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Slices the input array `Z` to extract the last element along its last dimension.\n\n    Args:\n    Z (np.ndarray): A NumPy array of arbitrary dimensions.\n\n    Returns:\n    np.ndarray: A sliced array containing the last element along the last dimension of `Z`.\n    \"\"\"\n    return Z[..., -1:]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 471, "library_problem_id": 180, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 180}}
{"id": 472, "code": ["<code>\nimport numpy as np\n\ndef slice_last_element_first_axis(a: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Slices the input array `a` to select the last element along the first axis.\n\n    Parameters:\n    a (np.ndarray): An n-dimensional NumPy array.\n\n    Returns:\n    np.ndarray: A sliced array containing only the last element along the first axis of `a`.\n    \"\"\"\n    # Determine the number of dimensions of the array\n    num_dims = a.ndim\n\n    # Create a slicing tuple that selects the last element along the first axis\n    slicing_tuple = (-1,) + (slice(None),) * (num_dims - 1)\n\n    # Perform the slicing operation\n    result = a[slicing_tuple]\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 472, "library_problem_id": 181, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 180}}
{"id": 473, "code": ["<code>\ndef is_array_in_list(target_array: np.ndarray, array_list: List[np.ndarray]) -> bool:\n    for arr in array_list:\n        if np.array_equal(target_array, arr):\n            return True\n    return False\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 473, "library_problem_id": 182, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 182}}
{"id": 474, "code": ["<code>\ndef is_array_in_list(target_array: np.ndarray, array_list: List[np.ndarray]) -> bool:\n    for array in array_list:\n        if np.array_equal(target_array, array, equal_nan=True):\n            return True\n    return False\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 474, "library_problem_id": 183, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 182}}
{"id": 475, "code": ["<code>\nimport numpy as np\nfrom scipy import interpolate as intp\n\ndef interpolate_upsampled_array() -> np.ndarray:\n    \"\"\"\n    Upsample a 2D NumPy array and apply linear interpolation to smooth the values.\n\n    Returns:\n        np.ndarray: A 2D array with interpolated values.\n    \"\"\"\n    # Original array\n    a = np.arange(0, 4, 1).reshape(2, 2)\n    \n    # Upsample the array\n    a_upsampled = a.repeat(2, axis=0).repeat(2, axis=1)\n    \n    # Define new grid for interpolation\n    x_new = np.linspace(0, 2, 4)\n    y_new = np.linspace(0, 2, 4)\n    \n    # Create interpolation function\n    f = intp.interp2d(np.arange(2), np.arange(2), a, kind='linear')\n    \n    # Apply interpolation\n    result = f(x_new, y_new)\n    \n    return result\n\n# Execute the function\nresult = interpolate_upsampled_array()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 475, "library_problem_id": 184, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 184}}
{"id": 476, "code": ["<code>\ndef generate_conditional_cumsum(df: pd.DataFrame) -> pd.DataFrame:\n    df['Q_cum'] = df.groupby('D')['Q'].cumsum()\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 476, "library_problem_id": 185, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 185}}
{"id": 477, "code": ["<code>\nimport numpy as np\n\n# Define the input matrix\na = np.matrix([[3, 4, 3, 1],\n               [1, 3, 2, 6],\n               [2, 4, 1, 5],\n               [3, 3, 5, 2]])\n\n# Perform Singular Value Decomposition\nU, i, V = np.linalg.svd(a, full_matrices=True)\n\n# Transform the singular values into a diagonal matrix\ni_diagonal = np.diag(i)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 477, "library_problem_id": 186, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 186}}
{"id": 478, "code": ["<code>\nimport numpy as np\n\ndef zero_non_diagonal_elements(matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Convert all non-diagonal elements of a square symmetric numpy ndarray to zero.\n\n    Args:\n    matrix (np.ndarray): A square symmetric numpy ndarray.\n\n    Returns:\n    np.ndarray: A numpy ndarray with non-diagonal elements set to zero.\n\n    Conditions:\n    - The input matrix is square and symmetric.\n    - Do not use explicit loops (e.g., for, while) in the implementation.\n    \"\"\"\n    return np.tril(matrix) + np.triu(matrix.T)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 478, "library_problem_id": 187, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 187}}
{"id": 479, "code": ["<code>\ndef generate_datetime_series(start: str, end: str, num_intervals: int) -> pd.DatetimeIndex:\n    # Convert start and end strings to datetime objects\n    t0 = pd.to_datetime(start)\n    tf = pd.to_datetime(end)\n    \n    # Calculate the total duration in seconds\n    total_seconds = (tf - t0).total_seconds()\n    \n    # Generate equally spaced time intervals in seconds\n    intervals = np.linspace(0, total_seconds, num_intervals)\n    \n    # Create datetime objects for each interval\n    datetime_series = [t0 + pd.Timedelta(seconds=interval) for interval in intervals]\n    \n    # Convert the list of datetime objects to a Pandas DatetimeIndex\n    return pd.DatetimeIndex(datetime_series)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 479, "library_problem_id": 188, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 188}}
{"id": 480, "code": ["<code>\ndef find_coordinate_index(x: np.ndarray, y: np.ndarray, a: int, b: int) -> int:\n    for i in range(len(x)):\n        if x[i] == a and y[i] == b:\n            return i\n    return -1\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 480, "library_problem_id": 189, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 189}}
{"id": 481, "code": ["<code>\nreturn list(np.where((x == a) & (y == b))[0])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 481, "library_problem_id": 190, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 189}}
{"id": 482, "code": ["<code>\n# Convert lists to numpy arrays\nx = np.array(x)\ny = np.array(y)\n\n# Compute the coefficients a, b, c\nresult = np.polyfit(x, y, 2)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 482, "library_problem_id": 191, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 191}}
{"id": 483, "code": ["<code>\nimport numpy as np\n\ndef find_polynomial_coefficients(x: list, y: list, degree: int) -> np.ndarray:\n    \"\"\"\n    Calculate the coefficients of a polynomial that best fits the given data points.\n\n    Parameters:\n    x (list): A list of x-coordinates of the data points.\n    y (list): A list of y-coordinates of the data points.\n    degree (int): The degree of the polynomial to fit.\n\n    Returns:\n    np.ndarray: An array of coefficients of the polynomial, ordered from the highest degree to the lowest.\n    \"\"\"\n    matrix = np.zeros((degree + 1, degree + 1))\n    vector = np.zeros(degree + 1)\n\n    for i in range(degree + 1):\n        for j in range(degree + 1):\n            matrix[i][j] = sum(x[k] ** (i + j) for k in range(len(x)))\n        vector[i] = sum(y[k] * x[k] ** i for k in range(len(x)))\n\n    return np.linalg.solve(matrix, vector)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 483, "library_problem_id": 192, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 191}}
{"id": 484, "code": ["<code>\ndef subtract_array_from_dataframe(temp_arr: list, temp_df: pd.DataFrame) -> pd.DataFrame:\n    return temp_df.apply(lambda row: row - temp_arr[row.name], axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 484, "library_problem_id": 193, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 193}}
{"id": 485, "code": ["<code>\nimport numpy as np\n\ndef transform_matrix(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Transforms a 3D matrix A by multiplying each vector along the third dimension with matrix B.\n\n    Args:\n        A (np.ndarray): A 3D numpy array with shape (x, y, 3).\n        B (np.ndarray): A 2D numpy array with shape (3, 3).\n\n    Returns:\n        np.ndarray: A 3D numpy array with shape (x, y, 3) resulting from the transformation.\n\n    Raises:\n        ValueError: If the dimensions of A and B are not compatible for the operation.\n    \"\"\"\n    # Ensure A has shape (x, y, 3) and B has shape (3, 3)\n    if A.shape[-1] != 3 or B.shape != (3, 3):\n        raise ValueError(\"Matrix A must have shape (x, y, 3) and B must have shape (3, 3).\")\n\n    # Perform the transformation\n    result = np.einsum('ijk,kl->ijl', A, B)\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 485, "library_problem_id": 194, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 194}}
{"id": 486, "code": ["<code>\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef normalize_entire_array(a: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Normalize the entire 2D numpy array using MinMaxScaler.\n\n    This function scales all elements of the array `a` such that the minimum \n    value becomes 0 and the maximum value becomes 1, considering the entire \n    array as a single dataset.\n\n    Args:\n        a (np.ndarray): A 2D numpy array to be normalized.\n\n    Returns:\n        np.ndarray: A 2D numpy array with normalized values.\n    \"\"\"\n    # Reshape the array to a single column for fitting the scaler\n    reshaped_a = a.reshape(-1, 1)\n    \n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n    \n    # Fit and transform the reshaped array\n    normalized_reshaped = scaler.fit_transform(reshaped_a)\n    \n    # Reshape back to the original shape\n    normalized_a = normalized_reshaped.reshape(a.shape)\n    \n    return normalized_a\n\na = np.array([[-1, 2], [-0.5, 6]])\nresult = normalize_entire_array(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 486, "library_problem_id": 195, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 195}}
{"id": 487, "code": ["<code>\n```python\n\"\"\"\nFunction to perform min-max scaling on a numpy array along each row.\n\nThis function takes a 2D numpy array as input and rescales the values in each row to be between 0 and 1. \nThe rescaling is done using the formula:\n    X_rescaled = (X - X_min) / (X_max - X_min)\nwhere X is an element in the row, X_min is the minimum value in the row, and X_max is the maximum value in the row.\n\nArgs:\n    arr (np.ndarray): A 2D numpy array with numerical values.\n\nReturns:\n    np.ndarray: A 2D numpy array with values rescaled to the range [0, 1] along each row.\n\nExample:\n    >>> arr = np.array([[1.0, 2.0, 3.0], [0.1, 5.1, 100.1], [0.01, 20.1, 1000.1]])\n    >>> result = rescale_rows(arr)\n    >>> print(result)\n    array([[0. , 0.5, 1. ],\n           [0. , 0.05, 1. ],\n           [0. , 0.02008999, 1. ]])\n\"\"\"\n\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef rescale_rows(arr: np.ndarray) -> np.ndarray:\n    # Initialize the MinMaxScaler with feature_range set to (\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 487, "library_problem_id": 196, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 488, "code": ["<code>\n```python\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef normalize_matrices(data: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Normalize each 2D matrix within a 3D numpy array using MinMaxScaler.\n    \n    Args:\n        data (np.ndarray): A 3D numpy array where each element is a 2D matrix.\n        \n    Returns:\n        np.ndarray: A 3D numpy array with each 2D matrix normalized such that\n                    the minimum value in each matrix is 0 and the maximum value is 1.\n    \"\"\"\n    # Initialize an empty list to store normalized matrices\n    normalized_data = []\n    \n    # Iterate over each 2D matrix in the 3D array\n    for matrix in data:\n        # Reshape the matrix to a 1D array for MinMaxScaler\n        reshaped_matrix = matrix.flatten().reshape(-1, 1)\n        \n        # Apply MinMaxScaler to the reshaped matrix\n        scaler = MinMaxScaler()\n        normalized_matrix = scaler.fit_transform(reshaped_matrix)\n        \n        # Reshape back to the original matrix shape\n        normalized_matrix = normalized_matrix.reshape(matrix.shape)\n        \n        # Append the normalized matrix to the list\n        normalized_data.append(normalized_matrix)\n    \n    # Convert the list of normalized matrices back to a 3D numpy array\n    return np.array(normalized_data)\n\na = np.array([[[1, 0\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 488, "library_problem_id": 197, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 195}}
{"id": 489, "code": ["<code>\nimport numpy as np\n\ndef modify_array(arr: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Modify a two-dimensional numpy array based on specific conditions using Boolean indexing.\n\n    Args:\n    arr (np.ndarray): A two-dimensional numpy array.\n\n    Returns:\n    np.ndarray: The modified numpy array where:\n        - Values less than -10 are set to 0.\n        - Values greater than or equal to 15 are set to 30.\n        - All other values are increased by 5.\n    \"\"\"\n    # Create a copy of the array to avoid modifying the original array\n    arr_temp = arr.copy()\n\n    # Boolean mask for values less than -10\n    mask_less_than_minus_10 = arr_temp < -10\n\n    # Boolean mask for values greater than or equal to 15\n    mask_greater_equal_15 = arr_temp >= 15\n\n    # Apply the transformations\n    arr_temp[mask_less_than_minus_10] = 0\n    arr_temp[mask_greater_equal_15] = 30\n    arr_temp[~mask_less_than_minus_10 & ~mask_greater_equal_15] += 5\n\n    return arr_temp\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 489, "library_problem_id": 198, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 198}}
{"id": 490, "code": ["<code>\nimport numpy as np\n\ndef modify_array(arr: np.ndarray, n1: list, n2: list) -> np.ndarray:\n    for i in range(arr.shape[0]):\n        mask1 = arr[i] < n1[i]\n        mask2 = arr[i] >= n2[i]\n        mask3 = ~mask1 & ~mask2  # Elements between n1 and n2\n        arr[i][mask1] = 0\n        arr[i][mask2] = 30\n        arr[i][mask3] += 5\n    return arr\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 490, "library_problem_id": 199, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 198}}
{"id": 491, "code": ["<code>\ndef count_true_differences(n: int, m: int, tolerance: float = 1e-9) -> int:\n    import numpy as np\n\n    tag = np.random.rand(n, m)\n    s1 = np.sum(tag, axis=1)\n    s2 = np.sum(tag[:, ::-1], axis=1)\n    truly_different_count = np.sum(np.abs(s1 - s2) > tolerance)\n\n    return truly_different_count\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 491, "library_problem_id": 200, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 200}}
{"id": 492, "code": ["<code>\n# Use np.isclose to compare elements with a tolerance, and np.isnan to handle NaN values\n    differences = ~np.isclose(s1, s2, equal_nan=True)\n    \n    # Count the number of true differences\n    result = np.sum(differences)\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 492, "library_problem_id": 201, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 200}}
{"id": 493, "code": ["<code>\ndef are_all_arrays_equal(arrays: List[np.ndarray]) -> bool:\n    if not arrays:\n        return True\n    first_array = arrays[0]\n    return all(np.array_equal(first_array, array) for array in arrays[1:])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 493, "library_problem_id": 202, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Origin", "perturbation_origin_id": 202}}
{"id": 494, "code": ["<code>\nimport numpy as np\n\ndef check_all_arrays_have_nan(arrays: list[np.ndarray]) -> bool:\n    return all(np.any(np.isnan(array)) for array in arrays)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 494, "library_problem_id": 203, "library": "Numpy", "test_case_cnt": 5, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 202}}
{"id": 495, "code": ["<code>\nimport numpy as np\n\ndef zero_pad_array(array: np.ndarray, target_shape: tuple) -> np.ndarray:\n    \"\"\"\n    Zero-pads a given 2D numpy array to match the target shape.\n\n    Parameters:\n    array (np.ndarray): The input 2D array to be padded.\n    target_shape (tuple): A tuple representing the target shape (rows, columns).\n\n    Returns:\n    np.ndarray: A new 2D array padded with zeros to match the target shape.\n    \"\"\"\n    # Calculate the padding widths\n    pad_width = [(0, target_shape[0] - array.shape[0]), (0, target_shape[1] - array.shape[1])]\n\n    # Pad the array with zeros\n    padded_array = np.pad(array, pad_width)\n\n    return padded_array\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 495, "library_problem_id": 204, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 204}}
{"id": 496, "code": ["<code>\nimport numpy as np\n\ndef zero_pad_array(a: np.ndarray, target_shape: tuple) -> np.ndarray:\n    result = np.zeros(target_shape)\n    result[:a.shape[0], :a.shape[1]] = a\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 496, "library_problem_id": 205, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 497, "code": ["<code>\nimport numpy as np\n\ndef pad_array(array: np.ndarray, target_shape: Tuple[int, int], pad_value: int) -> np.ndarray:\n    \"\"\"\n    Pads a given 2D array to the specified target shape using the specified pad value.\n    \n    Parameters:\n    - array (np.ndarray): The input array to be padded.\n    - target_shape (Tuple[int, int]): The desired shape of the output array.\n    - pad_value (int): The value to use for padding.\n    \n    Returns:\n    - np.ndarray: The padded array with the specified target shape.\n    \"\"\"\n    # Initialize the result array with the pad value\n    result = np.full(target_shape, pad_value)\n    \n    # Determine the original shape of the input array\n    original_shape = array.shape\n    \n    # Copy the original array into the result array\n    result[:original_shape[0], :original_shape[1]] = array\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 497, "library_problem_id": 206, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 204}}
{"id": 498, "code": ["<code>\nimport numpy as np\n\ndef zero_pad_array(arr: np.ndarray = np.ones((41, 13)), target_shape: tuple = (93, 13)) -> np.ndarray:\n    \"\"\"\n    Zero-pad a 2D array to match the specified target shape.\n\n    Parameters:\n    arr (np.ndarray): The input 2D array to be padded.\n    target_shape (tuple): The desired shape after padding, given as (rows, columns).\n\n    Returns:\n    np.ndarray: A new 2D array padded with zeros to match the target shape.\n    \"\"\"\n    # Calculate the padding amounts for rows and columns\n    pad_rows = target_shape[0] - arr.shape[0]\n    pad_cols = target_shape[1] - arr.shape[1]\n\n    # Apply zero-padding to the right and bottom of the array\n    padded_array = np.pad(arr, ((0, pad_rows), (0, pad_cols)), mode='constant', constant_values=0)\n\n    return padded_array\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 498, "library_problem_id": 207, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 204}}
{"id": 499, "code": ["<code>\nimport numpy as np\n\ndef zero_pad_array(array: np.ndarray, target_shape: tuple) -> np.ndarray:\n    \"\"\"\n    Zero-pad a given array to match the specified target shape.\n\n    Args:\n        array (np.ndarray): The input array to be padded.\n        target_shape (tuple): The target shape (rows, columns) to pad the array to.\n\n    Returns:\n        np.ndarray: The zero-padded array with the specified target shape.\n\n    Example:\n        >>> a = np.ones((41, 12))\n        >>> zero_pad_array(a, (93, 13))\n        array of shape (93, 13) with zero-padding applied\n    \"\"\"\n    current_shape = array.shape\n    pad_height = target_shape[0] - current_shape[0]\n    pad_width = target_shape[1] - current_shape[1]\n\n    # Calculate padding for top/bottom and left/right\n    pad_top = pad_height // 2\n    pad_bottom = pad_height - pad_top\n    pad_left = pad_width // 2\n    pad_right = pad_width - pad_left\n\n    # Apply padding\n    padded_array = np.pad(array, ((pad_top, pad_bottom), (pad_left, pad_right)), mode='constant', constant_values=0)\n    return padded_array\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 499, "library_problem_id": 208, "library": "Numpy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 204}}
{"id": 500, "code": ["<code>\n```python\n\"\"\"\nFunction to reshape a numpy array into a specified matrix form.\n\nThis function takes an integer `n` and generates a numpy array with elements\nranging from 0 to `n-1`. It then reshapes this array into a matrix with a\nspecified number of columns. The number of rows is calculated based on the\ntotal number of elements and the specified number of columns.\n\nArgs:\n    n (int): The number of elements in the initial numpy array.\n    columns (int): The number of columns for the reshaped matrix.\n\nReturns:\n    np.ndarray: A reshaped numpy array with the specified number of columns.\n\nRaises:\n    ValueError: If the number of elements in the array is not divisible by\n                the specified number of columns.\n\nExample:\n    >>> reshape_array(12, 3)\n    array([[ 0,  1,  2],\n           [ 3,  4,  5],\n           [ 6,  7,  8],\n           [ 9, 10, 11]])\n\"\"\"\n\nimport numpy as np\n\ndef reshape_array(n: int, columns: int) -> np.ndarray:\n    # Create a numpy array with elements from 0 to n-1\n    array = np.arange(n)\n    \n    # Calculate the number of rows needed for the specified number of columns\n    rows = n // columns\n    \n    # Check if the total number of elements is divisible by the number of columns\n    if n % columns != 0:\n        raise ValueError(\"The number of elements is\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 500, "library_problem_id": 209, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 209}}
{"id": 501, "code": ["<code>\ndef select_elements(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n    # Ensure `a` is a 3D array and `b` is a 2D array with compatible dimensions\n    assert a.ndim == 3 and b.ndim == 2, \"Array `a` must be 3D and `b` must be 2D.\"\n    assert a.shape[0] == b.shape[0] and a.shape[1] == b.shape[1], \"Dimensions of `a` and `b` must match.\"\n\n    # Use numpy advanced indexing to select elements from `a` based on `b`\n    result = a[np.arange(a.shape[0])[:, None], np.arange(a.shape[1]), b]\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 501, "library_problem_id": 210, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 210}}
{"id": 502, "code": ["<code>\nimport numpy as np\n\ndef select_elements(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n    result = a[np.arange(a.shape[0])[:, None], np.arange(a.shape[1]), b]\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 502, "library_problem_id": 211, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 210}}
{"id": 503, "code": ["<code>\nimport numpy as np\n\ndef select_elements(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n    # Ensure the input arrays have compatible dimensions\n    assert a.ndim == 3, \"Array 'a' must be 3-dimensional.\"\n    assert b.ndim == 2, \"Array 'b' must be 2-dimensional.\"\n    assert a.shape[:2] == b.shape, \"The first two dimensions of 'a' must match the dimensions of 'b'.\"\n    \n    # Use advanced indexing to select elements from 'a' based on indices in 'b'\n    N, M = b.shape\n    result = a[np.arange(N)[:, None], np.arange(M), b]\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 503, "library_problem_id": 212, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 210}}
{"id": 504, "code": ["<code>\nimport numpy as np\n\ndef compute_sum(a: np.ndarray, b: np.ndarray) -> int:\n    # Extract elements from `a` using indices from `b` and compute their sum\n    result = np.sum(a[np.arange(a.shape[0])[:, None], np.arange(a.shape[1]), b])\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 504, "library_problem_id": 213, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 505, "code": ["<code>\ndef compute_unindexed_sum(a: np.ndarray, b: np.ndarray) -> int:\n    total_sum = np.sum(a)\n    indexed_sum = np.sum(a[np.arange(a.shape[0])[:, None], np.arange(a.shape[1]), b])\n    return total_sum - indexed_sum\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 505, "library_problem_id": 214, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 210}}
{"id": 506, "code": ["<code>\nimport numpy as np\nimport pandas as pd\n\ndef filter_column_b(df: pd.DataFrame) -> np.ndarray:\n    # Extract column 'a' and 'b' from the DataFrame\n    column_a = df['a'].values\n    column_b = df['b'].values\n    \n    # Apply the condition and replace values in column 'b' accordingly\n    result = np.where((column_a > 1) & (column_a <= 4), column_b, np.nan)\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 506, "library_problem_id": 215, "library": "Numpy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 215}}
{"id": 507, "code": ["<code>\nimport numpy as np\n\ndef remove_peripheral_zeros(im: np.ndarray) -> np.ndarray:\n    # Identify rows that contain at least one non-zero element\n    non_zero_rows = np.any(im != 0, axis=1)\n    \n    # Identify columns that contain at least one non-zero element\n    non_zero_cols = np.any(im != 0, axis=0)\n    \n    # Use boolean indexing to select the sub-array with non-zero rows and columns\n    return im[np.ix_(non_zero_rows, non_zero_cols)]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 507, "library_problem_id": 216, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 216}}
{"id": 508, "code": ["<code>\n```python\n\"\"\"\nFunction to find the bounding box of non-zero elements in a numpy array.\n\nThis function identifies the smallest subarray that contains all the non-zero elements of the input array. \nIt effectively \"truncates\" the input array by removing rows and columns that consist entirely of zeros.\n\nArgs:\n    array (np.ndarray): A 2D numpy array of any size containing numerical data.\n\nReturns:\n    np.ndarray: A 2D numpy array representing the smallest subarray containing all non-zero elements \n                of the input array. If the input array is entirely zeros, an empty array is returned.\n\nExample:\n    Given the following input array:\n    [[0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 1, 0, 0, 0, 0],\n     [0, 0, 1, 1, 0, 0, 0],\n     [0, 0, 0, 0, 1, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 0]]\n\n    The function will return:\n    [[1, 0, 0, 0],\n     [1, 1, 0, 0],\n     [0, 0, 1, 0]]\n\nProcedure:\n1. Identify the indices\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 508, "library_problem_id": 217, "library": "Numpy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 216}}
{"id": 509, "code": ["<code>\ndef process_image(im: np.ndarray) -> np.ndarray:\n    # Identify rows and columns that contain any zero elements\n    non_zero_rows = np.any(im == 0, axis=1)\n    non_zero_cols = np.any(im == 0, axis=0)\n\n    # Determine the indices of the first and last rows and columns that contain zero elements\n    row_start, row_end = np.where(non_zero_rows)[0][[0, -1]]\n    col_start, col_end = np.where(non_zero_cols)[0][[0, -1]]\n\n    # Slice the array to include only the rows and columns between the identified indices\n    result = im[row_start:row_end + 1, col_start:col_end + 1]\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 509, "library_problem_id": 218, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 510, "code": ["<code>\n```python\nimport numpy as np\n\ndef remove_peripheral_zeros(image: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Remove rows and columns from the input image that are entirely filled with zeros.\n\n    Args:\n    image (np.ndarray): A 2D NumPy array representing a grayscale image, where each element is a pixel value.\n\n    Returns:\n    np.ndarray: A 2D NumPy array with peripheral zero rows and columns removed. If the input image is entirely zeros, returns an empty array.\n\n    Example:\n    Given the input image:\n    [[0, 0, 0, 0, 0, 0],\n     [0, 0, 5, 1, 2, 0],\n     [0, 1, 8, 0, 1, 0],\n     [0, 0, 0, 7, 1, 0],\n     [0, 0, 0, 0, 0, 0]]\n\n    The function should return:\n    [[0, 5, 1, 2],\n     [1, 8, 0, 1],\n     [0, 0, 7, 1]]\n\n    Mathematical/Logical Conditions:\n    - A row or column is considered peripheral if it contains only zeros.\n    - The function should efficiently identify and remove these rows and columns using NumPy operations.\n    - If the resulting image has no non-zero elements, return an empty array.\n    \"\"\"\n    # Identify rows and columns that are not entirely zeros\n    non_\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 510, "library_problem_id": 219, "library": "Numpy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 216}}
{"id": 511, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_xy_relationship(x: np.ndarray, y: np.ndarray) -> None:\n    \"\"\"\n    Plots the relationship between two numerical arrays x and y using a scatter plot.\n    \n    Args:\n        x (np.ndarray): A 1D numpy array representing the x-coordinates.\n        y (np.ndarray): A 1D numpy array representing the y-coordinates.\n    \n    Returns:\n        None: This function does not return any value. It displays a plot.\n    \n    The plot will display x vs y with a legend labeled \"x-y\".\n    \"\"\"\n    plt.figure(figsize=(8, 6))\n    plt.scatter(x, y, label='x-y')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Scatter Plot of x vs y')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n# Generate random data\nx = 10 * np.random.randn(10)\ny = x\n\n# Plot the data\nplot_xy_relationship(x, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 511, "library_problem_id": 0, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 512, "code": ["<code>\ndef enable_minor_ticks_on_y_axis():\n    \"\"\"\n    Generates a scatter plot with minor ticks enabled on the y-axis.\n\n    Returns:\n        None\n    \"\"\"\n    # Generate random data for x and y coordinates\n    x = np.random.rand(10)\n    y = np.random.rand(10)\n\n    # Create a scatter plot\n    plt.scatter(x, y)\n\n    # Enable minor ticks on the y-axis\n    plt.minorticks_on()\n    plt.gca().yaxis.set_minor_locator(plt.AutoMinorLocator())\n\n    # Display the plot\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 512, "library_problem_id": 1, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 1}}
{"id": 513, "code": ["<code>\ndef enable_minor_ticks():\n    \"\"\"\n    Generates a scatter plot with random data and enables minor ticks.\n\n    Args:\n        None\n\n    Returns:\n        None\n    \"\"\"\n    # Generate random data\n    x = np.random.rand(10)\n    y = np.random.rand(10)\n\n    # Create scatter plot\n    plt.scatter(x, y)\n\n    # Enable minor ticks\n    plt.minorticks_on()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 513, "library_problem_id": 2, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 514, "code": ["<code>\ndef plot_scatter_with_minor_ticks(x: np.ndarray, y: np.ndarray) -> None:\n    plt.scatter(x, y)\n    plt.minorticks_on()  # Enable minor ticks on both axes\n    plt.gca().xaxis.set_minor_locator(plt.AutoMinorLocator())  # Ensure minor ticks are set on x-axis\n    plt.gca().yaxis.set_minor_locator(plt.NullLocator())  # Disable minor ticks on y-axis\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 514, "library_problem_id": 3, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 1}}
{"id": 515, "code": ["<code>\ndef plot_lines_with_styles(x: np.ndarray) -> None:\n    \"\"\"\n    Plots multiple lines with different styles, using random y-values for each line.\n\n    Args:\n    - x (np.ndarray): An array of x-values for the lines.\n\n    Returns:\n    - None: This function does not return any value. It displays a plot.\n    \"\"\"\n    line_styles = ['-', '--', '-.', ':']\n    for style in line_styles:\n        y = np.random.rand(len(x))  # Generate random y-values for each line\n        plt.plot(x, y, linestyle=style, label=f'Style {style}')\n    \n    plt.xlabel('X-axis')\n    plt.ylabel('Y-axis')\n    plt.title('Line Plot with Different Styles')\n    plt.legend()\n    plt.show()\n\nx = np.arange(10)\nplot_lines_with_styles(x)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 515, "library_problem_id": 4, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 516, "code": ["<code>\n# Generate random y-values\ny1 = np.random.rand(10)\ny2 = np.random.rand(10)\ny3 = np.random.rand(10)\n\n# Set line styles\nline_style1 = '-'\nline_style2 = '--'\nline_style3 = ':'\n\n# Plot the lines\nplt.plot(x, y1, linestyle=line_style1, label='Line 1')\nplt.plot(x, y2, linestyle=line_style2, label='Line 2')\nplt.plot(x, y3, linestyle=line_style3, label='Line 3')\n\n# Add legend\nplt.legend()\n\n# Display the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 516, "library_problem_id": 5, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 517, "code": ["<code>\n# Create line plot with thin line and diamond markers\nplt.plot(x, y, linestyle='-', marker='D')\n\n# Display the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 517, "library_problem_id": 6, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 518, "code": ["<code>\n# Create a line plot with diamond markers\nplt.figure(figsize=(8, 6))\nplt.plot(x, y, marker='D', markersize=10, linewidth=2, linestyle='-')\nplt.title('Line Plot with Diamond Markers')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.grid(True)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 518, "library_problem_id": 7, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 519, "code": ["<code>\ndef plot_tips_boxplot() -> None:\n    sns.set_style(\"whitegrid\")\n    tips = sns.load_dataset(\"tips\")\n    ax = sns.boxplot(x=\"day\", y=\"total_bill\", data=tips)\n    ax.set_ylim(0, 40)\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 519, "library_problem_id": 8, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 8}}
{"id": 520, "code": ["<code>\ndef plot_with_highlighted_range(data: np.ndarray, highlight_start: int, highlight_end: int) -> None:\n    plt.plot(data, label='Data')\n    plt.axvspan(highlight_start, highlight_end, color='red', alpha=0.3, label='Highlighted Range')\n    plt.legend()\n    plt.show()\n\n# Generate random data\nx = 10 * np.random.randn(10)\n\n# Plot the data and highlight the x range from 2 to 4\nplot_with_highlighted_range(x, 2, 4)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 520, "library_problem_id": 9, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 521, "code": ["<code>\nplt.plot([0, 1], [0, 2])\n    plt.xlim(0, 1)\n    plt.ylim(0, 2)\n    plt.show()\n    # SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 521, "library_problem_id": 10, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 10}}
{"id": 522, "code": ["<code>\n```python\ndef draw_line_segment(start: tuple, end: tuple) -> None:\n    \"\"\"\n    Draws a line segment between two points in a 2D space using matplotlib.\n\n    Args:\n        start (tuple): A tuple (x1, y1) representing the starting point of the line segment.\n        end (tuple): A tuple (x2, y2) representing the ending point of the line segment.\n\n    Returns:\n        None: This function does not return any value. It displays a plot with the line segment.\n\n    Example:\n        draw_line_segment((0, 0), (1, 2))\n\n    Mathematical/Logical Conditions:\n        - The line segment is defined by two points: start (x1, y1) and end (x2, y2).\n        - The line segment is drawn by plotting these two points and connecting them with a line.\n    \"\"\"\n    # Extract coordinates from the input tuples\n    x1, y1 = start\n    x2, y2 = end\n\n    # Create a plot\n    plt.figure(figsize=(6, 4))\n    plt.plot([x1, x2], [y1, y2], marker='o')\n\n    # Set plot limits\n    plt.xlim(min(x1, x2) - 1, max(x1, x2) + 1)\n    plt.ylim(min(y1, y2) - 1, max(y1, y2) + 1)\n\n    # Add labels and title\n    plt.xlabel('X-axis\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 522, "library_problem_id": 11, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 10}}
{"id": 523, "code": ["<code>\n```python\ndef generate_seaborn_relation_plot():\n    \"\"\"\n    Generates a seaborn relational plot with data points colored by gender.\n\n    This function performs the following steps:\n    1. Sets the seaborn style to 'ticks'.\n    2. Seeds the numpy random number generator for reproducibility.\n    3. Creates a pandas DataFrame with random data for height, weight, and gender.\n    4. Plots a seaborn relational plot with 'Height (cm)' on the x-axis and 'Weight (kg)' on the y-axis.\n    5. Colors the data points based on the 'Gender' column.\n\n    Returns:\n        None\n    \"\"\"\n    # Set seaborn style\n    sns.set(style=\"ticks\")\n\n    # Seed for reproducibility\n    np.random.seed(0)\n\n    # Number of data points\n    N = 37\n\n    # Gender categories\n    genders = [\"Female\", \"Male\", \"Non-binary\", \"No Response\"]\n\n    # Create DataFrame with random data\n    df = pd.DataFrame(\n        {\n            \"Height (cm)\": np.random.uniform(low=130, high=200, size=N),\n            \"Weight (kg)\": np.random.uniform(low=30, high=100, size=N),\n            \"Gender\": np.random.choice(genders, size=N),\n        }\n    )\n\n    # Create a seaborn relational plot\n    sns.relplot(\n        data=df,\n        x\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 523, "library_problem_id": 12, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 12}}
{"id": 524, "code": ["<code>\n# Create a line plot using Seaborn\nsns.lineplot(x=x, y=y)\n\n# Set the plot style to match Matplotlib\nsns.set_style(\"whitegrid\")\n\n# Display the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 524, "library_problem_id": 13, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 525, "code": ["<code>\ndef plot_sine_wave():\n    \"\"\"\n    Generates a line plot of the sine function over a specified range using Seaborn and Pandas.\n\n    The function performs the following steps:\n    1. Creates an array of x values ranging from 0 to 9.\n    2. Computes the sine of each x value to generate the corresponding y values.\n    3. Constructs a Pandas DataFrame from the x and y values.\n    4. Uses Seaborn to create a line plot of x versus y.\n    5. Displays the plot using Matplotlib.\n\n    Returns:\n        None\n    \"\"\"\n    # Step 1: Create an array of x values\n    x = np.arange(10)\n\n    # Step 2: Compute the sine of each x value\n    y = np.sin(x)\n\n    # Step 3: Construct a DataFrame from x and y\n    data = pd.DataFrame({'x': x, 'y': y})\n\n    # Step 4: Create a line plot using Seaborn\n    sns.lineplot(data=data, x='x', y='y')\n\n    # Step 5: Display the plot\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 525, "library_problem_id": 14, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 526, "code": ["<code>\ndef plot_random_data_with_custom_marker(x: np.ndarray, y: np.ndarray) -> None:\n    plt.plot(x, y, marker='+', linewidth=7)\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 526, "library_problem_id": 15, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 527, "code": ["<code>\ndef plot_cosine_with_legend():\n    \"\"\"\n    Generates and displays a plot of the cosine function with a legend.\n\n    The function performs the following steps:\n    1. Creates an array of 10 evenly spaced values from 0 to 2\u03c0.\n    2. Computes the cosine of each value in the array.\n    3. Plots the cosine values against the x values.\n    4. Adds a legend to the plot with the font size set to 20.\n    \"\"\"\n    x: np.ndarray = np.linspace(0, 2 * np.pi, 10)\n    y: np.ndarray = np.cos(x)\n\n    plt.plot(x, y, label=\"cos\")\n    plt.legend(fontsize=20)\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 527, "library_problem_id": 16, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 528, "code": ["<code>\ndef plot_cosine_wave_with_custom_legend_title(x: np.ndarray, y: np.ndarray, legend_title: str, title_font_size: int) -> None:\n    plt.figure()\n    plt.plot(x, y, label='cos(x)')\n    plt.legend(title=legend_title, title_fontsize=title_font_size)\n    plt.xlabel('x')\n    plt.ylabel('cos(x)')\n    plt.title('Cosine Wave')\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 528, "library_problem_id": 17, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 529, "code": ["<code>\n# SOLUTION START\nl.set_markerfacecolor((0,0,0,0.2))  # RGBA color, where the last value is alpha\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 529, "library_problem_id": 18, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 18}}
{"id": 530, "code": ["<code>\n# SOLUTION START\nplt.setp(l, markeredgecolor='black')\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 530, "library_problem_id": 19, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 531, "code": ["<code>\nline.set_color('r')\nline.set_markerfacecolor('r')\n# SOLUTION END\n\n# Display the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 531, "library_problem_id": 20, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 18}}
{"id": 532, "code": ["<code>\nplt.xticks(rotation=45)\n# SOLUTION END\n\nplt.legend()\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 532, "library_problem_id": 21, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 21}}
{"id": 533, "code": ["<code>\n# Add a legend to the plot\nplt.legend()\n\n# Set the title of the plot\nplt.title(\"Cosine Function\")\n\n# Display the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 533, "library_problem_id": 22, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 534, "code": ["<code>\ndef plot_cosine_wave_with_custom_ticks() -> None:\n    \"\"\"\n    Plots a cosine wave using matplotlib and sets custom x-axis tick labels.\n\n    This function generates a cosine wave over the interval [0, 2\u03c0] and plots it.\n    The x-axis tick labels are customized to appear at intervals of 2 units.\n\n    Args:\n        None\n\n    Returns:\n        None\n    \"\"\"\n    # Generate x values from 0 to 2\u03c0, inclusive, with 10 points\n    x: np.ndarray = np.linspace(0, 2 * np.pi, 10)\n    \n    # Compute the cosine of each x value\n    y: np.ndarray = np.cos(x)\n    \n    # Plot the cosine wave\n    plt.plot(x, y, label=\"cosine\")\n    \n    # Set x-axis tick labels at intervals of 2 units\n    plt.xticks(ticks=np.arange(0, 2 * np.pi + 1, 2))\n    \n    # Display the plot\n    plt.legend()\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 534, "library_problem_id": 23, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 21}}
{"id": 535, "code": ["<code>\n# Add legends to the plot\nplt.legend()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 535, "library_problem_id": 24, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 536, "code": ["<code>\ndef plot_2d_array_heatmap(array: np.ndarray) -> None:\n    plt.imshow(array, cmap='viridis', interpolation='nearest')\n    plt.colorbar()\n    plt.title('Heatmap of 2D Array')\n    plt.xlabel('Column Index')\n    plt.ylabel('Row Index')\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 536, "library_problem_id": 25, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 537, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef display_2d_array_in_black_and_white(array: np.ndarray) -> None:\n    plt.imshow(array, cmap='gray', interpolation='nearest')\n    plt.colorbar()  # Optional: Adds a colorbar to the side of the plot\n    plt.title(\"2D Array Displayed in Black and White\")\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 537, "library_problem_id": 26, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 538, "code": ["<code>\ndef plot_cosine_wave() -> None:\n    \"\"\"\n    Generates a plot of the cosine function over the interval [0, 2\u03c0].\n\n    This function performs the following steps:\n    1. Creates an array of 10 evenly spaced values between 0 and 2\u03c0.\n    2. Computes the cosine of each value in the array.\n    3. Plots the cosine values against the original array.\n    4. Sets the x-axis label to \"X\" and positions it at the right end of the x-axis.\n\n    Returns:\n        None: This function does not return any value.\n    \"\"\"\n    # Step 1: Create an array of 10 evenly spaced values between 0 and 2\u03c0\n    x = np.linspace(0, 2 * np.pi, 10)\n    \n    # Step 2: Compute the cosine of each value in the array\n    y = np.cos(x)\n    \n    # Step 3: Plot the cosine values against the original array\n    plt.plot(x, y)\n    \n    # Step 4: Set the x-axis label to \"X\" and position it at the right end of the x-axis\n    plt.xlabel(\"X\", loc='right')\n    \n    # Display the plot\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 538, "library_problem_id": 27, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 27}}
{"id": 539, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef rotate_x_axis_labels(ax: plt.Axes, rotation_angle: int = 90) -> None:\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=rotation_angle)\n\n# Load the dataset\ndf = sns.load_dataset(\"planets\")\n\n# Create a boxplot\ng = sns.boxplot(x=\"method\", y=\"orbital_period\", data=df)\n\n# Rotate the x-axis labels by 90 degrees\nrotate_x_axis_labels(g)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 539, "library_problem_id": 28, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 540, "code": ["<code>\n```python\ndef plot_with_wrapped_title(x: np.ndarray, y: np.ndarray, title: str, max_line_length: int = 40) -> None:\n    \"\"\"\n    Plots the given x and y data and wraps the title to fit within a specified maximum line length.\n\n    Args:\n        x (np.ndarray): The x-coordinates of the data points.\n        y (np.ndarray): The y-coordinates of the data points.\n        title (str): The title of the plot that needs to be wrapped.\n        max_line_length (int): The maximum number of characters per line for the title. Default is 40.\n\n    Returns:\n        None: This function does not return any value. It displays a plot with the wrapped title.\n\n    Example:\n        x = np.arange(10)\n        y = 2 * np.random.rand(10)\n        title = \"Some really really long long long title I really really need - and just can't - just can't - make it any - simply any - shorter - at all.\"\n        plot_with_wrapped_title(x, y, title)\n    \"\"\"\n    def wrap_title(title: str, max_line_length: int) -> str:\n        \"\"\"\n        Wraps the title into multiple lines based on the maximum line length.\n\n        Args:\n            title (str): The title to be wrapped.\n            max_line_length (int): The maximum number of characters per line.\n\n        Returns:\n            str: The wrapped title with newline characters inserted.\n        \"\"\"\n        words = title\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 540, "library_problem_id": 29, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 541, "code": ["<code>\ndef generate_inverted_y_axis_plot() -> None:\n    \"\"\"\n    Generates a plot with the y-axis inverted. The y-values are generated\n    randomly and scaled, while the x-values are sequential integers.\n\n    The function performs the following steps:\n    1. Generates 10 random y-values scaled by a factor of 2.\n    2. Creates x-values as a sequence of integers from 0 to 9.\n    3. Plots the x and y values.\n    4. Inverts the y-axis so that higher values appear at the bottom.\n\n    Returns:\n        None: The function displays a plot with an inverted y-axis.\n    \"\"\"\n    # Generate random y-values scaled by 2\n    y = 2 * np.random.rand(10)\n    \n    # Create x-values as a sequence from 0 to 9\n    x = np.arange(10)\n    \n    # Create the plot\n    plt.plot(x, y)\n    \n    # Invert the y-axis\n    plt.gca().invert_yaxis()\n    \n    # Display the plot\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 541, "library_problem_id": 30, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 542, "code": ["<code>\n# Generate random data\nx: np.ndarray = np.random.randn(10)\ny: np.ndarray = x\n\n# Create scatter plot\nplt.scatter(x, y)\n\n# Set x-axis ticks at 0 and 1.5 only\nplt.xticks([0, 1.5])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 542, "library_problem_id": 31, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 31}}
{"id": 543, "code": ["<code>\ndef generate_scatter_plot_with_custom_yticks():\n    \"\"\"\n    Generates a scatter plot with random data points and sets y-axis ticks to -1 and 1.\n\n    Args:\n    None\n\n    Returns:\n    None\n    \"\"\"\n    # Generate random data\n    x = np.random.randn(10)\n    y = np.random.randn(10)\n\n    # Create scatter plot\n    plt.scatter(x, y)\n\n    # Set y-axis ticks at -1 and 1\n    plt.yticks([-1, 1])\n\n    # Display the plot\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 543, "library_problem_id": 32, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 31}}
{"id": 544, "code": ["<code>\ndef plot_random_data():\n    # Generate random data\n    x = np.random.rand(10)\n    y = np.random.rand(10)\n    z = np.random.rand(10)\n\n    # Plot the data\n    plt.figure()\n    plt.plot(x, label='x', zorder=3)  # Plot x with the highest z-order\n    plt.plot(y, label='y', zorder=2)  # Plot y with a medium z-order\n    plt.plot(z, label='z', zorder=1)  # Plot z with the lowest z-order\n\n    # Add legend and show plot\n    plt.legend()\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 544, "library_problem_id": 33, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 545, "code": ["<code>\ndef generate_scatter_plot_with_styling(x: np.ndarray, y: np.ndarray) -> None:\n    plt.scatter(x, y, edgecolor='black', facecolor='blue')\n    plt.xlabel('X-axis')\n    plt.ylabel('Y-axis')\n    plt.title('Scatter Plot with Black Borders and Blue Face')\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 545, "library_problem_id": 34, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 546, "code": ["<code>\ndef plot_data_with_integer_ticks(x: np.ndarray, y: np.ndarray) -> None:\n    \"\"\"\n    Plot the data with integer ticks on both axes.\n\n    Args:\n        x (np.ndarray): The x-coordinates of the data points.\n        y (np.ndarray): The y-coordinates of the data points.\n\n    Returns:\n        None\n    \"\"\"\n    plt.plot(x, y, 'o')\n    plt.xticks(np.arange(min(x), max(x) + 1, 1))  # Set x-axis ticks as integers\n    plt.yticks(np.arange(0, np.ceil(max(y)) + 1, 1))  # Set y-axis ticks as integers\n    plt.xlabel('X-axis')\n    plt.ylabel('Y-axis')\n    plt.title('Plot with Integer Ticks')\n    plt.grid(True)\n    plt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 546, "library_problem_id": 35, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 547, "code": ["<code>\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef create_bar_plot(data: dict) -> None:\n    \"\"\"\n    Generates a bar plot using Seaborn to visualize the relationship between 'reports' and 'coverage'.\n    \n    Args:\n        data (dict): A dictionary containing two keys:\n            - 'reports': A list of integers representing the number of reports.\n            - 'coverage': A list of integers representing the coverage values.\n    \n    Returns:\n        None: The function generates a plot and does not return any value.\n    \n    The function ensures that the y-axis tick labels are displayed in standard notation, not scientific notation.\n    \"\"\"\n    # Convert the input data dictionary to a Pandas DataFrame\n    df = pd.DataFrame(data)\n    \n    # Create a categorical bar plot\n    sns.catplot(y=\"coverage\", x=\"reports\", kind=\"bar\", data=df, label=\"Total\")\n    \n    # Configure the y-axis to use standard notation for tick labels\n    plt.ticklabel_format(style='plain', axis='y')\n    \n    # Display the plot\n    plt.show()\n\n# Example usage\ndata = {\n    \"reports\": [4, 24, 31, 2, 3],\n    \"coverage\": [35050800, 54899767, 57890789, 62890798\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 547, "library_problem_id": 36, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 548, "code": ["<code>\ndef plot_dashed_line_on_seaborn_lineplot(x: np.ndarray, y: np.ndarray) -> None:\n    ax = sns.lineplot(x=x, y=y, linestyle='--')\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 548, "library_problem_id": 37, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 549, "code": ["<code>\ndef plot_trigonometric_functions(x: np.ndarray, y1: np.ndarray, y2: np.ndarray) -> None:\n    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10, 6))\n    \n    ax1.plot(x, y1, label='sin(x)', color='blue')\n    ax1.set_title('Sine Function')\n    ax1.set_ylabel('sin(x)')\n    ax1.legend()\n    \n    ax2.plot(x, y2, label='cos(x)', color='red')\n    ax2.set_title('Cosine Function')\n    ax2.set_xlabel('x')\n    ax2.set_ylabel('cos(x)')\n    ax2.legend()\n    \n    plt.tight_layout()\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 549, "library_problem_id": 38, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 550, "code": ["<code>\nfig, axs = plt.subplots(2)\n    fig.subplots_adjust(hspace=0.5)\n\n    axs[0].plot(x, y1)\n    axs[0].set_title('sin(x)')\n    axs[0].spines['top'].set_color('none')\n    axs[0].spines['bottom'].set_color('none')\n    axs[0].spines['left'].set_color('none')\n    axs[0].spines['right'].set_color('none')\n\n    axs[1].plot(x, y2)\n    axs[1].set_title('cos(x)')\n    axs[1].spines['top'].set_color('none')\n    axs[1].spines['bottom'].set_color('none')\n    axs[1].spines['left'].set_color('none')\n    axs[1].spines['right'].set_color('none')\n\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 550, "library_problem_id": 39, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 551, "code": ["<code>\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 551, "library_problem_id": 40, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 552, "code": ["<code>\ndef plot_sine_wave_without_xticks() -> None:\n    \"\"\"\n    Generates a sine wave plot using Seaborn and removes the x-axis tick labels.\n\n    This function performs the following steps:\n    1. Creates an array of x values ranging from 0 to 9.\n    2. Computes the sine of each x value.\n    3. Constructs a Pandas DataFrame with the x and y values.\n    4. Uses Seaborn to create a line plot of the sine wave.\n    5. Removes the x-axis tick labels from the plot.\n\n    Returns:\n        None: This function does not return any value. It directly modifies the plot.\n    \"\"\"\n    # Step 1: Create an array of x values ranging from 0 to 9\n    x = np.arange(10)\n\n    # Step 2: Compute the sine of each x value\n    y = np.sin(x)\n\n    # Step 3: Construct a Pandas DataFrame with the x and y values\n    df = pd.DataFrame({\"x\": x, \"y\": y})\n\n    # Step 4: Use Seaborn to create a line plot of the sine wave\n    sns.lineplot(x=\"x\", y=\"y\", data=df)\n\n    # Step 5: Remove the x-axis tick labels from the plot\n    plt.xticks(ticks=[], labels=[])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 552, "library_problem_id": 41, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 40}}
{"id": 553, "code": ["<code>\ndef plot_scatter_with_custom_grid(x: np.ndarray, y: np.ndarray, grid_positions: list) -> None:\n    plt.scatter(x, y)\n    plt.xticks(grid_positions)\n    plt.grid(axis='x', which='both', linestyle='--', color='gray', alpha=0.7)\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 553, "library_problem_id": 42, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 554, "code": ["<code>\ndef plot_scatter_with_custom_grid(x: np.ndarray, y: np.ndarray, grid_positions: list) -> None:\n    plt.scatter(x, y)\n    plt.yticks(grid_positions)\n    plt.grid(True, axis='y', which='both', linestyle='--', color='grey', alpha=0.7)\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 554, "library_problem_id": 43, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 555, "code": ["<code>\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_with_custom_ticks_and_grid(x: np.ndarray, y: np.ndarray) -> None:\n    \"\"\"\n    Creates a scatter plot of the given x and y data points and customizes the plot by:\n    - Displaying y-ticks and horizontal grid lines at y positions 3 and 4.\n    - Displaying x-ticks and vertical grid lines at x positions 1 and 2.\n\n    Args:\n        x (np.ndarray): An array of x-coordinates for the scatter plot.\n        y (np.ndarray): An array of y-coordinates for the scatter plot.\n\n    Returns:\n        None: This function does not return any value. It displays a plot.\n    \"\"\"\n    plt.scatter(x, y)\n    \n    # Set x-ticks at positions 1 and 2\n    plt.xticks([1, 2])\n    \n    # Set y-ticks at positions 3 and 4\n    plt.yticks([3, 4])\n    \n    # Enable grid lines\n    plt.grid(True)\n    \n    # Customize grid lines to show only at specified x and y positions\n    plt.gca().xaxis.grid(True, which='both', linestyle='--', linewidth=0.5)\n    plt.gca().yaxis.grid(True, which='both', linestyle='--', linewidth\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 555, "library_problem_id": 44, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 556, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_scatter_with_grid() -> None:\n    \"\"\"\n    Generates a scatter plot of random data points and displays a grid.\n\n    This function creates a scatter plot using random data points for the y-axis\n    and a sequence of integers for the x-axis. It then displays a grid on the plot\n    for better visualization.\n\n    Arguments:\n    None\n\n    Returns:\n    None\n    \"\"\"\n    # Generate x values as a sequence of integers from 0 to 9\n    x = np.arange(10)\n    \n    # Generate y values as an array of 10 random numbers from a standard normal distribution\n    y = np.random.randn(10)\n    \n    # Create a scatter plot with x and y values\n    plt.scatter(x, y)\n    \n    # Display grid lines on the plot\n    plt.grid(True)\n\n# SOLUTION START\nplot_scatter_with_grid()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 556, "library_problem_id": 45, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 557, "code": ["<code>\n```python\n# Import necessary libraries and generate a plot with a legend.\n\n# This script performs the following steps:\n# 1. Import essential libraries: numpy, pandas, matplotlib, and seaborn.\n# 2. Generate a random dataset using numpy.\n# 3. Create a plot using matplotlib with the generated data.\n# 4. Add a legend to the plot and position it in the lower right corner.\n\n# Libraries:\n# - numpy: Used for generating random numbers.\n# - pandas: Imported but not used in this script.\n# - matplotlib: Used for plotting the data.\n# - seaborn: Imported but not used in this script.\n\n# Steps:\n# 1. Generate a random dataset 'x' using numpy's random.randn function, scaled by 10.\n# 2. Set 'y' equal to 'x' to create a linear relationship.\n# 3. Plot 'x' against 'y' using matplotlib's plot function.\n# 4. Add a legend to the plot and position it in the lower right corner.\n\n# Returns:\n# - A plot with a line representing the relationship between 'x' and 'y', with a legend in the lower right corner.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Generate random data\nx: np.ndarray = 10 * np.random.randn(10)\ny: np.ndarray = x\n\n# Create a plot\nplt.plot(x, y, label=\"\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 557, "library_problem_id": 46, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 46}}
{"id": 558, "code": ["<code>\nimport matplotlib.pyplot as plt\n\ndef create_subplots_with_labels() -> None:\n    \"\"\"\n    Generates a 2x2 grid of subplots with logarithmic expressions as axis labels.\n    Adjusts subplot padding to ensure labels are fully visible.\n\n    Args:\n        None\n\n    Returns:\n        None\n    \"\"\"\n    # Create a 2x2 grid of subplots\n    fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 6))\n    axes = axes.flatten()\n\n    # Define axis labels using LaTeX formatted strings\n    x_label = r\"$\\ln\\left(\\frac{x_a-x_d}{x_a-x_e}\\right)$\"\n    y_label = r\"$\\ln\\left(\\frac{x_a-x_b}{x_a-x_c}\\right)$\"\n\n    # Set axis labels for each subplot\n    for ax in axes:\n        ax.set_xlabel(x_label)\n        ax.set_ylabel(y_label)\n\n    # Adjust subplot padding to ensure labels are fully visible\n    plt.tight_layout()\n\n    # Display the plot\n    plt.show()\n\n    # Clear the figure to free memory\n    plt.clf()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 558, "library_problem_id": 47, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 559, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_with_legend(x: np.ndarray, y: np.ndarray, z: np.ndarray) -> None:\n    plt.plot(x, y, label='Y')\n    plt.plot(x, z, label='Z')\n    plt.legend()\n    plt.show()\n\nx = np.arange(10)\ny = np.arange(10, 20)\nz = np.arange(10)\n\nplot_with_legend(x, y, z)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 559, "library_problem_id": 48, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 560, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef create_heatmap_with_top_x_axis(data: np.ndarray, column_labels: list, row_labels: list) -> None:\n    fig, ax = plt.subplots()\n    heatmap = ax.pcolor(data, cmap=plt.cm.Blues)\n\n    # Set the x-axis to the top of the plot\n    ax.xaxis.tick_top()\n\n    # Set the labels for the x and y axes\n    ax.set_xticks(np.arange(data.shape[1]) + 0.5, minor=False)\n    ax.set_yticks(np.arange(data.shape[0]) + 0.5, minor=False)\n    ax.set_xticklabels(column_labels, minor=False)\n    ax.set_yticklabels(row_labels, minor=False)\n\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 560, "library_problem_id": 49, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 561, "code": ["<code>\ndef plot_linear_relationship(x: np.ndarray, y: np.ndarray) -> None:\n    \"\"\"\n    Plots the relationship between two numpy arrays, x and y, on a 2D graph.\n\n    Args:\n        x (np.ndarray): A 1D numpy array representing the x-coordinates.\n        y (np.ndarray): A 1D numpy array representing the y-coordinates.\n\n    Returns:\n        None: This function does not return any value. It displays a plot.\n\n    Functionality:\n    - Plots the values of y over x on a 2D plot.\n    - Labels the x-axis as \"X\".\n    - Sets the space between the x-axis label and the x-axis to 20 units.\n    \"\"\"\n    plt.plot(x, y)\n    plt.xlabel(\"X\", labelpad=20)\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 561, "library_problem_id": 50, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 562, "code": ["<code>\ndef plot_y_over_x_without_xticks(x: np.ndarray, y: np.ndarray) -> None:\n    plt.plot(x, y)\n    plt.xticks([])\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 562, "library_problem_id": 51, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 563, "code": ["<code>\ndef plot_with_right_y_axis(x: np.ndarray, y: np.ndarray) -> None:\n    plt.plot(x, y)\n    plt.gca().yaxis.tick_right()  # Move y-axis ticks to the right\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 563, "library_problem_id": 52, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 564, "code": ["<code>\ndef plot_y_over_x_with_custom_y_axis(x: np.ndarray, y: np.ndarray) -> None:\n    plt.plot(x, y)\n    plt.ylabel('Y', labelpad=20, rotation=0, loc='right')\n    plt.yticks(ticks=np.arange(min(y), max(y)+1))\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 564, "library_problem_id": 53, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 52}}
{"id": 565, "code": ["<code>\n```python\ndef create_joint_regression_plot(data: pd.DataFrame, x_col: str, y_col: str, line_color: str, scatter_color: str, dist_color: str) -> None:\n    \"\"\"\n    Generates a seaborn joint regression plot for the specified columns in the given DataFrame.\n\n    Args:\n        data (pd.DataFrame): The DataFrame containing the data to plot.\n        x_col (str): The name of the column to be used for the x-axis.\n        y_col (str): The name of the column to be used for the y-axis.\n        line_color (str): The color to use for the regression line.\n        scatter_color (str): The color to use for the scatter plot points.\n        dist_color (str): The color to use for the distribution plots.\n\n    Returns:\n        None: This function does not return a value. It displays a plot.\n\n    Conditions:\n        - The joint plot will display a regression line with the specified line color.\n        - The scatter plot points will be colored as specified.\n        - The marginal distribution plots will remain in the specified distribution color.\n    \"\"\"\n    sns.set(style=\"whitegrid\")\n    joint_plot = sns.jointplot(\n        data=data,\n        x=x_col,\n        y=y_col,\n        kind='reg',\n        line_kws={'color': line_color},\n        scatter_kws={'color': scatter_color}\n    )\n    joint_plot.plot_marginals(sns.histplot, color\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 565, "library_problem_id": 54, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 566, "code": ["<code>\n```python\ndef create_joint_regression_plot(data: pd.DataFrame, x_col: str, y_col: str, reg_line_color: str = 'green', hist_color: str = 'blue') -> None:\n    \"\"\"\n    Generates a seaborn joint regression plot for the specified columns in the given DataFrame.\n    \n    Args:\n        data (pd.DataFrame): The DataFrame containing the data to be plotted.\n        x_col (str): The name of the column to be used as the x-axis variable.\n        y_col (str): The name of the column to be used as the y-axis variable.\n        reg_line_color (str, optional): The color of the regression line. Defaults to 'green'.\n        hist_color (str, optional): The color of the histograms. Defaults to 'blue'.\n    \n    Returns:\n        None: The function displays the plot and does not return any value.\n    \n    Example:\n        create_joint_regression_plot(tips, 'total_bill', 'tip')\n    \"\"\"\n    # Create a jointplot with kind='reg' to include a regression line\n    plot = sns.jointplot(data=data, x=x_col, y=y_col, kind='reg', line_kws={'color': reg_line_color})\n    \n    # Set the color of the histograms\n    plot.ax_marg_x.hist(data[x_col], color=hist_color, alpha=0.6)\n    plot.ax_marg_y.hist(data[y_col], color=\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 566, "library_problem_id": 55, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 567, "code": ["<code>\ndef create_joint_regression_plot(data: pd.DataFrame, x_col: str, y_col: str) -> None:\n    \"\"\"\n    Generate a seaborn joint regression plot for the specified columns in the given DataFrame.\n\n    This function creates a joint plot with a regression line between two specified columns\n    from the provided DataFrame. The plot will not use a scatterplot for the joint plot.\n\n    Args:\n        data (pd.DataFrame): The DataFrame containing the data to be plotted.\n        x_col (str): The name of the column to be used for the x-axis.\n        y_col (str): The name of the column to be used for the y-axis.\n\n    Returns:\n        None: The function will display the plot and does not return any value.\n\n    Example:\n        create_joint_regression_plot(tips, 'total_bill', 'tip')\n    \"\"\"\n    sns.jointplot(data=data, x=x_col, y=y_col, kind='reg')\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 567, "library_problem_id": 56, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 568, "code": ["<code>\ndef plot_bar_chart(data: pd.DataFrame) -> None:\n    \"\"\"\n    Generate a bar plot for the given DataFrame.\n\n    Args:\n        data (pd.DataFrame): A DataFrame containing the data to plot. \n                             It must have columns 'celltype', 's1', and 's2'.\n\n    Returns:\n        None: This function does not return any value. It displays a plot.\n\n    The function creates a bar plot for the columns 's1' and 's2' using 'celltype' as the x-axis labels.\n    The x-axis tick labels are set to be horizontal for better readability.\n    \"\"\"\n    # Create a bar plot for 's1' and 's2' with 'celltype' as x-axis labels\n    ax = data.plot(kind='bar', x='celltype', y=['s1', 's2'])\n\n    # Set x-axis tick labels to horizontal\n    ax.set_xticklabels(data['celltype'], rotation=0)\n\n    # Display the plot\n    plt.show()\n\n# SOLUTION START\nplot_bar_chart(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 568, "library_problem_id": 57, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 569, "code": ["<code>\ndef plot_bar_chart(data: pd.DataFrame) -> None:\n    \"\"\"\n    Generates a bar plot for the given DataFrame.\n\n    Args:\n        data (pd.DataFrame): A DataFrame containing the data to plot. \n                             It must have the columns 'celltype', 's1', and 's2'.\n\n    Returns:\n        None: This function does not return any value. It displays a plot.\n\n    The function performs the following steps:\n    1. Creates a bar plot for the columns 's1' and 's2'.\n    2. Uses the 'celltype' column as the labels for the x-axis.\n    3. Rotates the x-axis tick labels by 45 degrees for better readability.\n    \"\"\"\n    # Create a bar plot for 's1' and 's2' using 'celltype' as the x-axis labels\n    ax = data.plot(kind='bar', x='celltype', y=['s1', 's2'])\n    \n    # Rotate x-axis tick labels by 45 degrees\n    plt.xticks(rotation=45)\n    \n    # Display the plot\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 569, "library_problem_id": 58, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 57}}
{"id": 570, "code": ["<code>\ndef plot_red_x_axis(x: np.ndarray, y: np.ndarray) -> None:\n    plt.plot(x, y)\n    plt.xlabel(\"X\", color='red')\n    plt.xticks(color='red')\n    plt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 570, "library_problem_id": 59, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 59}}
{"id": 571, "code": ["<code>\ndef plot_custom_line(x: np.ndarray, y: np.ndarray) -> None:\n    plt.plot(x, y)\n    plt.xlabel(\"X\")\n    plt.gca().spines['bottom'].set_color('red')\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 571, "library_problem_id": 60, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 59}}
{"id": 572, "code": ["<code>\n# Define data\nx = np.arange(10)\ny = np.arange(10)\n\n# Create the plot\nplt.plot(x, y)\n\n# Set tick parameters\nplt.xticks(fontsize=10, rotation='vertical')\nplt.yticks(fontsize=10)\n\n# Display the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 572, "library_problem_id": 61, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 573, "code": ["<code>\ndef draw_vertical_lines(x_positions: list[float]) -> None:\n    for x in x_positions:\n        plt.axvline(x=x, color='r', linestyle='--', linewidth=1)\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 573, "library_problem_id": 62, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 574, "code": ["<code>\n```python\nimport matplotlib.pyplot as plt\nimport numpy\n\ndef plot_heatmap_with_labels(xlabels: list, ylabels: list, data_matrix: numpy.ndarray) -> None:\n    \"\"\"\n    Plots a heatmap using the provided data matrix and labels for the x and y axes.\n\n    Args:\n        xlabels (list): A list of labels for the x-axis.\n        ylabels (list): A list of labels for the y-axis.\n        data_matrix (numpy.ndarray): A 2D numpy array containing the data to be visualized in the heatmap.\n\n    Returns:\n        None: This function does not return any value. It displays a heatmap plot.\n\n    Conditions:\n        - The x-axis tick labels should appear on top of the heatmap.\n        - The order of the y-axis labels should be inverted, displaying from top to bottom.\n    \"\"\"\n    fig, ax = plt.subplots()\n    cax = ax.matshow(data_matrix, cmap='viridis')\n\n    # Set x-axis labels and position them on top\n    ax.set_xticks(range(len(xlabels)))\n    ax.set_xticklabels(xlabels)\n    ax.xaxis.set_ticks_position('top')\n\n    # Set y-axis labels and invert their order\n    ax.set_yticks(range(len(ylabels)))\n    ax.set_yticklabels(ylabels[::-1])\n\n    # Display the color bar\n    plt.colorbar(cax\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 574, "library_problem_id": 63, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 575, "code": ["<code>\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\n\ndef plot_radiation_temperature():\n    \"\"\"\n    Generates a plot with two y-axes to display radiation and temperature over time.\n\n    The plot includes:\n    - A primary y-axis for Swdown and Rn radiation values.\n    - A secondary y-axis for temperature values.\n    - Legends for all three curves.\n\n    Mathematical/Logical Conditions:\n    - Time is represented as an array of integers from 0 to 9.\n    - Swdown and Rn are random values scaled to represent radiation in the range [-10, 90].\n    - Temperature is a random value scaled to represent temperature in the range [0, 30].\n\n    Returns:\n        None\n    \"\"\"\n    # Configure matplotlib to use regular math text\n    rc(\"mathtext\", default=\"regular\")\n\n    # Generate data\n    time = np.arange(10)\n    temp = np.random.random(10) * 30\n    Swdown = np.random.random(10) * 100 - 10\n    Rn = np.random.random(10) * 100 - 10\n\n    # Create figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot Swdown and Rn on the primary y-axis\n    ax.plot(time, Swdown, \"-\", label=\"Swdown\")\n    ax.plot(time, Rn, \"-\", label=\"Rn\")\n\n    #\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 575, "library_problem_id": 64, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 576, "code": ["<code>\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef create_side_by_side_subplots(x: np.ndarray, y: np.ndarray) -> None:\n    \"\"\"\n    Create two side-by-side subplots and plot y over x in each subplot.\n\n    Args:\n        x (np.ndarray): A 1D numpy array representing the x-axis values.\n        y (np.ndarray): A 1D numpy array representing the y-axis values.\n\n    Returns:\n        None: This function does not return any value. It displays the plots.\n\n    Conditions:\n        - The length of x and y must be the same.\n        - Each subplot should have the title \"Y\".\n    \"\"\"\n    # Ensure x and y have the same length\n    assert len(x) == len(y), \"x and y must have the same length.\"\n\n    # Create a figure with two subplots side by side\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n    # Plot y over x in the first subplot\n    axes[0].plot(x, y)\n    axes[0].set_title(\"Y\")\n\n    # Plot y over x in the second subplot\n    axes[1].plot(x, y)\n    axes[1].set_title(\"Y\")\n\n    # Display the plots\n    plt.show()\n\n# Example usage\nx = np.arange(10)\ny\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 576, "library_problem_id": 65, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 577, "code": ["<code>\ndef create_scatter_plot(data: pd.DataFrame, x_col: str, y_col: str, marker_size: int) -> None:\n    \"\"\"\n    Generates a scatter plot using Seaborn for the specified columns in the DataFrame.\n\n    Args:\n        data (pd.DataFrame): The DataFrame containing the data to be plotted.\n        x_col (str): The name of the column to be used for the x-axis.\n        y_col (str): The name of the column to be used for the y-axis.\n        marker_size (int): The size of the markers in the scatter plot.\n\n    Returns:\n        None: This function does not return any value. It displays a scatter plot.\n\n    Example:\n        create_scatter_plot(df, \"bill_length_mm\", \"bill_depth_mm\", 30)\n    \"\"\"\n    sns.scatterplot(data=data, x=x_col, y=y_col, s=marker_size)\n    plt.show()\n\n# Load the penguins dataset and select relevant columns\ndf = sns.load_dataset(\"penguins\")[[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]]\n\n# Create a scatter plot of 'bill_length_mm' vs 'bill_depth_mm' with marker size 30\ncreate_scatter_plot(df, \"bill_length_mm\", \"bill_depth_mm\", 30)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 577, "library_problem_id": 66, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 578, "code": ["<code>\ndef create_annotated_scatter_plot(x_values: list[float], y_values: list[float], annotations: list[int]) -> None:\n    plt.scatter(x_values, y_values)\n    for i, annotation in enumerate(annotations):\n        plt.annotate(str(annotation), (x_values[i], y_values[i]))\n    plt.xlabel('X Values')\n    plt.ylabel('Y Values')\n    plt.title('Annotated Scatter Plot')\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 578, "library_problem_id": 67, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
{"id": 579, "code": ["<code>\nplt.plot(x, y, label='y over x')\nplt.legend(title='Legend')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 579, "library_problem_id": 68, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 68}}
{"id": 580, "code": ["<code>\ndef plot_line_chart(x: np.ndarray, y: np.ndarray) -> None:\n    plt.plot(x, y, label='y over x')\n    legend = plt.legend(title='Legend')\n    plt.setp(legend.get_title(), fontsize='medium', fontweight='bold')\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 580, "library_problem_id": 69, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 68}}
{"id": 581, "code": ["<code>\ndef create_histogram_with_outline(data: np.ndarray, line_width: float = 1.2) -> None:\n    plt.hist(data, edgecolor='black', linewidth=line_width)\n    plt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 581, "library_problem_id": 70, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 582, "code": ["<code>\n```python\ndef create_custom_subplots(x: np.ndarray, y: np.ndarray) -> None:\n    \"\"\"\n    Create two subplots with specific size requirements.\n\n    The first subplot should be three times wider than the second subplot,\n    but both should have the same height.\n\n    Args:\n        x (np.ndarray): The x-coordinates for the plots.\n        y (np.ndarray): The y-coordinates for the plots.\n\n    Returns:\n        None: This function does not return any value. It displays the plots.\n\n    Usage:\n        x = np.arange(10)\n        y = np.arange(10)\n        create_custom_subplots(x, y)\n    \"\"\"\n    # Define the width ratios for the subplots\n    width_ratios = [3, 1]  # First subplot is three times wider than the second\n\n    # Create the figure and the subplots with the specified width ratios\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), gridspec_kw={'width_ratios': width_ratios})\n\n    # Plot data on the first subplot\n    ax1.plot(x, y)\n    ax1.set_title('First Subplot')\n\n    # Plot data on the second subplot\n    ax2.plot(x, y)\n    ax2.set_title('Second Subplot')\n\n    # Display the plots\n    plt.tight_layout()\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 582, "library_problem_id": 71, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 583, "code": ["<code>\n```python\ndef plot_histograms(x: np.ndarray, y: np.ndarray, bins: np.ndarray, alpha: float = 0.5) -> None:\n    \"\"\"\n    Plot two histograms of the given data arrays `x` and `y` on a single chart using matplotlib.\n\n    Args:\n        x (np.ndarray): A 1D numpy array containing the data for the first histogram.\n        y (np.ndarray): A 1D numpy array containing the data for the second histogram.\n        bins (np.ndarray): A 1D numpy array defining the bin edges for the histograms.\n        alpha (float, optional): The transparency level for the histograms. Default is 0.5.\n\n    Returns:\n        None: This function does not return any value. It displays a plot.\n\n    Conditions:\n        - The `x` and `y` arrays must be of the same length.\n        - The `bins` array must define a valid range for the data in `x` and `y`.\n        - The `alpha` value should be between 0 (completely transparent) and 1 (completely opaque).\n\n    Example:\n        plot_histograms(np.random.rand(10), np.random.rand(10), np.linspace(-1, 1, 100))\n    \"\"\"\n    plt.hist(x, bins=bins, alpha=alpha, label='x')\n    plt.hist(y, bins=bins, alpha=alpha, label='y')\n    plt.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 583, "library_problem_id": 72, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 72}}
{"id": 584, "code": ["<code>\n```python\ndef plot_grouped_histograms(x: np.ndarray, y: np.ndarray, bins: int = 10) -> None:\n    \"\"\"\n    Plots grouped histograms of two datasets on a single chart using matplotlib.\n    \n    The histograms are grouped to ensure they do not overlap with each other,\n    providing a clear visual comparison between the two datasets.\n\n    Args:\n        x (np.ndarray): The first dataset to plot.\n        y (np.ndarray): The second dataset to plot.\n        bins (int, optional): The number of bins to use for the histograms. Default is 10.\n\n    Returns:\n        None: This function does not return any value. It displays a plot.\n\n    Example:\n        x = np.random.rand(10)\n        y = np.random.rand(10)\n        plot_grouped_histograms(x, y)\n    \"\"\"\n    # Define the width of each bar in the histogram\n    bar_width = 0.35\n    \n    # Create an array of bin edges\n    bin_edges = np.linspace(0, 1, bins + 1)\n    \n    # Calculate the histogram values for each dataset\n    hist_x, _ = np.histogram(x, bins=bin_edges)\n    hist_y, _ = np.histogram(y, bins=bin_edges)\n    \n    # Define the positions of the bars on the x-axis\n    x_positions = bin_edges[:-1]\n    \n    # Plot the histograms with an\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 584, "library_problem_id": 73, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 72}}
{"id": 585, "code": ["<code>\n```python\ndef plot_infinite_line_through_points(a: float, b: float, c: float, d: float) -> None:\n    \"\"\"\n    Plot an infinite line passing through two given points (a, b) and (c, d).\n    \n    The function sets the x-axis and y-axis limits to be between 0 and 5.\n    \n    Args:\n        a (float): x-coordinate of the first point.\n        b (float): y-coordinate of the first point.\n        c (float): x-coordinate of the second point.\n        d (float): y-coordinate of the second point.\n    \n    Returns:\n        None: This function does not return any value. It displays a plot.\n    \n    Mathematical/Logical Conditions:\n        - The slope of the line is calculated as (d - b) / (c - a).\n        - The line equation is y = slope * (x - a) + b.\n        - The line is plotted over a range of x-values that extends beyond the given points.\n    \"\"\"\n    # Calculate the slope of the line\n    slope = (d - b) / (c - a)\n    \n    # Define x values for plotting the line, extending beyond the given points\n    x_values = [0, 5]\n    \n    # Calculate corresponding y values using the line equation\n    y_values = [slope * (x - a) + b for x in x_values]\n    \n    # Plot the line\n    plt.plot(x_values, y_values, label=f\"Line through ({a}, {b})\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 585, "library_problem_id": 74, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 586, "code": ["<code>\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef create_colormaps_with_shared_colorbar(x: np.ndarray, y: np.ndarray) -> None:\n    \"\"\"\n    Generates two colormaps from the given 2D numpy arrays `x` and `y`, \n    displays them in separate subplots, and uses a single shared colorbar.\n\n    Args:\n        x (np.ndarray): A 2D numpy array representing the data for the first colormap.\n        y (np.ndarray): A 2D numpy array representing the data for the second colormap.\n\n    Returns:\n        None: This function does not return any value. It displays the plot with colormaps.\n    \n    Conditions:\n        - Both `x` and `y` must be 2D numpy arrays of the same shape.\n        - The colorbar should reflect the range of values across both colormaps.\n    \"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    \n    # Create the first colormap\n    cax1 = axes[0].imshow(x, cmap='viridis')\n    axes[0].set_title('Colormap X')\n    \n    # Create the second colormap\n    cax2 = axes[1].imshow(y, cmap='viridis')\n    axes[1].set_title('Colormap Y')\n    \n    # Create a single colorbar for both colormaps\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 586, "library_problem_id": 75, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 587, "code": ["<code>\ndef plot_columns_as_lines(data: np.ndarray, labels: list) -> None:\n    \"\"\"\n    Plots each column of a 2D numpy array as an individual line on a graph.\n\n    Args:\n        data (np.ndarray): A 2D numpy array where each column will be plotted as a separate line.\n        labels (list): A list of strings representing the labels for each line. \n                       The length of the list should match the number of columns in the data.\n\n    Returns:\n        None: This function does not return any value. It displays a plot.\n\n    Preconditions:\n        - The input 'data' must be a 2D numpy array.\n        - The number of elements in 'labels' must be equal to the number of columns in 'data'.\n\n    Example:\n        If data is a 2D numpy array with shape (10, 2) and labels is ['a', 'b'],\n        the function will plot two lines, one for each column, labeled 'a' and 'b'.\n    \"\"\"\n    for i in range(data.shape[1]):\n        plt.plot(data[:, i], label=labels[i])\n    plt.legend()\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 587, "library_problem_id": 76, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 588, "code": ["<code>\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_subplots(x: np.ndarray, y: np.ndarray, z: np.ndarray, a: np.ndarray) -> None:\n    \"\"\"\n    Plots two subplots: one for y over x and another for z over a.\n    \n    Args:\n        x (np.ndarray): The x-coordinates for the first subplot.\n        y (np.ndarray): The y-coordinates for the first subplot.\n        z (np.ndarray): The z-coordinates for the second subplot.\n        a (np.ndarray): The a-coordinates for the second subplot.\n    \n    Returns:\n        None: This function does not return any value. It displays a plot.\n    \n    Conditions:\n        - The length of x and y must be the same.\n        - The length of z and a must be the same.\n        - The main title \"Y and Z\" is set above the two subplots.\n    \"\"\"\n    # Create a figure and two subplots\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n    \n    # Plot y over x in the first subplot\n    ax1.plot(x, y)\n    ax1.set_title('Y over X')\n    \n    # Plot z over a in the second subplot\n    ax2.plot(a, z)\n    ax2.set_title('Z over A')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 588, "library_problem_id": 77, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 589, "code": ["<code>\ndef plot_log_scale_line(points: list[tuple[int, int]]) -> None:\n    \"\"\"\n    Plots a line graph for the given points with a logarithmic scale on the y-axis.\n\n    Args:\n        points (list of tuple of int): A list of tuples where each tuple represents a point (x, y).\n\n    Returns:\n        None: This function does not return any value. It displays a plot.\n\n    Conditions:\n        - The y-axis of the plot should be on a logarithmic scale.\n        - The points should be plotted in the order they appear in the list.\n\n    Example:\n        Given points = [(3, 5), (5, 10), (10, 150)], the function will plot a line connecting these points\n        with the y-axis in log scale.\n    \"\"\"\n    # Extract x and y coordinates from the points\n    x_values, y_values = zip(*points)\n\n    # Create the plot\n    plt.figure()\n    plt.plot(x_values, y_values, marker='o')\n\n    # Set the y-axis to logarithmic scale\n    plt.yscale('log')\n\n    # Add labels and title\n    plt.xlabel('X-axis')\n    plt.ylabel('Y-axis (log scale)')\n    plt.title('Line Plot with Logarithmic Y-axis')\n\n    # Display the plot\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 589, "library_problem_id": 78, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 78}}
{"id": 590, "code": ["<code>\ndef plot_linear_relationship(x: np.ndarray, y: np.ndarray) -> None:\n    plt.figure(figsize=(8, 6))\n    plt.plot(x, y, marker='o')\n    plt.title('Relationship between x and y', fontsize=20)\n    plt.xlabel('x values', fontsize=18)\n    plt.ylabel('y values', fontsize=16)\n    plt.grid(True)\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 590, "library_problem_id": 79, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 79}}
{"id": 591, "code": ["<code>\ndef plot_with_custom_ticks(x: np.ndarray, y: np.ndarray) -> None:\n    \"\"\"\n    Plots the given y values over x values using Matplotlib and customizes the x-axis tick labels.\n\n    Args:\n        x (np.ndarray): An array of x values.\n        y (np.ndarray): An array of y values.\n\n    Returns:\n        None: This function does not return any value. It displays a plot.\n\n    Conditions:\n        - The x and y arrays must be of the same length.\n        - The x-axis tick labels should be set from 1 to the length of x.\n\n    Example:\n        >>> x = np.arange(10)\n        >>> y = np.arange(10)\n        >>> plot_with_custom_ticks(x, y)\n    \"\"\"\n    # Create a figure and an axis\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n\n    # Plot y over x\n    ax.plot(x, y)\n\n    # Set x-axis tick labels from 1 to the length of x\n    ax.set_xticks(x)\n    ax.set_xticklabels(np.arange(1, len(x) + 1))\n\n    # Display the plot\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 591, "library_problem_id": 80, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 592, "code": ["<code>\ndef plot_colored_lines(lines: List[List[Tuple[int, int]]], colors: np.ndarray) -> None:\n    for line, color in zip(lines, colors):\n        x_values, y_values = zip(*line)\n        plt.plot(x_values, y_values, color=color)\n    \n    plt.xlabel('X-axis')\n    plt.ylabel('Y-axis')\n    plt.title('Colored Line Segments')\n    plt.grid(True)\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 592, "library_problem_id": 81, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 593, "code": ["<code>\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_log_log(x: np.ndarray, y: np.ndarray) -> None:\n    \"\"\"\n    Plots the given y values over x values on a log-log scale.\n\n    Args:\n        x (np.ndarray): An array of x values.\n        y (np.ndarray): An array of y values.\n\n    Returns:\n        None: This function does not return any value. It displays a plot.\n\n    Conditions:\n        - The plot should be on a log-log scale.\n        - The axes should be marked with numbers like 1, 10, 100.\n        - Scientific notation should not be used for axis labels.\n    \"\"\"\n    plt.figure()\n    plt.loglog(x, y)\n    plt.xticks([1, 10, 100, 1000], ['1', '10', '100', '1000'])\n    plt.yticks([1, 10, 100, 1000], ['1', '10', '100', '1000'])\n    plt.xlabel('X-axis')\n    plt.ylabel('Y-axis')\n    plt.title('Log-Log Plot of Y over X')\n    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n    plt.show()\n\n# Example usage\nx = np\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 593, "library_problem_id": 82, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 594, "code": ["<code>\ndef generate_cumulative_line_plots(df: pd.DataFrame) -> None:\n    plt.figure(figsize=(10, 6))\n    for column in df.columns:\n        plt.plot(df.index, df[column].cumsum(), marker='o', label=column)\n    plt.title('Cumulative Line Plots')\n    plt.xlabel('Date')\n    plt.ylabel('Cumulative Sum')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 594, "library_problem_id": 83, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 595, "code": ["<code>\n```python\ndef plot_normalized_histogram(data: list) -> None:\n    \"\"\"\n    Plots a normalized histogram of the given data, where the histogram is renormalized\n    so that the sum of the bin heights equals 1. The y-axis tick labels are formatted\n    as percentages (e.g., 10%, 20%, etc.).\n\n    Args:\n        data (list): A list of numerical values to be plotted in the histogram.\n\n    Returns:\n        None: This function does not return any value. It displays a plot.\n\n    Mathematical/Logical Conditions:\n    - The sum of the heights of the histogram bars will be normalized to 1.\n    - The y-axis will be formatted to display tick labels in percentage format.\n\n    Example:\n        plot_normalized_histogram([1000, 1000, 5000, 3000, 4000, 16000, 2000])\n    \"\"\"\n    # Calculate the histogram and normalize it\n    counts, bins, patches = plt.hist(data, bins='auto', density=True)\n\n    # Format y-axis tick labels as percentages\n    plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y * 100:.0f}%'))\n\n    # Set y-axis ticks at 10% intervals\n    plt.yticks(np.arange(0, 1.1, 0.1))\n\n    # Display the plot\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 595, "library_problem_id": 84, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 84}}
{"id": 596, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_line_with_markers(x: np.ndarray, y: np.ndarray) -> None:\n    plt.plot(x, y, marker='o', linestyle='-', alpha=0.5, markerfacecolor='blue', markeredgewidth=0.5)\n    plt.show()\n\nx = np.arange(10)\ny = np.arange(10)\n\nplot_line_with_markers(x, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 596, "library_problem_id": 85, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 597, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_side_by_side(x: np.ndarray, y: np.ndarray, a: np.ndarray, z: np.ndarray) -> None:\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n    ax1.plot(x, y, label='y')\n    ax1.set_title('Plot of y over x')\n\n    ax2.plot(a, z, label='a')\n    ax2.set_title('Plot of a over z')\n\n    fig.legend(loc='upper center', ncol=2)\n\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 597, "library_problem_id": 86, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 86}}
{"id": 598, "code": ["<code>\n```python\ndef plot_penguin_regressions(df: pd.DataFrame) -> None:\n    \"\"\"\n    Creates two subplots with seaborn regression plots using the penguins dataset.\n    \n    Args:\n        df (pd.DataFrame): A DataFrame containing the penguins dataset with the following columns:\n            - \"bill_length_mm\": The length of the penguin's bill in millimeters.\n            - \"bill_depth_mm\": The depth of the penguin's bill in millimeters.\n            - \"flipper_length_mm\": The length of the penguin's flipper in millimeters.\n            - \"body_mass_g\": The body mass of the penguin in grams.\n    \n    Returns:\n        None: This function does not return any value. It displays the plots.\n    \n    Plot Details:\n        - First Subplot: A regression plot of \"bill_depth_mm\" as a function of \"bill_length_mm\".\n        - Second Subplot: A regression plot of \"flipper_length_mm\" as a function of \"bill_length_mm\".\n        - The y-axes of the subplots are not shared.\n    \"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=False)\n    \n    # First subplot: Regression plot of \"bill_depth_mm\" over \"bill_length_mm\"\n    sns.regplot(ax=axes[0], x=\"bill_length_mm\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 598, "library_problem_id": 87, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 87}}
{"id": 599, "code": ["<code>\ndef modify_x_axis_tick_labels():\n    \"\"\"\n    Modify the second x-axis tick label of a plot to \"second\" while keeping other labels as numerical values.\n    \n    Returns:\n        None\n    \"\"\"\n    # Step 1: Create data for plotting\n    x = np.arange(10)\n    y = np.arange(10)\n    \n    # Step 2: Initialize the plot\n    fig, ax = plt.subplots(1, 1)\n    \n    # Step 3: Set x-axis limits\n    plt.xlim(1, 10)\n    \n    # Step 4: Define x-axis tick positions\n    plt.xticks(range(1, 10))\n    \n    # Step 5: Plot the data\n    ax.plot(y, x)\n    \n    # Step 6: Modify the second x-axis tick label\n    tick_labels = [str(i) for i in range(1, 10)]\n    tick_labels[1] = \"second\"\n    ax.set_xticklabels(tick_labels)\n    \n    # Display the plot\n    plt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 599, "library_problem_id": 88, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 600, "code": ["<code>\nplt.plot(x, y, label='\u03bb')\n    plt.legend()\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 600, "library_problem_id": 89, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 601, "code": ["<code>\n# Add extra x-ticks at specified positions\nplt.xticks(range(0, 10, 2) + [int(xtick * 10) for xtick in extra_ticks], \n           labels=list(range(0, 11, 2)) + [str(xtick) for xtick in extra_ticks])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 601, "library_problem_id": 90, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 602, "code": ["<code>\ndef plot_with_rotated_xticklabels(x: np.ndarray, y: np.ndarray, rotation_angle: float = -60, alignment: str = 'left') -> None:\n    plt.plot(x, y)\n    plt.xticks(rotation=rotation_angle, ha=alignment)\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 602, "library_problem_id": 91, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 603, "code": ["<code>\ndef plot_with_custom_ticks(x: np.ndarray, y: np.ndarray) -> None:\n    plt.plot(x, y)\n    plt.yticks(rotation=-60)\n    plt.xticks(rotation='vertical', va='top')\n    plt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 603, "library_problem_id": 92, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 604, "code": ["<code>\ndef plot_with_transparent_xticks(start_year: int, end_year: int, y_values: np.ndarray, alpha: float = 0.5) -> None:\n    x = np.arange(start_year, end_year)\n    plt.plot(x, y_values)\n\n    # Set the transparency of xtick labels\n    xtick_labels = [item.get_text() for item in plt.gca().get_xticklabels()]\n    for label in xtick_labels:\n        text = plt.text(float(label.split()[0]), 0, label.split()[1], color='black', alpha=alpha)\n        text.set_bbox(dict(facecolor='white', edgecolor='None', alpha=alpha))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 604, "library_problem_id": 93, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 91}}
{"id": 605, "code": ["<code>\n```python\n\"\"\"\nThis script generates a simple line plot using matplotlib with specific margin adjustments.\nThe x-axis should have no margin before the first tick, while the y-axis should have a margin greater than zero.\n\nImports:\n- numpy as np: For numerical operations and array handling.\n- pandas as pd: For data manipulation (not used in this script but commonly used with numpy and matplotlib).\n- matplotlib.pyplot as plt: For creating static, interactive, and animated visualizations in Python.\n\nSteps:\n1. Create an array of integers from 0 to 9 for both x and y axes.\n2. Plot these arrays against each other to form a line plot.\n3. Adjust the plot margins as specified:\n   - Set the x-axis margin before the first tick to zero.\n   - Ensure the y-axis has a margin greater than zero.\n\nReturns:\n- A matplotlib plot with the specified margin settings.\n\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Step 1: Create data for plotting\nx: np.ndarray = np.arange(10)\ny: np.ndarray = np.arange(10)\n\n# Step 2: Plot the data\nplt.plot(x, y)\n\n# Step 3: Adjust margins\nplt.margins(x=0, y=0.1)  # x-axis margin set to zero, y-axis margin set to 0.1 (greater than zero)\n\n# Display the plot\nplt.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 605, "library_problem_id": 94, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 606, "code": ["<code>\n# SOLUTION START\n\n# Generate data\nx: np.ndarray = np.arange(10)\ny: np.ndarray = np.arange(10)\n\n# Create plot\nplt.plot(x, y)\n\n# Adjust margins\nplt.margins(x=0.1, y=0)  # Set x-axis margin > 0 and y-axis margin to 0\n\n# Display plot\nplt.show()\n\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 606, "library_problem_id": 95, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 94}}
{"id": 607, "code": ["<code>\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n    fig.suptitle('Figure')\n    axs[0].plot(x, y)\n    axs[0].set_title('Subplot 1')\n    axs[1].plot(x, y)\n    axs[1].set_title('Subplot 2')\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 607, "library_problem_id": 96, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 96}}
{"id": 608, "code": ["<code>\ndef plot_dataframe_line_chart(df: pd.DataFrame, x_label: str, y_label: str) -> None:\n    \"\"\"\n    Plots a line chart from the given DataFrame.\n\n    Args:\n        df (pd.DataFrame): A DataFrame containing the data to plot. \n                           It should have numerical values and appropriate column and index labels.\n        x_label (str): The label for the x-axis of the plot.\n        y_label (str): The label for the y-axis of the plot.\n\n    Returns:\n        None: This function does not return any value. It displays a plot.\n\n    Example:\n        values = [[1, 2], [3, 4]]\n        df = pd.DataFrame(values, columns=[\"Type A\", \"Type B\"], index=[\"Index 1\", \"Index 2\"])\n        plot_dataframe_line_chart(df, \"X\", \"Y\")\n    \"\"\"\n    # Plot the DataFrame using a line chart\n    df.plot(kind='line')\n\n    # Set the labels for the x-axis and y-axis\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n\n    # Display the plot\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 608, "library_problem_id": 97, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 609, "code": ["<code>\n\"\"\"\nThis script generates a scatter plot using the provided x and y data arrays.\nThe plot should use markers with a vertical line hatch pattern, and the hatch\npattern should be dense.\n\nImports:\n- numpy as np: For numerical operations and array handling.\n- pandas as pd: For data manipulation (not used in this script).\n- matplotlib.pyplot as plt: For creating visualizations.\n\nVariables:\n- x (np.ndarray): An array of integers from 0 to 9.\n- y (np.ndarray): An array of integers from 0 to 9.\n\nRequirements:\n- Create a scatter plot with x as the x-axis data and y as the y-axis data.\n- Use a marker style that includes a vertical line hatch pattern.\n- Ensure the hatch pattern is dense.\n\nReturns:\n- A matplotlib figure displaying the scatter plot with the specified marker style.\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\n# SOLUTION START\nplt.scatter(x, y, marker='|', hatch='|')\nplt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 609, "library_problem_id": 98, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 610, "code": ["<code>\ndef create_custom_scatter_plot(x: np.ndarray, y: np.ndarray) -> None:\n    plt.scatter(x, y, marker='|', edgecolors='none', hatch='|')\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 610, "library_problem_id": 99, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 611, "code": ["<code>\ndef create_scatter_plot(x: np.ndarray, y: np.ndarray) -> None:\n    plt.scatter(x, y, marker='*')\n    plt.xlabel('X-axis')\n    plt.ylabel('Y-axis')\n    plt.title('Scatter Plot with Star Hatch Marker')\n    plt.grid(True)\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 611, "library_problem_id": 100, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 612, "code": ["<code>\ndef create_custom_scatter_plot(x: np.ndarray, y: np.ndarray, marker_size: int = 100) -> None:\n    \"\"\"\n    Generates a scatter plot using the provided x and y data arrays.\n    \n    Parameters:\n    x (np.ndarray): A 1D numpy array representing the x-coordinates of the data points.\n    y (np.ndarray): A 1D numpy array representing the y-coordinates of the data points.\n    marker_size (int): An integer specifying the size of the markers in the scatter plot. Default is 100.\n    \n    The function combines a star hatch ('*') and a vertical line hatch ('|') for the marker style.\n    \"\"\"\n    # Create a scatter plot with specified marker size and combined hatches\n    plt.scatter(x, y, s=marker_size, edgecolors='black', facecolors='none', hatch='*|')\n    plt.xlabel('X-axis')\n    plt.ylabel('Y-axis')\n    plt.title('Custom Scatter Plot with Combined Hatches')\n    plt.grid(True)\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 612, "library_problem_id": 101, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 98}}
{"id": 613, "code": ["<code>\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_heatmap(data: np.ndarray) -> None:\n    \"\"\"\n    Plots a heatmap of the given 2D numpy array `data` within a specified rectangular region.\n\n    The function sets the x-axis and y-axis limits to be between 0 and 10.\n    It then plots the heatmap of `data` in the rectangle defined by:\n    - Right boundary at x = 5\n    - Left boundary at x = 1\n    - Bottom boundary at y = 1\n    - Top boundary at y = 4\n\n    Args:\n        data (np.ndarray): A 2D numpy array representing the data to be visualized as a heatmap.\n\n    Returns:\n        None: This function does not return any value. It directly plots the heatmap using matplotlib.\n\n    Preconditions:\n        - `data` must be a 2D numpy array.\n        - The shape of `data` should be compatible with the plotting region.\n\n    Example:\n        >>> data = np.random.random((10, 10))\n        >>> plot_heatmap(data)\n    \"\"\"\n    plt.figure()\n    plt.xlim(0, 10)\n    plt.ylim(0, 10)\n    plt.imshow(data, extent=[1, 5, 1, 4], origin='lower', aspect='auto')\n    plt.colorbar()\n    plt.show()\n\n# Example usage\ndata = np.random.random((10\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 613, "library_problem_id": 102, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 614, "code": ["<code>\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0.1, 2 * np.pi, 41)\ny = np.exp(np.sin(x))\n\nplt.stem(x, y, orientation='horizontal')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 614, "library_problem_id": 103, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 615, "code": ["<code>\ndef create_colored_bar_plot(data: dict, colors: dict) -> None:\n    labels = list(data.keys())\n    heights = list(data.values())\n    bar_colors = [colors[label] for label in labels]\n    plt.bar(labels, heights, color=bar_colors)\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 615, "library_problem_id": 104, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 616, "code": ["<code>\ndef plot_vertical_line_with_label(x_position: float, label: str) -> None:\n    plt.axvline(x=x_position, color='b', linestyle='-', label=label)\n    plt.legend()\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 616, "library_problem_id": 105, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 617, "code": ["<code>\n```python\nimport matplotlib.pyplot as plt\nfrom typing import List\n\ndef create_polar_bar_plot(labels: List[str], heights: List[float]) -> None:\n    \"\"\"\n    Generates a polar bar plot using the provided labels and heights.\n\n    Args:\n        labels (List[str]): A list of category labels for each bar.\n        heights (List[float]): A list of heights corresponding to each label.\n\n    Returns:\n        None: The function displays a polar bar plot and does not return any value.\n\n    Conditions:\n        - The length of `labels` and `heights` must be equal.\n        - Each element in `heights` must be a non-negative number.\n\n    Example:\n        create_polar_bar_plot([\"a\", \"b\"], [3, 4])\n    \"\"\"\n    # Ensure the lengths of labels and heights are equal\n    assert len(labels) == len(heights), \"Length of labels and heights must be equal.\"\n\n    # Ensure all heights are non-negative\n    assert all(h >= 0 for h in heights), \"All heights must be non-negative.\"\n\n    # Create a polar subplot\n    fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n\n    # Calculate the angles for each bar\n    angles = [n / float(len(labels)) * 2 * 3.141592653589793 for n in range(len(labels))]\n\n    # Create the bar plot\n    bars = ax.bar(angles, heights,\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 617, "library_problem_id": 106, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 106}}
{"id": 618, "code": ["<code>\ndef create_donut_plot(labels: List[str], data: List[float], wedge_width: float) -> None:\n    plt.pie(data, labels=labels, wedgeprops={'width': wedge_width})\n    plt.axis('equal')\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 618, "library_problem_id": 107, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 619, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\n\nplt.plot(x, y)\nplt.grid(color='blue', linestyle='--')\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 619, "library_problem_id": 108, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 108}}
{"id": 620, "code": ["<code>\ndef plot_with_minor_grid(x: np.ndarray, y: np.ndarray) -> None:\n    plt.plot(x, y)\n    plt.minorticks_on()\n    plt.grid(which='minor', linestyle='--', color='gray')\n    plt.grid(which='major', linestyle='', color='')\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 620, "library_problem_id": 109, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 621, "code": ["<code>\ndef create_pie_chart(labels: List[str], sizes: List[int], colors: List[str]) -> None:\n    plt.pie(sizes, labels=[f'**{label}**' for label in labels], colors=colors, autopct='%1.1f%%', startangle=140)\n    plt.axis('equal')\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 621, "library_problem_id": 110, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 110}}
{"id": 622, "code": ["<code>\ndef create_pie_chart(labels: List[str], sizes: List[int], colors: List[str]) -> None:\n    plt.pie(sizes, labels=[f'**{label}**' for label in labels], colors=colors, autopct='%1.1f%%')\n    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n    plt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 622, "library_problem_id": 111, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 623, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the data\nx: np.ndarray = np.arange(10)\ny: np.ndarray = np.arange(10)\n\n# Plot the data\nplt.plot(x, y, marker='o', markerfacecolor='none', markeredgecolor='blue')\n\n# Display the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 623, "library_problem_id": 112, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 112}}
{"id": 624, "code": ["<code>\ndef plot_penguin_bill_length_distribution() -> None:\n    # Load the dataset and select relevant columns\n    df = sns.load_dataset(\"penguins\")[\n        [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n    ]\n    \n    # Plot the distribution of bill lengths\n    sns.distplot(df[\"bill_length_mm\"], color=\"blue\")\n    \n    # Plot a vertical line at x = 55 with green color\n    plt.axvline(x=55, color=\"green\", linestyle='--')\n    \n    # Display the plot\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 624, "library_problem_id": 113, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 625, "code": ["<code>\n```python\ndef plot_side_by_side_bars(blue_bar: tuple, orange_bar: tuple) -> None:\n    \"\"\"\n    Plots two sets of bars (blue and orange) side-by-side in a single bar plot.\n\n    Args:\n        blue_bar (tuple): A tuple containing the heights of the blue bars.\n        orange_bar (tuple): A tuple containing the heights of the orange bars.\n\n    Returns:\n        None: This function does not return any value. It displays a bar plot.\n\n    Conditions:\n        - The bars should be plotted side-by-side without overlapping.\n        - The number of blue bars should match the number of orange bars.\n\n    Example:\n        blue_bar = (23, 25, 17)\n        orange_bar = (19, 18, 14)\n        plot_side_by_side_bars(blue_bar, orange_bar)\n    \"\"\"\n    # Number of bars\n    n_bars = len(blue_bar)\n    \n    # Ensure the number of blue and orange bars are equal\n    assert n_bars == len(orange_bar), \"The number of blue and orange bars must be the same.\"\n    \n    # Define the positions for the bars\n    index = np.arange(n_bars)\n    \n    # Define the width of each bar\n    bar_width = 0.35\n    \n    # Plot blue bars\n    plt.bar(index, blue_bar, bar_width, label='Blue Bar', color='blue')\n    \n    # Plot orange bars, offset by bar_width\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 625, "library_problem_id": 114, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 626, "code": ["<code>\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef create_subplots(x: np.ndarray, y: np.ndarray, z: np.ndarray, a: np.ndarray) -> None:\n    \"\"\"\n    Create two subplots using matplotlib. The first subplot plots 'y' over 'x',\n    and the second subplot plots 'z' over 'a'. Each line chart is labeled and\n    included in a single legend on the first subplot.\n\n    Args:\n        x (np.ndarray): The x-coordinates for the first subplot.\n        y (np.ndarray): The y-coordinates for the first subplot.\n        z (np.ndarray): The y-coordinates for the second subplot.\n        a (np.ndarray): The x-coordinates for the second subplot.\n\n    Returns:\n        None: This function does not return any value. It displays the plots.\n\n    Conditions:\n        - The length of 'x' and 'y' must be equal for the first subplot.\n        - The length of 'z' and 'a' must be equal for the second subplot.\n    \"\"\"\n    # Create a figure and two subplots\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n    # Plot y over x in the first subplot\n    ax1.plot(x, y, label='y over x')\n    ax1.set_title('First Subplot')\n    ax1.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 626, "library_problem_id": 115, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 115}}
{"id": 627, "code": ["<code>\nplt.scatter(x, y, c=y, cmap='Spectral')\n    plt.colorbar(label='y-value')\n    plt.xlabel('x-value')\n    plt.ylabel('y-value')\n    plt.title('Scatter plot of y over x with color based on y-value')\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 627, "library_problem_id": 116, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 116}}
{"id": 628, "code": ["<code>\ndef plot_linear_relationship(x: np.ndarray, y: np.ndarray) -> None:\n    plt.figure(figsize=(8, 6))\n    plt.plot(x, y, marker='o')\n    plt.xlabel('x-axis')\n    plt.ylabel('y-axis')\n    plt.title('Plot of y over x')\n    plt.xticks(np.arange(min(x), max(x) + 1, 1))  # Set tick interval to 1 on the x-axis\n    plt.grid(True)\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 628, "library_problem_id": 117, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 117}}
{"id": 629, "code": ["<code>\ng = sns.catplot(data=df, x=\"species\", y=\"bill_length_mm\", hue=\"sex\", kind=\"bar\", sharey=False, col=\"sex\", height=5, aspect=.7)\n    g.despine(left=True)\n    plt.show()\n    # SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 629, "library_problem_id": 118, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 118}}
{"id": 630, "code": ["<code>\nimport numpy as np\n\n    x = np.linspace(0, 1, 1000)\n    y = np.sqrt(radius**2 - (x - center[0])**2)\n\n    plt.plot(x, center[1] + y, label='upper arc')\n    plt.plot(x, center[1] - y, label='lower arc')\n    plt.plot(center[0] + y, x, label='right arc')\n    plt.plot(center[0] - y, x, label='left arc')\n\n    plt.xlim(0, 1)\n    plt.ylim(0, 1)\n\n    plt.legend()\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 630, "library_problem_id": 119, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 119}}
{"id": 631, "code": ["<code>\nplt.plot(x, y)\n    plt.title(r'$\\boldsymbol{\\phi}$', fontsize=20)\n    plt.show()\n    # SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 631, "library_problem_id": 120, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 120}}
{"id": 632, "code": ["<code>\n# Create the plot\nplt.plot(x, y, label=\"Line\")\n\n# Add legend with adjusted spacing\nplt.legend(handletextpad=0.1)\n\n# Display the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 632, "library_problem_id": 121, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 121}}
{"id": 633, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_line_with_legend(x: np.ndarray, y: np.ndarray, legend_label: str, legend_handle_length: float) -> None:\n    \"\"\"\n    Plots the given y values over x values and adds a legend with a specified label and handle length.\n\n    Args:\n        x (np.ndarray): An array of x values for the plot.\n        y (np.ndarray): An array of y values for the plot.\n        legend_label (str): The label to be used in the legend.\n        legend_handle_length (float): The length of the legend handle.\n\n    Returns:\n        None: This function does not return any value. It displays a plot.\n\n    Conditions:\n        - The length of x and y should be the same.\n        - The legend handle length should be a positive float.\n\n    Example:\n        plot_line_with_legend(np.arange(10), np.arange(10), \"Line\", 0.3)\n    \"\"\"\n    plt.plot(x, y, label=legend_label)\n    plt.legend(handlelength=legend_handle_length)\n    plt.show()\n\n# Example usage\nx = np.arange(10)\ny = np.arange(10)\nplot_line_with_legend(x, y, \"Line\", 0.3)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 633, "library_problem_id": 122, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 634, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = np.arange(10)\nplt.plot(x, y, label=\"Line\")\nplt.plot(y, x, label=\"Flipped\")\n\n# Display the legend with two columns\nplt.legend(ncol=2)\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 634, "library_problem_id": 123, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 635, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Step 1: Create an array `x` with values ranging from 0 to 9\nx = np.arange(10)\n\n# Step 2: Create an array `y` with values ranging from 0 to 9\ny = np.arange(10)\n\n# Step 3: Plot the line with specified markers and label\nplt.plot(x, y, marker=\"*\", label=\"Line\")\n\n# Step 4: Display the legend for the plot\nplt.legend()\n\n# Step 5: Highlight two specific markers on the line plot\n# Example: Highlight markers at indices 2 and 5\nplt.plot(x[2], y[2], marker=\"o\", markersize=10, color=\"red\")\nplt.plot(x[5], y[5], marker=\"o\", markersize=10, color=\"red\")\n\n# Display the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 635, "library_problem_id": 124, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 121}}
{"id": 636, "code": ["<code>\ndef plot_2d_matrix_with_colorbar(matrix: np.ndarray) -> None:\n    plt.imshow(matrix, cmap='viridis', interpolation='nearest')\n    plt.colorbar()\n    plt.title('2D Matrix Heatmap with Colorbar')\n    plt.xlabel('Column Index')\n    plt.ylabel('Row Index')\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 636, "library_problem_id": 125, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 125}}
{"id": 637, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_bold_figure_title(x: np.ndarray, y: np.ndarray) -> None:\n    \"\"\"\n    Plots the values of y over x and sets the plot title with specific formatting.\n\n    Args:\n        x (np.ndarray): A NumPy array representing the x-axis values.\n        y (np.ndarray): A NumPy array representing the y-axis values.\n\n    Returns:\n        None: This function does not return any value. It displays a plot.\n\n    The plot title should be \"Figure 1\" with the word \"Figure\" in bold and \"1\" not in bold.\n    \"\"\"\n    plt.plot(x, y)\n    plt.title(r\"$\\bf{Figure}$ 1\")\n    plt.show()\n\n# Example usage\nx = np.arange(10)\ny = np.arange(10)\nplot_bold_figure_title(x, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 637, "library_problem_id": 126, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 126}}
{"id": 638, "code": ["<code>\ndef create_pairplot(df: pd.DataFrame) -> None:\n    \"\"\"\n    Generates a pairplot using seaborn for the given DataFrame `df`.\n\n    The pairplot is created using the following specifications:\n    - `x` column is used for x_vars.\n    - `y` column is used for y_vars.\n    - `id` column is used for hue to differentiate data points.\n    - The legend is hidden in the output figure.\n\n    Args:\n        df (pd.DataFrame): A DataFrame containing the data to be plotted. \n                           It must have columns 'id', 'x', and 'y'.\n\n    Returns:\n        None: The function displays the plot and does not return any value.\n    \"\"\"\n    # Create the pairplot with specified parameters\n    pairplot = sns.pairplot(df, x_vars=[\"x\"], y_vars=[\"y\"], hue=\"id\", plot_kws={'legend': False})\n    \n    # Hide the legend\n    pairplot._legend.remove()\n\n    # Display the plot\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 638, "library_problem_id": 127, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 127}}
{"id": 639, "code": ["<code>\ndef plot_inverted_x_axis(x: np.ndarray, y: np.ndarray) -> None:\n    plt.plot(x, y)\n    plt.gca().invert_xaxis()\n    plt.xlabel('X Axis (Inverted)')\n    plt.ylabel('Y Axis')\n    plt.title('Plot of Y over X with Inverted X Axis')\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 639, "library_problem_id": 128, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 128}}
{"id": 640, "code": ["<code>\ndef plot_scatter_with_limits(x: np.ndarray, y: np.ndarray, x_limit: tuple, y_limit: tuple) -> None:\n    plt.scatter(x, y)\n    plt.xlim(x_limit)\n    plt.ylim(y_limit)\n    plt.gca().set_clip_on(False)\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 640, "library_problem_id": 129, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 129}}
{"id": 641, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define data points\nx = np.arange(10)\ny = np.arange(10)\n\n# Create scatter plot\nplt.scatter(x, y, facecolors='red', edgecolors='black')\n\n# Display the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 641, "library_problem_id": 130, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 130}}
{"id": 642, "code": ["<code>\ndef plot_repeated_subplots(x: np.ndarray, y: np.ndarray, subplot_shape: tuple = (2, 2), figure_size: tuple = (15, 15)) -> None:\n    fig, axes = plt.subplots(subplot_shape[0], subplot_shape[1], figsize=figure_size)\n    for ax in axes.flatten():\n        ax.plot(x, y)\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 642, "library_problem_id": 131, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 131}}
{"id": 643, "code": ["<code>\ndef generate_histogram(data: np.ndarray, bins: int, range_min: float, range_max: float, bar_width: float) -> None:\n    plt.hist(data, bins=bins, range=(range_min, range_max), width=bar_width)\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.title('Histogram of Data')\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 643, "library_problem_id": 132, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 132}}
{"id": 644, "code": ["<code>\ndef plot_with_shaded_error() -> None:\n    x = np.arange(10)\n    y = np.arange(1, 11)\n    error = np.random.random(y.shape)\n\n    # Plot y over x\n    plt.plot(x, y, label='Data')\n\n    # Plot the error as a shaded region\n    plt.fill_between(x, y - error, y + error, alpha=0.2, label='Error')\n\n    # Add labels and legend\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Plot with Shaded Error Region')\n    plt.legend()\n\n    # Display the plot\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 644, "library_problem_id": 133, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 133}}
{"id": 645, "code": ["<code>\ndef plot_contour_with_axes():\n    \"\"\"\n    Generates a contour plot of a 2D function and overlays the x=0 and y=0 axes.\n\n    The function creates a meshgrid over a specified range, computes the negative \n    Euclidean distance from the origin for each point, and plots the contour. \n    The x=0 and y=0 axes are highlighted in white.\n\n    Returns:\n        None\n    \"\"\"\n    # Define the range and resolution for the x and y axes\n    xvec = np.linspace(-5.0, 5.0, 100)\n    \n    # Create a meshgrid for the 2D space\n    x, y = np.meshgrid(xvec, xvec)\n    \n    # Compute the negative Euclidean distance from the origin\n    z = -np.hypot(x, y)\n    \n    # Plot the filled contour\n    plt.contourf(x, y, z)\n    \n    # Overlay the x=0 and y=0 axes in white\n    plt.axhline(0, color='white', linewidth=1)\n    plt.axvline(0, color='white', linewidth=1)\n    \n    # Display the plot\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 645, "library_problem_id": 134, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 134}}
{"id": 646, "code": ["<code>\n```python\ndef plot_bars_with_error_bars(box_position: np.ndarray, \n                              box_height: np.ndarray, \n                              box_errors: np.ndarray, \n                              error_colors: list) -> None:\n    \"\"\"\n    Plots a bar chart with error bars.\n\n    Args:\n        box_position (np.ndarray): An array of positions for the bars on the x-axis.\n        box_height (np.ndarray): An array of heights for each bar.\n        box_errors (np.ndarray): An array of error values for each bar.\n        error_colors (list): A list of color codes for each error bar.\n\n    Returns:\n        None: This function does not return any value. It displays a plot.\n\n    Conditions:\n        - The length of `box_position`, `box_height`, `box_errors`, and `error_colors` must be the same.\n        - Each element in `error_colors` should be a valid color code for matplotlib.\n\n    Example:\n        plot_bars_with_error_bars(np.arange(4), np.ones(4), np.arange(1, 5), [\"r\", \"r\", \"b\", \"b\"])\n    \"\"\"\n    fig, ax = plt.subplots()\n    ax.bar(box_position, box_height, color=\"yellow\")\n    ax.errorbar(box_position, box_height, yerr=box_errors, fmt='o', color='black', ecolor=error_colors, capsize=5)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 646, "library_problem_id": 135, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 135}}
{"id": 647, "code": ["<code>\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_subplots(x: np.ndarray, y: np.ndarray, z: np.ndarray, a: np.ndarray) -> None:\n    \"\"\"\n    Plots two side-by-side subplots. The first subplot plots 'y' over 'x' with the title \"Y\".\n    The second subplot plots 'z' over 'a' with the title \"Z\", positioned higher than the first subplot's title.\n\n    Args:\n        x (np.ndarray): The x-coordinates for the first subplot.\n        y (np.ndarray): The y-coordinates for the first subplot.\n        z (np.ndarray): The x-coordinates for the second subplot.\n        a (np.ndarray): The y-coordinates for the second subplot.\n\n    Returns:\n        None: This function does not return any value. It displays the plots.\n\n    Conditions:\n        - The title of the second subplot should be positioned higher than the title of the first subplot.\n    \"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n    # Plot y over x in the first subplot\n    ax1.plot(x, y)\n    ax1.set_title(\"Y\")\n\n    # Plot z over a in the second subplot\n    ax2.plot(z, a)\n    ax2.set_title(\"Z\", pad=2\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 647, "library_problem_id": 136, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 136}}
{"id": 648, "code": ["<code>\n```python\ndef create_subplots(x: np.ndarray, y: np.ndarray, num_rows: int = 4, num_cols: int = 4, fig_size: tuple = (5, 5)) -> None:\n    \"\"\"\n    Create a grid of subplots and plot y over x in each subplot.\n\n    Args:\n        x (np.ndarray): The x-coordinates for the plots.\n        y (np.ndarray): The y-coordinates for the plots.\n        num_rows (int, optional): The number of rows of subplots. Defaults to 4.\n        num_cols (int, optional): The number of columns of subplots. Defaults to 4.\n        fig_size (tuple, optional): The size of the figure. Defaults to (5, 5).\n\n    Returns:\n        None: This function does not return any value. It displays the plot.\n\n    Conditions:\n        - Ensure that the subplots have enough spacing to prevent tick label overlap.\n        - Each subplot should display y over x with axis tick labels visible.\n    \"\"\"\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=fig_size)\n    plt.subplots_adjust(wspace=0.5, hspace=0.5)  # Adjust spacing to prevent overlap\n\n    for i in range(num_rows):\n        for j in range(num_cols):\n            axes[i, j].plot(x, y)\n            axes[i, j].set_xticks(\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 648, "library_problem_id": 137, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 137}}
{"id": 649, "code": ["<code>\ndef plot_random_matrix(matrix: np.ndarray, figure_size: tuple = (8, 8)) -> None:\n    plt.figure(figsize=figure_size)\n    plt.matshow(matrix, fignum=1)\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 649, "library_problem_id": 138, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 138}}
{"id": 650, "code": ["<code>\n```python\n\"\"\"\nImports necessary libraries and loads a sample dataset of penguin measurements.\nThe task is to create a visual representation of the first 10 rows of specific\ncolumns from this dataset as a table using Matplotlib. The table should be\ndisplayed within a bounding box defined by the coordinates [0, 0, 1, 1].\n\nLibraries:\n- numpy: Used for numerical operations (not directly used in this task).\n- pandas: Used for data manipulation and analysis.\n- matplotlib.pyplot: Used for creating static, interactive, and animated visualizations.\n- seaborn: Used for statistical data visualization and loading sample datasets.\n\nDataset:\n- The dataset 'penguins' is loaded from seaborn, and the first 10 rows of the\n  columns 'bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', and 'body_mass_g'\n  are selected for visualization.\n\nFunctionality:\n- The task is to plot the selected DataFrame as a table using Matplotlib.\n- The bounding box for the table should be set to [0, 0, 1, 1].\n\nReturns:\n- A Matplotlib figure object displaying the table.\n\nExample:\n- The resulting table will be a visual representation of the selected DataFrame\n  with the specified bounding box.\n\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the dataset and select specific columns\ndf = sns.load_dataset(\"penguins\")[\n    [\"b\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 650, "library_problem_id": 139, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 139}}
{"id": 651, "code": ["<code>\nplt.plot(x, y, label='Line plot of y over x')\nplt.xlabel('x-axis')\nplt.ylabel('y-axis')\nplt.title('Line Chart of y over x')\n\n# Display x-axis tick labels on both top and bottom\nplt.tick_params(axis='x', which='both', top=True, bottom=True)\n\n# Show the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 651, "library_problem_id": 140, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 140}}
{"id": 652, "code": ["<code>\nfig, ax1 = plt.subplots()\n\n    # Plotting the line chart\n    ax1.plot(x, y, 'b-')\n\n    # Setting the labels and title\n    ax1.set_xlabel('X data')\n    ax1.set_ylabel('Y data', color='b')\n    ax1.tick_params('y', colors='b')\n    ax1.set_title('Line chart with dual axis ticks')\n\n    # Displaying the x-axis ticks on both the top and bottom of the figure\n    ax1.xaxis.tick_top()\n    ax1.xaxis.tick_bottom()\n\n    # Setting the x-axis limits\n    ax1.set_xlim(x.min(), x.max())\n\n    # Displaying the plot\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 652, "library_problem_id": 141, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 653, "code": ["<code>\ndef plot_line_chart(x: np.ndarray, y: np.ndarray) -> None:\n    plt.plot(x, y)\n    plt.xticks(ticks=x, labels=x)  # Show x-axis tick labels\n    plt.tick_params(axis='x', which='both', bottom=False, top=False)  # Hide x-axis ticks\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 653, "library_problem_id": 142, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 140}}
{"id": 654, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef create_catplots(df: pd.DataFrame) -> None:\n    \"\"\"\n    Generates categorical scatter plots using the provided DataFrame.\n    \n    The function creates scatter plots with the following specifications:\n    - X-axis: 'time'\n    - Y-axis: 'pulse'\n    - Hue: 'kind'\n    - Columns: 'diet'\n    \n    Additionally, the subplot titles are customized to:\n    - \"Group: Fat\" for the 'Fat' diet category\n    - \"Group: No Fat\" for the 'No Fat' diet category\n\n    Args:\n        df (pd.DataFrame): The DataFrame containing the data to be plotted.\n    \n    Returns:\n        None: The function displays the plots and does not return any value.\n    \"\"\"\n    # Create the categorical scatter plots\n    g = sns.catplot(\n        data=df,\n        x=\"time\",\n        y=\"pulse\",\n        hue=\"kind\",\n        col=\"diet\",\n        kind=\"scatter\"\n    )\n    \n    # Customize the subplot titles\n    g.set_titles(col_template=\"Group: {col_name}\")\n\n# Load the dataset\ndf = sns.load_dataset(\"exercise\")\n\n# Generate the catplots\ncreate_catplots(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 654, "library_problem_id": 143, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 143}}
{"id": 655, "code": ["<code>\ndef create_catplot(data: pd.DataFrame) -> sns.axisgrid.FacetGrid:\n    catplot = sns.catplot(\n        data=data,\n        x=\"time\",\n        y=\"pulse\",\n        hue=\"kind\",\n        col=\"diet\",\n        kind=\"scatter\"\n    )\n    catplot.set_axis_labels(\"Exercise Time\", \"Exercise Time\")\n    return catplot\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 655, "library_problem_id": 144, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 656, "code": ["<code>\ndef create_catplots(df: pd.DataFrame) -> None:\n    \"\"\"\n    Generates categorical scatter plots using the Seaborn library.\n\n    This function creates scatter plots with the following specifications:\n    - X-axis: 'time'\n    - Y-axis: 'pulse'\n    - Hue: 'kind'\n    - Column: 'diet'\n    - No Y-axis labels on any subplot\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the data to be plotted.\n\n    Returns:\n        None: This function does not return any value. It directly displays the plots.\n    \"\"\"\n    sns.catplot(\n        data=df,\n        x=\"time\",\n        y=\"pulse\",\n        hue=\"kind\",\n        col=\"diet\",\n        kind=\"scatter\",\n        facet_kws={'sharey': False}\n    )\n    plt.subplots_adjust(top=0.9)\n    plt.suptitle('Scatter Plots of Pulse vs Time by Diet and Kind')\n    plt.show()\n\n# Load the dataset and create the plots\ndf = sns.load_dataset(\"exercise\")\ncreate_catplots(df)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 656, "library_problem_id": 145, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 143}}
{"id": 657, "code": ["<code>\n# Generate data for plotting\nx = np.arange(10)\ny = np.arange(10)\n\n# Create the plot\nplt.plot(x, y, label='y')\n\n# Add legend with specified font size\nplt.legend(fontsize=8)\n\n# Display the plot\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 657, "library_problem_id": 146, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 146}}
{"id": 658, "code": ["<code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_line_graph(x: np.ndarray, y: np.ndarray) -> None:\n    plt.figure(figsize=(5, 5), dpi=300)\n    plt.plot(x, y)\n    plt.show()\n\nx = np.arange(10)\ny = np.arange(10)\nplot_line_graph(x, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 658, "library_problem_id": 147, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 147}}
{"id": 659, "code": ["<code>\ndef plot_with_legend(x: np.ndarray, y: np.ndarray) -> None:\n    plt.plot(x, y, label='y')\n    legend = plt.legend()\n    legend.get_frame().set_linewidth(0)  # Remove the border of the legend frame\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 659, "library_problem_id": 148, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 148}}
{"id": 660, "code": ["<code>\ndef plot_trigonometric_functions():\n    # Step 1: Generate an array of 400 evenly spaced values between 0 and 2\u03c0\n    t = np.linspace(0, 2 * math.pi, 400)\n\n    # Step 2: Calculate the sine of each value in 't'\n    a = np.sin(t)\n\n    # Step 3: Calculate the cosine of each value in 't'\n    b = np.cos(t)\n\n    # Step 4: Compute the sum of 'a' and 'b'\n    c = a + b\n\n    # Step 5: Plot the functions\n    plt.figure()\n    plt.plot(t, a, label='sin(t)')\n    plt.plot(t, b, label='cos(t)')\n    plt.plot(t, c, label='sin(t) + cos(t)')\n    plt.xlabel('t')\n    plt.ylabel('Function value')\n    plt.title('Trigonometric Functions')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 660, "library_problem_id": 149, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 149}}
{"id": 661, "code": ["<code>\ndef create_stripplot(data: pd.DataFrame) -> None:\n    sns.stripplot(data=data, x='sex', y='bill_length_mm', hue='species', dodge=True)\n    plt.legend([], [], frameon=False)  # Remove the legend\n    plt.show()\n# SOLUTION END\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 661, "library_problem_id": 150, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 150}}
{"id": 662, "code": ["<code>\n```python\ndef create_facet_grid_plot(df: pd.DataFrame) -> None:\n    \"\"\"\n    Create a Seaborn FacetGrid plot with point plots of column 'c' over column 'a',\n    faceted by unique values in column 'b'. Each subplot should display x-ticks at\n    intervals of 1, but x-tick labels should be shown at intervals of 2.\n\n    Args:\n        df (pd.DataFrame): A DataFrame containing the data to be plotted. It must have\n                           the following columns:\n                           - 'a': Numeric values for the x-axis.\n                           - 'b': Categorical values for faceting the plots.\n                           - 'c': Numeric values for the y-axis.\n\n    Returns:\n        None: The function creates and displays the plot but does not return any value.\n    \"\"\"\n    # Initialize the FacetGrid with 'b' as the row variable\n    g = sns.FacetGrid(df, row=\"b\", height=4, aspect=2)\n\n    # Map a point plot onto each facet\n    g.map(sns.pointplot, \"a\", \"c\", order=np.arange(1, 31))\n\n    # Customize the x-ticks and x-tick labels\n    for ax in g.axes.flat:\n        ax.set_xticks(np.arange(1, 31, 1))  # Set x-ticks at intervals of 1\n        ax.set_xticklabels(np.arange(1, 31, 2))  # Set x-t\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 662, "library_problem_id": 151, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 151}}
{"id": 663, "code": ["<code>\n```python\ndef create_3d_scatter_plot(x: np.ndarray, y: np.ndarray, z: np.ndarray, azimuth: int, elevation: int) -> None:\n    \"\"\"\n    Generates a 3D scatter plot using the provided x, y, and z coordinates.\n    \n    Args:\n        x (np.ndarray): An array of x-coordinates for the scatter plot.\n        y (np.ndarray): An array of y-coordinates for the scatter plot.\n        z (np.ndarray): An array of z-coordinates for the scatter plot.\n        azimuth (int): The azimuth angle in degrees for the view of the plot.\n        elevation (int): The elevation angle in degrees for the view of the plot.\n    \n    Returns:\n        None: This function does not return any value. It displays a 3D scatter plot.\n    \n    Conditions:\n        - The length of x, y, and z should be equal.\n        - Azimuth and elevation should be integers representing the desired view angles.\n    \"\"\"\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(x, y, z)\n    ax.view_init(elev=elevation, azim=azimuth)\n    plt.show()\n\n# Example usage\nx = np.random.random(10)\ny = np.random.random(10)\nz = np.random.random(10)\ncreate_3d_scatter_plot(x,\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 663, "library_problem_id": 152, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 152}}
{"id": 664, "code": ["<code>\nplt.plot(x, y)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.xticks(ticks=[], labels=[])\nplt.yticks(ticks=[], labels=[])\nplt.show()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 664, "library_problem_id": 153, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 153}}
{"id": 665, "code": ["<code>\n```python\ndef plot_random_matrix_subplots(nrow: int = 2, ncol: int = 2) -> None:\n    \"\"\"\n    Generates a figure with a grid of subplots, each displaying a random matrix as an image.\n    \n    Args:\n        nrow (int): Number of rows in the subplot grid. Default is 2.\n        ncol (int): Number of columns in the subplot grid. Default is 2.\n        \n    Returns:\n        None: The function creates and displays a plot.\n        \n    The function performs the following steps:\n    1. Generates a random 10x10 matrix.\n    2. Creates a figure with specified dimensions based on the number of rows and columns.\n    3. Configures a grid of subplots with no spacing between them.\n    4. Plots the random matrix in each subplot as an image.\n    5. Removes axis ticks from each subplot.\n    \"\"\"\n    # Generate a random 10x10 matrix\n    x = np.random.random((10, 10))\n    \n    # Create a figure with specified size\n    fig = plt.figure(figsize=(ncol + 1, nrow + 1))\n    \n    # Create a grid of subplots with no spacing\n    gs = gridspec.GridSpec(nrow, ncol, wspace=0, hspace=0)\n    \n    # Iterate over each subplot position\n    for i in range(nrow):\n        for j in range(ncol):\n            ax = plt.subplot(gs[i, j])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 665, "library_problem_id": 154, "library": "Matplotlib", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 154}}
{"id": 666, "code": ["<code>\ndef update_tensorflow_variable() -> int:\n    \"\"\"\n    Updates a TensorFlow variable `x` to the value 1 and returns it as a Python integer.\n\n    Returns:\n        int: The updated value of the TensorFlow variable `x`.\n    \"\"\"\n    # Initialize TensorFlow variable `x` with initial value 0\n    x = tf.Variable(0)\n\n    # Update the value of `x` to 1\n    x.assign(1)\n\n    # Retrieve and return the updated value of `x` as a Python integer\n    return int(x.numpy())\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 666, "library_problem_id": 0, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 667, "code": ["<code>\ndef update_tensorflow_variable() -> int:\n    # Step 1: Initialize the TensorFlow variable `x` with the value 0\n    x = tf.Variable(0, dtype=tf.int32)\n    \n    # Step 2: Assign the value 114514 to the variable `x`\n    x.assign(114514)\n    \n    # Step 3: Retrieve the value of `x` as a Python integer\n    result = int(x.numpy())\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 667, "library_problem_id": 1, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 668, "code": ["<code>\nimport tensorflow as tf\n\ndef create_one_hot_tensor(class_indices: list[int], num_classes: int = 10) -> tf.Tensor:\n    \"\"\"\n    Converts a list of class indices into a one-hot encoded tensor.\n\n    Args:\n        class_indices (list[int]): A list of integers representing class indices.\n        num_classes (int): The total number of classes. Default is 10.\n\n    Returns:\n        tf.Tensor: A tensor of shape (n, num_classes) with one-hot encoded rows.\n                   The tensor has dtype int32.\n    \"\"\"\n    # Convert the list of class indices to a one-hot encoded tensor\n    one_hot_tensor = tf.one_hot(class_indices, depth=num_classes, dtype=tf.int32)\n    return one_hot_tensor\n\nlabels = [0, 6, 5, 4, 2]\nresult = create_one_hot_tensor(labels)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 668, "library_problem_id": 2, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 2}}
{"id": 669, "code": ["<code>\n```python\n\"\"\"\nFunction to generate a tensor with specific class exclusions for a multi-class dataset.\n\nThis function takes a list of class indices and generates a tensor where each row corresponds to a one-hot encoded \nrepresentation of a class, with the specified class index set to 0 and all other indices set to 1. This is useful for \ncustom metrics in multi-class classification tasks where certain classes need to be excluded or treated differently.\n\nArgs:\n    labels (List[int]): A list of integer indices representing the classes to be excluded. Each index corresponds to \n                        the class position that should be set to 0 in the one-hot encoded tensor.\n\nReturns:\n    tf.Tensor: A tensor of shape (n, 10) where n is the length of the input list 'labels'. Each row has a single 0 \n               at the position specified by the corresponding index in 'labels', and 1s elsewhere. The dtype of the \n               tensor is int32.\n\nExample:\n    Given labels = [0, 6, 5, 4, 2], the function will return:\n    [[0 1 1 1 1 1 1 1 1 1]\n     [1 1 1 1 1 1 0 1 1 1]\n     [1 1 1 1 1 0 1 1 1 1]\n     [1 1 1 1 0 1 1 1 1 1]\n     [1 1 0 1 1 1 1 1 1 1]]\n\"\"\"\n\nimport tensorflow as tf\nfrom typing import List\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 669, "library_problem_id": 3, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 670, "code": ["<code>\nimport tensorflow as tf\n\ndef create_one_hot_tensor(class_indices: list[int], num_classes: int = 10) -> tf.Tensor:\n    \"\"\"\n    Converts a list of class indices into a one-hot encoded tensor.\n\n    Args:\n    class_indices (list[int]): A list of integers representing class indices.\n    num_classes (int): The total number of classes. Default is 10.\n\n    Returns:\n    tf.Tensor: A tensor of shape (n, num_classes) with one-hot encoded rows, where n is the length of class_indices.\n    \"\"\"\n    return tf.one_hot(class_indices, depth=num_classes, dtype=tf.int32)\n\n# Example usage\nlabels = [0, 6, 5, 4, 2]\nresult = create_one_hot_tensor(labels)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 670, "library_problem_id": 4, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 2}}
{"id": 671, "code": ["<code>\n```python\n\"\"\"\nFunction to convert a list of class indices into a one-hot encoded tensor suitable for use in TensorFlow.\n\nThis function is designed to work with TensorFlow 2.10.0 and is intended to help in building a custom metric\nto measure the accuracy of a specific class in a multi-class dataset during training.\n\nThe function takes a list of integer class indices and converts it into a one-hot encoded tensor. The tensor\nwill have a shape of (n, 10), where n is the length of the input list, and 10 is the total number of classes.\n\nArgs:\n    labels (List[int]): A list of integer class indices. Each index should be in the range [0, 9], representing\n                        the class label for each sample.\n\nReturns:\n    tf.Tensor: A tensor of shape (n, 10) with dtype int32, where each row is a one-hot encoded representation\n               of the corresponding class index from the input list.\n\nExample:\n    Given the input list [0, 6, 5, 4, 2], the function will return the following tensor:\n    [[1 0 0 0 0 0 0 0 0 0]\n     [0 0 0 0 0 0 1 0 0 0]\n     [0 0 0 0 0 1 0 0 0 0]\n     [0 0 0 0 1 0 0 0 0 0]\n     [0 0 1 0 0 0 0 0 0 0]]\n\"\"\"\n\nimport tensorflow\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 671, "library_problem_id": 5, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 2}}
{"id": 672, "code": ["<code>\nimport tensorflow as tf\n\ndef generate_reversed_one_hot_tensor(class_indices: list[int]) -> tf.Tensor:\n    \"\"\"\n    Generates a tensor of reversed one-hot encoded vectors for specified class indices.\n\n    Args:\n        class_indices (list[int]): A list of integers representing class indices.\n\n    Returns:\n        tf.Tensor: A tensor of shape (n, 10) with dtype int32, where each row is a reversed one-hot encoded vector.\n    \"\"\"\n    num_classes = 10\n    result = tf.ones((len(class_indices), num_classes), dtype=tf.int32)\n    for i, class_index in enumerate(class_indices):\n        result[i, class_index] = 0\n    return result\n\nlabels = [0, 6, 5, 4, 2]\nresult = generate_reversed_one_hot_tensor(labels)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 672, "library_problem_id": 6, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 2}}
{"id": 673, "code": ["<code>\n```python\nimport tensorflow as tf\n\n# Disable eager execution for compatibility with TensorFlow v1 functionalities\ntf.compat.v1.disable_eager_execution()\n\n# Define the input data\ninput_data = [10, 20, 30]\n\ndef custom_map_func(i):\n    \"\"\"\n    Custom map function to transform a single integer input into a list of three consecutive integers.\n\n    Args:\n    i (int): The input integer.\n\n    Returns:\n    list: A list containing the input integer and the next two consecutive integers.\n    \"\"\"\n    return [i, i + 1, i + 2]\n\n# Create a TensorFlow Dataset from the input data\ndataset = tf.data.Dataset.from_tensor_slices(input_data)\n\n# Apply the custom map function using tf.data.Dataset.flat_map to flatten the output\ndataset = dataset.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(\n    tf.compat.v1.py_func(func=custom_map_func, inp=[x], Tout=[tf.int64])\n))\n\n# Initialize an iterator to retrieve elements from the dataset\niterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\nnext_element = iterator.get_next()\n\n# Collect the results in a list\nresult = []\n\n# Execute the TensorFlow session to retrieve and print the results\nwith tf.compat.v1.Session() as sess:\n    try:\n        while\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 673, "library_problem_id": 7, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 7}}
{"id": 674, "code": ["<code>\nimport tensorflow as tf\n\n# Disable eager execution for compatibility with TensorFlow 1.x style sessions\ntf.compat.v1.disable_eager_execution()\n\ndef transform_dataset(input_data: list) -> list:\n    \"\"\"\n    Transforms a list of integers by expanding each integer into a sequence of three consecutive integers.\n    \n    Args:\n        input_data (list): A list of integers to be transformed.\n        \n    Returns:\n        list: A flattened list of integers where each input integer is expanded into three consecutive integers.\n    \"\"\"\n    def my_map_func(i):\n        # Expands each integer into a list of three consecutive integers\n        return tf.data.Dataset.from_tensor_slices([i, i + 1, i + 2])\n\n    # Create a TensorFlow Dataset from the input list\n    ds = tf.data.Dataset.from_tensor_slices(input_data)\n    \n    # Apply the map function to transform each element\n    ds = ds.flat_map(my_map_func)\n    \n    # Convert the dataset to a numpy array\n    ds = ds.as_numpy_iterator()\n\n    result = []\n    for element in ds:\n        # Append each element to the result list\n        result.append(element)\n    \n    return result\n\n# Example usage\nexample_input = [10, 20, 30]\nresult = transform_dataset(example_input)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 674, "library_problem_id": 8, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 7}}
{"id": 675, "code": ["<code>\ndef create_mask_tensor(lengths):\n    # Convert the list of lengths to a TensorFlow tensor\n    lengths_tensor = tf.constant(lengths, dtype=tf.int32)\n    \n    # Determine the total length of each row in the mask tensor\n    total_length = 8\n    \n    # Create a range tensor from 0 to total_length - 1\n    range_tensor = tf.range(total_length)\n    \n    # Expand dimensions of lengths_tensor to enable broadcasting\n    lengths_expanded = tf.expand_dims(lengths_tensor, axis=1)\n    \n    # Create the mask by comparing range_tensor with lengths_expanded\n    mask = tf.cast(range_tensor < lengths_expanded, dtype=tf.int32)\n    \n    return mask\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 675, "library_problem_id": 9, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 676, "code": ["<code>\nimport tensorflow as tf\nfrom typing import List\n\ndef create_mask(lengths: List[int]) -> tf.Tensor:\n    # Convert the list of lengths to a TensorFlow tensor\n    lengths_tensor = tf.constant(lengths, dtype=tf.int32)\n    \n    # Determine the total length of each row in the mask\n    total_length = 8\n    \n    # Create a range tensor from 0 to total_length - 1\n    range_tensor = tf.range(total_length)\n    \n    # Use broadcasting to compare each length with the range tensor\n    # This will create a boolean mask where True represents 0 and False represents 1\n    mask = range_tensor < tf.expand_dims(lengths_tensor, axis=1)\n    \n    # Convert the boolean mask to integers (0s and 1s)\n    result = tf.cast(mask, dtype=tf.int32)\n    \n    return result\n\n# Example usage\nlengths = [4, 3, 5, 2]\nresult = create_mask(lengths)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 676, "library_problem_id": 10, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 677, "code": ["<code>\nimport tensorflow as tf\n\ndef create_mask(lengths: list[int], max_length: int) -> tf.Tensor:\n    # Create a 2D tensor of zeros with shape (number of lengths, max_length)\n    mask = tf.zeros((len(lengths), max_length), dtype=tf.float32)\n    \n    # Iterate over each length and set the corresponding 1s in the mask\n    for i, length in enumerate(lengths):\n        mask[i, -length:] = 1.0  # Set the last 'length' elements to 1\n    \n    return mask\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 677, "library_problem_id": 11, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 678, "code": ["<code>\ndef create_mask_tensor(lengths: List[int], total_length: int) -> tf.Tensor:\n    # Initialize a zero tensor of shape (number of lengths, total_length)\n    mask = tf.zeros((len(lengths), total_length), dtype=tf.int32)\n    \n    # Iterate over each length and set the corresponding number of 1s\n    for i, length in enumerate(lengths):\n        # Ensure the length does not exceed the total_length\n        if length > total_length:\n            raise ValueError(f\"Length {length} exceeds total_length {total_length}\")\n        # Set the first 'length' elements of the i-th row to 1\n        mask = mask + tf.scatter_nd(indices=[[i]], updates=[tf.concat([tf.ones(length, dtype=tf.int32), tf.zeros(total_length - length, dtype=tf.int32)], axis=0)], shape=mask.shape)\n    \n    return mask\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 678, "library_problem_id": 12, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 679, "code": ["<code>\n```python\n\"\"\"\nFunction to create a binary mask tensor in TensorFlow.\n\nThis function generates a 2D tensor mask of 1s and 0s based on a given 1D tensor of lengths. \nEach row in the resulting mask tensor corresponds to an entry in the input lengths tensor. \nThe number of trailing 0s in each row equals the respective entry in the lengths tensor, \nand the remaining positions are filled with 1s, such that each row has a total length of `total_length`.\n\nArgs:\n    lengths (List[int]): A list of integers representing the number of trailing 0s for each row.\n    total_length (int): The total length of each row in the resulting mask tensor.\n\nReturns:\n    tf.Tensor: A 2D tensor of shape (len(lengths), total_length) where each row contains \n               the specified number of trailing 0s and leading 1s.\n\nExample:\n    lengths = [4, 3, 5, 2]\n    total_length = 8\n    The resulting tensor will be:\n    [[1. 1. 1. 1. 0. 0. 0. 0.]\n     [1. 1. 1. 1. 1. 0. 0. 0.]\n     [1. 1. 1. 0. 0. 0. 0. 0.]\n     [1. 1. 1. 1. 1. 1. 0. 0.]]\n\"\"\"\n\nimport tensorflow as tf\n\ndef create_mask_tensor(lengths: list[int], total\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 679, "library_problem_id": 13, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 680, "code": ["<code>\ndef cartesian_product(tensor_a: tf.Tensor, tensor_b: tf.Tensor) -> tf.Tensor:\n    \"\"\"\n    Computes the Cartesian product of two 1-D tensors using TensorFlow.\n\n    Args:\n    tensor_a (tf.Tensor): A 1-D tensor containing the first set of elements.\n    tensor_b (tf.Tensor): A 1-D tensor containing the second set of elements.\n\n    Returns:\n    tf.Tensor: A 2-D tensor where each row is a unique combination of elements from `tensor_a` and `tensor_b`.\n    \"\"\"\n    # Expand dimensions to match the broadcasting rules\n    tensor_a_expanded = tf.expand_dims(tensor_a, 1)\n    tensor_b_expanded = tf.expand_dims(tensor_b, 0)\n\n    # Perform element-wise multiplication to create the Cartesian product\n    cartesian_product = tf.multiply(tensor_a_expanded, tensor_b_expanded)\n\n    # Reshape the result to a 2-D tensor\n    cartesian_product = tf.reshape(cartesian_product, [-1, 2])\n\n    return cartesian_product\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 680, "library_problem_id": 14, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 681, "code": ["<code>\nimport tensorflow as tf\n\ndef cartesian_product(a: tf.Tensor, b: tf.Tensor) -> tf.Tensor:\n    # Expand dimensions of `a` and `b` to enable broadcasting\n    a_expanded = tf.expand_dims(a, axis=1)\n    b_expanded = tf.expand_dims(b, axis=0)\n    \n    # Use broadcasting to create a grid of all combinations\n    a_tiled = tf.tile(a_expanded, [1, tf.shape(b)[0]])\n    b_tiled = tf.tile(b_expanded, [tf.shape(a)[0], 1])\n    \n    # Stack the results to form pairs\n    result = tf.stack([tf.reshape(a_tiled, [-1]), tf.reshape(b_tiled, [-1])], axis=1)\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 681, "library_problem_id": 15, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 682, "code": ["<code>\ndef reshape_tensor(input_tensor: tf.Tensor) -> tf.Tensor:\n    \"\"\"\n    Reshapes the input tensor by removing the third dimension.\n\n    Args:\n        input_tensor (tf.Tensor): A 4D tensor with shape (50, 100, 1, 512).\n\n    Returns:\n        tf.Tensor: A 3D tensor with shape (50, 100, 512).\n\n    Example:\n        >>> np.random.seed(10)\n        >>> a = tf.constant(np.random.rand(50, 100, 1, 512))\n        >>> result = reshape_tensor(a)\n        >>> result.shape\n        TensorShape([50, 100, 512])\n    \"\"\"\n    # Implement the reshaping logic here\n    result = tf.squeeze(input_tensor, [2]) # remove the third dimension\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 682, "library_problem_id": 16, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 683, "code": ["<code>\nimport tensorflow as tf\nimport numpy as np\n\nnp.random.seed(10)\na = tf.constant(np.random.rand(50, 100, 512))\n\ndef reshape_tensor(tensor: tf.Tensor) -> tf.Tensor:\n    return tf.reshape(tensor, (50, 100, 1, 512))\n\nresult = reshape_tensor(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 683, "library_problem_id": 17, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 16}}
{"id": 684, "code": ["<code>\n# Reshape the tensor `a` to the desired shape\nresult = tf.reshape(a, (1, 50, 100, 1, 512))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 684, "library_problem_id": 18, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 16}}
{"id": 685, "code": ["<code>\nresult = tf.reduce_sum(A, axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 685, "library_problem_id": 19, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 686, "code": ["<code>\nresult = tf.reduce_prod(A, axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 686, "library_problem_id": 20, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 687, "code": ["<code>\ndef tensorflow_reciprocal(A: tf.Tensor) -> tf.Tensor:\n    \"\"\"\n    Computes the reciprocal of each element in the input tensor A using TensorFlow.\n\n    Args:\n    A (tf.Tensor): A 1-D TensorFlow tensor of type tf.float32 containing the elements \n                   for which the reciprocal is to be computed. The tensor contains \n                   both positive and negative values, as well as zero.\n\n    Returns:\n    tf.Tensor: A 1-D TensorFlow tensor of type tf.float32 where each element is the \n               reciprocal of the corresponding element in the input tensor A. \n               Note that the reciprocal of zero is undefined and should be handled \n               appropriately by TensorFlow.\n\n    Mathematical Condition:\n    The reciprocal of a number x is defined as 1/x. For elements in A that are zero, \n    TensorFlow will handle the division by zero according to its internal rules, \n    typically resulting in `inf` or `-inf` for positive or negative zero respectively.\n    \"\"\"\n    return tf.math.reciprocal(A)\n\nA = tf.constant([-0.5, -0.1, 0, 0.1, 0.5, 2], dtype=tf.float32)\nresult = tensorflow_reciprocal(A)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 687, "library_problem_id": 21, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 19}}
{"id": 688, "code": ["<code>\nimport tensorflow as tf\n\ndef compute_row_wise_l2_distance(a: tf.Tensor, b: tf.Tensor) -> tf.Tensor:\n    \"\"\"\n    Computes the row-wise L2 distance between two tensors.\n\n    Args:\n        a (tf.Tensor): A 2D tensor of shape (n, m).\n        b (tf.Tensor): A 2D tensor of shape (n, m).\n\n    Returns:\n        tf.Tensor: A 1D tensor containing the sum of squared differences for each row.\n    \"\"\"\n    # Calculate the element-wise squared difference\n    squared_difference = tf.square(tf.subtract(a, b))\n    \n    # Perform row-wise reduction to sum the squared differences\n    row_wise_sum = tf.reduce_sum(squared_difference, axis=1)\n    \n    return row_wise_sum\n\n# Define the tensors\na = tf.constant([\n    [1, 1, 1],\n    [1, 1, 1]\n])\nb = tf.constant([\n    [0, 0, 0],\n    [1, 1, 1]\n])\n\n# Compute the result\nresult = compute_row_wise_l2_distance(a, b)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 688, "library_problem_id": 22, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 689, "code": ["<code>\nimport tensorflow as tf\n\ndef compute_columnwise_l2_distance(a: tf.Tensor, b: tf.Tensor) -> tf.Tensor:\n    \"\"\"\n    Compute the column-wise L2 distance between two tensors.\n\n    Args:\n    a (tf.Tensor): A 2D tensor of shape (m, n).\n    b (tf.Tensor): A 2D tensor of shape (m, n).\n\n    Returns:\n    tf.Tensor: A 1D tensor of shape (n,) representing the L2 distance for each column.\n    \"\"\"\n    # Step 1: Compute the element-wise squared difference\n    squared_diff = tf.square(tf.subtract(a, b))\n    \n    # Step 2: Perform column-wise reduction to sum the squared differences\n    columnwise_l2_distance = tf.reduce_sum(squared_diff, axis=0)\n    \n    return columnwise_l2_distance\n\n# Example usage\na = tf.constant([\n  [1, 1, 1],\n  [1, 1, 1]\n])\nb = tf.constant([\n  [0, 0, 0],\n  [1, 1, 1]\n])\n\nresult = compute_columnwise_l2_distance(a, b)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 689, "library_problem_id": 23, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 22}}
{"id": 690, "code": ["<code>\n```python\n\"\"\"\nFunction to calculate the element-wise L2 distance between two tensors A and B.\n\nThis function computes the L2 distance (Euclidean distance) between corresponding elements of two input tensors A and B. \nThe L2 distance is calculated as the square root of the sum of the squared differences between corresponding elements.\n\nArguments:\n    A (tf.Tensor): A 2D tensor of shape (m, n) representing the first set of embeddings.\n    B (tf.Tensor): A 2D tensor of shape (m, n) representing the second set of embeddings.\n\nReturns:\n    tf.Tensor: A 1D tensor of shape (m,) where each element is the L2 distance for the corresponding row in A and B.\n\nExample:\n    A = tf.constant([\n      [1, 1, 1],\n      [1, 1, 1]\n    ])\n    B = tf.constant([\n      [0, 0, 0],\n      [1, 1, 1]\n    ])\n    result = f(A, B)\n    # result should be [3, 0]\n\"\"\"\n\nimport tensorflow as tf\n\ndef f(A: tf.Tensor, B: tf.Tensor) -> tf.Tensor:\n    # Calculate the squared differences element-wise\n    squared_diff = tf.square(tf.subtract(A, B))\n    \n    # Sum the squared differences along the last axis (columns) to reduce by row\n    row_sums = tf.reduce_sum(squared_diff\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 690, "library_problem_id": 24, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 691, "code": ["<code>\nimport tensorflow as tf\n\ndef extract_elements(x: list, y: list, z: list) -> tf.Tensor:\n    x = tf.constant(x)\n    y = tf.constant(y)\n    z = tf.constant(z)\n    result = tf.gather_nd(x, tf.stack([y, z], axis=1))\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 691, "library_problem_id": 25, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 692, "code": ["<code>\ndef extract_elements(x: list, row: list, col: list) -> tf.Tensor:\n    \"\"\"\n    Extracts elements from a 2D tensor using specified row and column indices.\n\n    Args:\n    x (list): A 2D list representing the matrix.\n    row (list): A list of row indices.\n    col (list): A list of column indices.\n\n    Returns:\n    tf.Tensor: A 1D tensor containing the extracted elements.\n    \"\"\"\n    x = tf.constant(x)\n    row = tf.constant(row)\n    col = tf.constant(col)\n    # Implement the solution to extract the elements\n    result = tf.gather_nd(x, tf.stack([row, col], axis=1))\n    return result\n\n# Example usage\nx = [[1,2,3],[4,5,6]]\nrow = [0,1]\ncol = [0,2]\nresult = extract_elements(x, row, col)\nprint(result)  # Output: [1,6]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 692, "library_problem_id": 26, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 25}}
{"id": 693, "code": ["<code>\nimport tensorflow as tf\n\ndef f(x: tf.Tensor, y: tf.Tensor, z: tf.Tensor) -> tf.Tensor:\n    # Use tf.gather_nd to select elements based on y and z indices\n    indices = tf.stack([y, z], axis=1)\n    result = tf.gather_nd(x, indices)\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 693, "library_problem_id": 27, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 694, "code": ["<code>\n# Compute tensor_C using vectorized operations\ntensor_C = tf.einsum('ijk,ilk->ijl', tensor_A, tensor_B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 694, "library_problem_id": 28, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 695, "code": ["<code>\nresult = tf.einsum('ijk,ilk->ijl', A, B)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 695, "library_problem_id": 29, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 696, "code": ["<code>\nimport tensorflow as tf\n\nx = [\n    b'\\xd8\\xa8\\xd9\\x85\\xd8\\xb3\\xd8\\xa3\\xd9\\x84\\xd8\\xa9',\n    b'\\xd8\\xa5\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd9\\x82\\xd8\\xb6\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd8\\xac\\xd9\\x86\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a',\n    b'\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a'\n]\n\nresult = tf.strings.unicode_transcode(x, \"UTF-8\", \"UTF-8\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 696, "library_problem_id": 30, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 697, "code": ["<code>\n```python\n\"\"\"\nProblem:\nConvert a list of byte sequences to a list of strings using TensorFlow 2.10.0.\n\nContext:\nYou have a list of byte sequences representing UTF-8 encoded strings. \nThe goal is to decode these byte sequences into human-readable strings using TensorFlow.\n\nExample Input:\nx = [\n    b'\\xd8\\xa8\\xd9\\x85\\xd8\\xb3\\xd8\\xa3\\xd9\\x84\\xd8\\xa9',\n    b'\\xd8\\xa5\\xd9\\x86\\xd8\\xb4\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd9\\x82\\xd8\\xb6\\xd8\\xa7\\xd8\\xa1',\n    b'\\xd8\\xac\\xd9\\x86\\xd8\\xa7\\xd8\\xa6\\xd9\\x8a',\n    b'\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a'\n]\n\nExpected Output:\nA list of decoded strings.\n\nSolution:\nImplement a function using TensorFlow to decode the byte sequences.\n\"\"\"\n\nimport tensorflow as tf\nfrom typing import List\n\ndef decode_bytes_to_strings(byte_list: List[bytes]) -> List[str]:\n    \"\"\"\n    Decodes a list of byte sequences into a list of strings using UTF-8 encoding.\n\n    Args:\n    byte_list (List[bytes]): A list of byte sequences to be\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 697, "library_problem_id": 31, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 698, "code": ["<code>\ndef average_non_zero_features(X: tf.Tensor) -> tf.Tensor:\n    # Compute a mask of non-zero entries\n    non_zero_mask = tf.not_equal(X, 0)\n    \n    # Cast the mask to float for multiplication\n    non_zero_mask_float = tf.cast(non_zero_mask, tf.float32)\n    \n    # Sum the non-zero entries\n    sum_non_zero = tf.reduce_sum(X * non_zero_mask_float, axis=-2)\n    \n    # Count the number of non-zero entries\n    count_non_zero = tf.reduce_sum(non_zero_mask_float, axis=-2)\n    \n    # Avoid division by zero by using tf.maximum\n    count_non_zero = tf.maximum(count_non_zero, 1)\n    \n    # Compute the average of non-zero entries\n    average_non_zero = sum_non_zero / count_non_zero\n    \n    return average_non_zero\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 698, "library_problem_id": 32, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 699, "code": ["<code>\n```python\ndef compute_non_zero_variance(x: tf.Tensor) -> tf.Tensor:\n    # Convert input to tensor if not already\n    x = tf.convert_to_tensor(x, dtype=tf.float32)\n    \n    # Create a mask for non-zero entries\n    non_zero_mask = tf.not_equal(x, 0)\n    \n    # Compute the count of non-zero entries along the feature dimension\n    non_zero_count = tf.reduce_sum(tf.cast(non_zero_mask, tf.float32), axis=-2, keepdims=True)\n    \n    # Compute the mean of non-zero entries\n    sum_non_zero = tf.reduce_sum(x * tf.cast(non_zero_mask, tf.float32), axis=-2, keepdims=True)\n    mean_non_zero = sum_non_zero / tf.maximum(non_zero_count, 1)\n    \n    # Compute the squared differences from the mean\n    squared_diff = tf.square(x - mean_non_zero) * tf.cast(non_zero_mask, tf.float32)\n    \n    # Compute the variance of non-zero entries\n    variance_non_zero = tf.reduce_sum(squared_diff, axis=-2, keepdims=True) / tf.maximum(non_zero_count, 1)\n    \n    # Return the variance, maintaining the original shape\n    return variance_non_zero\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 699, "library_problem_id": 33, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 700, "code": ["<code>\n```python\n\"\"\"\nFunction to compute the average of non-zero entries along the second to last dimension of a tensor.\n\nThis function takes a tensor `x` of shape (batch_size, ..., features) and computes the average of the \nnon-zero entries along the second to last dimension (features). The averaging excludes zero-padded values \nand divides the sum by the number of non-zero entries.\n\nArgs:\n    x (tf.Tensor): A tensor of shape (batch_size, ..., features) with potential zero-padded features.\n\nReturns:\n    tf.Tensor: A tensor of the same shape as `x` with averaged non-zero entries along the second to last dimension.\n\"\"\"\n\nimport tensorflow as tf\n\ndef average_non_zero_features(x: tf.Tensor) -> tf.Tensor:\n    # Calculate the sum of non-zero entries along the second to last dimension\n    non_zero_sum = tf.reduce_sum(x, axis=-2)\n    \n    # Count the number of non-zero entries along the second to last dimension\n    non_zero_count = tf.reduce_sum(tf.cast(x != 0, tf.float32), axis=-2)\n    \n    # Avoid division by zero by replacing zero counts with one\n    non_zero_count = tf.where(non_zero_count == 0, tf.ones_like(non_zero_count), non_zero_count)\n    \n    # Compute the average by dividing the sum by the count of non-zero entries\n    average = non_zero_sum\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 700, "library_problem_id": 34, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 701, "code": ["<code>\n# Import TensorFlow 2.3.0\nimport tensorflow as tf\n\n# Set a random seed for reproducibility\ntf.random.set_seed(10)\n\n# Create two random normal tensors of shape [100, 100]\nA = tf.random.normal([100, 100])\nB = tf.random.normal([100, 100])\n\n# Perform matrix multiplication and reduce sum using eager execution\nresult = tf.reduce_sum(tf.matmul(A, B))\n\n# The result is now available directly without needing a session\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 701, "library_problem_id": 35, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 702, "code": ["<code>\nimport tensorflow as tf\n\ndef get_predicted_class_indices(scores: tf.Tensor) -> tf.Tensor:\n    # Use tf.argmax to find the index of the maximum value in each row\n    predicted_indices = tf.argmax(scores, axis=1, output_type=tf.int64)\n    return predicted_indices\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 702, "library_problem_id": 36, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 703, "code": ["<code>\ndef get_max_indices(scores: tf.Tensor) -> tf.Tensor:\n    \"\"\"\n    Transforms the input tensor by replacing each row with the index of the maximum value in that row.\n\n    Args:\n    scores (tf.Tensor): A 2D tensor of shape [n, m] containing prediction scores.\n\n    Returns:\n    tf.Tensor: A 1D tensor of shape [n] containing the indices of the maximum values for each row.\n    \"\"\"\n    return tf.argmax(scores, axis=1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 703, "library_problem_id": 37, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 704, "code": ["<code>\n```python\nimport tensorflow as tf\n\ndef get_prediction_indices(scores: tf.Tensor) -> tf.Tensor:\n    \"\"\"\n    Converts a tensor of prediction scores into a tensor of indices representing\n    the class with the highest score for each sample.\n\n    Args:\n        scores (tf.Tensor): A 2D tensor of shape [n_samples, n_classes] containing\n                            prediction scores for each class.\n\n    Returns:\n        tf.Tensor: A 1D tensor of shape [n_samples] containing the indices of the\n                   highest score for each sample.\n\n    Example:\n        Given a tensor:\n        tf.Tensor(\n            [[0.3232, -0.2321, 0.2332, -0.1231, 0.2435, 0.6728],\n             [0.2323, -0.1231, -0.5321, -0.1452, 0.5435, 0.1722],\n             [0.9823, -0.1321, -0.6433, 0.1231, 0.023, 0.0711]]\n        )\n\n        The function will return:\n        tf.Tensor([5, 4, 0])\n    \"\"\"\n    # Use tf.argmax to find the index of the maximum value in each row\n    prediction_indices = tf.argmax(scores, axis=\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 704, "library_problem_id": 38, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 705, "code": ["<code>\nimport tensorflow as tf\n\ndef find_min_indices_per_column(tensor: tf.Tensor) -> tf.Tensor:\n    return tf.argmin(tensor, axis=0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 705, "library_problem_id": 39, "library": "Tensorflow", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 36}}
{"id": 706, "code": ["<code>\n```python\n\"\"\"\nRefactored Python function prompt for saving a TensorFlow model using the SavedModel format.\n\nProblem:\nThe user is attempting to save an Artificial Neural Network (ANN) model using TensorFlow's SavedModel format. \nHowever, the model is being saved as an HDF5 file instead. The user is using TensorFlow version 2.10.0 and Keras version 2.3.1.\n\nObjective:\nEnsure the model is saved in the SavedModel format, which should create a directory containing 'saved_model.pb', \n'variables', and 'assets'.\n\nCode Requirements:\n- Use TensorFlow's `model.save()` method without specifying a file extension to save in the SavedModel format.\n- Ensure compatibility with TensorFlow 2.x and Keras.\n\nFunctionality:\n1. Define a Sequential model with specified input and output dimensions.\n2. Add layers to the model with specified activation functions and initializers.\n3. Compile the model with a specified optimizer and loss function.\n4. Train the model with provided training and validation data.\n5. Save the model in the SavedModel format.\n\nParameters:\n- Xtrain: Training data features.\n- ytrain: Training data labels.\n- Xtest: Validation data features.\n- ytest: Validation data labels.\n- inputdim: Integer, the number of input features.\n- outputdim: Integer, the number of output classes.\n- activation: String, activation function for hidden layers.\n- opt: String, optimizer for compiling the model.\n- epochs: Integer, number of epochs for\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 706, "library_problem_id": 40, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 40}}
{"id": 707, "code": ["<code>\nimport tensorflow as tf\n\ndef generate_random_tensor(seed: int = 10) -> tf.Tensor:\n    \"\"\"\n    Generates a tensor of 10 random integers uniformly distributed over the set {1, 2, 3, 4}.\n\n    Args:\n        seed (int): The seed value for random number generation to ensure reproducibility.\n\n    Returns:\n        tf.Tensor: A tensor of shape (10,) containing random integers from the set {1, 2, 3, 4}.\n    \"\"\"\n    # Set the random seed for reproducibility\n    tf.random.set_seed(seed)\n    \n    # Generate the random tensor\n    result = tf.random.uniform(shape=(10,), minval=1, maxval=5, dtype=tf.int32)\n    \n    return result\n\n# Example usage\nresult = generate_random_tensor()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 707, "library_problem_id": 41, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 708, "code": ["<code>\nimport tensorflow as tf\n\ndef generate_random_tensor(seed_x: int) -> tf.Tensor:\n    # Set the random seed for reproducibility\n    tf.random.set_seed(seed_x)\n    \n    # Define the possible values\n    values = [2, 3, 4, 5]\n    \n    # Generate a tensor of random integers\n    result = tf.random.uniform(\n        shape=(114,), \n        minval=0, \n        maxval=len(values), \n        dtype=tf.int32\n    )\n    \n    # Map the generated indices to the desired values\n    result = tf.gather(values, result)\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 708, "library_problem_id": 42, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 709, "code": ["<code>\n\"\"\"\nFunction to generate a tensor of random integers using TensorFlow.\n\nThis function generates a tensor containing 10 random integers sampled from a uniform distribution over the set {1, 2, 3, 4}. The random seed is set to ensure reproducibility.\n\nArgs:\n    seed_x (int): The seed value for the random number generator. Default is 10.\n\nReturns:\n    tf.Tensor: A tensor of shape (10,) containing random integers from the set {1, 2, 3, 4}.\n\"\"\"\n\nimport tensorflow as tf\n\ndef generate_random_tensor(seed_x: int = 10) -> tf.Tensor:\n    # Set the random seed for reproducibility\n    tf.random.set_seed(seed_x)\n    \n    # Generate a tensor of 10 random integers from the set {1, 2, 3, 4}\n    random_tensor = tf.random.uniform(shape=(10,), minval=1, maxval=5, dtype=tf.int32)\n    \n    return random_tensor\n\n# Example usage:\n# result = generate_random_tensor()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 709, "library_problem_id": 43, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 710, "code": ["<code>\nimport tensorflow as tf\n\n# Retrieve the version of TensorFlow and store it in the variable 'result'\nresult = tf.__version__\n\n# Return the version of TensorFlow\nreturn result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 710, "library_problem_id": 44, "library": "Tensorflow", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 711, "code": ["<code>\nimport numpy as np\n\ndef fit_logarithmic_model(x, y):\n    # Ensure that all elements of x are positive\n    if np.any(x <= 0):\n        raise ValueError(\"All elements of x must be positive for the logarithmic model.\")\n    \n    # Transform the data to fit the logarithmic model\n    log_x = np.log(x)\n    \n    # Use numpy's polyfit function to find the coefficients A and B\n    coefficients = np.polyfit(log_x, y, 1)\n    \n    return coefficients\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 711, "library_problem_id": 0, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 712, "code": ["<code>\nimport numpy as np\n\ndef fit_logarithmic_model(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Fits a logarithmic model of the form y = A + B * log(x) to the given data.\n\n    Args:\n        x (np.ndarray): A numpy array of independent variable data points.\n        y (np.ndarray): A numpy array of dependent variable data points.\n\n    Returns:\n        np.ndarray: A numpy array containing the coefficients [A, B] of the fitted model.\n    \"\"\"\n    # Transform x to log(x)\n    log_x = np.log(x)\n    \n    # Use numpy.polyfit to fit the transformed data\n    coefficients = np.polyfit(log_x, y, 1)\n    \n    return coefficients\n\n# Example usage\nx = np.array([1, 7, 20, 50, 79])\ny = np.array([10, 19, 30, 35, 51])\nresult = fit_logarithmic_model(x, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 712, "library_problem_id": 1, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 713, "code": ["<code>\nimport numpy as np\nfrom scipy.optimize import curve_fit\nfrom typing import Tuple\n\ndef fit_exponential(x: np.ndarray, y: np.ndarray, p0: Tuple[float, float, float]) -> np.ndarray:\n    \"\"\"\n    Fits an exponential model of the form y = A * exp(B * x) + C to the given data.\n\n    Parameters:\n    x (np.ndarray): The independent variable data points.\n    y (np.ndarray): The dependent variable data points.\n    p0 (Tuple[float, float, float]): Initial guess for the parameters A, B, and C.\n\n    Returns:\n    np.ndarray: An array containing the optimized parameters [A, B, C].\n    \"\"\"\n    # Define the exponential function to fit\n    def exponential_model(x: np.ndarray, A: float, B: float, C: float) -> np.ndarray:\n        return A * np.exp(B * x) + C\n\n    # Use curve_fit to find the best fit parameters\n    params, _ = curve_fit(exponential_model, x, y, p0=p0)\n\n    return params\n\n# Example usage\ny = np.array([1, 7, 20, 50, 79])\nx = np.array([10, 19, 30, 35, 51])\np0 = (4, 0.1, 1)\nresult = fit_exponential(x, y, p0)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 713, "library_problem_id": 2, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 714, "code": ["<code>\nfrom scipy import stats\nimport numpy as np\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate two random samples\nx = np.random.normal(0, 1, 1000)\ny = np.random.normal(1.1, 0.9, 1000)\n\n# Perform two-sample KS test\nstatistic, p_value = stats.ks_2samp(x, y)\n\n# Output the results\nprint(f\"KS Statistic: {statistic}, p-value: {p_value}\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 714, "library_problem_id": 3, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 3}}
{"id": 715, "code": ["<code>\n```python\n\"\"\"\nProblem:\nPerform a two-sample Kolmogorov-Smirnov (KS) test using SciPy to determine if two distributions are identical.\n\nObjective:\nGiven two datasets, perform a two-sample KS test to test the null hypothesis that the two distributions are identical. Determine if the null hypothesis can be rejected based on a specified significance level (alpha).\n\nInstructions:\n1. Import necessary libraries.\n2. Generate two datasets using a normal distribution.\n3. Perform a two-sample KS test using SciPy.\n4. Compare the p-value from the test with the significance level (alpha) to determine if the null hypothesis can be rejected.\n5. Return a boolean indicating whether the null hypothesis can be rejected (True) or not (False).\n\nParameters:\n- alpha (float): Significance level for the hypothesis test.\n\nReturns:\n- result (bool): True if the null hypothesis can be rejected, False otherwise.\n\nExample:\nGiven two datasets x and y, and alpha = 0.01, determine if the distributions are identical.\n\"\"\"\n\nfrom scipy import stats\nimport numpy as np\n\nnp.random.seed(42)\n\n# Generate two datasets\nx = np.random.normal(0, 1, 1000)\ny = np.random.normal(0, 1, 1000)\n\n# Significance level\nalpha = 0.01\n\n# Perform two-sample KS test\ntest_stat, p_value = stats.ks_2samp(x, y)\n\n# Determine if null hypothesis can be rejected\nresult = p_\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 715, "library_problem_id": 4, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 3}}
{"id": 716, "code": ["<code>\ndef minimize_function(initial_guess):\n    def f(variables):\n        a, b, c = variables\n        return ((a + b - c) - 2)**2 + ((3*a - b - c))**2 + sin(b) + cos(b) + 4\n\n    result = optimize.minimize(f, initial_guess, method='BFGS')\n    return result.x.tolist()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 716, "library_problem_id": 5, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 5}}
{"id": 717, "code": ["<code>\n```python\n\"\"\"\nFunction to Convert Z-scores to Left-tailed P-values\n\nThis function takes a list or array of Z-scores from the standard normal distribution\nand converts them to left-tailed p-values using the cumulative distribution function (CDF).\n\nArgs:\n    z_scores (np.ndarray): A NumPy array of Z-scores. These scores represent the number\n                           of standard deviations a data point is from the mean of a \n                           standard normal distribution.\n\nReturns:\n    np.ndarray: A NumPy array of left-tailed p-values corresponding to the input Z-scores.\n                The p-value represents the probability that a standard normal random\n                variable will be less than or equal to the given Z-score.\n\nMathematical/Logical Conditions:\n    - The Z-score is a measure of how many standard deviations an element is from the mean.\n    - The left-tailed p-value is calculated using the cumulative distribution function (CDF)\n      of the standard normal distribution, which gives the probability that a random variable\n      is less than or equal to a given value.\n\nExample:\n    z_scores = np.array([-3, -2, 0, 2, 2.5])\n    p_values = convert_z_to_p(z_scores)\n    # p_values will be an array of left-tailed p-values for each Z-score\n\n\"\"\"\nimport numpy as np\nimport scipy.stats\n\ndef convert_z_to_p(z_scores: np.ndarray) -> np.ndarray:\n    return sc\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 717, "library_problem_id": 6, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 718, "code": ["<code>\nimport scipy.stats\nimport numpy as np\n\ndef convert_z_scores_to_p_values(z_scores: list[float]) -> list[float]:\n    \"\"\"\n    Convert a list of Z-scores to left-tailed p-values using the standard normal distribution.\n\n    Args:\n    z_scores (list of float): A list of Z-scores from the standard normal distribution.\n\n    Returns:\n    list of float: A list of left-tailed p-values corresponding to the input Z-scores.\n    \"\"\"\n    # Calculate the left-tailed p-values using the cumulative distribution function (CDF)\n    p_values = scipy.stats.norm.cdf(z_scores)\n    return p_values\n\n# Example usage\nz_scores = [-3, -2, 0, 2, 2.5]\np_values = convert_z_scores_to_p_values(z_scores)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 718, "library_problem_id": 7, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 719, "code": ["<code>\nimport scipy.stats\n\ndef convert_p_values_to_z_scores(p_values: list[float]) -> list[float]:\n    \"\"\"\n    Convert a list of left-tailed p-values to their corresponding z-scores from the standard normal distribution.\n\n    Args:\n    p_values (list of float): A list of p-values, where each value is a probability from the left tail of the standard normal distribution.\n\n    Returns:\n    list of float: A list of z-scores corresponding to the input p-values.\n\n    Mathematical/Logical Conditions:\n    - The conversion is based on the inverse cumulative distribution function (CDF) of the standard normal distribution.\n    - For a given p-value, the z-score is determined by finding the point on the Z-distribution where the cumulative probability equals the p-value.\n    \"\"\"\n    return scipy.stats.norm.ppf(p_values)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 719, "library_problem_id": 8, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 6}}
{"id": 720, "code": ["<code>\ndef compute_lognormal_cdf(mu: float, sigma: float, x: float) -> float:\n    \"\"\"\n    Calculate the cumulative distribution function (CDF) for a lognormal distribution.\n\n    Parameters:\n    - mu (float): The mean of the underlying normal distribution.\n    - sigma (float): The standard deviation of the underlying normal distribution.\n    - x (float): The point at which to evaluate the CDF.\n\n    Returns:\n    - float: The CDF value for the lognormal distribution at point x, ranging from 0 to 1.\n\n    The lognormal distribution is defined such that if Y is a normal random variable with mean `mu` and standard deviation `sigma`, then exp(Y) follows a lognormal distribution.\n    \"\"\"\n    import numpy as np\n    from scipy.stats import lognorm\n\n    # Calculate the shape parameter for the lognormal distribution\n    shape = sigma\n\n    # Calculate the scale parameter for the lognormal distribution\n    scale = np.exp(mu)\n\n    # Compute the CDF of the lognormal distribution at point x\n    cdf_value = lognorm.cdf(x, s=shape, scale=scale)\n\n    return cdf_value\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 720, "library_problem_id": 9, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 721, "code": ["<code>\nfrom scipy.stats import lognorm\nimport numpy as np\n\ndef calculate_lognormal_stats(mu, stddev):\n    # Create a lognormal distribution object\n    dist = lognorm(s=stddev, scale=np.exp(mu))\n\n    # Calculate expected value and median\n    expected_value = dist.mean()\n    median = dist.median()\n\n    return expected_value, median\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 721, "library_problem_id": 10, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 722, "code": ["<code>\nfrom scipy import sparse\nimport numpy as np\n\n# Convert the dense matrix to a sparse CSR matrix\nsa = sparse.csr_matrix(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n\n# Convert the vector to a sparse CSR matrix with appropriate dimensions\nsb = sparse.csr_matrix(np.array([0, 1, 2])).transpose()\n\n# Perform element-wise multiplication\nresult = sa.multiply(sb)\n\n# The result is a sparse CSR matrix with the same element-wise multiplication as the dense operation\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 722, "library_problem_id": 11, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 723, "code": ["<code>\nfrom scipy import sparse\nimport numpy as np\n\ndef f(sA: sparse.csr_matrix, sB: np.ndarray) -> sparse.csr_matrix:\n    # Ensure the dimensions are compatible for element-wise multiplication\n    assert sA.shape[1] == sB.shape[0], \"Dimension mismatch: sA columns must match sB size.\"\n    \n    # Perform element-wise multiplication by broadcasting sB across the rows of sA\n    result = sA.multiply(sB)\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 723, "library_problem_id": 12, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 724, "code": ["<code>\n```python\nimport numpy as np\nimport scipy.interpolate\n\ndef interpolate_moisture_value(data: list, target_point: tuple) -> float:\n    \"\"\"\n    Interpolates the moisture value V at a given target point (x, y, z) using\n    the provided irregular grid data.\n\n    Args:\n    - data (list): A list of lists, where each sublist contains four elements [x, y, z, V].\n    - target_point (tuple): A tuple (x, y, z) representing the point at which to interpolate V.\n\n    Returns:\n    - float: The interpolated moisture value V at the target point.\n    \"\"\"\n    # Extract spatial coordinates and moisture values from the data\n    points = np.array([point[:3] for point in data])\n    V = np.array([point[3] for point in data])\n    \n    # Define the target point for interpolation\n    request = np.array([target_point])\n    \n    # Perform interpolation using LinearNDInterpolator\n    interpolator = scipy.interpolate.LinearNDInterpolator(points, V)\n    interpolated_value = interpolator(request)\n    \n    # Return the interpolated value\n    return interpolated_value[0]\n\n# Sample data\ndata = [\n    [27.827, 18.530, -30.417, 0.205],\n    [24.002, 17.759, -24.782, 0.197],\n    [2\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 724, "library_problem_id": 13, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 13}}
{"id": 725, "code": ["<code>\ndef interpolate_moisture(data, targets):\n    # Extract coordinates and moisture values from the data\n    points = np.array([d[:3] for d in data])  # Extract (x, y, z) coordinates\n    V = np.array([d[3] for d in data])        # Extract moisture values\n\n    # Convert target points to a numpy array\n    request = np.array(targets)\n\n    # Create a LinearNDInterpolator for scattered data interpolation\n    interpolator = scipy.interpolate.LinearNDInterpolator(points, V)\n\n    # Interpolate moisture values at the target points\n    result = interpolator(request)\n\n    return result.tolist()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 725, "library_problem_id": 14, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 13}}
{"id": 726, "code": ["<code>\ndef calculate_rotated_coordinates(data_orig: np.ndarray, x0: int, y0: int, angle: float) -> tuple:\n    # Calculate the center of the image\n    center_x, center_y = np.array(data_orig.shape[1::-1]) / 2\n\n    # Translate the point to the origin\n    translated_x = x0 - center_x\n    translated_y = y0 - center_y\n\n    # Convert angle from degrees to radians\n    angle_rad = np.deg2rad(angle)\n\n    # Apply the rotation matrix\n    cos_angle = np.cos(angle_rad)\n    sin_angle = np.sin(angle_rad)\n    x_rotated = translated_x * cos_angle - translated_y * sin_angle\n    y_rotated = translated_x * sin_angle + translated_y * cos_angle\n\n    # Translate the point back to the image center\n    x_rot = x_rotated + center_x\n    y_rot = y_rotated + center_y\n\n    return x_rot, y_rot\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 726, "library_problem_id": 15, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 15}}
{"id": 727, "code": ["<code>\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\ndef extract_main_diagonal(sparse_matrix: csr_matrix) -> np.ndarray:\n    if sparse_matrix.shape[0] != sparse_matrix.shape[1]:\n        raise ValueError(\"Input matrix must be square.\")\n    \n    return sparse_matrix.diagonal()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 727, "library_problem_id": 16, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 728, "code": ["<code>\ndef poisson_simul(rate: float, T: float) -> list[float]:\n    \"\"\"\n    Simulate event times in the range [0, T] using a Poisson process.\n\n    Args:\n        rate (float): The rate parameter (lambda) of the Poisson process.\n        T (float): The upper bound of the time range for simulation.\n\n    Returns:\n        list[float]: A list of event times simulated according to the Poisson process.\n    \"\"\"\n    time = random.expovariate(rate)\n    times = [0]\n    while times[-1] < T:\n        times.append(time + times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 728, "library_problem_id": 17, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 729, "code": ["<code>\ndef poisson_simul(rate: float, T: float) -> list[float]:\n    \"\"\"\n    Simulate event times according to a Poisson process.\n\n    Args:\n        rate (float): The rate parameter (lambda) of the Poisson process.\n        T (float): The maximum time up to which events are simulated.\n\n    Returns:\n        list[float]: A list of event times simulated according to the Poisson process.\n    \"\"\"\n    time = random.expovariate(rate)\n    times = [0]\n    while times[-1] < T:\n        times.append(time + times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\n\ndef test_uniformity(times: list[float], T: float) -> tuple:\n    \"\"\"\n    Test if the given times are uniformly distributed over the interval [0, T].\n\n    Args:\n        times (list[float]): A list of event times to be tested.\n        T (float): The maximum time up to which events are simulated.\n\n    Returns:\n        tuple: The result of the Kolmogorov-Smirnov test, containing the KS statistic\n               and the p-value.\n    \"\"\"\n    # Normalize times to the range [0, 1] for the uniform distribution test\n    normalized_times = np.array(times) / T\n    # Perform the Kolmogorov-Smirnov test for uniformity\n    return stats.kstest(normalized_times, 'uniform')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 729, "library_problem_id": 18, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 730, "code": ["<code>\n```python\ndef poisson_simul(rate: float, T: float) -> list[float]:\n    \"\"\"\n    Simulate event times using a Poisson process.\n\n    Args:\n        rate (float): The rate parameter of the Poisson process.\n        T (float): The upper bound of the time interval [0, T].\n\n    Returns:\n        list[float]: A list of event times simulated from the Poisson process.\n    \"\"\"\n    time = random.expovariate(rate)\n    times = [0]\n    while times[-1] < T:\n        times.append(time + times[-1])\n        time = random.expovariate(rate)\n    return times[1:]\n\ndef test_uniformity(times: list[float], T: float, alpha: float) -> bool:\n    \"\"\"\n    Test if the event times are uniformly distributed over [0, T].\n\n    Args:\n        times (list[float]): The list of event times to test.\n        T (float): The upper bound of the time interval [0, T].\n        alpha (float): The significance level for the test (e.g., 0.05 for 95% confidence).\n\n    Returns:\n        bool: True if the null hypothesis of uniform distribution is not rejected,\n              False otherwise.\n    \"\"\"\n    # Normalize times to the range [0, 1] for the uniform test\n    normalized_times = np.array(times) / T\n    # Perform the Kolmogorov-Smirnov test\n    statistic, p_value = kstest(normalized_times, 'uniform')\n    # Interpret the\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 730, "library_problem_id": 19, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 17}}
{"id": 731, "code": ["<code>\nfrom scipy import sparse\n\ndef concatenate_csr_matrices(c1: sparse.csr_matrix, c2: sparse.csr_matrix) -> sparse.csr_matrix:\n    # Ensure the number of rows in both matrices are equal\n    assert c1.shape[0] == c2.shape[0], \"Matrices must have the same number of rows for horizontal concatenation.\"\n    \n    # Perform horizontal concatenation using scipy's hstack function\n    Feature = sparse.hstack([c1, c2], format='csr')\n    \n    return Feature\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 731, "library_problem_id": 20, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 732, "code": ["<code>\nfrom scipy import sparse\n\ndef horizontal_stack_sparse_matrices(c1: sparse.csr_matrix, c2: sparse.csr_matrix) -> sparse.csr_matrix:\n    # Check if the number of rows in both matrices are equal\n    if c1.shape[0] != c2.shape[0]:\n        raise ValueError(\"The number of rows in c1 and c2 must be the same to concatenate them horizontally.\")\n    \n    # Use scipy's hstack to concatenate the matrices horizontally\n    Feature = sparse.hstack([c1, c2], format='csr')\n    \n    return Feature\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 732, "library_problem_id": 21, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 733, "code": ["<code>\ndef concatenate_csr_matrices(c1: sparse.csr_matrix, c2: sparse.csr_matrix) -> sparse.csr_matrix:\n    \"\"\"\n    Concatenates two CSR matrices vertically.\n\n    Args:\n        c1 (sparse.csr_matrix): The first CSR matrix.\n        c2 (sparse.csr_matrix): The second CSR matrix.\n\n    Returns:\n        sparse.csr_matrix: A new CSR matrix resulting from the vertical concatenation of `c1` and `c2`.\n    \"\"\"\n    return sparse.vstack([c1, c2])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 733, "library_problem_id": 22, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 20}}
{"id": 734, "code": ["<code>\n# Calculate pairwise distances between the two sets of points\ndistances = scipy.spatial.distance.cdist(points1, points2)\n\n# Use scipy.optimize.linear_sum_assignment to find the optimal assignment\nrow_ind, col_ind = scipy.optimize.linear_sum_assignment(distances)\n\n# Store the result\nresult = col_ind\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 734, "library_problem_id": 23, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 735, "code": ["<code>\n# Convert the points to a format that can be used by scipy.optimize.linear_sum_assignment\ndistances = scipy.spatial.distance.cdist(points1, points2, metric='cityblock')\n\n# Use the linear sum assignment function to find the optimal mapping\n_, result = scipy.optimize.linear_sum_assignment(distances)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 735, "library_problem_id": 24, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 23}}
{"id": 736, "code": ["<code>\nfrom scipy import sparse\nimport numpy as np\n\n# Step 1: Create a 2x2 matrix of ones\na = np.ones((2, 2))\n\n# Step 2: Convert the dense matrix to a CSR sparse matrix\nb = sparse.csr_matrix(a)\n\n# Step 3: Remove diagonal elements from the CSR matrix\n# Explanation: Set diagonal elements to zero and eliminate zero entries\nb.setdiag(0)\nb.eliminate_zeros()\n\n# The variable 'b' now contains the CSR matrix with diagonal elements removed from storage\nb\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 736, "library_problem_id": 25, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 737, "code": ["<code>\ndef count_regions_above_threshold(array: np.ndarray, threshold: float) -> int:\n    \"\"\"\n    Counts the number of contiguous regions in a 2D numpy array where the cell values exceed a given threshold.\n\n    Parameters:\n    - array (np.ndarray): A 2D numpy array of shape (512, 512) with float values ranging from 0.0 to 1.0.\n    - threshold (float): The threshold value to determine which cells are part of a region.\n\n    Returns:\n    - int: The number of contiguous regions where cell values exceed the threshold.\n    \"\"\"\n    # Create a binary mask where True indicates values above the threshold\n    binary_mask = array > threshold\n\n    # Label connected components in the binary mask\n    labeled_array, num_features = ndimage.label(binary_mask, structure=np.ones((3, 3)))\n\n    return num_features\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 737, "library_problem_id": 26, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 738, "code": ["<code>\n```python\ndef count_regions_below_threshold(array: np.ndarray, threshold: float) -> int:\n    \"\"\"\n    Count the number of contiguous regions in a 2D numpy array where values are below a given threshold.\n\n    Parameters:\n    - array (np.ndarray): A 2D numpy array of shape (512, 512) with float values ranging from 0.0 to 1.0.\n    - threshold (float): A float value representing the threshold. Regions are counted where array values are below this threshold.\n\n    Returns:\n    - int: The number of contiguous regions where the array values are below the threshold.\n    \"\"\"\n    # Initialize a label array\n    label_array = np.zeros_like(array, dtype=np.int)\n\n    # Initialize label counter\n    label = 1\n\n    # Iterate over the array\n    for x in range(array.shape[0]):\n        for y in range(array.shape[1]):\n            # If the value is below the threshold and the label hasn't been assigned yet\n            if array[x, y] < threshold and label_array[x, y] == 0:\n                # Perform a depth-first search to label the connected components\n                label_array = dfs(array, x, y, label, label_array)\n                # Increment the label counter\n                label += 1\n\n    # Return the number of labels (i.e., the number of regions)\n    return label - 1\n\n\ndef dfs(array: np.ndarray, x: int, y: int, label:\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 738, "library_problem_id": 27, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 26}}
{"id": 739, "code": ["<code>\nimport numpy as np\nfrom scipy import ndimage\n\ndef count_regions_exceeding_threshold(img: np.ndarray) -> int:\n    threshold = 0.75\n    # Create a binary mask where True indicates values exceeding the threshold\n    binary_mask = img > threshold\n    \n    # Label connected components in the binary mask\n    labeled_array, num_features = ndimage.label(binary_mask)\n    \n    return num_features\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 739, "library_problem_id": 28, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 740, "code": ["<code>\nimport numpy as np\nfrom scipy import ndimage\n\ndef find_region_distances(array: np.ndarray, threshold: float) -> list:\n    \"\"\"\n    Identifies contiguous regions in a 2D numpy array where values exceed a given threshold and calculates\n    the Euclidean distance from the center of mass of each region to the origin (0, 0).\n\n    Args:\n        array (np.ndarray): A 2D numpy array of shape (512, 512) with float values ranging from 0.0 to 1.0.\n        threshold (float): The threshold value to identify regions. Values greater than this are considered.\n\n    Returns:\n        list: A list of Euclidean distances from the center of mass of each identified region to the origin (0, 0).\n    \"\"\"\n    # Create a binary mask where values exceed the threshold\n    mask = array > threshold\n\n    # Label contiguous regions in the mask\n    labeled_array, num_features = ndimage.label(mask)\n\n    # Calculate the center of mass for each region\n    centers_of_mass = ndimage.center_of_mass(array, labeled_array, range(1, num_features + 1))\n\n    # Calculate distances from each center of mass to the origin (0, 0)\n    distances = [np.sqrt(x**2 + y**2) for y, x in centers_of_mass]\n\n    return distances\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 740, "library_problem_id": 29, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 26}}
{"id": 741, "code": ["<code>\nimport numpy as np\nfrom scipy.sparse import lil_matrix, csr_matrix, spmatrix\n\ndef make_symmetric(sparse_matrix: spmatrix) -> spmatrix:\n    \"\"\"\n    Convert a given sparse SciPy matrix into a symmetric matrix.\n\n    Args:\n        sparse_matrix (spmatrix): A SciPy sparse matrix (e.g., `lil_matrix`, `csr_matrix`).\n\n    Returns:\n        spmatrix: A symmetric version of the input sparse matrix.\n    \"\"\"\n    csr_matrix = sparse_matrix.tocsr()\n    symmetric_matrix = csr_matrix + csr_matrix.transpose()\n    return symmetric_matrix.asformat(sparse_matrix.getformat())\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 741, "library_problem_id": 30, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 30}}
{"id": 742, "code": ["<code>\ndef make_symmetric(sA: lil_matrix) -> lil_matrix:\n    for i, row in enumerate(sA.rows):\n        for j_index, j in enumerate(row):\n            sA[j, i] = sA[i, j_index]\n    return sA\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 742, "library_problem_id": 31, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 30}}
{"id": 743, "code": ["<code>\ndef remove_isolated_cells(array: np.ndarray) -> np.ndarray:\n    import numpy as np\n    import scipy.ndimage\n\n    # Define the structure for connectivity (4-connectivity)\n    structure = np.array([[0, 1, 0],\n                          [1, 1, 1],\n                          [0, 1, 0]])\n\n    # Label connected components\n    labeled_array, num_features = scipy.ndimage.label(array, structure=structure)\n\n    # Find sizes of connected components\n    component_sizes = np.bincount(labeled_array.ravel())\n\n    # Create a mask for components of size 1\n    size_one_mask = component_sizes == 1\n\n    # Remove isolated cells by setting them to 0\n    array[size_one_mask[labeled_array]] = 0\n\n    return array\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 743, "library_problem_id": 32, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 744, "code": ["<code>\nimport numpy as np\nimport scipy.ndimage\n\ndef remove_isolated_cells(array: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Remove isolated nonzero cells from a 2D numpy array. A cell is considered isolated if it is nonzero and\n    completely surrounded by zeros in its 8-connected neighborhood.\n\n    Args:\n        array (np.ndarray): A 2D numpy array representing the image.\n\n    Returns:\n        np.ndarray: A new 2D numpy array with isolated cells set to zero.\n    \"\"\"\n    # Define the structure for 8-connectivity\n    structure = np.ones((3, 3), dtype=int)\n    \n    # Identify isolated cells\n    labeled_array, num_features = scipy.ndimage.label(array, structure=structure)\n    sizes = scipy.ndimage.sum(array > 0, labeled_array, range(num_features + 1))\n    \n    # Create a mask for isolated cells\n    mask_size_one = sizes == 1\n    remove_pixel_mask = mask_size_one[labeled_array]\n    \n    # Set isolated cells to zero\n    array[remove_pixel_mask] = 0\n    \n    return array\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 744, "library_problem_id": 33, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 32}}
{"id": 745, "code": ["<code>\n# Calculate the mean and standard deviation of the sparse vector\nmean = col.sum() / col.nnz\nstandard_deviation = np.sqrt(col.power(2).sum() / col.nnz - mean**2)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 745, "library_problem_id": 34, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 34}}
{"id": 746, "code": ["<code>\ndef find_sparse_vector_extremes(col: csr_matrix) -> tuple:\n    \"\"\"\n    Calculate the maximum and minimum values of a sparse vector stored in a csr_matrix column.\n\n    Args:\n    col (csr_matrix): A sparse matrix column vector of shape (988, 1).\n\n    Returns:\n    tuple: A tuple containing the maximum and minimum values of the sparse vector.\n    \"\"\"\n    # Initialize max and min values as None\n    max_val = None\n    min_val = None\n\n    # Iterate over the non-zero elements of the column\n    for i, j, v in zip(col.row, col.col, col.data):\n        # If max_val and min_val are None, set them to the first non-zero value\n        if max_val is None and min_val is None:\n            max_val = v\n            min_val = v\n        # If the current value is greater than max_val, update max_val\n        elif v > max_val:\n            max_val = v\n        # If the current value is less than min_val, update min_val\n        elif v < min_val:\n            min_val = v\n\n    return max_val, min_val\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 746, "library_problem_id": 35, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 34}}
{"id": 747, "code": ["<code>\nimport numpy as np\nfrom scipy.sparse import csr_matrix\nfrom scipy import stats\n\ndef calculate_median_mode(sparse_vector: csr_matrix) -> tuple:\n    non_zero_elements = sparse_vector.data\n    non_zero_indices = sparse_vector.indices\n\n    sorted_non_zero_elements = np.sort(non_zero_elements)\n    median_index = len(sorted_non_zero_elements) // 2\n    median = sorted_non_zero_elements[median_index] if len(sorted_non_zero_elements) % 2 != 0 else \\\n        (sorted_non_zero_elements[median_index - 1] + sorted_non_zero_elements[median_index]) / 2\n\n    mode_result = stats.mode(non_zero_elements)\n    mode = mode_result.mode[0]\n\n    return median, mode\n\nnp.random.seed(10)\narr = np.random.randint(4, size=(988, 988))\nsA = csr_matrix(arr)\ncol = sA.getcol(0)\n\nMedian, Mode = calculate_median_mode(col)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 747, "library_problem_id": 36, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 34}}
{"id": 748, "code": ["<code>\n```python\ndef fit_fourier_series(x: np.ndarray, y: np.ndarray, num_harmonics: int, tau: float) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Fit a Fourier series to the given data using the specified number of harmonics.\n\n    Args:\n        x (np.ndarray): The independent variable data points.\n        y (np.ndarray): The dependent variable data points.\n        num_harmonics (int): The number of harmonics to include in the Fourier series.\n        tau (float): The period of the Fourier series.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: The optimized parameters of the Fourier series and the covariance of the parameters.\n    \"\"\"\n    def fourier_series(x, *coefficients):\n        \"\"\"\n        Compute the Fourier series for the given coefficients and input data.\n\n        Args:\n            x (np.ndarray): The independent variable data points.\n            *coefficients: The coefficients for the Fourier series terms.\n\n        Returns:\n            np.ndarray: The computed Fourier series values.\n        \"\"\"\n        result = np.zeros_like(x)\n        for n in range(1, num_harmonics + 1):\n            result += coefficients[n - 1] * np.cos(n * np.pi / tau * x)\n        return result\n\n    # Initial guess for the coefficients (all set to 1.0)\n    initial_guess = np.ones(num_harmonics)\n\n    # Fit the Fourier series to the data\n    p\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 748, "library_problem_id": 37, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 749, "code": ["<code>\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef calculate_pairwise_distances(raster_array: np.ndarray, resolution: float) -> np.ndarray:\n    unique_ids = np.unique(raster_array[raster_array > 0])\n    num_regions = len(unique_ids)\n    distance_matrix = np.zeros((num_regions, num_regions))\n\n    # Create a dictionary to store coordinates of each region\n    region_coords = {region_id: np.argwhere(raster_array == region_id) for region_id in unique_ids}\n\n    for i, region_id1 in enumerate(unique_ids):\n        for j, region_id2 in enumerate(unique_ids):\n            if i < j:\n                coords1 = region_coords[region_id1]\n                coords2 = region_coords[region_id2]\n                # Calculate pairwise distances between all points in the two regions\n                distances = cdist(coords1, coords2, metric='euclidean')\n                # Find the minimum distance and convert it to real-world units\n                min_distance = np.min(distances) * resolution\n                distance_matrix[i, j] = min_distance\n                distance_matrix[j, i] = min_distance\n\n    return distance_matrix\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 749, "library_problem_id": 38, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 38}}
{"id": 750, "code": ["<code>\ndef calculate_pairwise_manhattan_distances(raster_array: np.ndarray) -> np.ndarray:\n    unique_ids = np.unique(raster_array)\n    unique_ids = unique_ids[unique_ids != 0]  # Exclude background ID (0)\n    num_regions = len(unique_ids)\n    distance_matrix = np.zeros((num_regions, num_regions))\n\n    # Extract coordinates for each region\n    region_coordinates = {region_id: np.argwhere(raster_array == region_id) for region_id in unique_ids}\n\n    # Calculate pairwise distances\n    for i, region_id_1 in enumerate(unique_ids):\n        for j, region_id_2 in enumerate(unique_ids):\n            if i < j:\n                coords_1 = region_coordinates[region_id_1]\n                coords_2 = region_coordinates[region_id_2]\n                distances = cdist(coords_1, coords_2, metric='cityblock')\n                min_distance = np.min(distances)\n                distance_matrix[i, j] = min_distance\n                distance_matrix[j, i] = min_distance\n\n    return distance_matrix\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 750, "library_problem_id": 39, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 38}}
{"id": 751, "code": ["<code>\n```python\nimport numpy as np\nimport scipy.spatial.distance\n\ndef calculate_pairwise_distances(raster_array: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Calculate the pairwise Euclidean distances between unique ID patches/regions\n    in a given raster array. The function returns an N*N array where each entry\n    represents the minimum distance between the nearest edges of each raster patch.\n\n    Args:\n        raster_array (np.ndarray): A 2D numpy array representing the raster with unique ID patches.\n\n    Returns:\n        np.ndarray: A 2D numpy array of shape (N, N) where N is the number of unique regions.\n                    Each entry [i, j] contains the minimum Euclidean distance between region i and region j.\n    \"\"\"\n    # Extract unique region IDs, excluding the background (assumed to be 0)\n    unique_ids = np.unique(raster_array)\n    unique_ids = unique_ids[unique_ids != 0]\n\n    # Initialize an empty list to store the coordinates of each region\n    region_coords = {region_id: [] for region_id in unique_ids}\n\n    # Populate the region coordinates\n    for i in range(raster_array.shape[0]):\n        for j in range(raster_array.shape[1]):\n            region_id = raster_array[i, j]\n            if region_id != 0:\n                region_coords[region_id].append((i, j))\n\n    # Calculate pairwise distances\n    num_regions\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 751, "library_problem_id": 40, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 38}}
{"id": 752, "code": ["<code>\n```python\n\"\"\"\nFunction to extrapolate data points using B-spline in both directions.\n\nThis function takes two 2D numpy arrays `x` and `y` representing data points,\nand extrapolates the curves using B-spline with specified parameters. The\nextrapolation is performed for each column in the arrays, and the result is\na 2D numpy array with shape (5, 100).\n\nParameters:\n    x (np.ndarray): A 2D numpy array of shape (9, 5) containing x-coordinates\n                    of the data points.\n    y (np.ndarray): A 2D numpy array of shape (9, 5) containing y-coordinates\n                    of the data points.\n\nReturns:\n    np.ndarray: A 2D numpy array of shape (5, 100) containing the extrapolated\n                y-values for each group of x, y.\n\nExtrapolation Conditions:\n    - The B-spline is constructed using `scipy.interpolate.splrep` with a\n      smoothing factor `s=4` and a degree `k=2`.\n    - The extrapolation is performed over a linspace `x_val` ranging from -1\n      to 1 with 100 points.\n\nExample Usage:\n    result = extrapolate_curves(x, y)\n\"\"\"\n\nimport numpy as np\nfrom scipy import interpolate\n\ndef extrapolate_curves(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n    # Initialize the result array\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 752, "library_problem_id": 41, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 753, "code": ["<code>\nimport numpy as np\nimport scipy.stats as ss\n\n# Define the datasets\nx1 = [38.7, 41.5, 43.8, 44.5, 45.5, 46.0, 47.7, 58.0]\nx2 = [39.2, 39.3, 39.7, 41.4, 41.8, 42.9, 43.3, 45.8]\nx3 = [34.0, 35.0, 39.0, 40.0, 43.0, 43.0, 44.0, 45.0]\nx4 = [34.0, 34.8, 34.8, 35.4, 37.2, 37.8, 41.2, 42.8]\n\n# Pass the datasets as a list of arrays to the anderson_ksamp function\nstatistic, critical_values, significance_level = ss.anderson_ksamp([np.array(x1), np.array(x2), np.array(x3), np.array(x4)])\n\n# Output the results\nprint(\"Statistic:\", statistic)\nprint(\"Critical Values:\", critical_values)\nprint(\"Significance Level:\", significance_level)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 753, "library_problem_id": 42, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 754, "code": ["<code>\nfrom scipy.stats import anderson_ksamp\n\ndef compare_datasets(x1: list[float], x2: list[float]) -> bool:\n    \"\"\"\n    Compares two datasets using the Anderson-Darling k-sample test.\n\n    Args:\n    x1 (list of float): The first dataset, a 1-dimensional array of numerical values.\n    x2 (list of float): The second dataset, a 1-dimensional array of numerical values.\n\n    Returns:\n    bool: `True` if the datasets are from the same population at the 5% significance level, `False` otherwise.\n    \"\"\"\n    # Convert the datasets to numpy arrays\n    x1 = np.array(x1)\n    x2 = np.array(x2)\n\n    # Create a list of datasets to pass to the function\n    datasets = [x1, x2]\n\n    # Perform the Anderson-Darling k-sample test\n    result = anderson_ksamp(datasets)\n\n    # The critical value for a 5% significance level is the second value in the result\n    critical_value = result.critical_values[1]\n\n    # If the test statistic is greater than the critical value, we reject the null hypothesis\n    # that the datasets are drawn from the same population\n    if result.statistic > critical_value:\n        return False\n    else:\n        return True\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 754, "library_problem_id": 43, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 755, "code": ["<code>\ndef rolling_kendall_tau(df: pd.DataFrame, window_size: int) -> pd.DataFrame:\n    # Initialize the result DataFrame with the original data\n    result = df.copy()\n    \n    # Get the column names\n    columns = df.columns\n    \n    # Iterate over each pair of columns\n    for i in range(len(columns)):\n        for j in range(i + 1, len(columns)):\n            col1, col2 = columns[i], columns[j]\n            # Compute rolling Kendall Tau correlation\n            result[f'{col1}{col2}'] = df[[col1, col2]].rolling(window=window_size).apply(\n                lambda x: stats.kendalltau(x[:, 0], x[:, 1])[0], raw=False\n            )\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 755, "library_problem_id": 44, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 44}}
{"id": 756, "code": ["<code>\ndef is_csr_matrix_only_zeroes(matrix: csr_matrix) -> bool:\n    return matrix.nnz == 0\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 756, "library_problem_id": 45, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 757, "code": ["<code>\n```python\n\"\"\"\nFunction to Determine if a SciPy LIL Matrix is Empty\n\nThis function checks if a given SciPy LIL (List of Lists) sparse matrix is empty, \nmeaning it contains only zeroes. The function utilizes the `nonzero()` method to \nidentify non-zero elements within the matrix.\n\nArguments:\n    my_lil_matrix (scipy.sparse.lil_matrix): A sparse matrix in LIL format to be checked.\n\nReturns:\n    bool: Returns True if the matrix contains only zeroes, otherwise False.\n\nMathematical/Logical Conditions:\n    - A matrix is considered empty if the number of non-zero elements is zero.\n    - This is determined by checking the length of the first element of the tuple \n      returned by `nonzero()`, which contains the indices of non-zero elements.\n\nExample Usage:\n    from scipy.sparse import lil_matrix\n\n    # Example matrices\n    matrix1 = lil_matrix([[1, 2, 0], [0, 0, 3], [4, 0, 5]])\n    matrix2 = lil_matrix([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n    matrix3 = lil_matrix((2, 3))\n    matrix4 = lil_matrix([[0, 0, 0], [0, 1, 0], [0, 0, 0]])\n\n    # Check if matrices are empty\n    print(is_lil_matrix_only_zeroes(matrix1\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 757, "library_problem_id": 46, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 45}}
{"id": 758, "code": ["<code>\nimport numpy as np\nfrom scipy.linalg import block_diag\n\ndef convert_to_block_diag(array_3d: np.ndarray) -> np.ndarray:\n    # Extract the number of blocks (n) from the shape of the input array\n    n, a, b = array_3d.shape\n    \n    # Use a list comprehension to extract each 2D block from the 3D array\n    blocks = [array_3d[i] for i in range(n)]\n    \n    # Use scipy's block_diag function to create the block diagonal matrix\n    block_diag_matrix = block_diag(*blocks)\n    \n    return block_diag_matrix\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 758, "library_problem_id": 47, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 759, "code": ["<code>\nimport numpy as np\nfrom scipy import stats\n\ndef extract_p_value(pre_course_scores: np.ndarray, during_course_scores: np.ndarray) -> float:\n    # Perform the Wilcoxon rank-sum test\n    result = stats.ranksums(pre_course_scores, during_course_scores)\n    \n    # Extract and return the p-value from the test result\n    return result.pvalue\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 759, "library_problem_id": 48, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 760, "code": ["<code>\n```python\n\"\"\"\nFunction to compute the p-value from the Wilcoxon rank-sum test.\n\nThis function takes two arrays of scores and performs the Wilcoxon rank-sum test\nto determine if there is a statistically significant difference between the two sets\nof scores. The function returns the p-value from the test.\n\nArgs:\n    pre_course_scores (np.ndarray): An array of scores before the course.\n    during_course_scores (np.ndarray): An array of scores during the course.\n\nReturns:\n    float: The p-value from the Wilcoxon rank-sum test.\n\nExample:\n    >>> import numpy as np\n    >>> from scipy import stats\n    >>> pre_course_scores = np.random.randn(10)\n    >>> during_course_scores = np.random.randn(10)\n    >>> p_value = compute_ranksum_pvalue(pre_course_scores, during_course_scores)\n    >>> print(p_value)\n\"\"\"\n\nimport numpy as np\nfrom scipy import stats\n\ndef compute_ranksum_pvalue(pre_course_scores: np.ndarray, during_course_scores: np.ndarray) -> float:\n    # Perform the Wilcoxon rank-sum test\n    result = stats.ranksums(pre_course_scores, during_course_scores)\n    # Extract and return the p-value from the result\n    return result.pvalue\n\n# Example usage\nexample_pre_course_scores = np.random.randn(10)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 760, "library_problem_id": 49, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 761, "code": ["<code>\nimport numpy as np\n\ndef calculate_kurtosis(data: np.ndarray) -> float:\n    mean = np.mean(data)\n    n = len(data)\n    sum_of_squares = np.sum((data - mean)**2)\n    sum_of_fourth_powers = np.sum((data - mean)**4)\n    kurtosis = (n * sum_of_fourth_powers / sum_of_squares**2) - 3\n    return kurtosis\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 761, "library_problem_id": 50, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 50}}
{"id": 762, "code": ["<code>\ndef calculate_kurtosis(data: np.ndarray) -> float:\n    n = len(data)\n    mean = np.mean(data)\n    std_dev = np.std(data, ddof=0)\n    fourth_moment = np.sum((data - mean) ** 4) / n\n    kurtosis = (fourth_moment / (std_dev ** 4)) - 3\n    return kurtosis\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 762, "library_problem_id": 51, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 50}}
{"id": 763, "code": ["<code>\n```python\nimport numpy as np\nimport scipy.interpolate\n\ndef interpolate_fuel_consumption(x: np.ndarray, y: np.ndarray, z: np.ndarray, s: np.ndarray, t: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Interpolates the fuel consumption values for given speed and temperature data points.\n\n    Args:\n        x (np.ndarray): The x-coordinates of the grid points.\n        y (np.ndarray): The y-coordinates of the grid points.\n        z (np.ndarray): The function values at the grid points.\n        s (np.ndarray): The speed values for which interpolation is required.\n        t (np.ndarray): The temperature values for which interpolation is required.\n\n    Returns:\n        np.ndarray: A vector of interpolated fuel consumption values for each (speed, temperature) pair.\n    \"\"\"\n    # Create the interpolating function using cubic interpolation\n    interpolating_function = scipy.interpolate.interp2d(x, y, z, kind='cubic')\n\n    # Compute the interpolated values for each (s, t) pair\n    interpolated_values = np.array([interpolating_function(si, ti)[0] for si, ti in zip(s, t)])\n\n    return interpolated_values\n\n# Example usage\ns = np.linspace(-1, 1, 50)\nt = np.linspace(-2, 0, 50)\nx, y = np.ogrid[-1:1:1\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 763, "library_problem_id": 52, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 52}}
{"id": 764, "code": ["<code>\nimport numpy as np\nimport scipy.interpolate\n\ndef interpolate_fuel_consumption(speed: np.ndarray, temperature: np.ndarray, \n                                 x: np.ndarray, y: np.ndarray, z: np.ndarray) -> np.ndarray:\n    # Create the interpolation function using cubic interpolation\n    interpolation_function = scipy.interpolate.interp2d(x, y, z, kind='cubic')\n    \n    # Initialize an array to store the interpolated results\n    interpolated_values = np.empty(len(speed))\n    \n    # Iterate over each pair of (speed, temperature) and compute the interpolated value\n    for i in range(len(speed)):\n        interpolated_values[i] = interpolation_function(speed[i], temperature[i])\n    \n    return interpolated_values\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 764, "library_problem_id": 53, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 52}}
{"id": 765, "code": ["<code>\n```python\n\"\"\"\nFunction to determine the Voronoi cell occupancy by extra points.\n\nThis function calculates which Voronoi cell each of a set of extra points belongs to, \ngiven an initial set of points that define the Voronoi tessellation. It returns an \narray indicating the count of extra points in each Voronoi cell.\n\nArgs:\n    points (List[List[float]]): A list of 2D coordinates representing the initial points \n                                used to generate the Voronoi tessellation.\n    extra_points (List[List[float]]): A list of 2D coordinates representing the extra points \n                                      to be checked against the Voronoi cells.\n\nReturns:\n    np.ndarray: An array where each element at index i represents the number of extra points \n                contained within the Voronoi cell corresponding to the i-th initial point.\n\nExample:\n    points = [[0,0], [1,4], [2,3], [4,1], [1,1], [2,2], [5,3]]\n    extra_points = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\n    result = count_extra_points_in_voronoi_cells(points, extra_points)\n    # result might be something like [1, 0, 0, 3, 0, 1, 0] depending on the Voronoi diagram\n\"\"\"\n\nimport numpy as np\nfrom scipy.spatial import Voronoi\n\ndef count_extra_points_in_vor\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 765, "library_problem_id": 54, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 766, "code": ["<code>\nimport numpy as np\nfrom scipy.spatial import Voronoi\n\ndef count_points_in_voronoi_cells(points, extra_points):\n    vor = Voronoi(points)\n    point_counts = np.zeros(len(points), dtype=int)\n\n    for extra_point in extra_points:\n        # Find the closest Voronoi region for the extra point\n        distances = np.linalg.norm(vor.points - extra_point, axis=1)\n        closest_region_index = np.argmin(distances)\n        point_counts[closest_region_index] += 1\n\n    return point_counts\n\npoints = [[0,0], [1,4], [2,3], [4,1], [1,1], [2,2], [5,3]]\nextra_points = [[0.5,0.2], [3, 0], [4,0],[5,0], [4,3]]\nresult = count_points_in_voronoi_cells(points, extra_points)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 766, "library_problem_id": 55, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 767, "code": ["<code>\n```python\nimport numpy as np\nimport scipy.sparse as sparse\n\ndef create_sparse_matrix(vectors: list[np.ndarray], max_vector_size: int) -> sparse.csr_matrix:\n    \"\"\"\n    Create a sparse matrix from a list of numpy vectors by padding zeros to the end of vectors\n    that have fewer elements than the maximum size.\n\n    Args:\n    - vectors (list of np.ndarray): A list of numpy arrays where each array represents a vector.\n    - max_vector_size (int): The maximum number of elements any vector can have.\n\n    Returns:\n    - sparse.csr_matrix: A sparse matrix with all vectors padded to the maximum size.\n    \"\"\"\n    # Initialize a list to store padded vectors\n    padded_vectors = []\n\n    # Iterate over each vector in the list\n    for vector in vectors:\n        # Determine the current vector size\n        current_size = vector.size\n        \n        # Check if padding is needed\n        if current_size < max_vector_size:\n            # Pad the vector with zeros to match the maximum size\n            padded_vector = np.pad(vector, (0, max_vector_size - current_size), 'constant')\n        else:\n            padded_vector = vector\n        \n        # Append the padded vector to the list\n        padded_vectors.append(padded_vector)\n    \n    # Convert the list of padded vectors to a sparse matrix\n    sparse_matrix = sparse.csr_matrix(padded_vectors)\n    \n    return sparse_matrix\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 767, "library_problem_id": 56, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 768, "code": ["<code>\nimport numpy as np\nimport scipy.ndimage\n\ndef shifted_median_filter(a: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Apply a median filter to a binary array with a 3x3 kernel, shifted one cell to the right.\n\n    Parameters:\n    a (np.ndarray): A 2D binary array of shape (9, 9).\n\n    Returns:\n    np.ndarray: A 2D array of the same shape as `a`, filtered with a shifted median filter.\n    \"\"\"\n    # Apply the median filter with the kernel shifted one cell to the right\n    b = scipy.ndimage.median_filter(a, size=3, origin=(0, -1))\n    return b\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 768, "library_problem_id": 57, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 57}}
{"id": 769, "code": ["<code>\ndef get_matrix_value(matrix, row, column):\n    # Get the indices of the non-zero elements in the specified row\n    row_indices = matrix.indices[matrix.indptr[row]:matrix.indptr[row+1]]\n    \n    # Find the index of the column in the row_indices array\n    column_index = np.where(row_indices == column)[0]\n    \n    # If the column is not in the row, return 0\n    if len(column_index) == 0:\n        return 0\n    \n    # Get the data array of the matrix\n    data = matrix.data\n    \n    # Return the value at the specified (row, column) position in the matrix\n    return data[matrix.indptr[row] + column_index[0]]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 769, "library_problem_id": 58, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 770, "code": ["<code>\ndef fetch_sparse_matrix_values(matrix: csr_matrix, row_indices: list[int], column_indices: list[int]) -> list[float]:\n    # Initialize an empty list to store the result\n    result = []\n    \n    # Iterate over the provided row and column indices\n    for row, col in zip(row_indices, column_indices):\n        # Access the value directly from the sparse matrix using the row and column indices\n        value = matrix[row, col]\n        # Append the value to the result list\n        result.append(value)\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 770, "library_problem_id": 59, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 771, "code": ["<code>\nimport numpy as np\nfrom scipy.interpolate import interp1d\n\ndef interpolate_array(array: np.ndarray, x: np.ndarray, x_new: np.ndarray) -> np.ndarray:\n    # Validate input dimensions\n    assert array.shape[0] == len(x), \"The first dimension of 'array' must match the length of 'x'.\"\n    \n    # Interpolate along the first axis using vectorized operations\n    interpolator = interp1d(x, array, axis=0, kind='linear', fill_value='extrapolate')\n    new_array = interpolator(x_new)\n    \n    return new_array\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 771, "library_problem_id": 60, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 60}}
{"id": 772, "code": ["<code>\ndef normal_cdf(u: float, o2: float, x: float) -> float:\n    \"\"\"Calculate the cumulative distribution function for a normal distribution.\"\"\"\n    # Normalize x with respect to mean (u) and standard deviation (sqrt(o2))\n    dev = abs((x - u) / math.sqrt(o2))\n    \n    # Integrate the standard normal PDF from -dev to dev\n    P_inner, _ = spi.quad(normal_pdf, -dev, dev)\n    \n    # Calculate the probability\n    P_outer = 1 - P_inner\n    P = P_inner + P_outer / 2\n    return P\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 772, "library_problem_id": 61, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 773, "code": ["<code>\ndef NormalDistro(u: float, o2: float, x: float) -> float:\n    \"\"\"\n    Calculate the probability of a normally distributed random variable up to position `x`.\n\n    Args:\n        u (float): The mean of the normal distribution.\n        o2 (float): The variance of the normal distribution.\n        x (float): The position up to which the probability is calculated.\n\n    Returns:\n        float: The cumulative probability up to position `x`.\n    \"\"\"\n    # Normalize the position `x` with respect to mean `u` and standard deviation `sqrt(o2)`\n    dev = abs((x - u) / math.sqrt(o2))\n    \n    # Integrate the PDF of the standard normal distribution from -dev to dev\n    P_inner, _ = scipy.integrate.quad(NDfx, -dev, dev)\n    \n    # Calculate the complementary probability\n    P_outer = 1 - P_inner\n    \n    # Combine the probabilities to get the final result\n    P = P_inner + P_outer / 2\n    \n    return P\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 773, "library_problem_id": 62, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 61}}
{"id": 774, "code": ["<code>\nimport numpy as np\n\ndef generate_dct_matrix(N: int) -> np.ndarray:\n    \"\"\"\n    Generate an NxN orthogonal DCT matrix.\n\n    Args:\n    - N (int): The size of the DCT matrix to generate.\n\n    Returns:\n    - np.ndarray: An NxN orthogonal DCT matrix.\n    \"\"\"\n    dct_matrix = np.zeros((N, N))\n    alpha = lambda u, v: np.sqrt(1/N) if u == 0 else np.sqrt(2/N)\n    for u in range(N):\n        for v in range(N):\n            dct_matrix[u, v] = alpha(u, v) * np.cos(((2*v+1)*u*np.pi)/(2*N))\n    return dct_matrix\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 774, "library_problem_id": 63, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 775, "code": ["<code>\nfrom scipy.sparse import diags\nimport numpy as np\n\ndef generate_tridiagonal(matrix: np.ndarray) -> np.ndarray:\n    # Define the offsets for the diagonals: -1 for sub-diagonal, 0 for main diagonal, 1 for super-diagonal\n    offsets = [-1, 0, 1]\n    \n    # Use scipy.sparse.diags to create the tridiagonal matrix\n    tridiagonal_matrix = diags(matrix, offsets, shape=(5, 5)).toarray()\n    \n    return tridiagonal_matrix\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 775, "library_problem_id": 64, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 776, "code": ["<code>\nimport numpy as np\nimport scipy.special\n\ndef compute_binomial_matrix(N: int, p: float) -> np.ndarray:\n    \"\"\"\n    Compute a 2D binomial distribution probability matrix M of size (N+1) x (N+1).\n\n    Args:\n    - N (int): The maximum number of trials, must be a non-negative integer.\n    - p (float): The probability of success in each trial, must satisfy 0 <= p <= 1.\n\n    Returns:\n    - np.ndarray: A 2D numpy array representing the binomial distribution probability matrix.\n    \"\"\"\n    # Initialize the result matrix with zeros\n    M = np.zeros((N+1, N+1))\n\n    # Compute the binomial coefficients using scipy.special.comb\n    comb = scipy.special.comb(np.arange(N+1), np.arange(N+1), exact=True)\n\n    # Compute the probability for each binomial coefficient\n    prob = comb * (p ** np.arange(N+1)) * ((1 - p) ** (np.arange(N+1)))\n\n    # Fill the matrix M with the computed probabilities\n    M[np.arange(N+1), np.arange(N+1)] = prob\n\n    return M\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 776, "library_problem_id": 65, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 65}}
{"id": 777, "code": ["<code>\nimport pandas as pd\nimport io\nfrom scipy import stats\n\ndef compute_row_zscores(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Compute row-wise z-scores for a given DataFrame.\n\n    Args:\n    df (pd.DataFrame): Input DataFrame with gene identifiers as index and sample measurements as columns.\n\n    Returns:\n    pd.DataFrame: DataFrame with the same structure as input, containing z-scores for each value.\n    \"\"\"\n    # Apply z-score normalization row-wise\n    zscore_df = df.apply(stats.zscore, axis=1)\n    return zscore_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 777, "library_problem_id": 66, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 778, "code": ["<code>\nresult = df.apply(lambda x: stats.zscore(x))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 778, "library_problem_id": 67, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 66}}
{"id": 779, "code": ["<code>\nimport pandas as pd\nimport io\nfrom scipy import stats\n\ndef calculate_row_zscores(data: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Calculate the z-score for each row in the DataFrame and append it below the original data.\n\n    Args:\n    - data (pd.DataFrame): A DataFrame with gene expression data, indexed by 'probegenes'.\n\n    Returns:\n    - pd.DataFrame: A DataFrame with original data and corresponding z-scores for each row.\n    \"\"\"\n    # Initialize a list to store the results\n    results = []\n\n    # Iterate over each row to calculate z-scores\n    for index, row in data.iterrows():\n        # Append original data\n        results.append(pd.Series(row, name=(index, 'data')))\n        # Calculate and append z-scores\n        z_scores = stats.zscore(row)\n        results.append(pd.Series(z_scores, name=(index, 'zscore')))\n\n    # Concatenate results into a DataFrame\n    result_df = pd.concat(results, axis=1).T\n\n    return result_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 779, "library_problem_id": 68, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 780, "code": ["<code>\nimport pandas as pd\nimport io\nimport numpy as np\nfrom scipy import stats\n\n# Load the data into a DataFrame\ntemp = u\"\"\"probegenes,sample1,sample2,sample3\n1415777_at Pnliprp1,20,0.00,11\n1415805_at Clps,17,0.00,55\n1415884_at Cela3b,47,0.00,100\"\"\"\ndf = pd.read_csv(io.StringIO(temp), index_col='probegenes')\n\n# Compute z-scores for each column\nz_scores = df.apply(lambda x: stats.zscore(x, nan_policy='omit'), axis=0)\n\n# Round the data and z-scores to 3 decimal places\ndf_rounded = df.round(3)\nz_scores_rounded = z_scores.round(3)\n\n# Combine the original data and z-scores into a single DataFrame\nresult = pd.concat([df_rounded, z_scores_rounded], keys=['data', 'zscore'], axis=1).swaplevel(0, 1, axis=1).sort_index(axis=1)\n\n# Display the result\nresult\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 780, "library_problem_id": 69, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 66}}
{"id": 781, "code": ["<code>\n```python\n\"\"\"\nRefactored Prompt for DeepSeek Coder 6.7B\n\nObjective:\nDemonstrate the correct usage of scipy.optimize.line_search for a multivariable function.\nThe goal is to find the optimal step size (alpha) along a given search direction that minimizes the function.\n\nFunction Definitions:\n1. test_func(x: List[float]) -> float:\n   - Computes the sum of squares of the input vector elements.\n   - Mathematical Expression: f(x) = x[0]^2 + x[1]^2\n\n2. test_grad(x: List[float]) -> List[float]:\n   - Computes the gradient of the test_func.\n   - Gradient Expression: \u2207f(x) = [2*x[0], 2*x[1]]\n\nParameters:\n- starting_point: List[float] = [1.8, 1.7]\n  Initial point for the line search.\n- direction: List[float] = [-1.0, -1.0]\n  Search direction vector.\n\nExpected Output:\n- The function should return the optimal step size (alpha) that minimizes test_func along the specified direction.\n\nError Context:\n- The TypeError encountered suggests an issue with vector-scalar multiplication. Ensure numpy arrays are used for arithmetic operations.\n\nSolution:\n\"\"\"\nimport scipy\nimport scipy.optimize\nimport numpy as np\n\ndef test_func(x: np.ndarray) -> float:\n    \"\"\"Compute the sum of squares of the input vector elements.\"\"\"\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 781, "library_problem_id": 70, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 70}}
{"id": 782, "code": ["<code>\nimport numpy as np\nfrom scipy.spatial import distance\n\ndef create_distance_array(shape: tuple) -> np.ndarray:\n    rows, cols = shape\n    y, x = np.indices((rows, cols))\n    center = np.array([(rows - 1) / 2, (cols - 1) / 2])\n    points = np.dstack((y, x)).reshape(-1, 2)\n    distances = distance.cdist(points, [center])\n    return distances.reshape(rows, cols)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 782, "library_problem_id": 71, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 783, "code": ["<code>\ndef manhattan_distance_from_center(shape: tuple) -> np.ndarray:\n    rows, cols = shape\n    center = (rows // 2, cols // 2)\n    y_indices, x_indices = np.indices((rows, cols))\n    manhattan_distances = np.abs(y_indices - center[0]) + np.abs(x_indices - center[1])\n    return manhattan_distances.astype(float)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 783, "library_problem_id": 72, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 71}}
{"id": 784, "code": ["<code>\nimport numpy as np\nfrom scipy.spatial import distance\n\ndef calculate_euclidean_distances(shape: tuple[int, int] = (6, 6)) -> np.ndarray:\n    rows, cols = shape\n    center = np.array([(rows - 1) / 2, (cols - 1) / 2])\n    y_indices, x_indices = np.indices(shape)\n    points = np.stack((y_indices, x_indices), axis=-1).reshape(-1, 2)\n    distances = distance.cdist(points, [center])\n    return distances.reshape(shape)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 784, "library_problem_id": 73, "library": "Scipy", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 785, "code": ["<code>\nimport numpy as np\nimport scipy.ndimage\n\ndef resample_array(array: np.ndarray, target_shape: Tuple[int, int]) -> np.ndarray:\n    # Calculate the zoom factors for each dimension\n    zoom_factors = (target_shape[0] / array.shape[0], target_shape[1] / array.shape[1])\n    \n    # Perform the resampling using scipy.ndimage.zoom\n    resampled_array = scipy.ndimage.zoom(array, zoom_factors, order=1)\n    \n    return resampled_array\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 785, "library_problem_id": 74, "library": "Scipy", "test_case_cnt": 4, "perturbation_type": "Origin", "perturbation_origin_id": 74}}
{"id": 786, "code": ["<code>\ndef main():\n    \"\"\"\n    Main function to set up and solve the optimization problem.\n    \"\"\"\n    # Coefficient matrix a with shape (M, N)\n    a = np.array([\n        [0, 0, 1, 1, 1],\n        [1, 0, 1, 0, 1],\n        [0, 1, 0, 1, 0]\n    ])\n\n    # True values of x\n    x_true = np.array([10, 13, 5, 8, 40])\n\n    # Data without noise\n    y = objective_function(x_true, a)\n\n    # Initial guess for x\n    x0 = np.array([2, 3, 1, 4, 20])\n\n    # Minimize the residual function\n    result = minimize(residual, x0, args=(a, y), method='L-BFGS-B')\n\n    # Output the result\n    print(\"Optimal x:\", result.x)\n\nif __name__ == '__main__':\n    main()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 786, "library_problem_id": 75, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 787, "code": ["<code>\ndef main() -> None:\n    \"\"\"\n    Set up the optimization problem and find the optimal x using L-BFGS-B method.\n    \"\"\"\n    # Coefficient matrix a (3x5)\n    a = np.array([[0, 0, 1, 1, 1],\n                  [1, 0, 1, 0, 1],\n                  [0, 1, 0, 1, 0]])\n    \n    # True values of x\n    x_true = np.array([10, 13, 5, 8, 40])\n    \n    # Target vector y without noise\n    y = objective_function(x_true, a)\n    \n    # Initial guess for x\n    x0 = np.array([2, 3, 1, 4, 20])\n    \n    # Lower bounds for x\n    x_lower_bounds = x_true / 2\n    \n    # Perform optimization using L-BFGS-B\n    result = scipy.optimize.minimize(\n        fun=lambda x: np.sum(residual(x, a, y)),\n        x0=x0,\n        bounds=[(lb, None) for lb in x_lower_bounds],\n        method='L-BFGS-B'\n    )\n    \n    # Output the result\n    print(\"Optimal x:\", result.x)\n\nif __name__ == '__main__':\n    main()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 787, "library_problem_id": 76, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 75}}
{"id": 788, "code": ["<code>\nimport scipy.integrate\nimport numpy as np\n\ndef solve_time_varying_ode(N0: float, time_span: tuple) -> np.ndarray:\n    def dN1_dt_time_varying(t, N1):\n        return -100 * N1 + np.sin(t)\n\n    sol = scipy.integrate.solve_ivp(fun=dN1_dt_time_varying, t_span=time_span, y0=[N0])\n    result = sol.y\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 788, "library_problem_id": 77, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 77}}
{"id": 789, "code": ["<code>\nimport scipy.integrate\nimport numpy as np\n\ndef dN1_dt_varying(t: float, N1: float) -> float:\n    \"\"\"\n    Compute the derivative of N1 with respect to time, incorporating a time-varying input.\n\n    Args:\n        t (float): The current time point.\n        N1 (float): The current value of N1.\n\n    Returns:\n        float: The derivative of N1 at time t.\n    \"\"\"\n    # Define the time-varying input condition\n    if 0 < t < 2 * np.pi:\n        input_variation = t - np.sin(t)\n    else:\n        input_variation = 2 * np.pi\n\n    # Compute the derivative with the time-varying input\n    return -100 * N1 + input_variation\n\n# Initial condition\nN0 = 1\n\n# Define the time span for the integration\ntime_span = [0, 10]\n\n# Solve the ODE with the time-varying input\nsol = scipy.integrate.solve_ivp(fun=dN1_dt_varying, t_span=time_span, y0=[N0])\n\n# Extract the solution values at the time points\nresult = sol.y\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 789, "library_problem_id": 78, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 790, "code": ["<code>\n```python\nimport scipy.integrate\nimport numpy as np\n\ndef solve_ode_with_sinusoidal_input(N0: float, time_span: list, time_points: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Solves an ODE with a time-varying sinusoidal input using SciPy's solve_ivp.\n\n    Args:\n        N0 (float): Initial condition for the ODE.\n        time_span (list): A list containing the start and end times for the integration.\n        time_points (np.ndarray): An array of time points at which to evaluate the solution.\n\n    Returns:\n        np.ndarray: An array of solution values at the specified time points.\n    \"\"\"\n    def dN1_dt(t: float, N1: float) -> float:\n        \"\"\"\n        Defines the rate of change of N1 with respect to time, including a sinusoidal input.\n\n        Args:\n            t (float): Current time.\n            N1 (float): Current value of N1.\n\n        Returns:\n            float: The rate of change of N1.\n        \"\"\"\n        return -100 * N1 - np.cos(t)\n\n    # Solve the ODE using solve_ivp\n    sol = scipy.integrate.solve_ivp(\n        fun=dN1_dt,\n        t_span=time_span,\n        y0=[N0],\n        t_eval=time_points\n    )\n\n    # Extract the solution values at the specified time points\n    result = sol.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 790, "library_problem_id": 79, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 77}}
{"id": 791, "code": ["<code>\ndef optimize_reservoir(I: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Solves the reservoir optimization problem using scipy.optimize.minimize.\n\n    Args:\n        I (np.ndarray): Array of inflows for each time step.\n\n    Returns:\n        np.ndarray: Optimized decision variables.\n    \"\"\"\n    x0 = I  # Initial guess for decision variables\n    constraints = create_constraints(I)\n\n    result = minimize(objective_function, x0, method=\"SLSQP\", constraints=constraints)\n    return result.x\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 791, "library_problem_id": 80, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 792, "code": ["<code>\nfrom scipy import sparse\n\ndef merge_csr_matrices(a: sparse.csr_matrix, b: sparse.csr_matrix) -> sparse.csr_matrix:\n    \"\"\"\n    Merges two CSR matrices vertically into a single CSR matrix.\n\n    Args:\n        a (sparse.csr_matrix): The first sparse matrix.\n        b (sparse.csr_matrix): The second sparse matrix.\n\n    Returns:\n        sparse.csr_matrix: The merged sparse matrix in CSR format.\n\n    Example:\n        a = sparse.csr_matrix([[1, 2, 3], [4, 5, 6]])\n        b = sparse.csr_matrix([[7, 8, 9], [10, 11, 12]])\n        result = merge_csr_matrices(a, b)\n        # result is a CSR matrix equivalent to:\n        # [[1, 2, 3],\n        #  [4, 5, 6],\n        #  [7, 8, 9],\n        #  [10, 11, 12]]\n    \"\"\"\n    # Solution implementation\n    result = sparse.vstack([a, b], format='csr')\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 792, "library_problem_id": 81, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 81}}
{"id": 793, "code": ["<code>\n```python\nfrom scipy import sparse\n\ndef merge_csr_matrices_horizontally(a: sparse.csr_matrix, b: sparse.csr_matrix) -> sparse.csr_matrix:\n    \"\"\"\n    Merges two CSR (Compressed Sparse Row) matrices horizontally.\n\n    Args:\n    a (sparse.csr_matrix): The first sparse matrix to merge.\n    b (sparse.csr_matrix): The second sparse matrix to merge.\n\n    Returns:\n    sparse.csr_matrix: A new CSR matrix resulting from the horizontal concatenation of matrices 'a' and 'b'.\n\n    Preconditions:\n    - The number of rows in 'a' must be equal to the number of rows in 'b'.\n    - Both 'a' and 'b' must be in CSR format.\n\n    Postconditions:\n    - The resulting matrix will have the same number of rows as 'a' and 'b'.\n    - The number of columns in the resulting matrix will be the sum of the columns in 'a' and 'b'.\n\n    Example:\n    Given:\n    a = csr_matrix([[1, 2, 3], [4, 5, 6]])\n    b = csr_matrix([[7, 8, 9], [10, 11, 12]])\n    The result should be:\n    csr_matrix([[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]])\n    \"\"\"\n    result = sparse\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 793, "library_problem_id": 82, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 81}}
{"id": 794, "code": ["<code>\nimport numpy as np\nfrom scipy import integrate\n\ndef compute_integrals(filename: str) -> list:\n    \"\"\"\n    Computes the definite integral of the function 2*c*x from 0 to 1 for each value of c.\n\n    Args:\n        filename (str): The path to the data file containing constant values of c.\n\n    Returns:\n        list: A list of integral results for each value of c.\n    \"\"\"\n    # Load constant values from the file\n    c_values = np.loadtxt(filename, unpack=True, usecols=[1])\n    \n    # Initialize an empty list to store integral results\n    integral_results = []\n    \n    # Define integration limits\n    lower_limit = 0\n    upper_limit = 1\n    \n    # Iterate over each constant value c\n    for c in c_values:\n        # Define the integrand function\n        integrand = lambda x: 2 * c * x\n        \n        # Compute the definite integral using scipy.integrate.quad\n        result, error = integrate.quad(integrand, lower_limit, upper_limit)\n        \n        # Append the result to the list\n        integral_results.append(result)\n    \n    return integral_results\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 794, "library_problem_id": 83, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 83}}
{"id": 795, "code": ["<code>\nimport numpy as np\nfrom scipy.integrate import quad\nfrom typing import List\n\ndef compute_integrals(c_values: List[float], lower_limit: float = 0, upper_limit: float = 1) -> np.ndarray:\n    integral_results = []\n\n    for c in c_values:\n        # Define the integrand function\n        integrand = lambda x: 2 * c * x\n        \n        # Compute the definite integral using scipy.integrate.quad\n        result, _ = quad(integrand, lower_limit, upper_limit)\n        \n        # Append the result to the list\n        integral_results.append(result)\n\n    # Convert the list of results to a numpy array\n    return np.array(integral_results)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 795, "library_problem_id": 84, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 83}}
{"id": 796, "code": ["<code>\ndef add_scalar_to_sparse_matrix(V: sparse.dok_matrix, x: float) -> sparse.dok_matrix:\n    \"\"\"\n    Adds a scalar value to the non-zero elements of a sparse matrix in DOK format.\n\n    Parameters:\n    V (sparse.dok_matrix): A large square sparse matrix in Dictionary of Keys format.\n    x (float): The scalar value to be added to the non-zero elements of the matrix.\n\n    Returns:\n    sparse.dok_matrix: A new sparse matrix with the scalar added to its non-zero elements.\n    \"\"\"\n    # Create a new DOK matrix with the same shape as V\n    result = sparse.dok_matrix(V.shape)\n\n    # Iterate over the non-zero elements of V\n    for (row, col), value in V.items():\n        # Add x to the value and set it in the new matrix\n        result[row, col] = value + x\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 796, "library_problem_id": 85, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 797, "code": ["<code>\nfrom scipy import sparse\n\ndef add_scalar_to_sparse(V: sparse.coo_matrix, x: float) -> sparse.coo_matrix:\n    \"\"\"\n    Add a scalar value to non-zero elements of a sparse matrix in COO format.\n\n    Args:\n        V (sparse.coo_matrix): A square sparse matrix in COO format.\n        x (float): The scalar value to add to non-zero elements of the matrix.\n\n    Returns:\n        sparse.coo_matrix: The resulting sparse matrix after the scalar addition.\n    \"\"\"\n    # Create a new data array with the scalar added to non-zero elements\n    new_data = V.data + x\n\n    # Construct a new COO matrix with the updated data\n    result = sparse.coo_matrix((new_data, (V.row, V.col)), shape=V.shape)\n\n    return result\n\n# Example usage\nV = sparse.random(10, 10, density=0.05, format='coo', random_state=42)\nx = 100\nV = add_scalar_to_sparse(V, x)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 797, "library_problem_id": 86, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 798, "code": ["<code>\n# Create a new COO matrix with the same shape as V\nV_new = sparse.coo_matrix((V.data.copy(), V.indices.copy(), V.shape), dtype=V.dtype)\n\n# Add x to the data of V_new\nV_new.data += x\n\n# Add y to the data of V_new\nV_new.data += y\n\n# Return the new matrix\nreturn V_new\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 798, "library_problem_id": 87, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 799, "code": ["<code>\nfrom scipy import sparse\nimport numpy as np\nimport math\n\ndef normalize_csc_matrix_columns(matrix: sparse.csc_matrix) -> sparse.csc_matrix:\n    \"\"\"\n    Normalize each column of a CSC sparse matrix by its Euclidean norm.\n\n    Args:\n        matrix (sparse.csc_matrix): The input sparse matrix in CSC format.\n\n    Returns:\n        sparse.csc_matrix: The matrix with each column normalized by its Euclidean norm.\n    \"\"\"\n    # Ensure the matrix is in CSC format\n    matrix = matrix.tocsc()\n\n    # Iterate through each column\n    for col_index in range(matrix.shape[1]):\n        # Extract the column data\n        column_data = matrix[:, col_index].data\n\n        # Calculate the Euclidean norm of the column\n        euclidean_norm = math.sqrt(np.sum(column_data ** 2))\n\n        # Normalize the column by dividing each element by the Euclidean norm\n        if euclidean_norm > 0:\n            matrix.data[matrix.indptr[col_index]:matrix.indptr[col_index + 1]] /= euclidean_norm\n\n    return matrix\n\n# Example usage\nsa = sparse.random(10, 10, density=0.3, format='csc', random_state=42)\nnormalized_sa = normalize_csc_matrix_columns(sa)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 799, "library_problem_id": 88, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 88}}
{"id": 800, "code": ["<code>\nfrom scipy import sparse\nimport numpy as np\nimport math\n\ndef normalize_columns(matrix: sparse.csr_matrix) -> sparse.csr_matrix:\n    # Ensure the matrix is in CSR format\n    matrix = matrix.tocsr()\n\n    # Iterate over each column\n    for col in range(matrix.shape[1]):\n        # Extract the column data\n        column_data = matrix[:, col].toarray().flatten()\n        \n        # Compute the Euclidean norm of the column\n        norm = math.sqrt(np.sum(column_data**2))\n        \n        # Avoid division by zero\n        if norm != 0:\n            # Normalize the column\n            matrix[:, col] = matrix[:, col] / norm\n\n    return matrix\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 800, "library_problem_id": 89, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 88}}
{"id": 801, "code": ["<code>\nimport numpy as np\n\ndef convert_to_binary_matrix(matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Convert a given integer matrix to a binary matrix for social network analysis.\n\n    Args:\n    - matrix (np.ndarray): A 2D numpy array of integers representing email counts.\n\n    Returns:\n    - np.ndarray: A 2D numpy array of the same shape as `matrix`, where each element\n                  is 1 if the corresponding element in `matrix` is greater than zero,\n                  otherwise 0.\n    \"\"\"\n    binary_matrix = (matrix > 0).astype(int)\n    return binary_matrix\n\n# Example usage\na = np.array([[26, 3, 0], [3, 195, 1], [0, 1, 17]])\na = convert_to_binary_matrix(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 801, "library_problem_id": 90, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 90}}
{"id": 802, "code": ["<code>\nimport numpy as np\n\ndef convert_to_binary_matrix(matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Convert an integer matrix representing email interactions into a binary matrix.\n    \n    Args:\n        matrix (np.ndarray): A square matrix where each element represents the number of emails sent \n                             from one individual to another.\n    \n    Returns:\n        np.ndarray: A binary matrix where a value of 1 indicates that the corresponding individuals \n                    have not emailed each other, and a value of 0 indicates that at least one email \n                    has been exchanged between them.\n    \n    Example:\n        Input:\n        [[26, 3, 0],\n         [3, 195, 1],\n         [0, 1, 17]]\n        \n        Output:\n        [[0, 0, 1],\n         [0, 0, 0],\n         [1, 0, 0]]\n    \"\"\"\n    # Create a binary matrix where each element is 1 if the corresponding element in the input matrix is 0,\n    # and 0 otherwise.\n    binary_matrix = (matrix == 0).astype(int)\n    return binary_matrix\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 802, "library_problem_id": 91, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 90}}
{"id": 803, "code": ["<code>\nimport numpy as np\n\ndef find_closest_to_centroid(data: np.ndarray, centroids: np.ndarray, labels: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Identify the index of the element closest to the centroid for each cluster.\n\n    Parameters:\n    - data (np.ndarray): The original dataset, where each row is a sample point.\n    - centroids (np.ndarray): The centroid of each cluster, where each row is a centroid.\n    - labels (np.ndarray): An array of cluster labels for each sample in the dataset.\n\n    Returns:\n    - np.ndarray: An array of indices, where each index corresponds to the element in the original data\n      that is closest to the centroid of its respective cluster.\n    \"\"\"\n    result = []\n    for i in range(centroids.shape[0]):\n        cluster_indices = np.where(labels == i)[0]\n        cluster_data = data[cluster_indices]\n        distances = np.linalg.norm(cluster_data - centroids[i], axis=1)\n        closest_index = cluster_indices[np.argmin(distances)]\n        result.append(closest_index)\n    return np.array(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 803, "library_problem_id": 92, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 92}}
{"id": 804, "code": ["<code>\n# Calculate pairwise distances between data points and centroids\n    distances = scipy.spatial.distance.cdist(data, centroids)\n\n    # Find the index of the closest data point to each centroid\n    closest_indices = np.argmin(distances, axis=0)\n\n    # Extract the closest data points\n    result = data[closest_indices]\n\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 804, "library_problem_id": 93, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 92}}
{"id": 805, "code": ["<code>\nimport numpy as np\nimport scipy.spatial\n\ndef find_kth_closest_to_centroid(data: np.ndarray, centroids: np.ndarray, k: int) -> list:\n    result = []\n    for i, centroid in enumerate(centroids):\n        # Calculate distances from each data point to the current centroid\n        distances = scipy.spatial.distance.cdist(data, [centroid], 'euclidean').flatten()\n        # Get the indices of the sorted distances\n        sorted_indices = np.argsort(distances)\n        # Append the index of the k-th closest element to the result list\n        result.append(sorted_indices[k-1])\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 805, "library_problem_id": 94, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 92}}
{"id": 806, "code": ["<code>\nimport numpy as np\nfrom scipy.optimize import fsolve\n\ndef eqn(x, a, b):\n    \"\"\"\n    Equation to find the root for.\n\n    Parameters:\n    x (float): The independent variable.\n    a (float): The variable to solve for.\n    b (float): A parameter affecting the equation.\n\n    Returns:\n    float: The result of the equation x + 2*a - b**2.\n    \"\"\"\n    return x + 2*a - b**2\n\ndef solve_for_a(x_values, b_values):\n    \"\"\"\n    Solves for the variable 'a' given arrays of 'x' and 'b' values.\n\n    Parameters:\n    x_values (np.ndarray): Array of x values.\n    b_values (np.ndarray): Array of b values.\n\n    Returns:\n    np.ndarray: Array of roots for 'a' corresponding to each (x, b) pair.\n    \"\"\"\n    def root_function(a, x, b):\n        return eqn(x, a, b)\n\n    roots = np.array([fsolve(root_function, x0=0.5, args=(x, b))[0] for x, b in zip(x_values, b_values)])\n    return roots\n\nxdata = np.arange(4) + 3\nbdata = np.random.randint(0, 10, (4,))\nresult = solve_for_a(xdata, bdata)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 806, "library_problem_id": 95, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 807, "code": ["<code>\n```python\nimport numpy as np\nfrom scipy.optimize import fsolve\n\ndef eqn(b, x, a):\n    \"\"\"Equation to find the root for.\n\n    Args:\n        b (float): The independent variable.\n        x (float): Parameter x.\n        a (float): Parameter a.\n\n    Returns:\n        float: The result of the equation x + 2*a - b**2.\n    \"\"\"\n    return x + 2*a - b**2\n\ndef solve_for_b(x_values, a_values):\n    \"\"\"Finds the roots for b given arrays of x and a values.\n\n    Args:\n        x_values (np.ndarray): Array of x values.\n        a_values (np.ndarray): Array of a values.\n\n    Returns:\n        np.ndarray: A 2D array where each row contains the roots for the corresponding (x, a) pair.\n    \"\"\"\n    results = []\n    for x, a in zip(x_values, a_values):\n        # Define a lambda function to solve for b\n        func = lambda b: eqn(b, x, a)\n        # Use fsolve to find roots, starting from two initial guesses\n        roots = fsolve(func, x0=[-10, 10])\n        # Sort roots to ensure the smaller root is first\n        roots.sort()\n        results.append(roots)\n    return np.array(results)\n\nxdata = np.arange(4) + 3\nadata = np.random.randint(0, 10, (4,\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 807, "library_problem_id": 96, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 95}}
{"id": 808, "code": ["<code>\n# Define the cumulative distribution function (CDF) for the Bekkers distribution\ndef bekkers_cdf(x: float, a: float, m: float, d: float) -> float:\n    \"\"\"\n    Calculate the cumulative distribution function value for a given x using the Bekkers distribution.\n\n    Args:\n    x (float): The value at which to evaluate the CDF.\n    a (float): The scaling parameter of the distribution.\n    m (float): The location parameter of the distribution.\n    d (float): The scale parameter of the distribution.\n\n    Returns:\n    float: The cumulative distribution function value at x.\n    \"\"\"\n    # Integrate the PDF from range_start to x to get the CDF\n    cdf_value, _ = integrate.quad(bekkers, range_start, x, args=(a, m, d))\n    return cdf_value\n\n# Perform the Kolmogorov-Smirnov test\nresult = stats.kstest(sample_data, lambda x: bekkers_cdf(x, estimated_a, estimated_m, estimated_d))\n\n# Output the result (statistic, pvalue)\nresult\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 808, "library_problem_id": 97, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 97}}
{"id": 809, "code": ["<code>\n```python\n\"\"\"\nThis script evaluates the goodness of fit for a set of experimental values using the Kolmogorov-Smirnov test (KStest).\nThe test compares the empirical distribution of the sample data with a theoretical probability density function (PDF) defined by the 'bekkers' function.\n\nFunction:\n    bekkers(x: float, a: float, m: float, d: float) -> float:\n        Computes the probability density for a given value 'x' using the Bekkers distribution with parameters 'a', 'm', and 'd'.\n\nParameters:\n    x (float): The value at which the PDF is evaluated.\n    a (float): Scale parameter of the distribution.\n    m (float): Mean parameter of the distribution.\n    d (float): Standard deviation parameter of the distribution.\n\nReturns:\n    float: The probability density at 'x'.\n\nSteps:\n1. Define the Bekkers PDF function.\n2. Use the estimated parameters from curve fitting.\n3. Generate sample data from the fitted function.\n4. Perform the Kolmogorov-Smirnov test using scipy.stats.kstest.\n5. Determine if the null hypothesis can be rejected at a 95% confidence level.\n\nReturns:\n    bool: True if the null hypothesis is rejected (indicating a poor fit), False otherwise.\n\"\"\"\n\nimport numpy as np\nimport scipy as sp\nfrom scipy import integrate, stats\n\ndef bekkers(x: float, a: float, m: float, d: float) -> float:\n    \"\"\"Calculate the Bekkers probability density function.\"\"\"\n    p\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 809, "library_problem_id": 98, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 97}}
{"id": 810, "code": ["<code>\ndef rolling_integral(df: pd.DataFrame, column_name: str, window_size: int) -> pd.Series:\n    # Ensure the DataFrame index is a datetime type\n    if not pd.api.types.is_datetime64_any_dtype(df.index):\n        raise ValueError(\"DataFrame index must be of datetime type.\")\n    \n    # Compute the rolling integral using a custom apply function\n    def integrate_window(window):\n        # Convert the index to seconds for integration\n        time_seconds = (window.index - window.index[0]).total_seconds()\n        return integrate.trapz(window[column_name], time_seconds)\n    \n    # Apply the rolling window and compute the integral\n    rolling_integrals = df[column_name].rolling(window=window_size, min_periods=1).apply(integrate_window, raw=False)\n    \n    return rolling_integrals\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 810, "library_problem_id": 99, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 811, "code": ["<code>\nimport scipy.interpolate\n\ndef interpolate_quantity(points, values, eval_point):\n    interpolated_value = scipy.interpolate.griddata(points, values, eval_point, method='linear')\n    return interpolated_value\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 811, "library_problem_id": 100, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 100}}
{"id": 812, "code": ["<code>\nimport pandas as pd\n\ndef estimate_multinomial_parameters(df: pd.DataFrame) -> dict:\n    \"\"\"\n    Estimate the parameters of a multinomial distribution using Maximum Likelihood Estimation (MLE).\n\n    Args:\n    df (pd.DataFrame): A DataFrame with a single column 'A1' containing integer values representing category occurrences.\n\n    Returns:\n    dict: A dictionary where keys are categories (integers) and values are the estimated probabilities (floats) of each category.\n    \"\"\"\n    # Count occurrences of each category\n    category_counts = df['A1'].value_counts().sort_index()\n    \n    # Calculate total number of observations\n    total_observations = category_counts.sum()\n    \n    # Compute probabilities for each category\n    probabilities = category_counts / total_observations\n    \n    # Convert to dictionary\n    probability_dict = probabilities.to_dict()\n    \n    return probability_dict\n\n# Example usage\na = pd.DataFrame({'A1': [0, 1, 2, 3, 2, 1, 6, 0, 1, 1, 7, 10]})\nweights = estimate_multinomial_parameters(a)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 812, "library_problem_id": 101, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 101}}
{"id": 813, "code": ["<code>\n```python\n\"\"\"\nOptimize a linear function with multiple parameters using scipy.optimize.\n\nThis script aims to find the optimal parameters for a linear function that minimizes the sum of squared errors\nbetween the predicted and actual values. The optimization is constrained by parameter bounds to ensure physically\nsensible solutions.\n\nImports:\n    - numpy as np: For array manipulation and mathematical operations.\n    - scipy.optimize as sciopt: For optimization functions.\n\nData:\n    - x: A 2D numpy array representing the input data.\n    - y: A 1D numpy array representing the target values.\n\nFunctions:\n    - fp(p, x): Computes the linear function with parameters p and input x.\n    - e(p, x, y): Computes the sum of squared errors between the predicted and actual values.\n\nOptimization:\n    - Uses scipy.optimize.minimize with bounds to find the optimal parameters.\n\nReturns:\n    - result: The optimization result containing the optimal parameters and other information.\n\"\"\"\n\nimport numpy as np\nimport scipy.optimize as sciopt\n\n# Input data\nx = np.array([[1247.04, 1274.9, 1277.81, 1259.51, 1246.06, 1230.2,\n               1207.37, 1192., 1180.84, 1182.76, 1194.76, 1222.65],\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 813, "library_problem_id": 102, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 814, "code": ["<code>\ndef find_relative_extrema(arr: np.ndarray, n: int) -> np.ndarray:\n    indices = []\n    for i in range(len(arr)):\n        is_extrema = True\n        for j in range(1, n+1):\n            if i-j >= 0 and arr[i] > arr[i-j]:\n                is_extrema = False\n                break\n            if i+j < len(arr) and arr[i] > arr[i+j]:\n                is_extrema = False\n                break\n        if is_extrema:\n            indices.append(i)\n    return np.array(indices)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 814, "library_problem_id": 103, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 103}}
{"id": 815, "code": ["<code>\nimport numpy as np\n\ndef find_relative_extrema(arr: np.ndarray, n: int) -> list[list[int]]:\n    # Initialize an empty list to store the indices of relative extrema\n    extrema_indices = []\n\n    # Iterate over each row in the array\n    for row_index, row in enumerate(arr):\n        # Iterate over each element in the row\n        for col_index in range(len(row)):\n            # Define the range of neighboring elements to consider\n            start = max(0, col_index - n)\n            end = min(len(row), col_index + n + 1)\n            \n            # Extract the neighboring elements\n            neighbors = row[start:end]\n            \n            # Check if the current element is a relative extrema\n            if row[col_index] <= min(neighbors):\n                extrema_indices.append([row_index, col_index])\n\n    return extrema_indices\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 815, "library_problem_id": 104, "library": "Scipy", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 103}}
{"id": 816, "code": ["<code>\n\"\"\"\nFunction to remove rows with outliers from a DataFrame based on numerical columns only.\n\nThis function identifies and removes rows from a DataFrame where any numerical column\ncontains an outlier. An outlier is defined as a value with a z-score greater than 3 or less than -3.\nThe function retains all original columns, including categorical ones, in the resulting DataFrame.\n\nArgs:\n    df (pd.DataFrame): The input DataFrame containing both numerical and categorical columns.\n\nReturns:\n    pd.DataFrame: A DataFrame with rows containing outliers in numerical columns removed.\n\nRaises:\n    TypeError: If the DataFrame contains non-numeric columns and the z-score calculation is attempted on them.\n\"\"\"\n\nfrom scipy import stats\nimport pandas as pd\nimport numpy as np\n\ndef remove_outliers(df: pd.DataFrame) -> pd.DataFrame:\n    # Select only the numeric columns from the DataFrame\n    numeric_df = df.select_dtypes(include=[np.number])\n    \n    # Calculate the z-scores of the numeric columns\n    z_scores = np.abs(stats.zscore(numeric_df))\n    \n    # Create a boolean mask where True indicates the row does not contain outliers\n    mask = (z_scores < 3).all(axis=1)\n    \n    # Apply the mask to the original DataFrame to retain all columns\n    return df[mask]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 816, "library_problem_id": 105, "library": "Scipy", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 105}}
{"id": 817, "code": ["<code>\nimport numpy as np\nfrom sklearn.datasets import load_iris\nimport pandas as pd\n\n# Load the dataset\ndata = load_iris()\n\ndef convert_bunch_to_dataframe(bunch: 'Bunch') -> pd.DataFrame:\n    \"\"\"\n    Convert a Scikit-learn Bunch object to a Pandas DataFrame.\n\n    Args:\n        bunch (Bunch): A Scikit-learn Bunch object containing 'data' and 'target' attributes.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the combined data and target from the Bunch object.\n    \"\"\"\n    # Extract data and target from the Bunch object\n    data_array = bunch.data\n    target_array = bunch.target\n\n    # Create a DataFrame from the data array\n    df = pd.DataFrame(data_array, columns=bunch.feature_names)\n\n    # Add the target array as a new column in the DataFrame\n    df['target'] = target_array\n\n    return df\n\n# Convert the Bunch object to a DataFrame\ndata1 = convert_bunch_to_dataframe(data)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 817, "library_problem_id": 0, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 818, "code": ["<code>\nfrom sklearn.datasets import load_iris\nimport pandas as pd\n\ndef convert_bunch_to_dataframe() -> pd.DataFrame:\n    \"\"\"\n    Converts the iris dataset from a scikit-learn Bunch object to a Pandas DataFrame.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the iris dataset with feature names as columns.\n    \"\"\"\n    # Load the iris dataset\n    data = load_iris()\n    \n    # Convert to DataFrame\n    df = pd.DataFrame(data.data, columns=data.feature_names)\n    \n    return df\n\n# Usage\niris_df = convert_bunch_to_dataframe()\nprint(iris_df.head())\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 818, "library_problem_id": 1, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 819, "code": ["<code>\ndef convert_bunch_to_dataframe(bunch: 'sklearn.utils.Bunch') -> pd.DataFrame:\n    \"\"\"\n    Convert a Scikit-learn Bunch object to a Pandas DataFrame.\n\n    Args:\n        bunch (sklearn.utils.Bunch): The Bunch object containing data and target.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the combined data and target from the Bunch object.\n    \"\"\"\n    # Extract data and target from the Bunch object\n    data = bunch.data\n    target = bunch.target\n    feature_names = bunch.feature_names\n\n    # Create a DataFrame from the data\n    df = pd.DataFrame(data, columns=feature_names)\n\n    # Add the target to the DataFrame\n    df['target'] = target\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 819, "library_problem_id": 2, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 0}}
{"id": 820, "code": ["<code>\ndata_array = np.c_[data['data'], data['target']]\n    columns = np.append(data['feature_names'], ['target'])\n    df = pd.DataFrame(data_array, columns=columns)\n    return df\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 820, "library_problem_id": 3, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 821, "code": ["<code>\ndef one_hot_encode_column(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n    # Extract unique elements from all lists in the specified column\n    unique_elements = set(element for sublist in df[column_name] for element in sublist)\n    \n    # Create a new DataFrame with one-hot encoded columns\n    for element in unique_elements:\n        df[element] = df[column_name].apply(lambda x: 1 if element in x else 0)\n    \n    # Drop the original column with lists\n    df.drop(columns=[column_name], inplace=True)\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 821, "library_problem_id": 4, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 822, "code": ["<code>\nimport pandas as pd\n\ndef one_hot_encode_column(df: pd.DataFrame, list_column: str) -> pd.DataFrame:\n    \"\"\"\n    Convert a DataFrame column containing lists of strings into one-hot encoded columns.\n\n    Args:\n    - df (pd.DataFrame): The input DataFrame.\n    - list_column (str): The name of the column containing lists of strings to be one-hot encoded.\n\n    Returns:\n    - pd.DataFrame: A DataFrame with the original columns and additional one-hot encoded columns.\n    \"\"\"\n    # Create a DataFrame with one-hot encoded columns for each unique string in the list column\n    one_hot_df = df[list_column].explode().str.get_dummies().groupby(level=0).sum()\n    \n    # Concatenate the original DataFrame with the one-hot encoded DataFrame\n    df_out = pd.concat([df.drop(columns=[list_column]), one_hot_df], axis=1)\n    \n    return df_out\n\n# Example usage\ndf = pd.DataFrame({\n    'Col1': ['C', 'A', 'B'],\n    'Col2': [33, 2.5, 42],\n    'Col3': [['Apple', 'Orange', 'Banana'], ['Apple', 'Grape'], ['Banana']]\n})\n\ndf_out = one_hot_encode_column(df, 'Col3')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 822, "library_problem_id": 5, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 823, "code": ["<code>\nimport pandas as pd\n\ndef one_hot_encode_last_column(df: pd.DataFrame) -> pd.DataFrame:\n    # Extract the last column containing lists\n    last_column = df.iloc[:, -1]\n    \n    # Create a DataFrame with one-hot encoding for each unique element\n    one_hot_encoded = pd.get_dummies(last_column.apply(pd.Series).stack()).sum(level=0)\n    \n    # Concatenate the original DataFrame (excluding the last column) with the one-hot encoded DataFrame\n    df_out = pd.concat([df.iloc[:, :-1], one_hot_encoded], axis=1)\n    \n    return df_out\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 823, "library_problem_id": 6, "library": "Sklearn", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 824, "code": ["<code>\ndef one_hot_encode_last_column(df: pd.DataFrame) -> pd.DataFrame:\n    # Extract the last column containing lists\n    last_column = df.iloc[:, -1]\n    \n    # Flatten the lists and find unique elements\n    unique_elements = set(element for sublist in last_column for element in sublist)\n    \n    # Create a DataFrame for one-hot encoding\n    one_hot_df = pd.DataFrame(0, index=df.index, columns=unique_elements)\n    \n    # Populate the one-hot encoded DataFrame\n    for idx, elements in enumerate(last_column):\n        one_hot_df.loc[idx, elements] = 1\n    \n    # Concatenate the original DataFrame with the one-hot encoded DataFrame\n    df_out = pd.concat([df.iloc[:, :-1], one_hot_df], axis=1)\n    \n    return df_out\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 824, "library_problem_id": 7, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Semantic", "perturbation_origin_id": 4}}
{"id": 825, "code": ["<code>\ndef one_hot_encode_last_column(df: pd.DataFrame) -> pd.DataFrame:\n    # Extract the last column containing lists\n    last_column = df.iloc[:, -1]\n    \n    # Flatten the lists and find unique elements\n    unique_elements = set(element for sublist in last_column for element in sublist)\n    \n    # Initialize a dictionary to store one-hot encoded columns\n    one_hot_dict = {element: [] for element in unique_elements}\n    \n    # Iterate over each row in the last column\n    for sublist in last_column:\n        # Create a set for quick lookup\n        sublist_set = set(sublist)\n        # Populate the one-hot encoded columns\n        for element in unique_elements:\n            one_hot_dict[element].append(1 if element in sublist_set else 0)\n    \n    # Create a DataFrame from the one-hot encoded dictionary\n    one_hot_df = pd.DataFrame(one_hot_dict)\n    \n    # Concatenate the original DataFrame (excluding the last column) with the one-hot encoded DataFrame\n    df_out = pd.concat([df.iloc[:, :-1], one_hot_df], axis=1)\n    \n    return df_out\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 825, "library_problem_id": 8, "library": "Sklearn", "test_case_cnt": 4, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 4}}
{"id": 826, "code": ["<code>\n```python\nimport numpy as np\nimport sklearn.svm as suppmach\nfrom sklearn.calibration import CalibratedClassifierCV\n\ndef load_data():\n    # Placeholder function to load data\n    pass\n\ndef fit_and_calibrate_svm(X: np.ndarray, y: np.ndarray, x_test: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Fits a LinearSVC model and calibrates it to provide probability estimates.\n\n    Args:\n        X (np.ndarray): Training feature data.\n        y (np.ndarray): Training labels.\n        x_test (np.ndarray): Test feature data.\n\n    Returns:\n        np.ndarray: Probability estimates for the test data.\n    \"\"\"\n    # Fit the LinearSVC model\n    svmmodel = suppmach.LinearSVC()\n\n    # Calibrate the model to obtain probabilities\n    calibrated_svm = CalibratedClassifierCV(base_estimator=svmmodel, cv=5)\n    calibrated_svm.fit(X, y)\n\n    # Predict probabilities for the test data\n    proba = calibrated_svm.predict_proba(x_test)\n\n    return proba\n\n# Load data\nX, y, x_test = load_data()\n\n# Ensure data is in the correct format\nassert isinstance(X, np.ndarray)\nassert isinstance(y, np.ndarray)\nassert isinstance(x_test, np.ndarray)\n\n# Obtain probability estimates\nproba = fit_\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 826, "library_problem_id": 9, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 827, "code": ["<code>\n```python\ndef load_data():\n    # Placeholder function to load data\n    # Returns:\n    #   X (np.ndarray): Feature matrix for training\n    #   y (np.ndarray): Target labels for training\n    #   x_predict (np.ndarray): Feature matrix for prediction\n    pass\n\ndef train_and_calibrate_model(X: np.ndarray, y: np.ndarray, x_predict: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Trains a LinearSVC model and calibrates it to provide probability estimates.\n\n    Args:\n        X (np.ndarray): Feature matrix for training the model.\n        y (np.ndarray): Target labels for training the model.\n        x_predict (np.ndarray): Feature matrix for which predictions are required.\n\n    Returns:\n        np.ndarray: Probability estimates for the predicted classes.\n    \"\"\"\n    # Initialize the LinearSVC model\n    base_model = svm.LinearSVC()\n\n    # Calibrate the model using CalibratedClassifierCV\n    calibrated_model = CalibratedClassifierCV(base_model, cv=5)\n\n    # Fit the calibrated model\n    calibrated_model.fit(X, y)\n\n    # Predict probabilities for the provided data\n    probabilities = calibrated_model.predict_proba(x_predict)\n\n    return probabilities\n\n# Load data\nX, y, x_predict = load_data()\n\n# Ensure data types are correct\nassert isinstance(X, np.ndarray), \"X should be a numpy array\"\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 827, "library_problem_id": 10, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 828, "code": ["<code>\nimport pandas as pd\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\ndef merge_one_hot_encoded_data(df_origin: pd.DataFrame, transform_output: csr_matrix) -> pd.DataFrame:\n    \"\"\"\n    Merges one-hot encoded data from a sparse matrix back into the original DataFrame.\n\n    Args:\n        df_origin (pd.DataFrame): The original DataFrame containing the data before transformation.\n        transform_output (csr_matrix): The one-hot encoded data in sparse matrix format.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with the one-hot encoded data merged back into the original DataFrame.\n\n    Steps:\n    1. Convert the sparse matrix to a dense format using `toarray()`.\n    2. Create a new DataFrame from the dense matrix.\n    3. Concatenate the new DataFrame with the original DataFrame using `pd.concat`.\n    4. Ensure the index of the original DataFrame is preserved in the merged DataFrame.\n    \"\"\"\n    # Convert the sparse matrix to a dense array\n    dense_array = transform_output.toarray()\n\n    # Create a DataFrame from the dense array\n    df_encoded = pd.DataFrame(dense_array, index=df_origin.index)\n\n    # Concatenate the original DataFrame with the encoded DataFrame\n    df_merged = pd.concat([df_origin, df_encoded], axis=1)\n\n    return df_merged\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 828, "library_problem_id": 11, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 11}}
{"id": 829, "code": ["<code>\n```python\nimport pandas as pd\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\ndef merge_sparse_matrix_with_dataframe(\n    df_origin: pd.DataFrame, \n    transform_output: csr_matrix, \n    column_names: list\n) -> pd.DataFrame:\n    \"\"\"\n    Converts a scipy.sparse.csr.csr_matrix to a pandas DataFrame and merges it with the original DataFrame.\n\n    Args:\n        df_origin (pd.DataFrame): The original DataFrame containing other columns.\n        transform_output (csr_matrix): The sparse matrix output from the transformation.\n        column_names (list): The list of column names for the transformed data.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the original data and the transformed data as new columns.\n\n    Raises:\n        ValueError: If the number of rows in the original DataFrame and the transformed data do not match.\n    \"\"\"\n    # Convert the sparse matrix to a dense format and then to a DataFrame\n    transformed_df = pd.DataFrame.sparse.from_spmatrix(transform_output, columns=column_names)\n\n    # Check if the number of rows match\n    if df_origin.shape[0] != transformed_df.shape[0]:\n        raise ValueError(\"The number of rows in the original DataFrame and the transformed data must match.\")\n\n    # Concatenate the original DataFrame with the transformed DataFrame\n    merged_df = pd.concat([df_origin, transformed_df], axis\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 829, "library_problem_id": 12, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 830, "code": ["<code>\ndense_array = transform_output.toarray()\n    new_df = pd.DataFrame(dense_array, columns=[f\"feature_{i}\" for i in range(dense_array.shape[1])])\n    merged_df = pd.concat([df, new_df], axis=1)\n    return merged_df\n    ### END SOLUTION\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 830, "library_problem_id": 13, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 11}}
{"id": 831, "code": ["<code>\n```python\ndef modify_pipeline(pipeline: Pipeline, action: str, step_name: str = None, step: tuple = None) -> Pipeline:\n    \"\"\"\n    Modify a sklearn Pipeline by inserting or deleting a step.\n\n    Args:\n        pipeline (Pipeline): The sklearn Pipeline object to modify.\n        action (str): The action to perform, either 'insert' or 'delete'.\n        step_name (str, optional): The name of the step to delete. Required if action is 'delete'.\n        step (tuple, optional): The step to insert, in the form (name, transformer). Required if action is 'insert'.\n\n    Returns:\n        Pipeline: The modified sklearn Pipeline object.\n\n    Raises:\n        ValueError: If the action is not 'insert' or 'delete'.\n        KeyError: If the step_name to delete is not found in the pipeline.\n    \"\"\"\n    # Retrieve the current steps from the pipeline\n    steps = pipeline.steps\n\n    if action == 'delete':\n        if step_name is None:\n            raise ValueError(\"step_name must be provided for delete action.\")\n        # Filter out the step to delete\n        steps = [(name, transformer) for name, transformer in steps if name != step_name]\n        if len(steps) == len(pipeline.steps):\n            raise KeyError(f\"Step '{step_name}' not found in the pipeline.\")\n    \n    elif action == 'insert':\n        if step is None:\n            raise ValueError(\"step must be provided for insert action.\")\n        # Insert the new step at the end of the pipeline\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 831, "library_problem_id": 14, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 14}}
{"id": 832, "code": ["<code>\n# Delete a step\nclf.steps.pop(1)  # Removes the step at index 1, which is 'dim_svm'\n\n# Insert a step\nclf.steps.insert(1, ('new_step', PCA()))  # Inserts a new step at index 1, named 'new_step'\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 832, "library_problem_id": 15, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 14}}
{"id": 833, "code": ["<code>\ndef modify_pipeline(pipeline: Pipeline, step_to_remove: str) -> Pipeline:\n    \"\"\"\n    Modify a given sklearn Pipeline by removing a specified step.\n\n    Args:\n        pipeline (Pipeline): The sklearn Pipeline object to modify.\n        step_to_remove (str): The name of the step to remove from the pipeline.\n\n    Returns:\n        Pipeline: A new Pipeline object with the specified step removed.\n    \"\"\"\n    # Extract current steps from the pipeline\n    steps = pipeline.steps\n    \n    # Filter out the step to be removed\n    modified_steps = [(name, step) for name, step in steps if name != step_to_remove]\n    \n    # Create a new pipeline with the modified steps\n    modified_pipeline = Pipeline(modified_steps)\n    \n    return modified_pipeline\n\n# Example usage\nimport numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Define initial pipeline steps\nestimators = [('reduce_dim', PCA()), ('poly', PolynomialFeatures()), ('svm', SVC())]\nclf = Pipeline(estimators)\n\n# Remove the 'poly' step\nmodified_clf = modify_pipeline(clf, 'poly')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 833, "library_problem_id": 16, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 14}}
{"id": 834, "code": ["<code>\n# Inserting a step\nclf.steps.insert(1, ('new_step', PolynomialFeatures()))\n\n# Deleting a step\ndel clf.steps[1]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 834, "library_problem_id": 17, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 17}}
{"id": 835, "code": ["<code>\n```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.decomposition import PCA\n\ndef modify_pipeline(clf, step_name, step, position='end'):\n    \"\"\"\n    Modify a pipeline by inserting or deleting a step.\n\n    Parameters:\n    clf (Pipeline): The pipeline to modify.\n    step_name (str): The name of the step to insert or delete.\n    step: The step to insert or delete.\n    position (str, optional): The position to insert the step. Default is 'end'.\n\n    Returns:\n    Pipeline: The modified pipeline.\n    \"\"\"\n    steps = clf.steps.copy()\n\n    if position == 'end':\n        steps.append((step_name, step))\n    elif position == 'beginning':\n        steps.insert(0, (step_name, step))\n    else:\n        for i, (name, _) in enumerate(steps):\n            if name == position:\n                steps.insert(i+1, (step_name, step))\n                break\n        else:\n            raise ValueError(f\"Position {position} not found in pipeline\")\n\n    return Pipeline(steps)\n\n# Create a pipeline with two steps\nclf = Pipeline([('AAA', PCA()), ('BBB', LinearSVC())])\n\n# Insert a new step at the beginning\nclf = modify_pipeline(clf, 'CCC', PCA(), position='beginning')\n\n# Delete a step\nclf = modify_pipeline(clf, 'BBB',\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 835, "library_problem_id": 18, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 17}}
{"id": 836, "code": ["<code>\nclf.steps.insert(2, ('t1919810', PCA()))\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 836, "library_problem_id": 19, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 17}}
{"id": 837, "code": ["<code>\n```python\n\"\"\"\nRefactored Prompt for DeepSeek Coder 6.7B\n\nObjective:\nPerform hyperparameter optimization using scikit-learn's GridSearchCV on an XGBoost model with early stopping to reduce search time and potentially improve prediction/regression task results.\n\nRequirements:\n- Use XGBoost via its Scikit-Learn API.\n- Implement early stopping during GridSearchCV.\n- Ensure the model works with the specified fit parameters.\n\nFunction Signature:\ndef perform_grid_search_with_early_stopping(\n    trainX: List[np.ndarray], \n    trainY: List[np.ndarray], \n    testX: List[np.ndarray], \n    testY: List[np.ndarray], \n    paramGrid: Dict[str, List[Any]], \n    cv: int, \n    n_jobs: int, \n    verbose: int\n) -> Tuple[float, np.ndarray]:\n    \"\"\"\n    Perform hyperparameter search using GridSearchCV on an XGBoost model with early stopping.\n\n    Args:\n        trainX (List[np.ndarray]): Training feature set.\n        trainY (List[np.ndarray]): Training target set.\n        testX (List[np.ndarray]): Validation feature set for early stopping.\n        testY (List[np.ndarray]): Validation target set for early stopping.\n        paramGrid (Dict[str, List[Any]]): Dictionary with parameters names (`str`) as keys and lists of parameter settings to try as values.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 837, "library_problem_id": 20, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 20}}
{"id": 838, "code": ["<code>\ndef optimize_xgboost_with_early_stopping(trainX: list, trainY: list, testX: list, testY: list, gridsearch: GridSearchCV) -> tuple:\n    \"\"\"\n    Perform hyperparameter optimization on an XGBoost model using GridSearchCV with early stopping.\n\n    Args:\n        trainX (list): Training feature data.\n        trainY (list): Training target data.\n        testX (list): Testing feature data.\n        testY (list): Testing target data.\n        gridsearch (GridSearchCV): Configured GridSearchCV instance for XGBoost.\n\n    Returns:\n        tuple: A tuple containing the best score (b) and predictions (c) from the optimized model.\n    \"\"\"\n    # Ensure the gridsearch object is correctly configured\n    assert isinstance(gridsearch, GridSearchCV)\n    \n    # Define fit parameters for early stopping\n    fit_params = {\n        \"early_stopping_rounds\": 42,\n        \"eval_metric\": \"mae\",\n        \"eval_set\": [(testX, testY)]\n    }\n    \n    # Fit the model using GridSearchCV with early stopping\n    gridsearch.fit(trainX, trainY, **fit_params)\n    \n    # Retrieve the best score and predictions\n    b = gridsearch.best_score_\n    c = gridsearch.predict(testX)\n    \n    return b, c\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 838, "library_problem_id": 21, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 20}}
{"id": 839, "code": ["<code>\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\n\ndef logistic_regression_cv_predict_proba(X: np.ndarray, y: np.ndarray, n_splits: int = 5) -> np.ndarray:\n    assert isinstance(X, np.ndarray), \"X must be a numpy array\"\n    assert isinstance(y, np.ndarray), \"y must be a numpy array\"\n    \n    cv = StratifiedKFold(n_splits=n_splits).split(X, y)\n    logreg = LogisticRegression()\n    probabilities = []\n\n    for train_idx, test_idx in cv:\n        X_train, X_test = X[train_idx], X[test_idx]\n        y_train = y[train_idx]\n        logreg.fit(X_train, y_train)\n        proba = logreg.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n        probabilities.extend(proba)\n\n    return np.array(probabilities)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 839, "library_problem_id": 22, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 22}}
{"id": 840, "code": ["<code>\nplaintext\nProblem:\n\nDevelop a Python function to compute the predicted probabilities of a Logistic Regression model using cross-validation. The function should utilize Stratified K-Folds cross-validation to ensure each fold is a good representative of the whole dataset. The predicted probabilities for each fold should be stored in a list or a NumPy array.\n\nRequirements:\n- Use the Logistic Regression model from scikit-learn.\n- Implement Stratified K-Folds cross-validation with 5 splits.\n- Ensure the input data `X` and `y` are NumPy arrays.\n- Return the predicted probabilities for each fold.\n\nFunction Signature:\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 840, "library_problem_id": 23, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 22}}
{"id": 841, "code": ["<code>\n# Transform the target variable 't' to obtain the scaled target 't_prime'\ndata['t_prime'] = scaler.transform(data[['t']])\n\n# Proceed with training a regression model on the scaled target 't_prime'\n# ...\n\n# After obtaining predictions for 't_prime', inverse the scaling\n# to retrieve the original time values\npredicted_t_prime = ...  # Predicted values from the regression model\npredicted_t = scaler.inverse_transform(predicted_t_prime)\n\n# Verify if the inverse transformation yields accurate predictions\n# compared to the original target 't'\nassert np.allclose(predicted_t, data['t']), \"Inverse transformation does not yield accurate predictions.\"\n\n# Placeholder for the inverse transformation of predicted values\ninversed = scaler.inverse_transform(predicted_t_prime)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 841, "library_problem_id": 24, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 24}}
{"id": 842, "code": ["<code>\ndef solve(data: pd.DataFrame, scaler: StandardScaler) -> np.ndarray:\n    \"\"\"\n    Scale the target variable 't' using StandardScaler, train a regression model,\n    and return the inverse-transformed predictions.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame containing the features and target variable 't'.\n        scaler (StandardScaler): An instance of StandardScaler used for scaling the target variable.\n\n    Returns:\n        np.ndarray: The inverse-transformed predictions of the target variable.\n    \"\"\"\n    # Fit the scaler on the target variable 't'\n    scaler.fit(data['t'].values.reshape(-1, 1))\n    \n    # Transform the target variable 't'\n    data['t'] = scaler.transform(data['t'].values.reshape(-1, 1))\n    \n    # Placeholder for regression model training and prediction\n    # model = train_regression_model(data)\n    # predicted_scaled_t = model.predict(data[features])\n    \n    # Inverse transform the predicted scaled values to get the original time values\n    # inversed_t = scaler.inverse_transform(predicted_scaled_t)\n    \n    # Return the inverse-transformed predictions\n    # return inversed_t\n\n    # Placeholder return statement\n    return np.array([])  # Replace with actual inverse-transformed predictions\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 842, "library_problem_id": 25, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 24}}
{"id": 843, "code": ["<code>\ndef extract_model_name(model) -> str:\n    \"\"\"\n    Extracts the class name of a scikit-learn model without its parameters.\n\n    Args:\n    - model: An instance of a scikit-learn model.\n\n    Returns:\n    - str: The name of the model class.\n    \"\"\"\n    return model.__class__.__name__\n\n# Example usage\nfor model in models:\n    scores = cross_val_score(model, X, y, cv=5)\n    model_name = extract_model_name(model)\n    print(f'Name Model: {model_name}, Mean Score: {np.mean(scores)}')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 843, "library_problem_id": 26, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 26}}
{"id": 844, "code": ["<code>\nfor model in models:\n    scores = cross_val_score(model, X, y, cv=5)\n    print(f'Name Model: {model.__class__.__name__}, Mean Score: {np.mean(scores)}')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 844, "library_problem_id": 27, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 845, "code": ["<code>\ndef get_model_name(model) -> str:\n    \"\"\"\n    Extracts the class name of a scikit-learn model object.\n\n    Args:\n    model: A scikit-learn model object. Example: LinearSVC(), RandomForestClassifier(), etc.\n\n    Returns:\n    str: The class name of the model without any parameters. Example: 'LinearSVC', 'RandomForestClassifier'.\n\n    Example:\n    >>> from sklearn.svm import LinearSVC\n    >>> model = LinearSVC()\n    >>> get_model_name(model)\n    'LinearSVC'\n    \"\"\"\n    return model.__class__.__name__\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 845, "library_problem_id": 28, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 26}}
{"id": 846, "code": ["<code>\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\nfrom sklearn.pipeline import Pipeline\nimport pandas as pd\n\ndef extract_tfidf_output(data: pd.DataFrame) -> np.ndarray:\n    \"\"\"\n    Extracts the output of the TfidfVectorizer step from a scikit-learn pipeline.\n\n    Args:\n        data (pd.DataFrame): A DataFrame containing a column 'test' with text data.\n\n    Returns:\n        np.ndarray: The output of the TfidfVectorizer after fit_transform, which is the input to the NMF step.\n    \"\"\"\n    # Initialize the pipeline with TfidfVectorizer and NMF\n    pipe = Pipeline([\n        (\"tf_idf\", TfidfVectorizer()),\n        (\"nmf\", NMF())\n    ])\n    \n    # Fit and transform the data using the TfidfVectorizer directly\n    tf_idf_out = pipe.named_steps[\"tf_idf\"].fit_transform(data['test'])\n    \n    return tf_idf_out\n\n# Example usage\ndata = pd.DataFrame([[\"Salut comment tu vas\", \"Hey how are you today\", \"I am okay and you ?\"]]).T\ndata.columns = [\"test\"]\n\ntf_idf_output = extract_tfidf_output(data)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 846, "library_problem_id": 29, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 29}}
{"id": 847, "code": ["<code>\n# Extract the intermediate output\ntf_idf_out = pipe.named_steps[\"tf_idf\"].fit_transform(data.test)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 847, "library_problem_id": 30, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 848, "code": ["<code>\nselect_out = pipe.named_steps['select'].transform(data)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 848, "library_problem_id": 31, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 29}}
{"id": 849, "code": ["<code>\n```python\n\"\"\"\nObjective:\nTrain a BaggingClassifier using a DecisionTreeClassifier as the base estimator. \nOptimize the hyperparameters of both classifiers using GridSearchCV to find the best combination of parameters.\n\nSteps:\n1. Define the parameter grid for GridSearchCV to search over.\n2. Initialize the DecisionTreeClassifier and BaggingClassifier with initial parameters.\n3. Use GridSearchCV to perform hyperparameter optimization.\n4. Fit the model on training data and obtain prediction probabilities.\n\nAssumptions:\n- The training data (X_train, y_train) is loaded and is of type numpy.ndarray.\n- The default arguments of GridSearchCV are sufficient for this task.\n\nReturns:\n- proba: numpy.ndarray, the predicted probabilities for the test data.\n\n\"\"\"\n\nimport numpy as np\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Load training data\nX_train, y_train = load_data()\nassert isinstance(X_train, np.ndarray), \"X_train must be a numpy array\"\nassert isinstance(y_train, np.ndarray), \"y_train must be a numpy array\"\n\n# Define parameter grid for hyperparameter tuning\nparam_grid = {\n    'base_estimator__max_depth': [1, 2, 3, 4, 5],  # DecisionTreeClassifier max_depth\n    'max_samples': [0.05, 0.1, 0.2, 0\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 849, "library_problem_id": 32, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 850, "code": ["<code>\n```python\n\"\"\"\nRefactor the given code to correctly fit a Random Forest Regressor model using the provided X and y data.\nEnsure that the data formats are compatible with the model requirements and resolve any errors related to mismatched dimensions.\n\nFunctionality:\n1. Load the data for X, y, and X_test.\n2. Ensure that X, y, and X_test are numpy arrays.\n3. Fit a Random Forest Regressor model using the provided X and y data.\n4. Make predictions using the fitted model on X_test data.\n\nReturns:\n    - A numpy array containing the predictions for X_test.\n\nExample:\n    >>> X, y, X_test = load_data()\n    >>> predictions = fit_random_forest_and_predict(X, y, X_test)\n\"\"\"\n\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\n\ndef fit_random_forest_and_predict(X: np.ndarray, y: np.ndarray, X_test: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Fit a Random Forest Regressor model and predict on test data.\n\n    Args:\n        X (np.ndarray): Feature matrix for training data.\n        y (np.ndarray): Target values for training data.\n        X_test (np.ndarray): Feature matrix for test data.\n\n    Returns:\n        np.ndarray: Predicted values for X_test.\n    \"\"\"\n    # Ensure that the input data are numpy arrays\n    assert isinstance(X, np.ndarray), \"X should be a numpy\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 850, "library_problem_id": 33, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 33}}
{"id": 851, "code": ["<code>\n```python\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Example data\nX = np.array([\n    [233.176], [234.270], [235.270], [523.176],\n    [237.176], [238.270], [239.270], [524.176],\n    [241.176], [242.270], [243.270], [524.176],\n    [245.176], [246.270], [247.270], [524.176]\n])\n\ny = np.array([\n    0.00, 1.36, 4.46, 8.72,\n    1.31, 1.73, 2.29, 3.12,\n    4.11, 5.07, 6.14, 7.34,\n    7.87, 8.46, 9.71, 1.07\n])\n\n# Initialize the regressor\nregressor = RandomForestRegressor(n_estimators=150, min_samples_split=2, random_state=42)\n\n# Fit the model\nrgr = regressor.fit(X, y)\n\n# Predict using the fitted model\npredict = rgr.predict(X)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 851, "library_problem_id": 34, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 33}}
{"id": 852, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Example usage of TfidfVectorizer with a custom preprocessor\ndef preprocess(s: str) -> str:\n    \"\"\"Convert the input string to uppercase.\"\"\"\n    return s.upper()\n\n# Initialize TfidfVectorizer with the custom preprocessor\ntfidf = TfidfVectorizer(preprocessor=preprocess)\n\n# Example input data\ndocuments = [\"This is a sample document.\", \"Another example document.\"]\n\n# Fit and transform the documents using the TfidfVectorizer\ntfidf_matrix = tfidf.fit_transform(documents)\n\n# Display the resulting TF-IDF matrix\nprint(tfidf_matrix.toarray())\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 852, "library_problem_id": 35, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 35}}
{"id": 853, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef prePro(text: str) -> str:\n    \"\"\"\n    Convert all uppercase letters in the input string to lowercase.\n\n    Args:\n        text (str): The input string to be processed.\n\n    Returns:\n        str: The processed string with all letters in lowercase.\n    \"\"\"\n    return text.lower()\n\n# Example usage\ntfidf = TfidfVectorizer(preprocessor=prePro)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 853, "library_problem_id": 36, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Semantic", "perturbation_origin_id": 35}}
{"id": 854, "code": ["<code>\ndef scale_dataframe(data: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Scales the input DataFrame using scikit-learn's preprocessing.scale function\n    while preserving the DataFrame's index and column names.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame to be scaled.\n\n    Returns:\n        pd.DataFrame: A new DataFrame with scaled values, retaining the original\n                      index and column names.\n    \"\"\"\n    # Implement the solution here\n    df_out = pd.DataFrame(preprocessing.scale(data), index=data.index, columns=data.columns)\n    return df_out\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 854, "library_problem_id": 37, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 37}}
{"id": 855, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\n\ndef scale_dataframe(data: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Scales the values in the DataFrame using sklearn's preprocessing.scale function.\n    \n    Args:\n        data (pd.DataFrame): The input DataFrame to be scaled.\n        \n    Returns:\n        pd.DataFrame: A new DataFrame with scaled values, retaining the original index and column names.\n    \"\"\"\n    # Scale the data using sklearn's preprocessing.scale\n    scaled_values = preprocessing.scale(data)\n    \n    # Create a new DataFrame with the scaled values, preserving index and column names\n    df_out = pd.DataFrame(scaled_values, index=data.index, columns=data.columns)\n    \n    return df_out\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 855, "library_problem_id": 38, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 37}}
{"id": 856, "code": ["<code>\ndef get_model_coefficients(grid: GridSearchCV) -> np.ndarray:\n    \"\"\"\n    Retrieve the coefficients of the best estimator from a fitted GridSearchCV object.\n\n    Args:\n        grid (GridSearchCV): A fitted GridSearchCV object containing the pipeline.\n\n    Returns:\n        np.ndarray: The coefficients of the best estimator's model.\n    \"\"\"\n    # Ensure the grid search has been fitted\n    assert hasattr(grid, 'best_estimator_'), \"GridSearchCV has not been fitted yet.\"\n\n    # Extract the best estimator's model from the pipeline\n    model = grid.best_estimator_.named_steps['model']\n\n    # Return the model's coefficients\n    return model.coef_\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 856, "library_problem_id": 39, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 857, "code": ["<code>\ndef extract_model_coefficients(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n    # Step 1: Define the pipeline\n    pipe = Pipeline([\n        (\"scale\", StandardScaler()),\n        (\"model\", RidgeClassifier(random_state=24))\n    ])\n    \n    # Step 2: Define the parameter grid for GridSearchCV\n    param_grid = {\"model__alpha\": [2e-4, 3e-3, 4e-2, 5e-1]}\n    \n    # Step 3: Initialize GridSearchCV with cross-validation\n    grid = GridSearchCV(pipe, param_grid=param_grid, cv=7)\n    \n    # Step 4: Fit the GridSearchCV with the data\n    grid.fit(X, y)\n    \n    # Step 5: Extract and return the coefficients from the best estimator\n    best_model = grid.best_estimator_\n    coefficients = best_model.named_steps['model'].coef_\n    \n    return coefficients\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 857, "library_problem_id": 40, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 858, "code": ["<code>\ndef select_features(X: pd.DataFrame, y: pd.Series) -> list[str]:\n    \"\"\"\n    Perform feature selection using ExtraTreesClassifier and SelectFromModel,\n    and return the names of the selected features.\n\n    Args:\n        X (pd.DataFrame): The feature data.\n        y (pd.Series): The target variable.\n\n    Returns:\n        list: A list of column names for the selected features.\n    \"\"\"\n    clf = ExtraTreesClassifier(random_state=42)\n    clf.fit(X, y)\n    \n    model = SelectFromModel(clf, prefit=True)\n    selected_features_mask = model.get_support()  # Boolean mask of selected features\n    selected_feature_names = X.columns[selected_features_mask].tolist()\n    \n    return selected_feature_names\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 858, "library_problem_id": 41, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 41}}
{"id": 859, "code": ["<code>\ndef get_selected_feature_names(X: pd.DataFrame, model: SelectFromModel) -> list:\n    \"\"\"\n    Get the names of the features selected by the SelectFromModel.\n\n    Args:\n        X (pd.DataFrame): The original feature DataFrame.\n        model (SelectFromModel): The fitted SelectFromModel instance.\n\n    Returns:\n        list: A list of selected feature names.\n    \"\"\"\n    # Boolean mask of selected features\n    selected_features_mask = model.get_support()\n\n    # Extract selected feature names\n    selected_feature_names = X.columns[selected_features_mask].tolist()\n    return selected_feature_names\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 859, "library_problem_id": 42, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 860, "code": ["<code>\n# Extract the names of the selected columns\nselected_column_names = X.columns[model.get_support(indices=True)]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 860, "library_problem_id": 43, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 41}}
{"id": 861, "code": ["<code>\ndef select_important_features(X: pd.DataFrame, y: pd.Series) -> list[str]:\n    \"\"\"\n    Perform feature selection using ExtraTreesClassifier and SelectFromModel.\n\n    Args:\n        X (pd.DataFrame): The feature DataFrame.\n        y (pd.Series): The target variable.\n\n    Returns:\n        list: A list of selected feature names.\n    \"\"\"\n    # Initialize and fit the ExtraTreesClassifier\n    clf = ExtraTreesClassifier(random_state=42)\n    clf.fit(X, y)\n\n    # Use SelectFromModel to select important features\n    model = SelectFromModel(clf, prefit=True)\n    selected_features_mask = model.get_support()  # Boolean mask of selected features\n\n    # Extract the names of the selected columns\n    selected_columns = X.columns[selected_features_mask].tolist()\n\n    return selected_columns\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 861, "library_problem_id": 44, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 41}}
{"id": 862, "code": ["<code>\n```python\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\ndef find_closest_samples(p: int, X: np.ndarray, n_clusters: int = 8, n_samples: int = 50) -> np.ndarray:\n    \"\"\"\n    Identifies the n_samples closest to the p-th cluster center from a fitted k-means model.\n\n    Args:\n        p (int): The index of the cluster center to find the closest samples to (0-based index).\n        X (np.ndarray): The dataset on which k-means clustering has been performed.\n        n_clusters (int, optional): The number of clusters used in k-means. Defaults to 8.\n        n_samples (int, optional): The number of closest samples to retrieve. Defaults to 50.\n\n    Returns:\n        np.ndarray: An array containing the n_samples closest to the p-th cluster center.\n\n    Raises:\n        ValueError: If p is not a valid cluster index.\n    \"\"\"\n    # Ensure the input data is a numpy array\n    assert isinstance(X, np.ndarray), \"Input data X must be a numpy array.\"\n\n    # Fit the k-means model\n    km = KMeans(n_clusters=n_clusters)\n    km.fit(X)\n\n    # Validate the cluster index\n    if p < 0 or p >= n_clusters:\n        raise ValueError(f\"Cluster index p must be between 0 and {n_clusters - 1}.\")\n\n    # Calcul\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 862, "library_problem_id": 45, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 45}}
{"id": 863, "code": ["<code>\n```python\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\ndef find_closest_samples(X: np.ndarray, p: int, n_samples: int = 50) -> np.ndarray:\n    \"\"\"\n    Identifies and returns the n_samples closest to the p-th cluster center from the dataset X.\n\n    Args:\n        X (np.ndarray): The dataset containing more than 5000 samples.\n        p (int): The index of the cluster center to which distances are calculated.\n        n_samples (int): The number of closest samples to return. Default is 50.\n\n    Returns:\n        np.ndarray: An array containing the n_samples closest to the specified cluster center.\n    \"\"\"\n    # Ensure the input is a numpy array\n    assert isinstance(X, np.ndarray), \"Input data X must be a numpy array.\"\n\n    # Fit the KMeans model\n    km = KMeans(n_clusters=p+1)\n    km.fit(X)\n\n    # Get the coordinates of the p-th cluster center\n    cluster_center_p = km.cluster_centers_[p]\n\n    # Calculate the Euclidean distance from each sample to the p-th cluster center\n    distances = np.linalg.norm(X - cluster_center_p, axis=1)\n\n    # Get the indices of the n_samples smallest distances\n    closest_indices = np.argsort(distances)[:n_samples]\n\n    # Return the samples corresponding to these indices\n    return X[closest\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 863, "library_problem_id": 46, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 864, "code": ["<code>\n```python\ndef find_closest_samples(p: int, X: np.ndarray, n_clusters: int = 8, n_samples: int = 100) -> np.ndarray:\n    \"\"\"\n    Identify the n_samples closest data points to the p-th cluster center after fitting k-means.\n\n    Args:\n        p (int): Index of the cluster center of interest (0-based index).\n        X (np.ndarray): The dataset on which k-means is fitted. Must be a 2D array.\n        n_clusters (int): The number of clusters to form as well as the number of centroids to generate.\n        n_samples (int): The number of closest samples to retrieve.\n\n    Returns:\n        np.ndarray: An array containing the n_samples closest data points to the specified cluster center.\n    \"\"\"\n    # Ensure the input data is a numpy array\n    assert isinstance(X, np.ndarray), \"Input data X must be a numpy array.\"\n\n    # Fit the k-means algorithm on the dataset\n    km = KMeans(n_clusters=n_clusters, random_state=42)\n    km.fit(X)\n\n    # Calculate the distances from each sample to the p-th cluster center\n    distances = np.linalg.norm(X - km.cluster_centers_[p], axis=1)\n\n    # Get the indices of the n_samples closest samples\n    closest_indices = np.argsort(distances)[:n_samples]\n\n    # Retrieve the closest samples\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 864, "library_problem_id": 47, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 865, "code": ["<code>\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\n\ndef load_data():\n    # This function should load and return the data and the cluster center index\n    # Replace this with the actual data loading logic\n    pass\n\ndef get_samples(p: int, X: np.ndarray, km: KMeans) -> np.ndarray:\n    \"\"\"\n    Identify the 50 samples closest to the specified cluster center.\n\n    Parameters:\n    - p (int): The index of the cluster center (0-based index).\n    - X (np.ndarray): The dataset on which k-means clustering has been performed.\n    - km (KMeans): The fitted KMeans object.\n\n    Returns:\n    - np.ndarray: An array containing the 50 samples closest to the specified cluster center.\n\n    Steps:\n    1. Ensure that the input dataset X is a numpy array.\n    2. Retrieve the coordinates of the p-th cluster center from the fitted KMeans object.\n    3. Calculate the Euclidean distance from each sample in X to the p-th cluster center.\n    4. Sort the samples based on their distance to the cluster center.\n    5. Select and return the 50 samples with the smallest distances.\n    \"\"\"\n    # Ensure the input dataset is a numpy array\n    assert isinstance(X, np.ndarray), \"X must be a numpy array\"\n\n    # Retrieve the coordinates of the p-th cluster center\n    cluster_center = km.cluster_centers_[p]\n\n    # Calculate the E\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 865, "library_problem_id": 48, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 45}}
{"id": 866, "code": ["<code>\n```python\n\"\"\"\nRefactored Prompt for DeepSeek Coder 6.7B\n\nObjective:\nTrain a GradientBoostingClassifier using categorical variables without encountering errors related to data type conversion.\n\nProblem Statement:\nThe current implementation attempts to include a categorical variable in the training data for GradientBoostingClassifier, resulting in a ValueError due to the inability to convert string data to float. The goal is to preprocess the data appropriately to enable model training without errors.\n\nRequirements:\n1. Load the Iris dataset and filter it to include only two classes.\n2. Split the data into training and testing sets (80% training, 20% testing).\n3. Introduce a fake categorical variable for testing purposes.\n4. Preprocess the categorical variable using one-hot encoding.\n5. Train a GradientBoostingClassifier on the preprocessed data.\n\nFunction Definitions:\n- load_data() -> Tuple[np.ndarray, np.ndarray]: Loads and filters the Iris dataset for two classes.\n- preprocess_data(X_train: np.ndarray) -> pd.DataFrame: Converts the training data to a DataFrame and applies one-hot encoding to the categorical variable.\n- train_model(X_train: pd.DataFrame, y_train: np.ndarray) -> GradientBoostingClassifier: Trains the GradientBoostingClassifier on the preprocessed data.\n\nSolution:\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn.ensemble import\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 866, "library_problem_id": 49, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 49}}
{"id": 867, "code": ["<code>\n```python\nfrom sklearn import datasets\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport pandas as pd\n\ndef load_data():\n    \"\"\"\n    Load and preprocess the Iris dataset for binary classification with categorical features.\n\n    Returns:\n        X_train (pd.DataFrame): Preprocessed training features with one-hot encoded categorical variables.\n        y_train (np.ndarray): Training labels.\n    \"\"\"\n    # Load the Iris dataset\n    iris = datasets.load_iris()\n\n    # Select only the first two classes for binary classification\n    X = iris.data[(iris.target == 0) | (iris.target == 1)]\n    Y = iris.target[(iris.target == 0) | (iris.target == 1)]\n\n    # Define train indices\n    train_indices = list(range(40)) + list(range(50, 90))\n\n    # Split the data into training sets\n    X_train = X[train_indices]\n    y_train = Y[train_indices]\n\n    # Convert training data to a DataFrame and add a categorical column\n    X_train = pd.DataFrame(X_train)\n    X_train[0] = ['a'] * 40 + ['b'] * 40\n\n    # One-hot encode the categorical column\n    X_train_encoded = pd.get_dummies(X_train, columns=[0])\n\n    return X_train_encoded, y_train\n\n# Load and preprocess the data\nX_train, y\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 867, "library_problem_id": 50, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 49}}
{"id": 868, "code": ["<code>\nfrom sklearn.svm import SVR\n\ndef fit_and_predict(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Fit an SVM model using a Gaussian (RBF) kernel to the provided data and predict the target values.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n        y (np.ndarray): Target vector of shape (n_samples,).\n\n    Returns:\n        np.ndarray: Predicted target values for the input feature matrix X.\n    \"\"\"\n    # Ensure input types are correct\n    assert isinstance(X, np.ndarray), \"X should be a numpy ndarray\"\n    assert isinstance(y, np.ndarray), \"y should be a numpy ndarray\"\n\n    # Initialize the SVR model with a Gaussian (RBF) kernel\n    model = SVR(kernel='rbf')\n\n    # Fit the model to the data\n    model.fit(X, y)\n\n    # Predict the target values using the fitted model\n    predictions = model.predict(X)\n\n    return predictions\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 868, "library_problem_id": 51, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 51}}
{"id": 869, "code": ["<code>\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the SVR model with a Gaussian kernel (RBF)\nsvr_model = SVR(kernel='rbf')\n\n# Fit the model to the training data\nsvr_model.fit(X_train, y_train)\n\n# Predict the target values for the test set\ny_pred = svr_model.predict(X_test)\n\n# Calculate and return the mean squared error of the predictions\nmse = mean_squared_error(y_test, y_pred)\nreturn mse\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 869, "library_problem_id": 52, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 51}}
{"id": 870, "code": ["<code>\n```python\nimport numpy as np\nfrom sklearn.svm import SVR\nfrom sklearn.datasets import make_regression\n\ndef load_data() -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Generates synthetic regression data for demonstration purposes.\n\n    Returns:\n        tuple: A tuple containing the feature matrix X and target vector y.\n    \"\"\"\n    X, y = make_regression(n_samples=100, n_features=1, noise=0.1)\n    return X, y\n\ndef fit_and_predict(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Fits an SVR model with a polynomial kernel of degree 2 to the data and predicts the target values.\n\n    Args:\n        X (np.ndarray): Feature matrix.\n        y (np.ndarray): Target vector.\n\n    Returns:\n        np.ndarray: Predicted target values for the input feature matrix X.\n    \"\"\"\n    # Ensure input types are correct\n    assert isinstance(X, np.ndarray), \"X must be a numpy array\"\n    assert isinstance(y, np.ndarray), \"y must be a numpy array\"\n\n    # Initialize SVR with polynomial kernel of degree 2\n    svr_model = SVR(kernel='poly', degree=2)\n\n    # Fit the model to the data\n    svr_model.fit(X, y)\n\n    # Predict using the fitted model\n    predictions = svr_model.predict(X)\n    return predictions\n\n# Load\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 870, "library_problem_id": 53, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 51}}
{"id": 871, "code": ["<code>\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.svm import SVR\n\ndef load_data() -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Load and return the dataset as a tuple of NumPy arrays (X, y).\n\n    Returns:\n        tuple: A tuple containing the feature matrix X and target vector y.\n    \"\"\"\n    # Placeholder for actual data loading logic\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n    return X, y\n\nX, y = load_data()\nassert isinstance(X, np.ndarray), \"X should be a NumPy array\"\nassert isinstance(y, np.ndarray), \"y should be a NumPy array\"\n\ndef perform_regression(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform regression using SVM with a polynomial kernel of degree 2.\n\n    Args:\n        X (np.ndarray): Feature matrix.\n        y (np.ndarray): Target vector.\n\n    Returns:\n        np.ndarray: Predicted values for the input features X.\n    \"\"\"\n    # Initialize the SVM regressor with a polynomial kernel of degree 2\n    model = SVR(kernel='poly', degree=2)\n    # Fit the model to the data\n    model.fit(X, y)\n    # Predict the target values for the input features\n    predictions = model.predict(X)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 871, "library_problem_id": 54, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 53}}
{"id": 872, "code": ["<code>\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef get_tf_idf_query_similarity(documents: list, queries: list) -> np.ndarray:\n    \"\"\"\n    Compute the cosine similarity between each query and a set of documents.\n\n    Args:\n        documents (list): A list of document strings.\n        queries (list): A list of query strings.\n\n    Returns:\n        np.ndarray: A 3x5 matrix where each element (i, j) is the cosine similarity\n                    between the i-th query and the j-th document.\n    \"\"\"\n    # Calculate TF-IDF for documents\n    vectorizer = TfidfVectorizer()\n    document_tfidf = vectorizer.fit_transform(documents)\n\n    # Calculate TF-IDF for queries using the same vectorizer\n    query_tfidf = vectorizer.transform(queries)\n\n    # Compute cosine similarity between each query and each document\n    cosine_similarities = cosine_similarity(query_tfidf, document_tfidf)\n\n    return cosine_similarities\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 872, "library_problem_id": 55, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 55}}
{"id": 873, "code": ["<code>\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef get_tf_idf_query_similarity(documents, queries):\n    \"\"\"\n    Computes the cosine similarity between each query and a set of documents.\n\n    Args:\n        documents (list of str): A list of document strings.\n        queries (list of str): A list of query strings.\n\n    Returns:\n        np.ndarray: A 2D array where each element [i, j] represents the cosine similarity\n                    between the i-th query and the j-th document.\n    \"\"\"\n    # Compute TF-IDF matrix for documents\n    vectorizer = TfidfVectorizer()\n    document_tfidf = vectorizer.fit_transform(documents)\n    \n    # Transform queries into TF-IDF representation\n    query_tfidf = vectorizer.transform(queries)\n    \n    # Compute cosine similarity\n    cosine_similarities = cosine_similarity(query_tfidf, document_tfidf)\n    \n    return cosine_similarities\n\n# Example usage\nqueries, documents = load_data()\ncosine_similarities_of_queries = get_tf_idf_query_similarity(documents, queries)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 873, "library_problem_id": 56, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 874, "code": ["<code>\ndef get_tf_idf_query_similarity(documents, queries):\n    \"\"\"\n    Compute the cosine similarity between each query and each document.\n\n    Args:\n        documents (List[str]): A list of document strings.\n        queries (List[str]): A list of query strings.\n\n    Returns:\n        np.ndarray: A 3x5 matrix where each element (i, j) represents the cosine similarity\n                    between the i-th query and the j-th document.\n    \"\"\"\n    # Calculate TF-IDF for documents and fit vectorizer\n    document_tfidf, vectorizer = get_term_frequency_inverse_data_frequency(documents)\n    \n    # Transform queries into the same TF-IDF space\n    query_tfidf = vectorizer.transform(queries)\n    \n    # Compute cosine similarity\n    cosine_similarities = (query_tfidf * document_tfidf.T).toarray()\n    \n    return cosine_similarities\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 874, "library_problem_id": 57, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 55}}
{"id": 875, "code": ["<code>\ndef transform_features_to_2d_array(features: list[list[str]]) -> np.ndarray:\n    \"\"\"\n    Transforms a list of variable-length feature lists into a 2D NumPy array suitable for sklearn feature selection.\n\n    Args:\n        features (list[list[str]]): A list of samples, each containing a list of feature strings.\n\n    Returns:\n        np.ndarray: A 2D array where each row represents a sample and each column represents a unique feature.\n                    The presence of a feature in a sample is indicated by 1, and its absence is indicated by 0.\n    \"\"\"\n    # Create a pandas DataFrame from the features list\n    df = pd.DataFrame(features)\n\n    # Use get_dummies function from pandas to convert categorical variable(s) into dummy/indicator variables\n    new_features = pd.get_dummies(df.stack()).sum(level=0)\n\n    # Convert the DataFrame to a NumPy array and return\n    return new_features.values\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 875, "library_problem_id": 58, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 876, "code": ["<code>\ndef convert_to_binary_matrix(features: list[list[str]]) -> pd.DataFrame:\n    \"\"\"\n    Convert a list of variable-length feature lists into a 2D binary array.\n\n    Args:\n        features (list[list[str]]): A list of samples, where each sample is a list of one-hot encoded string features.\n\n    Returns:\n        pd.DataFrame: A DataFrame where each row represents a sample and each column represents a unique feature.\n                      The value is 1 if the feature is present in the sample, otherwise 0.\n    \"\"\"\n    # Flatten the list of features and extract unique feature names\n    unique_features = sorted(set(feature for sample in features for feature in sample))\n    \n    # Initialize a DataFrame with zeros\n    binary_matrix = pd.DataFrame(0, index=range(len(features)), columns=unique_features)\n    \n    # Populate the DataFrame with 1s where features are present\n    for i, sample in enumerate(features):\n        binary_matrix.loc[i, sample] = 1\n    \n    return binary_matrix\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 876, "library_problem_id": 59, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 877, "code": ["<code>\nimport pandas as pd\nimport numpy as np\nfrom typing import List\n\ndef convert_features_to_2d_array(features: List[List[str]]) -> np.ndarray:\n    # Extract unique features across all samples\n    unique_features = sorted(set(f for sample in features for f in sample))\n    \n    # Initialize a DataFrame with zeros\n    df = pd.DataFrame(0, index=range(len(features)), columns=unique_features)\n    \n    # Populate the DataFrame with 1s where features are present\n    for i, sample in enumerate(features):\n        df.loc[i, sample] = 1\n    \n    # Convert the DataFrame to a numpy array\n    return df.to_numpy()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 877, "library_problem_id": 60, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 878, "code": ["<code>\nfrom sklearn.preprocessing import MultiLabelBinarizer\nimport numpy as np\nfrom typing import List\n\ndef solve(features: List[List[str]]) -> np.ndarray:\n    mlb = MultiLabelBinarizer()\n    return mlb.fit_transform(features)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 878, "library_problem_id": 61, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 879, "code": ["<code>\nimport pandas as pd\nimport numpy as np\n\ndef convert_features_to_2d_array(features: list[list[str]]) -> pd.DataFrame:\n    \"\"\"\n    Convert a list of variable-length feature lists into a 2D binary DataFrame.\n\n    Args:\n        features (list[list[str]]): A list where each element is a list of features (strings).\n\n    Returns:\n        pd.DataFrame: A DataFrame where each column represents a unique feature and each row\n                      corresponds to a sample, with binary values indicating the presence (1)\n                      or absence (0) of the feature.\n    \"\"\"\n    # Flatten the list of features and get unique feature names\n    unique_features = sorted(set(feature for sublist in features for feature in sublist))\n    \n    # Create a DataFrame with all zeros\n    feature_matrix = pd.DataFrame(0, index=np.arange(len(features)), columns=unique_features)\n    \n    # Populate the DataFrame with ones where features are present\n    for i, feature_list in enumerate(features):\n        feature_matrix.loc[i, feature_list] = 1\n    \n    return feature_matrix\n\n# Example usage\nfeatures = [\n    ['t1'],\n    ['t2', 't5', 't7'],\n    ['t1', 't2', 't3', 't4', 't5'],\n    ['t4', 't5', 't6']\n]\n\nnew_features = convert_features_to_2d_array(features)\nprint(new_features)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 879, "library_problem_id": 62, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 58}}
{"id": 880, "code": ["<code>\nimport numpy as np\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.spatial.distance import squareform\n\ndef hierarchical_clustering(data_matrix: np.ndarray, n_clusters: int) -> list:\n    \"\"\"\n    Perform hierarchical clustering on a given distance matrix.\n\n    Args:\n    data_matrix (np.ndarray): A 2D numpy array representing the distance matrix.\n    n_clusters (int): The number of clusters to form.\n\n    Returns:\n    list: A list of cluster labels for each professor.\n    \"\"\"\n    # Convert the distance matrix to a condensed distance matrix\n    condensed_distance_matrix = squareform(data_matrix, checks=False)\n    \n    # Perform Agglomerative Clustering\n    clustering = AgglomerativeClustering(\n        n_clusters=n_clusters,\n        affinity='precomputed',\n        linkage='complete'\n    )\n    \n    # Fit the model and get the cluster labels\n    cluster_labels = clustering.fit_predict(condensed_distance_matrix)\n    \n    return cluster_labels\n\n# Example usage\ndata_matrix = np.array([[0, 0.8, 0.9], [0.8, 0, 0.2], [0.9, 0.2, 0]])\ncluster_labels = hierarchical_clustering(data_matrix, 2)\nprint(cluster_labels)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 880, "library_problem_id": 63, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 63}}
{"id": 881, "code": ["<code>\ndef hierarchical_clustering(data_matrix: list[list[float]], n_clusters: int) -> list[int]:\n    \"\"\"\n    Perform hierarchical clustering on a given distance matrix using Agglomerative Clustering.\n\n    Args:\n        data_matrix (list[list[float]]): A square matrix representing the pairwise distances between entities.\n                                         The matrix should be symmetric with zeros on its diagonal.\n        n_clusters (int): The desired number of clusters to form.\n\n    Returns:\n        list[int]: A list of cluster labels corresponding to each entity in the distance matrix.\n\n    Example:\n        >>> data_matrix = [[0, 0.8, 0.9], [0.8, 0, 0.2], [0.9, 0.2, 0]]\n        >>> hierarchical_clustering(data_matrix, 2)\n        [0, 1, 1]\n    \"\"\"\n    import numpy as np\n    from sklearn.cluster import AgglomerativeClustering\n\n    # Convert the distance matrix to a numpy array\n    distance_matrix = np.array(data_matrix)\n\n    # Initialize the AgglomerativeClustering model with the specified number of clusters\n    model = AgglomerativeClustering(n_clusters=n_clusters, affinity='precomputed', linkage='complete')\n\n    # Fit the model to the distance matrix and obtain the cluster labels\n    cluster_labels = model.fit_predict(distance_matrix)\n\n    return cluster_labels\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 881, "library_problem_id": 64, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 882, "code": ["<code>\nimport numpy as np\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.spatial.distance import squareform\n\ndef hierarchical_clustering(simM: np.ndarray, n_clusters: int = 2) -> list:\n    \"\"\"\n    Perform hierarchical clustering on a distance matrix using AgglomerativeClustering.\n\n    Args:\n    - simM (np.ndarray): A 2D numpy array representing the distance matrix.\n    - n_clusters (int): The number of clusters to form. Default is 2.\n\n    Returns:\n    - list: A list of cluster labels for each item in the distance matrix.\n    \"\"\"\n    # Convert the distance matrix to a condensed distance matrix format\n    condensed_distances = squareform(simM)\n    \n    # Initialize the AgglomerativeClustering model with precomputed distances\n    clustering_model = AgglomerativeClustering(n_clusters=n_clusters, affinity='precomputed', linkage='complete')\n    \n    # Fit the model and predict cluster labels\n    cluster_labels = clustering_model.fit_predict(condensed_distances)\n    \n    return cluster_labels\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 882, "library_problem_id": 65, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 63}}
{"id": 883, "code": ["<code>\nimport numpy as np\nfrom scipy.cluster.hierarchy import linkage, fcluster\n\ndef hierarchical_clustering(data_matrix: list[list[float]], num_clusters: int) -> list[int]:\n    \"\"\"\n    Perform hierarchical clustering on a similarity matrix and return cluster labels.\n\n    Args:\n        data_matrix (list[list[float]]): A 2D list representing the similarity matrix.\n        num_clusters (int): The desired number of clusters.\n\n    Returns:\n        list[int]: A list of cluster labels for each item in the data matrix.\n    \"\"\"\n    # Convert the similarity matrix to a condensed distance matrix\n    distance_matrix = 1 - np.array(data_matrix)\n    \n    # Perform hierarchical clustering using the 'average' linkage method\n    Z = linkage(distance_matrix, method='average')\n    \n    # Form clusters and return the labels\n    cluster_labels = fcluster(Z, num_clusters, criterion='maxclust')\n    return cluster_labels\n\n# Example usage\ndata_matrix = [[0, 0.8, 0.9], [0.8, 0, 0.2], [0.9, 0.2, 0]]\ncluster_labels = hierarchical_clustering(data_matrix, 2)\nprint(cluster_labels)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 883, "library_problem_id": 66, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 66}}
{"id": 884, "code": ["<code>\nimport numpy as np\nfrom scipy.cluster.hierarchy import linkage, fcluster\n\ndef hierarchical_clustering(data_matrix: np.ndarray, num_clusters: int = 2) -> list:\n    \"\"\"\n    Perform hierarchical clustering on a distance matrix to form a specified number of clusters.\n\n    Args:\n        data_matrix (np.ndarray): A 2D numpy array representing the distance matrix.\n        num_clusters (int): The number of clusters to form. Default is 2.\n\n    Returns:\n        list: A list of cluster labels indicating the cluster assignment for each element.\n    \"\"\"\n    # Convert the distance matrix to a condensed distance matrix format\n    condensed_matrix = scipy.spatial.distance.squareform(data_matrix)\n    \n    # Perform hierarchical/agglomerative clustering\n    linkage_matrix = linkage(condensed_matrix, method='average')\n    \n    # Form flat clusters from the hierarchical clustering defined by the linkage matrix\n    cluster_labels = fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n    \n    return cluster_labels.tolist()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 884, "library_problem_id": 67, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 885, "code": ["<code>\nimport numpy as np\nimport scipy.cluster.hierarchy as sch\n\ndef hierarchical_clustering(similarity_matrix: np.ndarray, num_clusters: int) -> list:\n    \"\"\"\n    Perform hierarchical clustering on a similarity matrix.\n\n    Args:\n        similarity_matrix (np.ndarray): A 2D numpy array representing the similarity between items.\n        num_clusters (int): The desired number of clusters.\n\n    Returns:\n        list: A list of cluster labels corresponding to each item in the similarity matrix.\n    \"\"\"\n    # Convert the similarity matrix to a distance matrix\n    distance_matrix = 1 - similarity_matrix\n\n    # Perform hierarchical clustering using the linkage method\n    linkage_matrix = sch.linkage(distance_matrix, method='average')\n\n    # Assign cluster labels\n    cluster_labels = sch.fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n\n    return cluster_labels\n\n# Example usage\nsimM = np.array([[0, 0.6, 0.8], [0.6, 0, 0.111], [0.8, 0.111, 0]])\ncluster_labels = hierarchical_clustering(simM, 2)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 885, "library_problem_id": 68, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 66}}
{"id": 886, "code": ["<code>\nimport numpy as np\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler\n\ndef transform_data(data: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Transforms the input data by applying Box-Cox transformation, centering, and scaling.\n\n    Args:\n        data (np.ndarray): The input data array to be transformed.\n\n    Returns:\n        np.ndarray: The transformed data with reduced skewness, centered, and scaled.\n    \"\"\"\n    # Ensure input data is a NumPy ndarray\n    assert isinstance(data, np.ndarray), \"Input data must be a NumPy ndarray.\"\n\n    # Apply Box-Cox transformation\n    power_transformer = PowerTransformer(method='box-cox', standardize=False)\n    data_boxcox = power_transformer.fit_transform(data)\n\n    # Center and scale the data\n    scaler = StandardScaler()\n    centered_scaled_data = scaler.fit_transform(data_boxcox)\n\n    return centered_scaled_data\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 886, "library_problem_id": 69, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 69}}
{"id": 887, "code": ["<code>\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\ndef transform_data(data: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Transforms the input data by scaling and centering to reduce skewness.\n\n    Args:\n    data (np.ndarray): A NumPy array containing the dataset to be transformed.\n\n    Returns:\n    np.ndarray: A NumPy array with the data transformed by scaling and centering.\n\n    Conditions:\n    - The input data must be a NumPy array.\n    - The transformation should eliminate skewness by centering and scaling the data.\n    \"\"\"\n    assert isinstance(data, np.ndarray), \"Input data must be a NumPy array.\"\n    \n    scaler = StandardScaler()\n    centered_scaled_data = scaler.fit_transform(data)\n    \n    return centered_scaled_data\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 887, "library_problem_id": 70, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 69}}
{"id": 888, "code": ["<code>\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import PowerTransformer\n\ndef apply_box_cox_transformation(data: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Apply Box-Cox transformation to the input data to reduce skewness.\n\n    Args:\n        data (np.ndarray): A 2D numpy array where each column represents a feature.\n\n    Returns:\n        np.ndarray: Transformed data with reduced skewness.\n\n    Conditions:\n        - The input data must be positive since Box-Cox transformation requires positive values.\n        - The transformation will also center and scale the data.\n\n    Example:\n        >>> data = np.random.rand(1000, 2) * 10  # Example data\n        >>> transformed_data = apply_box_cox_transformation(data)\n    \"\"\"\n    assert isinstance(data, np.ndarray), \"Input data must be a numpy array.\"\n    assert np.all(data > 0), \"All elements in the data must be positive for Box-Cox transformation.\"\n\n    # Initialize PowerTransformer with method='box-cox'\n    transformer = PowerTransformer(method='box-cox', standardize=True)\n    \n    # Fit and transform the data\n    transformed_data = transformer.fit_transform(data)\n    \n    return transformed_data\n\n# Example usage\ndata = np.random.rand(1000, 2) * 10  # Example data\nbox_cox_data = apply_box_cox_transformation\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 888, "library_problem_id": 71, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 71}}
{"id": 889, "code": ["<code>\nimport numpy as np\nfrom scipy.stats import boxcox\n\ndef perform_box_cox_transformation(data: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform a Box-Cox transformation on the input data to reduce skewness.\n\n    Parameters:\n    - data (np.ndarray): A NumPy array containing the data to be transformed. \n      The data must be positive as required by the Box-Cox transformation.\n\n    Returns:\n    - np.ndarray: A NumPy array containing the transformed data.\n\n    Raises:\n    - ValueError: If the input data contains non-positive values.\n    \"\"\"\n    # Ensure the input data is positive\n    if np.any(data <= 0):\n        raise ValueError(\"Data must be positive for Box-Cox transformation.\")\n    \n    # Perform Box-Cox transformation using scipy\n    transformed_data, _ = boxcox(data)\n    \n    return transformed_data\n\n# Load your data\ndata = load_data()\nassert isinstance(data, np.ndarray)\n\n# Perform the transformation\nbox_cox_data = perform_box_cox_transformation(data)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 889, "library_problem_id": 72, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 71}}
{"id": 890, "code": ["<code>\nimport numpy as np\nfrom sklearn.preprocessing import PowerTransformer\n\ndef perform_yeo_johnson_transformation(data: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Apply the Yeo-Johnson transformation to the input data to reduce skewness.\n\n    Args:\n        data (np.ndarray): The input data array to be transformed. \n                           It should be a 2D array where each column represents a feature.\n\n    Returns:\n        np.ndarray: The transformed data with reduced skewness.\n\n    Raises:\n        ValueError: If the input data is not a 2D numpy array.\n    \"\"\"\n    if not isinstance(data, np.ndarray) or data.ndim != 2:\n        raise ValueError(\"Input data must be a 2D numpy array.\")\n\n    # Initialize the PowerTransformer with the Yeo-Johnson method\n    transformer = PowerTransformer(method='yeo-johnson', standardize=True)\n\n    # Fit and transform the data\n    transformed_data = transformer.fit_transform(data)\n\n    return transformed_data\n\n# Example usage\ndata = np.array([[1, 2, 3], [4, 5, 6]])  # Replace with actual data loading logic\nassert isinstance(data, np.ndarray), \"Data must be a numpy array.\"\n\nyeo_johnson_data = perform_yeo_johnson_transformation(data)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 890, "library_problem_id": 73, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 73}}
{"id": 891, "code": ["<code>\nimport numpy as np\nfrom sklearn.preprocessing import PowerTransformer\n\ndef apply_yeo_johnson_transformation(data: np.ndarray) -> np.ndarray:\n    if not isinstance(data, np.ndarray) or data.ndim != 2:\n        raise ValueError(\"Input data must be a 2D NumPy array.\")\n\n    transformer = PowerTransformer(method='yeo-johnson', standardize=True)\n\n    transformed_data = transformer.fit_transform(data)\n\n    return transformed_data\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 891, "library_problem_id": 74, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 73}}
{"id": 892, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef load_data() -> str:\n    \"\"\"\n    Load and return the text data as a string.\n    \n    Returns:\n        str: The text data to be processed.\n    \"\"\"\n    # Implementation to load data\n    pass\n\ndef preserve_punctuation(text: str) -> np.ndarray:\n    \"\"\"\n    Transform the input text to preserve specific punctuation marks (!, ?, \", ') \n    using CountVectorizer from scikit-learn.\n\n    Args:\n        text (str): The input text data.\n\n    Returns:\n        np.ndarray: The transformed text data as a feature matrix.\n    \"\"\"\n    # Define the token pattern to include the specified punctuation marks\n    token_pattern = r\"(?u)\\b\\w+\\b|[!?\\\"']\"\n    \n    # Initialize CountVectorizer with the custom token pattern\n    vectorizer = CountVectorizer(token_pattern=token_pattern)\n    \n    # Fit and transform the text data\n    transformed_text = vectorizer.fit_transform([text])\n    \n    return transformed_text.toarray()\n\ntext = load_data()\ntransformed_text = preserve_punctuation(text)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 892, "library_problem_id": 75, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 75}}
{"id": 893, "code": ["<code>\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef load_data(file_path: str) -> pd.DataFrame:\n    \"\"\"\n    Load the dataset from a CSV file without headers.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded dataset.\n    \"\"\"\n    return pd.read_csv(file_path, header=None, sep=',')\n\ndef split_dataset(dataset: pd.DataFrame, test_size: float = 0.2, random_state: int = 42) -> tuple:\n    \"\"\"\n    Split the dataset into training and testing sets, and further into features and target.\n\n    Args:\n        dataset (pd.DataFrame): The dataset to split.\n        test_size (float): The proportion of the dataset to include in the test split.\n        random_state (int): Controls the shuffling applied to the data before applying the split.\n\n    Returns:\n        tuple: A tuple containing x_train, x_test, y_train, y_test.\n    \"\"\"\n    # Separate features and target\n    x = dataset.iloc[:, :-1]\n    y = dataset.iloc[:, -1]\n\n    # Split the dataset into training and testing sets\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=random_state)\n\n    return x_train\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 893, "library_problem_id": 76, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 76}}
{"id": 894, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef split_dataframe(data: pd.DataFrame) -> tuple:\n    \"\"\"\n    Splits a DataFrame into training and testing sets, and further divides each set into features and target.\n\n    Args:\n        data (pd.DataFrame): The input DataFrame where the last column is the target variable.\n\n    Returns:\n        tuple: A tuple containing four elements:\n            - x_train (pd.DataFrame): Training set features.\n            - x_test (pd.DataFrame): Testing set features.\n            - y_train (pd.Series): Training set target.\n            - y_test (pd.Series): Testing set target.\n    \"\"\"\n    # Extract features and target\n    x = data.iloc[:, :-1]\n    y = data.iloc[:, -1]\n\n    # Split the data into training and testing sets\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n    return x_train, x_test, y_train, y_test\n\n# Example usage\ndata = load_data()\nx_train, x_test, y_train, y_test = split_dataframe(data)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 894, "library_problem_id": 77, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 895, "code": ["<code>\n```python\nimport numpy as np\nimport pandas as pd\n\ndef split_dataset(dataset: pd.DataFrame, test_size: float = 0.4, random_state: int = 42) -> tuple:\n    \"\"\"\n    Splits the dataset into training and testing sets, and further splits each set into features and target labels.\n\n    Args:\n        dataset (pd.DataFrame): The dataset to be split, with the last column as the target class.\n        test_size (float): The proportion of the dataset to include in the test split. Default is 0.4 (3:2 ratio).\n        random_state (int): Controls the shuffling applied to the data before applying the split. Default is 42.\n\n    Returns:\n        tuple: A tuple containing four elements:\n            - x_train (pd.DataFrame): Training set features.\n            - x_test (pd.DataFrame): Testing set features.\n            - y_train (pd.Series): Training set target labels.\n            - y_test (pd.Series): Testing set target labels.\n    \"\"\"\n    from sklearn.model_selection import train_test_split\n\n    # Separate features and target\n    x = dataset.iloc[:, :-1]\n    y = dataset.iloc[:, -1]\n\n    # Split the dataset\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=random_state)\n\n    return x_train, x_test, y_train, y_test\n\n# Example usage\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 895, "library_problem_id": 78, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 896, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom typing import Tuple\n\ndef solve(data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Splits the input dataset into training and testing sets, and further splits\n    each set into features (x) and target (y).\n\n    Args:\n    - data (pd.DataFrame): The input dataset with no headers, where the last column\n      is the target class and the rest are feature columns.\n\n    Returns:\n    - Tuple containing:\n      - x_train (np.ndarray): Training set features.\n      - y_train (np.ndarray): Training set target.\n      - x_test (np.ndarray): Testing set features.\n      - y_test (np.ndarray): Testing set target.\n    \"\"\"\n    # Split the dataset into features and target\n    x = data.iloc[:, :-1].values  # All columns except the last one\n    y = data.iloc[:, -1].values   # The last column\n\n    # Split the data into training and testing sets with an 80/20 ratio\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n    return x_train, y_train, x_test, y_test\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 896, "library_problem_id": 79, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 76}}
{"id": 897, "code": ["<code>\ndef cluster_mse_values(file_path: str) -> np.ndarray:\n    import pandas as pd\n    import numpy as np\n    from sklearn.cluster import KMeans\n    import matplotlib.pyplot as plt\n\n    # Load the CSV file\n    df = pd.read_csv(file_path, parse_dates=[\"date\"])\n    \n    # Extract MSE values\n    mse_values = df['mse'].values\n    \n    # Reshape MSE values for K-Means input\n    mse_values_reshaped = mse_values.reshape(-1, 1)\n    \n    # Apply K-Means clustering\n    kmeans = KMeans(n_clusters=2, n_init=10)\n    kmeans.fit(mse_values_reshaped)\n    labels = kmeans.labels_\n    \n    # Get centroid values\n    centroids = kmeans.cluster_centers_\n    \n    # Plot the results\n    plt.scatter(mse_values, np.zeros_like(mse_values), c=labels, cmap='viridis')\n    plt.scatter(centroids, np.zeros_like(centroids), marker='*', c='red', s=200)\n    plt.title('K-Means Clustering of MSE Values')\n    plt.xlabel('MSE')\n    plt.show()\n    \n    return labels\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 897, "library_problem_id": 80, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 80}}
{"id": 898, "code": ["<code>\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport pandas as pd\n\ndef cluster_mse_values(file_path: str) -> np.ndarray:\n    \"\"\"\n    Clusters 'mse' values from a CSV file into two clusters using KMeans.\n\n    Args:\n        file_path (str): The path to the CSV file containing 'date' and 'mse' columns.\n\n    Returns:\n        np.ndarray: An array of cluster labels for each 'mse' value.\n    \"\"\"\n    # Load the CSV file\n    df = pd.read_csv(file_path, parse_dates=[\"date\"])\n    \n    # Extract 'mse' values\n    mse_values = df['mse'].values\n    \n    # Create a second feature as a range of integers\n    range_values = np.arange(len(mse_values))\n    \n    # Combine features into a 2D array\n    X = np.column_stack((mse_values, range_values))\n    \n    # Apply KMeans clustering\n    kmeans = KMeans(n_clusters=2, n_init=10, random_state=42)\n    kmeans.fit(X)\n    \n    # Predict cluster labels\n    labels = kmeans.labels_\n    \n    # Calculate centroids (optional)\n    centroids = kmeans.cluster_centers_\n    \n    return labels\n\n# Example usage\nlabels = cluster_mse_values(\"file.csv\")\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 898, "library_problem_id": 81, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 80}}
{"id": 899, "code": ["<code>\n```python\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\n\ndef retrieve_selected_feature_names(corpus: list, y: list) -> list:\n    \"\"\"\n    Perform feature selection using LinearSVC with L1 penalty and retrieve the names of selected features.\n\n    Args:\n        corpus (list): A list of text documents for feature extraction.\n        y (list): A list of target labels corresponding to the documents.\n\n    Returns:\n        list: A list of selected feature names.\n\n    Raises:\n        AssertionError: If the input corpus or y is not of type list.\n    \"\"\"\n    assert isinstance(corpus, list), \"corpus must be a list\"\n    assert isinstance(y, list), \"y must be a list\"\n\n    # Initialize the vectorizer and transform the corpus into a feature matrix\n    vectorizer = TfidfVectorizer()\n    X = vectorizer.fit_transform(corpus)\n\n    # Initialize LinearSVC with L1 penalty\n    lsvc = LinearSVC(penalty='l1', dual=False, max_iter=1000)\n    lsvc.fit(X, y)\n\n    # Retrieve the indices of non-zero coefficients (selected features)\n    selected_indices = np.where(lsvc.coef_ != 0)[1]\n\n    # Get the feature names and filter by selected indices\n    feature_names = np.array(vectorizer.get_feature_names_\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 899, "library_problem_id": 82, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 82}}
{"id": 900, "code": ["<code>\n```python\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectFromModel\n\ndef select_features_with_linear_svc(corpus: list, y: list) -> np.ndarray:\n    \"\"\"\n    Perform feature selection using LinearSVC with L1 penalty.\n\n    Args:\n    - corpus (list): A list of text documents.\n    - y (list): A list of target labels corresponding to the documents.\n\n    Returns:\n    - np.ndarray: An array of selected feature names.\n    \"\"\"\n    vectorizer = TfidfVectorizer()\n    X = vectorizer.fit_transform(corpus)\n\n    # Initialize LinearSVC with L1 penalty\n    lsvc = LinearSVC(penalty='l1', dual=False, max_iter=10000)\n\n    # Create a pipeline with feature selection\n    pipeline = Pipeline([\n        ('vectorizer', vectorizer),\n        ('feature_selector', SelectFromModel(lsvc))\n    ])\n\n    # Fit the pipeline\n    pipeline.fit(corpus, y)\n\n    # Get the mask of selected features\n    feature_mask = pipeline.named_steps['feature_selector'].get_support()\n\n    # Retrieve the selected feature names\n    selected_feature_names = np.array(vectorizer.get_feature_names_out())[feature_mask]\n\n    return selected_feature_names\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 900, "library_problem_id": 83, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 901, "code": ["<code>\n```python\n\"\"\"\nFunction to perform feature selection using a linear model penalized with the L1 norm and retrieve the names of the selected features.\n\nThis function demonstrates how to use sklearn's LinearSVC with an L1 penalty to perform feature selection on a given corpus of text data. It retrieves the names of the features that are selected by the model.\n\nArgs:\n    corpus (list of str): A list of text documents to be vectorized.\n    y (list): A list of target labels corresponding to the documents in the corpus.\n    vectorizer (TfidfVectorizer): An instance of TfidfVectorizer used to transform the corpus into a feature matrix.\n    X (scipy.sparse.csr_matrix): The feature matrix obtained by transforming the corpus using the vectorizer.\n\nReturns:\n    list of str: A list of feature names that are selected by the LinearSVC model with an L1 penalty.\n\nNote:\n    - The LinearSVC model is used with penalty='l1' and default arguments for other parameters unless necessary.\n    - The function assumes that the corpus and y are both lists and that the vectorizer has already been fitted to the corpus.\n\"\"\"\n\nimport numpy as np\nfrom sklearn.svm import LinearSVC\n\ndef solve(corpus: list, y: list, vectorizer: TfidfVectorizer, X) -> list:\n    # Initialize the LinearSVC model with L1 penalty\n    model = LinearSVC(penalty='l1', dual=False, max_iter=1000)\n\n    # Fit the model to the data\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 901, "library_problem_id": 84, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 82}}
{"id": 902, "code": ["<code>\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom typing import List, Tuple\nimport numpy as np\n\ndef vectorize_corpus(corpus: List[str], vocabulary: List[str]) -> Tuple[List[str], np.ndarray]:\n    vectorizer = CountVectorizer(\n        stop_words=\"english\",\n        binary=True,\n        lowercase=False,\n        vocabulary=vocabulary\n    )\n    X = vectorizer.fit_transform(corpus)\n    feature_names = vectorizer.get_feature_names_out()\n    return feature_names, X.toarray()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 902, "library_problem_id": 85, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 85}}
{"id": 903, "code": ["<code>\n```python\n\"\"\"\nRefactor the function to vectorize text data using sklearn's CountVectorizer with a custom vocabulary.\nEnsure the vocabulary order is preserved as specified.\n\nFunction: vectorize_corpus\n\nArguments:\n- corpus: List[str] - A list of text documents to be vectorized.\n- vocabulary: List[str] - A list of keywords defining the vocabulary for vectorization.\n\nReturns:\n- Tuple[List[str], np.ndarray] - A tuple containing the list of feature names in the specified order and the binary matrix representation of the corpus.\n\nSteps:\n1. Initialize CountVectorizer with the given vocabulary, stop words, and binary flag.\n2. Fit and transform the corpus using the vectorizer.\n3. Retrieve feature names in the order specified by the vocabulary.\n4. Convert the transformed corpus to a binary array.\n\nExample:\ncorpus = [\n    'We are looking for Java developer',\n    'Frontend developer with knowledge in SQL and Jscript',\n    'And this is the third one.',\n    'Is this the first document?',\n]\nvocabulary = ['Jscript', '.Net', 'TypeScript', 'NodeJS', 'Angular', 'Mongo', 'CSS', 'Python', 'PHP', 'Photoshop', 'Oracle', 'Linux', 'C++', 'Java', 'TeamCity', 'Frontend', 'Backend', 'Full stack', 'UI Design', 'Web', 'Integration', 'Database design', 'UX']\n\nfeature_names, X = vectorize_corpus(corpus, vocabulary)\n\"\"\"\n\nimport\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 903, "library_problem_id": 86, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 85}}
{"id": 904, "code": ["<code>\n```python\n\"\"\"\nVectorize a given text corpus using sklearn's CountVectorizer with specific properties.\n\nThe function should:\n1. Use a predefined vocabulary for vectorization.\n2. Ensure the vocabulary order is preserved in the output.\n3. Transform the corpus into a binary matrix representation.\n\nArgs:\n    corpus (List[str]): A list of text documents to be vectorized.\n\nReturns:\n    Tuple[List[str], np.ndarray]: A tuple containing:\n        - A list of feature names in the order specified by the vocabulary.\n        - A binary matrix representation of the corpus, where each row corresponds to a document and each column corresponds to a feature from the vocabulary.\n\nExample:\n    Given the corpus:\n    [\n        'We are looking for Java developer',\n        'Frontend developer with knowledge in SQL and Jscript',\n        'And this is the third one.',\n        'Is this the first document?',\n    ]\n\n    And the vocabulary:\n    {'Jscript', '.Net', 'TypeScript', 'SQL', 'NodeJS', 'Angular', 'Mongo', 'CSS', 'Python', 'PHP', 'Photoshop', 'Oracle', 'Linux', 'C++', 'Java', 'TeamCity', 'Frontend', 'Backend', 'Full stack', 'UI Design', 'Web', 'Integration', 'Database design', 'UX'}\n\n    The function should return:\n    (['Jscript', '.Net', 'TypeScript', 'SQL', 'NodeJS', 'Angular', 'Mongo', 'CSS', 'Python', 'PHP', 'Photoshop',\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 904, "library_problem_id": 87, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 85}}
{"id": 905, "code": ["<code>\n```python\n\"\"\"\nProblem Statement:\nYou are tasked with vectorizing a given corpus of text data using the sklearn.feature_extraction.text.CountVectorizer. \nThe corpus consists of job descriptions, and you want to use a predefined vocabulary to create binary feature vectors. \nThe vocabulary should not be sorted alphabetically in the output, and the resulting feature matrix should reflect \nthe presence of vocabulary terms in the corpus.\n\nCorpus:\ncorpus = [\n    'We are looking for Java developer',\n    'Frontend developer with knowledge in SQL and Jscript',\n    'And this is the third one.',\n    'Is this the first document?',\n]\n\nVectorizer Properties:\n- stop_words: 'english'\n- binary: True\n- lowercase: False\n- vocabulary: {'Jscript', '.Net', 'TypeScript', 'NodeJS', 'Angular', 'Mongo', 'CSS', 'Python', 'PHP', \n               'Photoshop', 'Oracle', 'Linux', 'C++', 'Java', 'TeamCity', 'Frontend', 'Backend', \n               'Full stack', 'UI Design', 'Web', 'Integration', 'Database design', 'UX'}\n\nObjective:\n1. Ensure the vocabulary order is maintained as specified.\n2. Create a binary feature matrix that indicates the presence of each vocabulary term in the corpus.\n3. The output feature matrix should match the desired structure as closely as possible.\n\nSolution:\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 905, "library_problem_id": 88, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 85}}
{"id": 906, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndef calculate_slopes(df1: pd.DataFrame, columns: list) -> np.ndarray:\n    slopes = np.array([])  # Initialize an empty array to store slope coefficients\n    \n    for col in columns:\n        # Remove NaN values for the current column\n        df2 = df1[['Time', col]].dropna()\n        \n        # Prepare the data for linear regression\n        X = df2[['Time']].values\n        Y = df2[col].values\n        \n        # Perform linear regression\n        model = LinearRegression().fit(X, Y)\n        \n        # Extract the slope coefficient\n        slope = model.coef_[0]\n        \n        # Append the slope to the slopes array\n        slopes = np.append(slopes, slope)\n    \n    return slopes\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 906, "library_problem_id": 89, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 89}}
{"id": 907, "code": ["<code>\n```python\n\"\"\"\nFunction to compute the slope coefficients of linear regressions for each column in a DataFrame\nstarting from a specified column, excluding NaN values, and store the results in a 1D numpy array.\n\nArgs:\n    df1 (pd.DataFrame): The input DataFrame containing time series data with columns to be processed.\n    start_col (str): The column name from which to start processing (e.g., 'A1').\n\nReturns:\n    np.ndarray: A 1D numpy array containing the slope coefficients for each column starting from 'start_col'.\n\nNotes:\n    - The DataFrame is expected to have a 'Time' column and multiple other columns with potential NaN values.\n    - The function iterates over each column starting from 'start_col' and performs linear regression\n      using 'Time' as the independent variable and the column values as the dependent variable.\n    - NaN values are excluded from the regression calculation.\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\ndef compute_slopes(df1: pd.DataFrame, start_col: str) -> np.ndarray:\n    slopes = []\n    start_index = df1.columns.get_loc(start_col)\n    \n    for col in df1.columns[start_index:]:\n        df2 = df1[['Time', col]].dropna()\n        if df2.empty:\n            slopes.append(np.nan)\n            continue\n        \n        X = df2[['Time']].values\n        Y = df2[col].values\n        model\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 907, "library_problem_id": 90, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 89}}
{"id": 908, "code": ["<code>\ndef transform_sex_column(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Transform the 'Sex' column in the DataFrame using LabelEncoder.\n\n    Args:\n        df (pd.DataFrame): The DataFrame containing the Titanic dataset.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the 'Sex' column transformed.\n    \"\"\"\n    # Initialize the LabelEncoder\n    label_encoder = LabelEncoder()\n    \n    # Fit the LabelEncoder to the 'Sex' column and transform it\n    df['Sex'] = label_encoder.fit_transform(df['Sex'])\n    \n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 908, "library_problem_id": 91, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 91}}
{"id": 909, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef transform_sex_column(file_path: str) -> pd.DataFrame:\n    \"\"\"\n    Transforms the 'Sex' column in the DataFrame loaded from the specified CSV file.\n    \n    Args:\n    - file_path (str): The path to the CSV file containing the data.\n    \n    Returns:\n    - pd.DataFrame: A DataFrame with the 'Sex' column transformed, where 'male' is encoded as 1 and 'female' as 0.\n    \"\"\"\n    # Load the data from the CSV file\n    df = pd.read_csv(file_path)\n    \n    # Create an instance of LabelEncoder\n    label_encoder = LabelEncoder()\n    \n    # Fit the LabelEncoder to the 'Sex' column and transform it\n    df['Sex'] = label_encoder.fit_transform(df['Sex'])\n    \n    return df\n\n# Example usage\ntransformed_df = transform_sex_column('data.csv')\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 909, "library_problem_id": 92, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 910, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef load_data() -> pd.DataFrame:\n    \"\"\"\n    Load the Titanic dataset from a CSV file.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the Titanic dataset.\n    \"\"\"\n    return pd.read_csv('titanic.csv')\n\ndef transform_sex_column(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Transform the 'Sex' column in the DataFrame using LabelEncoder.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame containing the 'Sex' column.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the 'Sex' column transformed.\n    \"\"\"\n    # Initialize the LabelEncoder\n    label_encoder = LabelEncoder()\n    \n    # Fit and transform the 'Sex' column\n    df['Sex'] = label_encoder.fit_transform(df['Sex'])\n    \n    return df\n\n# Load the data\ndf = load_data()\n\n# Transform the 'Sex' column\ndf = transform_sex_column(df)\n\n# The DataFrame 'df' now has the 'Sex' column encoded as 0 and 1\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 910, "library_problem_id": 93, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 91}}
{"id": 911, "code": ["<code>\n```python\n\"\"\"\nElastic Net Regression Correction\n\nThis prompt addresses the issue of a NameError encountered when running an Elastic Net regression due to an undefined 'sklearn'. \nThe task is to correct the code and ensure it runs without errors. The corrected code should import necessary libraries, \nload the data, and perform the regression analysis, outputting the R^2 scores for both training and test datasets.\n\nFunctionality:\n- Import necessary libraries for Elastic Net regression.\n- Load training and test datasets.\n- Fit an Elastic Net model to the training data.\n- Calculate and print R^2 scores for both training and test datasets.\n\nAssumptions:\n- The data loading function `load_data()` is defined elsewhere and returns numpy arrays for X_train, y_train, X_test, and y_test.\n\nReturns:\n- training_set_score (float): R^2 score for the training dataset.\n- test_set_score (float): R^2 score for the test dataset.\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import ElasticNet\nimport statsmodels.api as sm\n\ndef run_elastic_net_regression():\n    # Load data\n    X_train, y_train, X_test, y_test = load_data()\n    \n    # Ensure data is in the correct format\n    assert isinstance(X_train, np.ndarray), \"X_train must be a numpy array\"\n    assert isinstance(y_train, np.ndarray), \"y_train must be a numpy array\"\n    assert isinstance(X_test, np.ndarray),\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 911, "library_problem_id": 94, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 94}}
{"id": 912, "code": ["<code>\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef normalize_entire_array(np_array: np.ndarray) -> np.ndarray:\n    # Reshape the array to a single column for global normalization\n    reshaped_array = np_array.reshape(-1, 1)\n    \n    # Initialize MinMaxScaler\n    scaler = MinMaxScaler()\n    \n    # Fit and transform the reshaped array\n    normalized_array = scaler.fit_transform(reshaped_array)\n    \n    # Reshape back to the original 2x2 shape\n    return normalized_array.reshape(np_array.shape)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 912, "library_problem_id": 95, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 95}}
{"id": 913, "code": ["<code>\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef normalize_entire_array(np_array: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Normalize the entire numpy array using MinMaxScaler as a single entity.\n\n    Args:\n    np_array (np.ndarray): A 3x3 numpy array containing the data to be normalized.\n\n    Returns:\n    np.ndarray: A 3x3 numpy array with normalized values ranging from 0 to 1.\n\n    Steps:\n    1. Reshape the array to a single dimension to treat it as a single entity.\n    2. Apply MinMaxScaler to the reshaped array.\n    3. Reshape the normalized data back to the original 3x3 shape.\n    \"\"\"\n    # Reshape the array to a single column vector\n    reshaped_array = np_array.reshape(-1, 1)\n    \n    # Initialize MinMaxScaler\n    scaler = MinMaxScaler()\n    \n    # Fit and transform the reshaped array\n    normalized_data = scaler.fit_transform(reshaped_array)\n    \n    # Reshape the normalized data back to the original 3x3 shape\n    transformed = normalized_data.reshape(np_array.shape)\n    \n    return transformed\n\n# Example usage\nnp_array = load_data()  # Assume load_data() returns a 3x3 numpy array\ntransformed = normalize_entire_array(np_array)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 913, "library_problem_id": 96, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 914, "code": ["<code>\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef normalize_array(a: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Normalize the entire 2x2 numpy array as a whole using MinMaxScaler.\n\n    Args:\n    - a (np.ndarray): A 2x2 numpy array to be normalized.\n\n    Returns:\n    - np.ndarray: A 2x2 numpy array with normalization applied to the entire array.\n    \"\"\"\n    # Flatten the array to apply MinMaxScaler across all elements\n    flattened = a.flatten().reshape(-1, 1)\n    \n    # Initialize MinMaxScaler\n    scaler = MinMaxScaler()\n    \n    # Fit and transform the flattened array\n    normalized_flattened = scaler.fit_transform(flattened)\n    \n    # Reshape the normalized array back to 2x2\n    normalized_array = normalized_flattened.reshape(a.shape)\n    \n    return normalized_array\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 914, "library_problem_id": 97, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 95}}
{"id": 915, "code": ["<code>\n```python\n\"\"\"\nFunction to predict stock market trends using a Decision Tree Regressor.\n\nThis function utilizes historical stock data to predict future trends based on\nthe last closing price and moving averages. It addresses issues with data\npreparation and model prediction, ensuring compatibility with the input data\ndimensions.\n\nImports:\n    - tree from sklearn: For Decision Tree Regressor.\n    - pandas as pd: For data manipulation.\n    - pandas_datareader as web: For fetching stock data.\n    - numpy as np: For numerical operations.\n\nData:\n    - Stock data for 'GOOG' from Yahoo Finance between '2012-05-01' and '2016-05-20'.\n\nSteps:\n    1. Fetch stock data and calculate buy/sell signals.\n    2. Prepare feature and target datasets.\n    3. Train a Decision Tree Regressor.\n    4. Predict using the trained model.\n\nReturns:\n    - Predicted values for the given input data.\n\"\"\"\n\nfrom sklearn import tree\nimport pandas as pd\nimport pandas_datareader as web\nimport numpy as np\n\n# Fetch stock data\ndf = web.DataReader('goog', 'yahoo', start='2012-5-1', end='2016-5-20')\n\n# Calculate buy/sell signals\ndf['B/S'] = (df['Close'].diff() < 0).astype(int)\n\n# Prepare data for the specified date range\nclosing = df.loc['2013\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 915, "library_problem_id": 98, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 98}}
{"id": 916, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Original data\nX = [['asdf', '1'], ['asdf', '0']]\ny = ['2', '3']\n\n# Initialize the DecisionTreeClassifier\nclf = DecisionTreeClassifier()\n\n# Solution: Transform string data to numerical data\n# Define a function to convert string data to numerical data\ndef convert_string_data_to_numerical(X: list) -> np.ndarray:\n    \"\"\"\n    Convert a list of string data into a numerical numpy array suitable for training a DecisionTreeClassifier.\n\n    Args:\n    X (list): A list of lists containing string data.\n\n    Returns:\n    np.ndarray: A numpy array with numerical data converted from the input string data.\n    \"\"\"\n    # Example conversion logic (this is a placeholder, actual logic may vary)\n    # Convert categorical string data to numerical data using pandas\n    df = pd.DataFrame(X)\n    df_encoded = df.apply(lambda col: pd.factorize(col, sort=True)[0])\n    return df_encoded.to_numpy()\n\n# Convert the string data to numerical data\nnew_X = convert_string_data_to_numerical(X)\n\n# Fit the classifier with the transformed data\nclf.fit(new_X, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 916, "library_problem_id": 99, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Origin", "perturbation_origin_id": 99}}
{"id": 917, "code": ["<code>\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\n# Original data\nX = [['asdf', '1'], ['asdf', '0']]\n\n# Initialize the DecisionTreeClassifier\nclf = DecisionTreeClassifier()\n\n# Solution: Preprocess the data\n# Convert categorical string data to numerical data using LabelEncoder\nlabel_encoders = [LabelEncoder() for _ in range(len(X[0]))]\n\n# Transform each column of X\nnew_X = np.array(X)\nfor i in range(new_X.shape[1]):\n    new_X[:, i] = label_encoders[i].fit_transform(new_X[:, i])\n\n# Fit the classifier with the transformed data\nclf.fit(new_X, ['2', '3'])\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 917, "library_problem_id": 100, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 918, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\n# Original data\nX = [['dsa', '2'], ['sato', '3']]\ny = ['4', '5']\n\n# Convert string data to numerical data using LabelEncoder\ndef preprocess_data(X):\n    \"\"\"Preprocesses the input data by encoding string features to numerical values.\n\n    Args:\n        X (list or np.ndarray): The input feature matrix containing string data.\n\n    Returns:\n        np.ndarray: The transformed feature matrix with numerical data.\n    \"\"\"\n    label_encoders = []\n    X_encoded = np.array(X)\n    \n    for i in range(X_encoded.shape[1]):\n        le = LabelEncoder()\n        X_encoded[:, i] = le.fit_transform(X_encoded[:, i])\n        label_encoders.append(le)\n    \n    return X_encoded\n\n# Preprocess the data\nnew_X = preprocess_data(X)\n\n# Initialize and train the DecisionTreeClassifier\nclf = DecisionTreeClassifier()\nclf.fit(new_X, y)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 918, "library_problem_id": 101, "library": "Sklearn", "test_case_cnt": 0, "perturbation_type": "Surface", "perturbation_origin_id": 99}}
{"id": 919, "code": ["<code>\ndef perform_logistic_regression(filename: str) -> np.ndarray:\n    # Load the dataset\n    dataframe = pd.read_csv(filename, dtype='category')\n    \n    # Preprocess the data\n    dataframe = dataframe.drop([\"Name\"], axis=1)\n    cleanup = {\"Class\": {\"Primary Hunter\": 0, \"Primary Scavenger\": 1}}\n    dataframe.replace(cleanup, inplace=True)\n    \n    # Separate the data into independent (X) and dependent (y) variables\n    X = dataframe.iloc[:, :-1].astype(float)\n    y = dataframe.iloc[:, -1].astype(int)\n    \n    # Initialize the Logistic Regression model\n    logReg = LogisticRegression()\n    \n    # Fit the model\n    logReg.fit(X, y)\n    \n    # Make predictions\n    predict = logReg.predict(X)\n    \n    return predict\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 919, "library_problem_id": 102, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 102}}
{"id": 920, "code": ["<code>\ndef predict_classes(filename: str) -> np.ndarray:\n    # Load the dataset\n    dataframe = pd.read_csv(filename, dtype='category')\n    \n    # Drop the 'Name' column\n    dataframe = dataframe.drop([\"Name\"], axis=1)\n    \n    # Convert 'Class' column to numerical values\n    cleanup = {\"Class\": {\"Primary Hunter\": 0, \"Primary Scavenger\": 1}}\n    dataframe.replace(cleanup, inplace=True)\n    \n    # Separate features and target variable\n    X = dataframe.iloc[:, :-1].astype(float)\n    y = dataframe.iloc[:, -1].astype(int)\n    \n    # Initialize and fit the logistic regression model\n    logReg = LogisticRegression()\n    logReg.fit(X, y)\n    \n    # Predict the class labels\n    predict = logReg.predict(X)\n    \n    return predict\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 920, "library_problem_id": 103, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 102}}
{"id": 921, "code": ["<code>\ndef split_data_by_date(features_dataframe: pd.DataFrame, train_size: float) -> (pd.DataFrame, pd.DataFrame):\n    \"\"\"\n    Splits the given DataFrame into a training set and a test set based on date order.\n    \n    Args:\n        features_dataframe (pd.DataFrame): The input DataFrame containing a 'date' column with sorted dates.\n        train_size (float): The proportion of the dataset to include in the training set.\n        \n    Returns:\n        (pd.DataFrame, pd.DataFrame): A tuple containing the training and test DataFrames.\n        \n    The test set will contain dates that are strictly newer than those in the training set.\n    \"\"\"\n    # Calculate the index at which to split the data\n    split_index = int(len(features_dataframe) * train_size)\n    \n    # Split the data\n    train_dataframe = features_dataframe.iloc[:split_index]\n    test_dataframe = features_dataframe.iloc[split_index:]\n    \n    return train_dataframe, test_dataframe\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 921, "library_problem_id": 104, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 104}}
{"id": 922, "code": ["<code>\ndef split_data_by_date(features_dataframe: pd.DataFrame, train_size: float = 0.8) -> (pd.DataFrame, pd.DataFrame):\n    \"\"\"\n    Splits the given DataFrame into training and testing sets based on date order.\n    \n    Args:\n        features_dataframe (pd.DataFrame): The input DataFrame containing a 'date' column with sorted dates.\n        train_size (float): The proportion of the dataset to include in the train split (default is 0.8).\n        \n    Returns:\n        train_dataframe (pd.DataFrame): The DataFrame containing the training data.\n        test_dataframe (pd.DataFrame): The DataFrame containing the testing data.\n    \"\"\"\n    # Calculate the split index\n    split_index = int(len(features_dataframe) * (1 - train_size))\n    \n    # Split the data\n    test_dataframe = features_dataframe.iloc[:split_index]\n    train_dataframe = features_dataframe.iloc[split_index:]\n    \n    return train_dataframe, test_dataframe\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 922, "library_problem_id": 105, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 923, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef solve(features_dataframe: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    # Define the proportion of data to be used as the training set\n    train_size = 0.2\n    \n    # Calculate the number of samples for the training set\n    num_train_samples = int(len(features_dataframe) * train_size)\n    \n    # Split the data into training and testing sets\n    train_dataframe = features_dataframe.iloc[:num_train_samples]\n    test_dataframe = features_dataframe.iloc[num_train_samples:]\n    \n    return train_dataframe, test_dataframe\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 923, "library_problem_id": 106, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 104}}
{"id": 924, "code": ["<code>\ndef apply_minmax_scaling(df: pd.DataFrame) -> pd.DataFrame:\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Define the columns to scale\n    columns_to_scale = ['X2', 'X3']\n\n    # Apply MinMax scaling for each group defined by 'Month'\n    for month, group in df.groupby('Month'):\n        # Fit and transform the data for the current group\n        scaled_values = scaler.fit_transform(group[columns_to_scale])\n        \n        # Assign the scaled values to new columns in the original DataFrame\n        df.loc[group.index, 'X2_scale'] = scaled_values[:, 0]\n        df.loc[group.index, 'X3_scale'] = scaled_values[:, 1]\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 924, "library_problem_id": 107, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 107}}
{"id": 925, "code": ["<code>\ndef apply_minmax_scaler(df: pd.DataFrame) -> pd.DataFrame:\n    # Initialize the MinMaxScaler\n    scaler = MinMaxScaler()\n\n    # Define the columns to scale\n    columns_to_scale = ['A2', 'A3']\n\n    # Apply MinMax scaling within each month and add new columns for scaled values\n    df[['new_A2', 'new_A3']] = df.groupby('Month')[columns_to_scale].transform(lambda x: scaler.fit_transform(x))\n\n    return df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 925, "library_problem_id": 108, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 107}}
{"id": 926, "code": ["<code>\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef tokenize_with_special_chars(text: str) -> list:\n    \"\"\"\n    Tokenizes the input text while preserving special characters such as '@' and '#'.\n\n    Args:\n    - text (str): The input string to be tokenized.\n\n    Returns:\n    - list: A list of tokens including special characters.\n    \"\"\"\n    # Define a custom token pattern to include special characters\n    custom_token_pattern = r\"(?u)\\b\\w+['@#-]*\\w*\\b\"\n    \n    # Initialize CountVectorizer with the custom token pattern\n    count = CountVectorizer(lowercase=False, token_pattern=custom_token_pattern)\n    \n    # Fit and transform the input text\n    vocabulary = count.fit_transform([text])\n    \n    # Retrieve and return the feature names (tokens)\n    return count.get_feature_names_out()\n\n# Example usage\nwords = \"Hello @friend, this is a good day. #good.\"\nfeature_names = tokenize_with_special_chars(words)\nprint(feature_names)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 926, "library_problem_id": 109, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 109}}
{"id": 927, "code": ["<code>\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef tokenize_words(words: str) -> list:\n    \"\"\"\n    Tokenizes the input string `words` using CountVectorizer, ensuring special characters\n    like '@' and '#' are included in the tokens. The tokens are then sorted in the specified order.\n\n    Args:\n    - words (str): The input string containing words and special characters.\n\n    Returns:\n    - list: A list of tokens including special characters, sorted in the desired order.\n    \"\"\"\n    count = CountVectorizer(lowercase=False, token_pattern=r\"(?u)\\b\\w+\\b|[@#]\\w+\")\n    vocabulary = count.fit_transform([words])\n    feature_names = count.get_feature_names_out()\n    # Sort the feature names in the desired order\n    sorted_feature_names = sorted(feature_names, key=lambda x: (x[0] not in '@#', x))\n    return sorted_feature_names\n\nwords = \"ha @ji me te no ru bu ru wa, @na n te ko to wa na ka tsu ta wa. wa ta shi da ke no mo na ri za, mo u to kku ni #de a 't te ta ka ra\"\nresult = tokenize_words(words)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 927, "library_problem_id": 110, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 109}}
{"id": 928, "code": ["<code>\ndef extract_gridsearch_results(gridsearch: GridSearchCV) -> pd.DataFrame:\n    \"\"\"\n    Extracts the full results from a fitted GridSearchCV object and returns them as a pandas DataFrame.\n\n    Args:\n        gridsearch (GridSearchCV): A fitted GridSearchCV object.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the full results of the GridSearchCV, including scores for all candidate models.\n    \"\"\"\n    # Ensure the input is a fitted GridSearchCV object\n    assert isinstance(gridsearch, GridSearchCV), \"Input must be a fitted GridSearchCV object.\"\n\n    # Extract the results from the GridSearchCV object\n    results = gridsearch.cv_results_\n\n    # Convert the results to a pandas DataFrame\n    results_df = pd.DataFrame(results)\n\n    return results_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 928, "library_problem_id": 111, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 111}}
{"id": 929, "code": ["<code>\ndef extract_gridsearch_results(gridsearch: GridSearchCV) -> pd.DataFrame:\n    \"\"\"\n    Extracts and returns the full results from a fitted GridSearchCV object.\n\n    Args:\n        gridsearch (GridSearchCV): A fitted GridSearchCV object containing candidate model evaluations.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the full results of the GridSearchCV, sorted by mean_fit_time.\n\n    Preconditions:\n        - The input 'gridsearch' must be an instance of a fitted GridSearchCV object.\n        - The GridSearchCV object must have been executed with a scoring metric.\n\n    Postconditions:\n        - The returned DataFrame includes columns for parameter settings, mean test scores, and mean_fit_time.\n        - The DataFrame is sorted in ascending order by mean_fit_time.\n    \"\"\"\n    import numpy as np\n    import pandas as pd\n    from sklearn.model_selection import GridSearchCV\n\n    # Ensure the input is a fitted GridSearchCV object\n    assert isinstance(gridsearch, GridSearchCV), \"Input must be a fitted GridSearchCV object.\"\n\n    # Extract the results from the GridSearchCV object\n    results = gridsearch.cv_results_\n\n    # Create a DataFrame from the results\n    results_df = pd.DataFrame(results)\n\n    # Sort the DataFrame by mean_fit_time\n    sorted_results_df = results_df.sort_values(by='mean_fit_time', ascending=True)\n\n    return sorted_results_df\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 929, "library_problem_id": 112, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 111}}
{"id": 930, "code": ["<code>\ndef train_and_save_model(data: pd.DataFrame, model_path: str) -> None:\n    \"\"\"\n    Train the IsolationForest model on the provided data and save the model to a file.\n\n    Args:\n        data (pd.DataFrame): The clean dataset used to train the IsolationForest model.\n        model_path (str): The file path where the trained model will be saved.\n\n    Returns:\n        None\n    \"\"\"\n    # Initialize the IsolationForest model\n    model = IsolationForest()\n\n    # Fit the model to the clean data\n    model.fit(data)\n\n    # Save the trained model to the specified file path\n    joblib.dump(model, model_path)\n\ndef load_model(model_path: str) -> IsolationForest:\n    \"\"\"\n    Load the IsolationForest model from a file.\n\n    Args:\n        model_path (str): The file path from which the trained model will be loaded.\n\n    Returns:\n        IsolationForest: The loaded IsolationForest model.\n    \"\"\"\n    # Load the model from the specified file path\n    model = joblib.load(model_path)\n    return model\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 930, "library_problem_id": 113, "library": "Sklearn", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 113}}
{"id": 931, "code": ["<code>\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef compute_similarity_matrix(df: pd.DataFrame) -> np.ndarray:\n    \"\"\"\n    Compute the cosine similarity matrix for item descriptions.\n\n    Args:\n        df (pd.DataFrame): A DataFrame with columns 'items' and 'description'.\n\n    Returns:\n        np.ndarray: A 2D numpy array representing the cosine similarity matrix.\n    \"\"\"\n    # Initialize the TF-IDF vectorizer\n    tfidf_vectorizer = TfidfVectorizer()\n\n    # Transform the descriptions into TF-IDF vectors\n    tfidf_matrix = tfidf_vectorizer.fit_transform(df['description'])\n\n    # Compute the cosine similarity matrix\n    cosine_similarity_matrix = cosine_similarity(tfidf_matrix)\n\n    return cosine_similarity_matrix\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 931, "library_problem_id": 114, "library": "Sklearn", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 114}}
{"id": 932, "code": ["<code>\ndef update_learning_rate(optimizer: torch.optim.Optimizer, new_lr: float) -> None:\n    \"\"\"\n    Update the learning rate of the given optimizer to a new value.\n\n    Args:\n        optimizer (torch.optim.Optimizer): The optimizer whose learning rate needs to be updated.\n        new_lr (float): The new learning rate value to be set.\n\n    Returns:\n        None\n\n    Example:\n        >>> optim = torch.optim.SGD(model.parameters(), lr=0.01)\n        >>> update_learning_rate(optim, 0.001)\n    \"\"\"\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = new_lr\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 932, "library_problem_id": 0, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 0}}
{"id": 933, "code": ["<code>\nimport torch\n\ndef update_learning_rate(optimizer: torch.optim.Optimizer, new_lr: float) -> None:\n    \"\"\"\n    Update the learning rate of the given optimizer to a new value.\n\n    Args:\n        optimizer (torch.optim.Optimizer): The optimizer whose learning rate needs to be updated.\n        new_lr (float): The new learning rate value to be set.\n\n    Returns:\n        None\n\n    Example:\n        # Assuming `optim` is an instance of torch.optim.Optimizer\n        update_learning_rate(optim, 0.001)\n    \"\"\"\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = new_lr\n\n# Example usage\noptim = torch.optim.SGD(..., lr=0.01)\n# After some condition is met, update the learning rate\nupdate_learning_rate(optim, 0.001)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 933, "library_problem_id": 1, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 934, "code": ["<code>\ndef update_learning_rate(optimizer: torch.optim.Optimizer, new_lr: float) -> None:\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = new_lr\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 934, "library_problem_id": 2, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 0}}
{"id": 935, "code": ["<code>\ndef update_learning_rate(optimizer: torch.optim.Optimizer, new_lr: float) -> None:\n    \"\"\"\n    Updates the learning rate of the given optimizer to a new value.\n\n    Args:\n        optimizer (torch.optim.Optimizer): The optimizer whose learning rate needs to be updated.\n        new_lr (float): The new learning rate value to be set.\n\n    Returns:\n        None\n\n    Example:\n        optim = torch.optim.SGD(model.parameters(), lr=0.005)\n        update_learning_rate(optim, 0.001)\n    \"\"\"\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = new_lr\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 935, "library_problem_id": 3, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 0}}
{"id": 936, "code": ["<code>\n```python\nimport numpy as np\nimport torch\nfrom gensim.models import Word2Vec\nfrom gensim.test.utils import common_texts\n\ndef load_data() -> torch.Tensor:\n    \"\"\"\n    Load and preprocess input data to be embedded.\n    \n    Returns:\n        torch.Tensor: The input data tensor to be embedded.\n    \"\"\"\n    # Placeholder for actual data loading logic\n    return torch.tensor([[0, 1, 2], [3, 4, 5]])\n\ndef transfer_weights_to_torch(word2vec_model: Word2Vec) -> torch.nn.Embedding:\n    \"\"\"\n    Transfer Word2Vec weights from gensim to a PyTorch embedding layer.\n    \n    Args:\n        word2vec_model (Word2Vec): A trained gensim Word2Vec model.\n    \n    Returns:\n        torch.nn.Embedding: A PyTorch embedding layer initialized with Word2Vec weights.\n    \"\"\"\n    # Extract weights from gensim Word2Vec model\n    weights = torch.FloatTensor(word2vec_model.wv.vectors)\n    \n    # Create a PyTorch embedding layer with the same number of embeddings and embedding dimension\n    embedding_layer = torch.nn.Embedding.from_pretrained(weights)\n    \n    return embedding_layer\n\ndef embed_input_data(input_tensor: torch.Tensor, embedding_layer: torch.nn.Embedding) -> torch.Tensor:\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 936, "library_problem_id": 4, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 4}}
{"id": 937, "code": ["<code>\n```python\ndef load_data() -> torch.Tensor:\n    \"\"\"\n    Placeholder function to load input data as a PyTorch tensor.\n    \n    Returns:\n        torch.Tensor: The input data tensor.\n    \"\"\"\n    # Implement data loading logic here\n    pass\n\ndef get_embedded_input(input_tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Embed input data using pre-trained Word2Vec weights transferred to a PyTorch embedding layer.\n\n    Args:\n        input_tensor (torch.Tensor): The input data tensor to be embedded.\n\n    Returns:\n        torch.Tensor: The embedded input data.\n    \"\"\"\n    # Load pre-trained Word2Vec model\n    word2vec = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n    \n    # Extract Word2Vec weights and vocabulary\n    weights = torch.FloatTensor(word2vec.wv.vectors)\n    vocab_size, embedding_dim = weights.shape\n    \n    # Create PyTorch embedding layer\n    embedding_layer = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n    \n    # Transfer Word2Vec weights to PyTorch embedding layer\n    embedding_layer.weight.data.copy_(weights)\n    \n    # Embed input data using the PyTorch embedding layer\n    embedded_input = embedding_layer(input_tensor)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 937, "library_problem_id": 5, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 4}}
{"id": 938, "code": ["<code>\ndef convert_tensor_to_dataframe(tensor: torch.Tensor) -> pd.DataFrame:\n    numpy_array = tensor.numpy()\n    dataframe = pd.DataFrame(numpy_array)\n    return dataframe\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 938, "library_problem_id": 6, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 6}}
{"id": 939, "code": ["<code>\ndef convert_tensor_to_dataframe(tensor: torch.Tensor) -> pd.DataFrame:\n    # Convert the PyTorch tensor to a NumPy array\n    numpy_array = tensor.numpy()\n    \n    # Create a Pandas DataFrame from the NumPy array\n    dataframe = pd.DataFrame(numpy_array)\n    \n    return dataframe\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 939, "library_problem_id": 7, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 940, "code": ["<code>\ndef tensor_to_dataframe(x: torch.Tensor) -> pd.DataFrame:\n    # Convert the PyTorch tensor to a NumPy array\n    numpy_array = x.numpy()\n    \n    # Create a Pandas DataFrame from the NumPy array\n    dataframe = pd.DataFrame(numpy_array)\n    \n    return dataframe\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 940, "library_problem_id": 8, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 6}}
{"id": 941, "code": ["<code>\ndef slice_tensor_with_logical_index(B: torch.LongTensor, A_log: torch.ByteTensor) -> torch.LongTensor:\n    \"\"\"\n    Slices a PyTorch tensor using a logical index on the columns.\n\n    Args:\n        B (torch.LongTensor): A 2D tensor from which columns are to be selected.\n        A_log (torch.ByteTensor): A 1D logical index tensor indicating which columns to select (1 for select, 0 for ignore).\n\n    Returns:\n        torch.LongTensor: A tensor containing the selected columns from B.\n\n    Example:\n        B = torch.LongTensor([[1, 2, 3], [4, 5, 6]])\n        A_log = torch.ByteTensor([1, 0, 1])\n        result = slice_tensor_with_logical_index(B, A_log)\n        # result should be:\n        # tensor([[1, 3],\n        #         [4, 6]])\n    \"\"\"\n    # Convert A_log to a boolean mask\n    boolean_mask = A_log.bool()\n    \n    # Use the boolean mask to index the columns of B\n    C = B[:, boolean_mask]\n    \n    return C\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 941, "library_problem_id": 9, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 9}}
{"id": 942, "code": ["<code>\nimport torch\n\ndef logical_indexing(tensor: torch.Tensor, logical_index: torch.Tensor) -> torch.Tensor:\n    # Ensure logical_index is a 1D tensor of appropriate type\n    if logical_index.dim() != 1 or logical_index.dtype not in [torch.uint8, torch.bool]:\n        raise TypeError(\"logical_index must be a 1D tensor of type torch.ByteTensor or torch.BoolTensor.\")\n    \n    # Ensure logical_index length matches the number of columns in tensor\n    if logical_index.size(0) != tensor.size(1):\n        raise ValueError(\"Length of logical_index must match the number of columns in the tensor.\")\n    \n    # Perform logical indexing\n    return tensor[:, logical_index.bool()]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 942, "library_problem_id": 10, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 943, "code": ["<code>\nimport torch\n\ndef logical_indexing_example() -> torch.LongTensor:\n    \"\"\"\n    Demonstrates logical indexing on a PyTorch tensor's columns using a logical index vector.\n\n    Returns:\n        torch.LongTensor: A tensor containing the columns selected by the logical index.\n    \"\"\"\n    # Define the logical index vector\n    A_log = torch.ByteTensor([1, 1, 0])  # Logical index indicating which columns to select\n\n    # Define the tensor from which columns will be selected\n    B = torch.LongTensor([[999, 777, 114514], [9999, 7777, 1919810]])\n\n    # Convert the logical index to a boolean mask for indexing\n    mask = A_log.bool()\n\n    # Use the boolean mask to select columns from the tensor\n    C = B[:, mask]\n\n    return C\n\n# Example usage\nresult = logical_indexing_example()\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 943, "library_problem_id": 11, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 944, "code": ["<code>\ndef logical_indexing_on_columns(A_log: torch.ByteTensor, B: torch.LongTensor) -> torch.LongTensor:\n    \"\"\"\n    Perform logical indexing on the columns of a PyTorch tensor.\n\n    Args:\n    - A_log (torch.ByteTensor): A logical index vector where 0 indicates columns to select.\n    - B (torch.LongTensor): The tensor from which columns are to be selected.\n\n    Returns:\n    - torch.LongTensor: A tensor containing the selected columns.\n    \"\"\"\n    # Convert ByteTensor to LongTensor for indexing\n    A_log = A_log.nonzero(as_tuple=True)[0]\n\n    # Perform logical indexing on the columns\n    C = torch.index_select(B, 1, A_log)\n\n    return C\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 944, "library_problem_id": 12, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Semantic", "perturbation_origin_id": 9}}
{"id": 945, "code": ["<code>\nimport torch\n\ndef solve(A_log: torch.ByteTensor, B: torch.LongTensor) -> torch.LongTensor:\n    # Convert A_log to a boolean mask for indexing\n    mask = A_log.bool()\n    # Use the boolean mask to select columns from B\n    C = B[:, mask]\n    return C\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 945, "library_problem_id": 13, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 9}}
{"id": 946, "code": ["<code>\n```python\nimport torch\n\ndef logical_indexing(tensor: torch.Tensor, logical_index: torch.ByteTensor) -> torch.Tensor:\n    \"\"\"\n    Select columns from a PyTorch tensor using a logical index where the index is 0.\n\n    Args:\n        tensor (torch.Tensor): The input tensor from which columns are to be selected.\n        logical_index (torch.ByteTensor): A logical index tensor with 0s and 1s.\n\n    Returns:\n        torch.Tensor: A tensor containing columns from the input tensor where the logical index is 0.\n\n    Raises:\n        TypeError: If the logical index is not a ByteTensor or if its length does not match the number of columns in the input tensor.\n    \"\"\"\n    # Ensure logical_index is a ByteTensor and matches the number of columns in tensor\n    if not isinstance(logical_index, torch.ByteTensor):\n        raise TypeError(\"Logical index must be a ByteTensor.\")\n    if logical_index.size(0) != tensor.size(1):\n        raise ValueError(\"Logical index size must match the number of columns in the input tensor.\")\n\n    # Create a mask for columns where logical_index is 0\n    mask = logical_index == 0\n\n    # Use the mask to select columns\n    result = tensor[:, mask]\n\n    return result\n\n# Example usage\nA_log = torch.ByteTensor([0, 0, 1])\nB = torch.LongTensor([[999, 777, 114514\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 946, "library_problem_id": 14, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 947, "code": ["<code>\nimport torch\n\ndef select_columns_using_index_select(idx: torch.LongTensor, B: torch.LongTensor) -> torch.LongTensor:\n    \"\"\"\n    Select columns from the tensor B using the specified index tensor idx.\n\n    Args:\n    idx (torch.LongTensor): A 1D tensor containing the indices of the columns to select.\n    B (torch.LongTensor): A 2D tensor from which columns are to be selected.\n\n    Returns:\n    torch.LongTensor: A 2D tensor containing the selected columns from B in the order specified by idx.\n    \"\"\"\n    # Use index_select to select the specified columns from B\n    C = B.index_select(dim=1, index=idx)\n    return C\n\n# Example usage\nidx = torch.LongTensor([1, 2])\nB = torch.LongTensor([[2, 1, 3], [5, 4, 6]])\nC = select_columns_using_index_select(idx, B)\nprint(C)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 947, "library_problem_id": 15, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 9}}
{"id": 948, "code": ["<code>\nimport numpy as np\nimport torch\n\ndef convert_to_tensor(x_array: np.ndarray) -> torch.Tensor:\n    \"\"\"\n    Convert a NumPy array of dtype=object to a PyTorch Tensor.\n\n    Args:\n        x_array (np.ndarray): A NumPy array with dtype=object, containing sub-arrays of dtype=float16.\n\n    Returns:\n        torch.Tensor: A PyTorch Tensor with the same shape and data type as the input array.\n    \"\"\"\n    # Convert object dtype array to a single NumPy array\n    stacked_array = np.stack(x_array)\n\n    # Convert the stacked NumPy array to a PyTorch Tensor\n    x_tensor = torch.tensor(stacked_array, dtype=torch.float16)\n\n    return x_tensor\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 948, "library_problem_id": 16, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 16}}
{"id": 949, "code": ["<code>\n```python\ndef convert_numpy_to_torch_tensor():\n    # Define the numpy array with dtype=object\n    x = np.array([\n        np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n        np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n        np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n        np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n        np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n        np.array([4.0, 4.56, 9.78, 1.23, 4.56, 77.77], dtype=np.double),\n        np.array([1.23, 4.56, 9.78, 1.23, 4.56, 9.78], dtype=np.double),\n        np.array([4.0,\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 949, "library_problem_id": 17, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 950, "code": ["<code>\ndef convert_to_tensor(input_array: np.ndarray) -> torch.Tensor:\n    \"\"\"\n    Convert a numpy array of dtype=object to a PyTorch Tensor.\n\n    Args:\n        input_array (np.ndarray): A numpy array with dtype=object, where each element is a numpy array\n                                  of dtype=float16.\n\n    Returns:\n        torch.Tensor: A PyTorch Tensor containing the data from the input numpy array.\n    \"\"\"\n    # Ensure the input is a numpy array with dtype=object\n    assert input_array.dtype == object, \"Input array must have dtype=object\"\n    \n    # Convert each sub-array to a numpy array with dtype=float16 and stack them\n    stacked_array = np.stack(input_array)\n    \n    # Convert the stacked numpy array to a PyTorch Tensor\n    tensor = torch.tensor(stacked_array, dtype=torch.float16)\n    \n    return tensor\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 950, "library_problem_id": 18, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 16}}
{"id": 951, "code": ["<code>\ndef convert_lengths_to_mask(lens: List[int]) -> torch.LongTensor:\n    max_len = max(lens)\n    batch_size = len(lens)\n    mask = torch.zeros(batch_size, max_len)\n    for idx, length in enumerate(lens):\n        mask[idx, :length] = 1\n    return mask.long()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 951, "library_problem_id": 19, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 19}}
{"id": 952, "code": ["<code>\nimport torch\nfrom typing import List\n\ndef lengths_to_mask(lens: List[int]) -> torch.LongTensor:\n    max_len = max(lens)  # Determine the maximum sentence length\n    mask = torch.zeros((len(lens), max_len), dtype=torch.long)  # Initialize the mask tensor with zeros\n    for i, length in enumerate(lens):\n        mask[i, :length] = 1  # Set 1s up to the sentence length for each row\n    return mask\n\n# Example usage\nlens = [1, 9, 3, 5]\nmask = lengths_to_mask(lens)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 952, "library_problem_id": 20, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 953, "code": ["<code>\nimport torch\n\ndef convert_lengths_to_mask(lens: torch.LongTensor) -> torch.LongTensor:\n    \"\"\"\n    Convert a list of sentence lengths into a batch mask tensor.\n\n    Args:\n        lens (torch.LongTensor): A 1D tensor containing the lengths of each sentence.\n\n    Returns:\n        torch.LongTensor: A 2D tensor where each row is a mask for the corresponding sentence length.\n                          The mask contains 1s up to the sentence length and 0s thereafter.\n    \"\"\"\n    max_len = lens.max().item()\n    mask = torch.arange(max_len).expand(len(lens), max_len) < lens.unsqueeze(1)\n    return mask.long()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 953, "library_problem_id": 21, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 19}}
{"id": 954, "code": ["<code>\ndef get_mask(lens: list[int]) -> torch.LongTensor:\n    # Determine the maximum sentence length\n    max_len = max(lens)\n    \n    # Initialize a tensor of zeros with shape (number of sentences, max sentence length)\n    mask = torch.zeros((len(lens), max_len), dtype=torch.long)\n    \n    # Fill in the mask with '1's up to the length of each sentence\n    for i, length in enumerate(lens):\n        mask[i, :length] = 1\n    \n    return mask\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 954, "library_problem_id": 22, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 19}}
{"id": 955, "code": ["<code>\nimport torch\n\ndef create_3d_diagonal_tensor(tensor_2d: torch.Tensor) -> torch.Tensor:\n    # Get the shape of the input tensor\n    index_in_batch, diag_ele = tensor_2d.shape\n    \n    # Initialize a 3D tensor filled with zeros\n    tensor_3d = torch.zeros((index_in_batch, diag_ele, diag_ele), dtype=tensor_2d.dtype)\n    \n    # Iterate over each batch index\n    for i in range(index_in_batch):\n        # Fill the diagonal of the matrix with the corresponding row from tensor_2d\n        tensor_3d[i] = torch.diag(tensor_2d[i])\n    \n    return tensor_3d\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 955, "library_problem_id": 23, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 23}}
{"id": 956, "code": ["<code>\ndef convert_to_3d_diagonal(tensor_2d: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Converts a 2D tensor into a 3D tensor where each slice along the first dimension\n    is a diagonal matrix constructed from the corresponding row of the input tensor.\n\n    Args:\n        tensor_2d (torch.Tensor): A 2D tensor of shape (index_in_batch, diag_ele).\n\n    Returns:\n        torch.Tensor: A 3D tensor of shape (index_in_batch, diag_ele, diag_ele),\n                      where each slice along the first dimension is a diagonal matrix.\n    \"\"\"\n    # Get the shape of the input tensor\n    index_in_batch, diag_ele = tensor_2d.shape\n    \n    # Initialize an empty 3D tensor to store the result\n    tensor_3d = torch.zeros((index_in_batch, diag_ele, diag_ele), dtype=tensor_2d.dtype)\n    \n    # Iterate over each batch index\n    for i in range(index_in_batch):\n        # Construct a diagonal matrix for each row in the input tensor\n        tensor_3d[i] = torch.diag(tensor_2d[i])\n    \n    return tensor_3d\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 956, "library_problem_id": 24, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 23}}
{"id": 957, "code": ["<code>\nimport torch\n\ndef stack_tensors(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n    if a.shape[1] != b.shape[1]:\n        raise ValueError(\"The second dimension of tensors `a` and `b` must be the same.\")\n    \n    # Concatenate tensors along the first dimension\n    ab = torch.cat((a, b), dim=0)\n    return ab\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 957, "library_problem_id": 25, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 25}}
{"id": 958, "code": ["<code>\ndef concatenate_tensors(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Concatenates two PyTorch tensors along the first dimension.\n\n    Args:\n        a (torch.Tensor): A tensor of shape (m, n).\n        b (torch.Tensor): A tensor of shape (p, n), where p can be different from m.\n\n    Returns:\n        torch.Tensor: A concatenated tensor of shape (m + p, n).\n\n    Raises:\n        ValueError: If the second dimension of `a` and `b` do not match.\n    \"\"\"\n    if a.size(1) != b.size(1):\n        raise ValueError(\"The second dimension of both tensors must be the same.\")\n\n    # Concatenate tensors along the first dimension\n    ab = torch.cat((a, b), dim=0)\n    return ab\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 958, "library_problem_id": 26, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 959, "code": ["<code>\ndef solve(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Combine tensors `a` and `b` into a single tensor `ab` with shape (3, 11).\n\n    Parameters:\n    - a (torch.Tensor): A tensor of shape (2, 11).\n    - b (torch.Tensor): A tensor of shape (1, 11).\n\n    Returns:\n    - torch.Tensor: A combined tensor of shape (3, 11).\n    \"\"\"\n    # Concatenate tensors `a` and `b` along the first dimension\n    ab = torch.cat((a, b), dim=0)\n    return ab\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 959, "library_problem_id": 27, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 25}}
{"id": 960, "code": ["<code>\ndef zero_fill_after_lengths(a: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n    batch_size, sentence_length, embedding_dim = a.shape\n    for i in range(batch_size):\n        a[i, lengths[i]:, :] = 0\n    return a\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 960, "library_problem_id": 28, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 28}}
{"id": 961, "code": ["<code>\nimport torch\n\ndef fill_tensor_with_value(a: torch.Tensor, lengths: torch.Tensor, value: float = 2333) -> torch.Tensor:\n    for i in range(a.size(0)):  # Iterate over the batch dimension\n        a[i, lengths[i]:, :] = value  # Fill from the specified index to the end of the sentence length dimension\n    return a\n\n# Example usage\na = torch.rand((10, 1000, 96))\nlengths = torch.randint(1000, (10,))\na = fill_tensor_with_value(a, lengths)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 961, "library_problem_id": 29, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 28}}
{"id": 962, "code": ["<code>\nimport torch\n\ndef zero_fill_tensor(a: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n    for i in range(a.size(0)):\n        a[i, :lengths[i], :] = 0\n    return a\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 962, "library_problem_id": 30, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 28}}
{"id": 963, "code": ["<code>\ndef fill_tensor_before_lengths(a: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n    batch_size, sentence_length, embedding_dim = a.shape\n    for i in range(batch_size):\n        a[i, :lengths[i], :] = 2333\n    return a\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 963, "library_problem_id": 31, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 28}}
{"id": 964, "code": ["<code>\ndef convert_list_of_tensors_to_tensor(list_of_tensors: list[torch.Tensor]) -> torch.Tensor:\n    \"\"\"\n    Convert a list of PyTorch tensors into a single tensor.\n\n    Args:\n        list_of_tensors (list[torch.Tensor]): A list where each element is a PyTorch tensor of the same shape.\n\n    Returns:\n        torch.Tensor: A single tensor containing all the tensors from the input list.\n\n    Raises:\n        ValueError: If the input list is empty or if the tensors have different shapes.\n    \"\"\"\n    if not list_of_tensors:\n        raise ValueError(\"Input list cannot be empty.\")\n\n    tensor_shape = list_of_tensors[0].shape\n    for tensor in list_of_tensors:\n        if tensor.shape != tensor_shape:\n            raise ValueError(\"All tensors must have the same shape.\")\n\n    return torch.stack(list_of_tensors)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 964, "library_problem_id": 32, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 32}}
{"id": 965, "code": ["<code>\nimport torch\n\ndef convert_list_to_tensor(tensor_list: list) -> torch.Tensor:\n    \"\"\"\n    Converts a list of PyTorch tensors into a single tensor containing all the tensors.\n\n    Args:\n        tensor_list (list): A list of PyTorch tensors, where each tensor has the same shape.\n\n    Returns:\n        torch.Tensor: A single tensor containing all the tensors from the input list.\n\n    Raises:\n        ValueError: If the input list is empty or contains tensors of different shapes.\n    \"\"\"\n    # Ensure all tensors in the list have the same shape\n    if not tensor_list:\n        raise ValueError(\"The input list is empty.\")\n    first_shape = tensor_list[0].shape\n    if not all(tensor.shape == first_shape for tensor in tensor_list):\n        raise ValueError(\"All tensors must have the same shape.\")\n\n    # Stack the tensors along a new dimension\n    return torch.stack(tensor_list)\n\n# Example usage\ntensor_list = [torch.randn(3), torch.randn(3), torch.randn(3)]\nnew_tensors = convert_list_to_tensor(tensor_list)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 965, "library_problem_id": 33, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 966, "code": ["<code>\ndef convert_to_tensor_of_tensors(list_of_tensors: list) -> torch.Tensor:\n    \"\"\"\n    Converts a list of 1-dimensional PyTorch tensors into a 2-dimensional tensor.\n\n    Args:\n        list_of_tensors (list): A list containing PyTorch tensors of the same size.\n\n    Returns:\n        torch.Tensor: A 2-dimensional tensor where each row corresponds to a tensor from the input list.\n\n    Example:\n        list_of_tensors = [torch.randn(3), torch.randn(3), torch.randn(3)]\n        tensor_of_tensors = convert_to_tensor_of_tensors(list_of_tensors)\n    \"\"\"\n    # Use torch.stack to combine the list of tensors into a single tensor\n    return torch.stack(list_of_tensors)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 966, "library_problem_id": 34, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 32}}
{"id": 967, "code": ["<code>\nimport torch\nfrom typing import List\n\ndef stack_tensors(tensor_list: List[torch.Tensor]) -> torch.Tensor:\n    \"\"\"\n    Convert a list of PyTorch tensors into a single tensor by stacking them along a new dimension.\n\n    Args:\n        tensor_list (List[torch.Tensor]): A list of PyTorch tensors, each with the same shape.\n\n    Returns:\n        torch.Tensor: A single tensor containing all input tensors stacked along a new dimension.\n\n    Raises:\n        ValueError: If the input list is empty or if tensors have different shapes.\n    \"\"\"\n    # Ensure the list is not empty and all tensors have the same shape\n    if not tensor_list:\n        raise ValueError(\"The input list must not be empty.\")\n    first_shape = tensor_list[0].shape\n    if any(tensor.shape != first_shape for tensor in tensor_list):\n        raise ValueError(\"All tensors must have the same shape.\")\n\n    # Stack tensors along a new dimension\n    return torch.stack(tensor_list)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 967, "library_problem_id": 35, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 32}}
{"id": 968, "code": ["<code>\n```python\nimport numpy as np\nimport torch\n\ndef select_elements(t: torch.Tensor, idx: np.ndarray) -> torch.Tensor:\n    \"\"\"\n    Select elements from each row of a given PyTorch tensor based on indices provided by a NumPy array.\n\n    Args:\n    - t (torch.Tensor): A 2D tensor from which elements are to be selected.\n    - idx (np.ndarray): A 1D array of indices indicating which element to select from each row of the tensor.\n\n    Returns:\n    - torch.Tensor: A 1D tensor containing the selected elements from each row.\n\n    Example:\n    >>> t = torch.tensor([[-0.2, 0.3], [-0.5, 0.1], [-0.4, 0.2]])\n    >>> idx = np.array([1, 0, 1])\n    >>> select_elements(t, idx)\n    tensor([0.3, -0.5, 0.2])\n    \"\"\"\n    # Ensure input types are correct\n    assert isinstance(t, torch.Tensor), \"Input t must be a PyTorch tensor.\"\n    assert isinstance(idx, np.ndarray), \"Input idx must be a NumPy array.\"\n\n    # Use advanced indexing to select elements\n    result = t[torch.arange(t.size(0)), idx]\n\n    return result\n\n# Example usage\nt = torch.tensor([[-0.2, 0.3], [-0.5, 0.1], [-0.4, 0.\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 968, "library_problem_id": 36, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 36}}
{"id": 969, "code": ["<code>\ndef index_tensor_by_array(t: torch.Tensor, idx: np.ndarray) -> torch.Tensor:\n    \"\"\"\n    Indexes each row of a given PyTorch tensor using a NumPy array to produce a new tensor.\n\n    Args:\n        t (torch.Tensor): A 2D tensor of shape (n, m) where n is the number of rows and m is the number of columns.\n        idx (np.ndarray): A 1D NumPy array of length n containing indices to select from each row of the tensor.\n\n    Returns:\n        torch.Tensor: A 1D tensor of length n containing elements selected from each row of the input tensor.\n    \"\"\"\n    # Ensure the input types are correct\n    assert isinstance(t, torch.Tensor), \"Input t must be a PyTorch tensor.\"\n    assert isinstance(idx, np.ndarray), \"Input idx must be a NumPy array.\"\n    \n    # Ensure the dimensions are compatible\n    assert t.shape[0] == idx.shape[0], \"The number of rows in the tensor must match the length of the index array.\"\n    \n    # Solution\n    result = t[torch.arange(t.size(0)), idx]\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 969, "library_problem_id": 37, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 36}}
{"id": 970, "code": ["<code>\nimport numpy as np\nimport torch\n\ndef select_elements(t: torch.Tensor, idx: np.ndarray) -> torch.Tensor:\n    # Ensure the input types are correct\n    assert isinstance(t, torch.Tensor), \"Input t must be a torch.Tensor\"\n    assert isinstance(idx, np.ndarray), \"Input idx must be a np.ndarray\"\n    \n    # Convert idx to a PyTorch tensor for advanced indexing\n    idx_tensor = torch.from_numpy(idx)\n    \n    # Use advanced indexing to select elements based on idx\n    result = t[torch.arange(t.size(0)), 1 - idx_tensor]\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 970, "library_problem_id": 38, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 36}}
{"id": 971, "code": ["<code>\ndef select_slices(ids: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Select slices from tensor `x` based on indices provided in `ids`.\n\n    Args:\n    - ids (torch.Tensor): A tensor of shape (70, 1) containing indices.\n    - x (torch.Tensor): A tensor of shape (70, 3, 2) from which slices are selected.\n\n    Returns:\n    - torch.Tensor: A tensor of shape (70, 2) containing the selected slices.\n    \"\"\"\n    # Ensure ids is of shape (70,) for proper indexing\n    ids = ids.squeeze(1)\n    \n    # Use advanced indexing to select the appropriate slices\n    result = x[torch.arange(x.size(0)), ids]\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 971, "library_problem_id": 39, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 39}}
{"id": 972, "code": ["<code>\ndef select_slices(ids: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Select slices from the tensor `x` based on indices provided in `ids`.\n\n    Args:\n    - ids (torch.Tensor): A tensor of shape (30, 1) containing indices for selection.\n    - x (torch.Tensor): A tensor of shape (30, 3, 114) from which slices are selected.\n\n    Returns:\n    - torch.Tensor: A tensor of shape (30, 114) with the selected slices.\n    \"\"\"\n    # Expand ids to match the dimensions required for gather\n    ids_expanded = ids.unsqueeze(-1).expand(-1, -1, x.size(2))\n    \n    # Gather the slices from x based on ids\n    result = x.gather(1, ids_expanded).squeeze(1)\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 972, "library_problem_id": 40, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 39}}
{"id": 973, "code": ["<code>\ndef select_slices(ids: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Selects sub-vectors from the tensor `x` based on the binary indices in `ids`.\n\n    Args:\n        ids (torch.Tensor): A tensor of shape (70, 3) with binary values indicating which sub-vector to select.\n        x (torch.Tensor): A tensor of shape (70, 3, 2) containing sub-vectors to be selected from.\n\n    Returns:\n        torch.Tensor: A tensor of shape (70, 2) containing the selected sub-vectors.\n    \"\"\"\n    # Solution implementation here\n    result = torch.zeros((70, 2))  # Initialize the result tensor\n    for i in range(70):\n        # Find the index of the selected sub-vector in the current row\n        selected_index = torch.argmax(ids[i])\n        # Assign the selected sub-vector to the result tensor\n        result[i] = x[i, selected_index]\n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 973, "library_problem_id": 41, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 39}}
{"id": 974, "code": ["<code>\ndef predict_class_from_softmax(softmax_output: torch.Tensor) -> torch.Tensor:\n    # Use torch.argmax to find the index of the maximum probability in each row\n    # The result is a 1D tensor of indices, each representing the predicted class\n    predicted_classes = torch.argmax(softmax_output, dim=1)\n    \n    # Reshape the result to a column tensor of shape (n, 1)\n    return predicted_classes.unsqueeze(1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 974, "library_problem_id": 42, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 42}}
{"id": 975, "code": ["<code>\nimport torch\n\ndef transform_softmax_output(softmax_output: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Transforms a softmax output tensor of shape (n, 3) into a tensor of shape (n, 1),\n    where each entry indicates the class with the highest probability.\n\n    Args:\n        softmax_output (torch.Tensor): A tensor of shape (n, 3) representing the probabilities\n                                       of each input belonging to classes 0, 1, or 2.\n\n    Returns:\n        torch.Tensor: A tensor of shape (n, 1) where each entry is the class index (0, 1, or 2)\n                      with the highest probability for the corresponding input.\n    \"\"\"\n    # Use torch.argmax to find the index of the maximum probability for each input\n    class_indices = torch.argmax(softmax_output, dim=1)\n\n    # Reshape the result to be a column tensor of shape (n, 1)\n    return class_indices.unsqueeze(1)\n\n# Example usage\nsoftmax_output = torch.tensor([[0.7, 0.2, 0.1],\n                               [0.2, 0.6, 0.2],\n                               [0.1, 0.1, 0.8]])\nresult = transform_softmax_output(softmax_output)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 975, "library_problem_id": 43, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 976, "code": ["<code>\nimport torch\n\ndef find_lowest_probability_class(softmax_output: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Transforms a softmax output tensor of shape (n, 3) into a tensor of shape (n, 1),\n    where each entry indicates the class with the lowest probability for each input sample.\n\n    Args:\n        softmax_output (torch.Tensor): A tensor of shape (n, 3) representing the probabilities\n                                       of each input sample belonging to classes 0, 1, and 2.\n\n    Returns:\n        torch.Tensor: A tensor of shape (n, 1) where each entry is the index of the class\n                      with the lowest probability for the corresponding input sample.\n    \"\"\"\n    # Use torch.argmin to find the index of the minimum probability for each row\n    lowest_prob_class = torch.argmin(softmax_output, dim=1, keepdim=True)\n    return lowest_prob_class\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 976, "library_problem_id": 44, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 42}}
{"id": 977, "code": ["<code>\ndef solve(softmax_output: torch.Tensor) -> torch.Tensor:\n    # Use torch.argmax to find the index of the maximum probability in each row\n    # The result is a 1D tensor of class indices\n    class_indices = torch.argmax(softmax_output, dim=1)\n    \n    # Reshape the result to an n x 1 tensor\n    return class_indices.unsqueeze(1)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 977, "library_problem_id": 45, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 42}}
{"id": 978, "code": ["<code>\ndef find_lowest_probability_class(softmax_output: torch.Tensor) -> torch.LongTensor:\n    \"\"\"\n    Transforms the softmax output of a logistic regression model into a tensor indicating\n    the class with the lowest probability for each input.\n\n    Args:\n        softmax_output (torch.Tensor): An n x 3 tensor where each row represents the\n                                       probabilities of the input belonging to classes 0, 1, and 2.\n\n    Returns:\n        torch.LongTensor: A 1 x n tensor where each element is the index of the class\n                          with the lowest probability for the corresponding input.\n    \"\"\"\n    # Calculate the index of the minimum probability for each input\n    lowest_probability_indices = torch.argmin(softmax_output, dim=1)\n    return lowest_probability_indices\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 978, "library_problem_id": 46, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 42}}
{"id": 979, "code": ["<code>\ndef cross_entropy2d(input: torch.Tensor, target: torch.Tensor, weight: torch.Tensor = None, size_average: bool = True) -> torch.Tensor:\n    # Ensure input dimensions are compatible\n    n, c, h, w = input.size()\n    if target.size() != (n, h, w):\n        raise RuntimeError(\"Target size must be (batch_size, height, width)\")\n\n    # Compute log probabilities\n    log_p = F.log_softmax(input, dim=1)\n\n    # Reshape log probabilities to (n*h*w, c)\n    log_p = log_p.permute(0, 2, 3, 1).contiguous().view(-1, c)\n\n    # Flatten target to (n*h*w,)\n    target = target.view(-1)\n\n    # Mask to filter out invalid targets\n    mask = target >= 0\n    if not mask.any():\n        raise RuntimeError(\"Target contains no valid class indices\")\n\n    # Compute negative log likelihood loss\n    loss = F.nll_loss(log_p[mask], target[mask], weight=weight, reduction='sum' if not size_average else 'mean')\n\n    return loss\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 979, "library_problem_id": 47, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 47}}
{"id": 980, "code": ["<code>\ndef count_equal_elements(tensor1: torch.Tensor, tensor2: torch.Tensor) -> int:\n    # Calculate the number of equal elements at corresponding positions\n    cnt_equal = torch.sum(tensor1 == tensor2).item()\n    return cnt_equal\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 980, "library_problem_id": 48, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 48}}
{"id": 981, "code": ["<code>\ndef count_equal_elements(tensor1: torch.Tensor, tensor2: torch.Tensor) -> int:\n    \"\"\"\n    Counts the number of elements that are equal between two 11x1 tensors.\n\n    Args:\n    tensor1 (torch.Tensor): The first tensor with dimensions 11x1.\n    tensor2 (torch.Tensor): The second tensor with dimensions 11x1.\n\n    Returns:\n    int: The count of positions where the elements of tensor1 and tensor2 are equal.\n\n    Preconditions:\n    - Both tensor1 and tensor2 must have the shape (11, 1).\n    - The tensors must be of the same data type.\n\n    Example:\n    >>> tensor1 = torch.tensor([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11]])\n    >>> tensor2 = torch.tensor([[1], [2], [0], [4], [0], [6], [7], [0], [9], [0], [11]])\n    >>> count_equal_elements(tensor1, tensor2)\n    7\n    \"\"\"\n    return torch.sum(tensor1 == tensor2).item()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 981, "library_problem_id": 49, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 982, "code": ["<code>\nimport torch\n\ndef count_not_equal_elements(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> int:\n    if tensor_a.shape != tensor_b.shape:\n        raise ValueError(\"The dimensions of the tensors must match.\")\n    return torch.sum(tensor_a != tensor_b).item()\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 982, "library_problem_id": 50, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 48}}
{"id": 983, "code": ["<code>\ndef Count(A: torch.Tensor, B: torch.Tensor) -> int:\n    # Ensure both tensors are of the same shape\n    assert A.shape == B.shape, \"Tensors must have the same shape.\"\n    \n    # Calculate the number of equal elements\n    equal_elements = (A == B).sum().item()\n    \n    return equal_elements\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 983, "library_problem_id": 51, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 48}}
{"id": 984, "code": ["<code>\ndef count_equal_last_x_elements(tensor_a: torch.Tensor, tensor_b: torch.Tensor, x: int) -> int:\n    \"\"\"\n    Counts the number of equal elements in the last x elements of two tensors.\n\n    Args:\n        tensor_a (torch.Tensor): The first tensor with dimensions (2*x, 1).\n        tensor_b (torch.Tensor): The second tensor with dimensions (2*x, 1).\n        x (int): The number of elements from the end of each tensor to compare.\n\n    Returns:\n        int: The count of equal elements in the last x elements of both tensors.\n\n    Conditions:\n        - Both tensors must have the same dimensions (2*x, 1).\n        - The comparison is performed on the last x elements of each tensor.\n    \"\"\"\n    # Extract the last x elements from each tensor\n    last_x_a = tensor_a[-x:]\n    last_x_b = tensor_b[-x:]\n    \n    # Compare the elements and count the number of equal pairs\n    cnt_equal = torch.sum(last_x_a == last_x_b).item()\n    \n    return cnt_equal\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 984, "library_problem_id": 52, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 985, "code": ["<code>\ndef count_non_equal_last_x_elements(tensor_a: torch.Tensor, tensor_b: torch.Tensor, x: int) -> int:\n    \"\"\"\n    Counts the number of elements that are not equal in the last x elements of two tensors.\n\n    Args:\n        tensor_a (torch.Tensor): The first tensor with dimensions (2*x, 1).\n        tensor_b (torch.Tensor): The second tensor with dimensions (2*x, 1).\n        x (int): The number of elements from the end of each tensor to compare.\n\n    Returns:\n        int: The count of non-equal elements in the last x elements of the two tensors.\n\n    Preconditions:\n        - tensor_a and tensor_b must have the same shape.\n        - The shape of tensor_a and tensor_b must be (2*x, 1).\n        - x must be a positive integer such that 2*x is the size of the first dimension of the tensors.\n    \"\"\"\n    # Extract the last x elements from each tensor\n    last_x_a = tensor_a[-x:]\n    last_x_b = tensor_b[-x:]\n\n    # Compare the elements and count the number of non-equal pairs\n    cnt_not_equal = torch.sum(last_x_a != last_x_b).item()\n\n    return cnt_not_equal\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 985, "library_problem_id": 53, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Difficult-Rewrite", "perturbation_origin_id": 48}}
{"id": 986, "code": ["<code>\ndef split_tensor(tensor: torch.Tensor, split_dim: int, chunk_size: int) -> List[torch.Tensor]:\n    # Validate input dimensions\n    assert tensor.dim() == 5, \"Input tensor must be 5-dimensional.\"\n    assert 0 <= split_dim < tensor.dim(), \"split_dim must be a valid dimension index.\"\n    assert chunk_size > 0, \"chunk_size must be positive.\"\n    \n    # Get the size of the specified dimension\n    dim_size = tensor.size(split_dim)\n    \n    # Calculate the number of chunks\n    num_chunks = dim_size - chunk_size + 1\n    \n    # Generate the list of tensors\n    tensors = [tensor.narrow(split_dim, i, chunk_size) for i in range(num_chunks)]\n    \n    return tensors\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 986, "library_problem_id": 54, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 54}}
{"id": 987, "code": ["<code>\ndef split_tensor(tensor: torch.Tensor, split_dim: int, chunk_size: int) -> list:\n    # Validate input tensor dimensions\n    assert tensor.ndim == 5, \"Input tensor must be 5-dimensional.\"\n    \n    # Get the size of the specified dimension\n    dim_size = tensor.size(split_dim)\n    \n    # Calculate the number of chunks\n    num_chunks = dim_size - chunk_size + 1\n    \n    # Initialize list to store the resulting tensors\n    tensors = []\n    \n    # Iterate and create overlapping chunks\n    for i in range(num_chunks):\n        # Slice the tensor to create a chunk\n        chunk = tensor.narrow(split_dim, i, chunk_size)\n        tensors.append(chunk)\n    \n    return tensors\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 987, "library_problem_id": 55, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 54}}
{"id": 988, "code": ["<code>\ndef update_output_with_mask(mask: torch.Tensor, clean_input_spectrogram: torch.Tensor, output: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Updates the `output` tensor by setting its elements to the corresponding elements\n    from `clean_input_spectrogram` wherever the `mask` tensor has a value of 1.\n\n    Args:\n        mask (torch.Tensor): A tensor of shape [1, 400] containing binary values (0 or 1).\n        clean_input_spectrogram (torch.Tensor): A tensor of shape [1, 400, 161] containing the input spectrogram data.\n        output (torch.Tensor): A tensor of shape [1, 400, 161] to be updated based on the mask.\n\n    Returns:\n        torch.Tensor: The updated `output` tensor with elements set according to the mask.\n    \"\"\"\n    # Ensure mask is broadcastable to the shape of clean_input_spectrogram and output\n    mask_expanded = mask.unsqueeze(-1).expand_as(clean_input_spectrogram)\n    \n    # Update the output tensor where mask is 1\n    output = torch.where(mask_expanded == 1, clean_input_spectrogram, output)\n    \n    return output\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 988, "library_problem_id": 56, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 56}}
{"id": 989, "code": ["<code>\nimport torch\n\ndef apply_mask(mask, clean_input_spectrogram, output):\n    # Ensure mask is broadcastable to the shape of output\n    mask_expanded = mask.unsqueeze(-1).expand_as(output)\n    # Apply the mask: where mask is 0, take values from clean_input_spectrogram\n    output = torch.where(mask_expanded == 0, clean_input_spectrogram, output)\n    return output\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 989, "library_problem_id": 57, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 56}}
{"id": 990, "code": ["<code>\nimport torch\n\ndef compute_signed_min(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Compute the element-wise minimum of the absolute values from two tensors, preserving the sign of the original element.\n\n    Args:\n        x (torch.Tensor): The first input tensor.\n        y (torch.Tensor): The second input tensor.\n\n    Returns:\n        torch.Tensor: A tensor containing elements with the minimum absolute values from `x` and `y`, with the original sign.\n    \"\"\"\n    # Compute the sign of each element in x and y\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    \n    # Compute the element-wise minimum of the absolute values\n    abs_min = torch.min(torch.abs(x), torch.abs(y))\n    \n    # Determine the mask for elements where x has the minimum absolute value\n    mask_x = torch.abs(x) <= torch.abs(y)\n    \n    # Apply the sign of the original element to the minimum values\n    signed_min = torch.where(mask_x, abs_min * sign_x, abs_min * sign_y)\n    \n    return signed_min\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 990, "library_problem_id": 58, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 58}}
{"id": 991, "code": ["<code>\nimport torch\n\ndef compute_signed_max(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    abs_x = torch.abs(x)\n    abs_y = torch.abs(y)\n    max_abs = torch.max(abs_x, abs_y)\n    signed_max = torch.where(abs_x >= abs_y, sign_x * max_abs, sign_y * max_abs)\n    return signed_max\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 991, "library_problem_id": 59, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Semantic", "perturbation_origin_id": 58}}
{"id": 992, "code": ["<code>\ndef solve(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n    # Compute the sign of each element in both tensors\n    sign_x = torch.sign(x)\n    sign_y = torch.sign(y)\n    \n    # Compute the absolute values of both tensors\n    abs_x = torch.abs(x)\n    abs_y = torch.abs(y)\n    \n    # Determine the minimum absolute values element-wise\n    min_abs = torch.min(abs_x, abs_y)\n    \n    # Create a mask to determine which tensor's sign to use\n    mask = abs_x <= abs_y\n    \n    # Initialize the result tensor with zeros\n    result = torch.zeros_like(x)\n    \n    # Assign the signed minimum values to the result tensor\n    result[mask] = min_abs[mask] * sign_x[mask]\n    result[~mask] = min_abs[~mask] * sign_y[~mask]\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 992, "library_problem_id": 60, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Surface", "perturbation_origin_id": 58}}
{"id": 993, "code": ["<code>\ndef predict_all_characters(input: torch.Tensor) -> tuple:\n    \"\"\"\n    Predicts the class of the input tensor and returns the confidence score and class name.\n\n    Args:\n    - input (torch.Tensor): The input tensor for prediction.\n\n    Returns:\n    - Tuple[torch.Tensor, str]: A tuple containing the confidence score and the predicted class name.\n    \"\"\"\n    # Obtain the raw output from the model\n    output = MyNet(input)\n    \n    # Apply softmax to get confidence scores in the range (0, 1)\n    softmax_scores = torch.nn.functional.softmax(output, dim=1)\n    \n    # Get the maximum confidence score and the corresponding class index\n    conf, class_index = torch.max(softmax_scores, 1)\n    \n    # Define class names\n    class_names = '012'\n    \n    # Return the confidence score and the class name\n    return conf, class_names[class_index.item()]\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 993, "library_problem_id": 61, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 61}}
{"id": 994, "code": ["<code>\ndef combine_tensors_with_overlap(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n    # Ensure input tensors have the same number of rows\n    assert a.shape[0] == b.shape[0], \"Tensors must have the same number of rows\"\n    \n    # Calculate the average of the overlapping column\n    overlap_avg = (a[:, -1] + b[:, 0]) / 2\n    \n    # Concatenate the tensors with the averaged column in between\n    result = torch.cat((a[:, :-1], overlap_avg.unsqueeze(1), b[:, 1:]), dim=1)\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 994, "library_problem_id": 62, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Origin", "perturbation_origin_id": 62}}
{"id": 995, "code": ["<code>\ndef solve(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n    # Ensure the tensors have the same number of rows\n    assert a.shape[0] == b.shape[0], \"Tensors must have the same number of rows\"\n    \n    # Calculate the average of the overlapping columns\n    overlap_avg = (a[:, -1] + b[:, 0]) / 2\n    \n    # Concatenate the tensors with the overlap average\n    result = torch.cat((a[:, :-1], overlap_avg.unsqueeze(1), b[:, 1:]), dim=1)\n    \n    return result\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 995, "library_problem_id": 63, "library": "Pytorch", "test_case_cnt": 3, "perturbation_type": "Surface", "perturbation_origin_id": 62}}
{"id": 996, "code": ["<code>\nimport torch\n\ndef pad_tensor_with_zeros(t: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Pads the given 2D tensor `t` with zeros on all sides.\n\n    Args:\n    t (torch.Tensor): A 2D tensor of shape (m, n).\n\n    Returns:\n    torch.Tensor: A new tensor of shape (m+2, n+2) with zero padding.\n    \"\"\"\n    # Get the shape of the original tensor\n    m, n = t.shape\n    \n    # Create a new tensor with zero padding\n    padded_t = torch.zeros((m + 2, n + 2), dtype=t.dtype)\n    \n    # Copy the original tensor into the center of the new tensor\n    padded_t[1:m+1, 1:n+1] = t\n    \n    return padded_t\n\n# Example usage\nt = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]], dtype=torch.float32)\nresult = pad_tensor_with_zeros(t)\nprint(result)\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 996, "library_problem_id": 64, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Origin", "perturbation_origin_id": 64}}
{"id": 997, "code": ["<code>\nimport torch\n\ndef pad_tensor_with_zeros(t: torch.Tensor) -> torch.Tensor:\n    # Get the shape of the original tensor\n    m, n = t.shape\n    \n    # Create a new tensor of zeros with shape (m+2, n+2)\n    padded_tensor = torch.zeros((m + 2, n + 2), dtype=t.dtype)\n    \n    # Place the original tensor in the center of the new tensor\n    padded_tensor[1:m+1, 1:n+1] = t\n    \n    return padded_tensor\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 997, "library_problem_id": 65, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Surface", "perturbation_origin_id": 64}}
{"id": 998, "code": ["<code>\ndef pad_tensor_with_border(t: torch.Tensor) -> torch.Tensor:\n    # Ensure the input tensor is 2D\n    assert t.dim() == 2, \"Input tensor must be 2-dimensional\"\n    \n    # Get the shape of the input tensor\n    m, n = t.shape\n    \n    # Create a new tensor filled with -1 values with shape (m+2, n+2)\n    padded_tensor = torch.full((m + 2, n + 2), -1, dtype=t.dtype)\n    \n    # Copy the original tensor into the center of the new tensor\n    padded_tensor[1:m+1, 1:n+1] = t\n    \n    return padded_tensor\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 998, "library_problem_id": 66, "library": "Pytorch", "test_case_cnt": 2, "perturbation_type": "Semantic", "perturbation_origin_id": 64}}
{"id": 999, "code": ["<code>\n```python\nimport torch\n\ndef compute_dot_product_with_weights(batch_size: int = 10, seq_len1: int = 2, seq_len2: int = 3, hid_dim: int = 32) -> torch.Tensor:\n    \"\"\"\n    Computes the dot product between batch data and trainable weights.\n\n    Args:\n        batch_size (int): The number of samples in the batch.\n        seq_len1 (int): The first sequence dimension.\n        seq_len2 (int): The second sequence dimension.\n        hid_dim (int): The hidden dimension size.\n\n    Returns:\n        torch.Tensor: The result of the dot product operation, reshaped to (batch_size, seq_len1, seq_len2).\n    \"\"\"\n    # Generate random batch data\n    data = torch.randn(batch_size, seq_len1, seq_len2, hid_dim)\n    # Reshape data to (batch_size, seq_len1 * seq_len2, hid_dim)\n    data = data.view(batch_size, seq_len1 * seq_len2, hid_dim)\n    # Initialize trainable weights\n    W = torch.randn(hid_dim, 1)  # Reshape W to (hid_dim, 1) for matrix multiplication\n    # Perform batch matrix multiplication and squeeze the last dimension\n    result = torch.bmm(data, W).squeeze(-1)\n    # Reshape the result to (batch_size, seq_len\n</code>\nEND SOLUTION\n</code>"], "metadata": {"problem_id": 999, "library_problem_id": 67, "library": "Pytorch", "test_case_cnt": 1, "perturbation_type": "Origin", "perturbation_origin_id": 67}}
